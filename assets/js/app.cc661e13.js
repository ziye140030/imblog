(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(n){function e(e){for(var o,r,i=e[0],l=e[1],c=e[2],d=0,m=[];d<i.length;d++)r=i[d],Object.prototype.hasOwnProperty.call(a,r)&&a[r]&&m.push(a[r][0]),a[r]=0;for(o in l)Object.prototype.hasOwnProperty.call(l,o)&&(n[o]=l[o]);for(p&&p(e);m.length;)m.shift()();return s.push.apply(s,c||[]),t()}function t(){for(var n,e=0;e<s.length;e++){for(var t=s[e],o=!0,i=1;i<t.length;i++){var l=t[i];0!==a[l]&&(o=!1)}o&&(s.splice(e--,1),n=r(r.s=t[0]))}return n}var o={},a={1:0},s=[];function r(e){if(o[e])return o[e].exports;var t=o[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,r),t.l=!0,t.exports}r.e=function(n){var e=[],t=a[n];if(0!==t)if(t)e.push(t[2]);else{var o=new Promise((function(e,o){t=a[n]=[e,o]}));e.push(t[2]=o);var s,i=document.createElement("script");i.charset="utf-8",i.timeout=120,r.nc&&i.setAttribute("nonce",r.nc),i.src=function(n){return r.p+"assets/js/"+({}[n]||n)+"."+{2:"5530ac35",3:"0352ef92",4:"eef75fb3",5:"ff0128d3",6:"9233c8ba",7:"6ac946b2",8:"6ac87516",9:"110e5553",10:"2d0df2a5",11:"5eca3f4f",12:"000e424f",13:"d35f48a7",14:"d21bcc14",15:"bb17458d",16:"8ba4b22c",17:"5c165ba6",18:"39fda8a3",19:"31bca7fe",20:"2647de20",21:"44bb34ac",22:"045c3eec",23:"71a82ea5",24:"27169789",25:"2c50d526",26:"b04032a5",27:"5f0f85c8",28:"9824180b",29:"d8a04e43",30:"b2c233b5",31:"cef434d1",32:"b1abe5c1",33:"00dc451a",34:"252282a6",35:"fbd57bac",36:"a9407b64",37:"c44a5298",38:"976712a7",39:"cd572f10",40:"38b41e51",41:"e4ca8e43",42:"13c9d4af",43:"b03d9849",44:"8398e8cf",45:"ec632fdc",46:"dd574d55",47:"ab623724",48:"34ea7aa2",49:"0a0b9eb7",50:"65692633",51:"d9dde319",52:"bc5cf8a3",53:"aa1f302d",54:"541e7bff",55:"64453af7",56:"5f2660ae",57:"4b2c03e7",58:"333ef6f4",59:"50c18168",60:"d91921c3",61:"5ecf9fc6",62:"85909a07",63:"0f9b17f6",64:"f734321b",65:"4f36c96b",66:"9405b55d",67:"a294d0a3",68:"3f539faa",69:"8b833214",70:"42add28d",71:"da59fb0c",72:"c5db2036",73:"355c1bbf",74:"b37cbbce",75:"cc8bf69e",76:"b1ca7f2e",77:"10f40ba5",78:"ccd6927f",79:"87ae66c0",80:"ff648a8c",81:"e00fed98",82:"abc72fcf",83:"39b970d8",84:"dc5bc5f6",85:"7dfc87e4",86:"1830134e",87:"85a74bcc",88:"7e5782e1",89:"e90e0d13",90:"2354d1f3",91:"5458d664",92:"12fca424",93:"37139e21",94:"966d3bdb",95:"9c44c86e",96:"36fa2e18",97:"acd3e8c6",98:"ddcf4b1a",99:"a2d5af04",100:"d2864acc",101:"299ca1f8",102:"3e135c28",103:"bce6619e",104:"9ccdb0d0",105:"4f76a5e1",106:"189480e1",107:"537609be",108:"481d9c6a",109:"a8c1085a",110:"f8270c34",111:"bafa5b75",112:"6094cd5f",113:"79ca4714",114:"b001833b",115:"e6070513",116:"666ddd03",117:"4a256a2b",118:"f1e3ea77",119:"e2c3254a",120:"e54f57d6",121:"8070e5e3",122:"3664033b",123:"22ebac4e",124:"f22556ce",125:"07d5d849",126:"2b6eea89",127:"05fd3232",128:"07108ccf",129:"77d20ad5",130:"e48c927a",131:"349fa17d",132:"c92f3d4b",133:"5162df80",134:"5e15d059",135:"1ebe9412",136:"29b44c8e",137:"0511ba65",138:"0360d58e",139:"52117293",140:"8303166a",141:"c886b9b3",142:"2d4d36c5",143:"482b783c",144:"5ff5f2ab",145:"8da4569b",146:"61829b48",147:"58814577",148:"65c2b04b"}[n]+".js"}(n);var l=new Error;s=function(e){i.onerror=i.onload=null,clearTimeout(c);var t=a[n];if(0!==t){if(t){var o=e&&("load"===e.type?"missing":e.type),s=e&&e.target&&e.target.src;l.message="Loading chunk "+n+" failed.\n("+o+": "+s+")",l.name="ChunkLoadError",l.type=o,l.request=s,t[1](l)}a[n]=void 0}};var c=setTimeout((function(){s({type:"timeout",target:i})}),12e4);i.onerror=i.onload=s,document.head.appendChild(i)}return Promise.all(e)},r.m=n,r.c=o,r.d=function(n,e,t){r.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},r.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},r.t=function(n,e){if(1&e&&(n=r(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(r.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var o in n)r.d(t,o,function(e){return n[e]}.bind(null,o));return t},r.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return r.d(e,"a",e),e},r.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},r.p="/",r.oe=function(n){throw console.error(n),n};var i=window.webpackJsonp=window.webpackJsonp||[],l=i.push.bind(i);i.push=e,i=i.slice();for(var c=0;c<i.length;c++)e(i[c]);var p=l;s.push([107,0]),t()}([function(n,e,t){"use strict";function o(n,e,t,o,a,s,r,i){var l,c="function"==typeof n?n.options:n;if(e&&(c.render=e,c.staticRenderFns=t,c._compiled=!0),o&&(c.functional=!0),s&&(c._scopeId="data-v-"+s),r?(l=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),a&&a.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(r)},c._ssrRegister=l):a&&(l=i?function(){a.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:a),l)if(c.functional){c._injectStyles=l;var p=c.render;c.render=function(n,e){return l.call(e),p(n,e)}}else{var d=c.beforeCreate;c.beforeCreate=d?[].concat(d,l):[l]}return{exports:n,options:c}}t.d(e,"a",(function(){return o}))},function(n,e,t){var o=t(56),a=o.all;n.exports=o.IS_HTMLDDA?function(n){return"function"==typeof n||n===a}:function(n){return"function"==typeof n}},function(n,e,t){var o=t(29),a=Function.prototype,s=a.call,r=o&&a.bind.bind(s,s);n.exports=o?r:function(n){return function(){return s.apply(n,arguments)}}},function(n,e){var t=function(n){return n&&n.Math==Math&&n};n.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||Function("return this")()},function(n,e){n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e,t){var o=t(4);n.exports=!o((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){var o=t(2),a=t(33),s=o({}.hasOwnProperty);n.exports=Object.hasOwn||function(n,e){return s(a(n),e)}},function(n,e,t){var o=t(70),a="object"==typeof self&&self&&self.Object===Object&&self,s=o||a||Function("return this")();n.exports=s},function(n,e,t){var o=t(1),a=t(56),s=a.all;n.exports=a.IS_HTMLDDA?function(n){return"object"==typeof n?null!==n:o(n)||n===s}:function(n){return"object"==typeof n?null!==n:o(n)}},function(n,e,t){var o=t(167),a=t(170);n.exports=function(n,e){var t=a(n,e);return o(t)?t:void 0}},function(n,e,t){"use strict";t.d(e,"e",(function(){return o})),t.d(e,"b",(function(){return s})),t.d(e,"j",(function(){return r})),t.d(e,"g",(function(){return l})),t.d(e,"h",(function(){return c})),t.d(e,"i",(function(){return p})),t.d(e,"c",(function(){return d})),t.d(e,"f",(function(){return m})),t.d(e,"l",(function(){return u})),t.d(e,"m",(function(){return h})),t.d(e,"d",(function(){return b})),t.d(e,"k",(function(){return f})),t.d(e,"n",(function(){return y})),t.d(e,"a",(function(){return v}));t(18);const o=/#.*$/,a=/\.(md|html)$/,s=/\/$/,r=/^[a-z]+:/i;function i(n){return decodeURI(n).replace(o,"").replace(a,"")}function l(n){return r.test(n)}function c(n){return/^mailto:/.test(n)}function p(n){return/^tel:/.test(n)}function d(n){if(l(n))return n;if(!n)return"404";const e=n.match(o),t=e?e[0]:"",a=i(n);return s.test(a)?n:a+".html"+t}function m(n,e){const t=n.hash,a=function(n){const e=n&&n.match(o);if(e)return e[0]}(e);if(a&&t!==a)return!1;return i(n.path)===i(e)}function u(n,e,t){if(l(e))return{type:"external",path:e};t&&(e=function(n,e,t){const o=n.charAt(0);if("/"===o)return n;if("?"===o||"#"===o)return e+n;const a=e.split("/");t&&a[a.length-1]||a.pop();const s=n.replace(/^\//,"").split("/");for(let n=0;n<s.length;n++){const e=s[n];".."===e?a.pop():"."!==e&&a.push(e)}""!==a[0]&&a.unshift("");return a.join("/")}(e,t));const o=i(e);for(let e=0;e<n.length;e++)if(i(n[e].regularPath)===o)return Object.assign({},n[e],{type:"page",path:d(n[e].path)});return console.error(`[vuepress] No matching page found for sidebar item "${e}"`),{}}function h(n,e,t,o){const{pages:a,themeConfig:s}=t,r=o&&s.locales&&s.locales[o]||s;if("auto"===(n.frontmatter.sidebar||r.sidebar||s.sidebar))return g(n);const i=r.sidebar||s.sidebar;if(i){const{base:t,config:o}=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(const o in e)if(0===(t=n,/(\.html|\/)$/.test(t)?t:t+"/").indexOf(encodeURI(o)))return{base:o,config:e[o]};var t;return{}}(e,i);return"auto"===o?g(n):o?o.map(n=>function n(e,t,o,a=1){if("string"==typeof e)return u(t,e,o);if(Array.isArray(e))return Object.assign(u(t,e[0],o),{title:e[1]});{a>3&&console.error("[vuepress] detected a too deep nested sidebar group.");const s=e.children||[];return 0===s.length&&e.path?Object.assign(u(t,e.path,o),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,initialOpenGroupIndex:e.initialOpenGroupIndex,children:s.map(e=>n(e,t,o,a+1)),collapsable:!1!==e.collapsable}}}(n,a,t)):[]}return[]}function g(n){const e=b(n.headers||[]);return[{type:"group",collapsable:!1,title:n.title,path:null,children:e.map(e=>({type:"auto",title:e.title,basePath:n.path,path:n.path+"#"+e.slug,children:e.children||[]}))}]}function b(n){let e;return(n=n.map(n=>Object.assign({},n))).forEach(n=>{2===n.level?e=n:e&&(e.children||(e.children=[])).push(n)}),n.filter(n=>2===n.level)}function f(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function y(n){return Object.prototype.toString.call(n).match(/\[object (.*?)\]/)[1].toLowerCase()}function _(n){let e=n.frontmatter.date||n.lastUpdated||new Date,t=new Date(e);return"Invalid Date"==t&&e&&(t=new Date(e.replace(/-/g,"/"))),t.getTime()}function v(n,e){return _(e)-_(n)}},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){var o=t(5),a=t(65),s=t(102),r=t(28),i=t(55),l=TypeError,c=Object.defineProperty,p=Object.getOwnPropertyDescriptor;e.f=o?s?function(n,e,t){if(r(n),e=i(e),r(t),"function"==typeof n&&"prototype"===e&&"value"in t&&"writable"in t&&!t.writable){var o=p(n,e);o&&o.writable&&(n[e]=t.value,t={configurable:"configurable"in t?t.configurable:o.configurable,enumerable:"enumerable"in t?t.enumerable:o.enumerable,writable:!1})}return c(n,e,t)}:c:function(n,e,t){if(r(n),e=i(e),r(t),a)try{return c(n,e,t)}catch(n){}if("get"in t||"set"in t)throw l("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e,t){var o=t(16),a=t(152),s=t(153),r=o?o.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":r&&r in Object(n)?a(n):s(n)}},function(n,e,t){var o=t(5),a=t(13),s=t(36);n.exports=o?function(n,e,t){return a.f(n,e,s(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e,t){var o=t(8).Symbol;n.exports=o},function(n){n.exports=JSON.parse('{"name":"vuepress-plugin-vdoing-comment","version":"1.0.7","description":"Comment plugin in vuepress vdoing theme, supports Gitalk, Valine, Artalk...","main":"index.js","scripts":{"test":"echo \\"Error: no test specified\\" && exit 1"},"repository":{"type":"git","url":"git+ssh://git@github.com/terwer/vuepress-plugin-vdoing-comment.git"},"keywords":["vuepress","comment","plugin","vue","gitalk","valine","artalk"],"author":"terwer","license":"MIT","bugs":{"url":"https://github.com/terwer/vuepress-plugin-vdoing-comment/issues"},"homepage":"https://github.com/terwer/vuepress-plugin-vdoing-comment/blob/main/README.md","dependencies":{"ejs":"^3.1.8","gitalk":"^1.5.0","gitalk-fix":"^1.5.2","i":"^0.3.6","npm":"^6.9.0","valine":"^1.3.9","artalk":"^2.4.4"}}')},function(n,e,t){"use strict";var o=t(19),a=t(33),s=t(34),r=t(132),i=t(134);o({target:"Array",proto:!0,arity:1,forced:t(4)((function(){return 4294967297!==[].push.call({length:4294967296},1)}))||!function(){try{Object.defineProperty([],"length",{writable:!1}).push()}catch(n){return n instanceof TypeError}}()},{push:function(n){var e=a(this),t=s(e),o=arguments.length;i(t+o);for(var l=0;l<o;l++)e[t]=arguments[l],t++;return r(e,t),t}})},function(n,e,t){var o=t(3),a=t(52).f,s=t(15),r=t(115),i=t(38),l=t(66),c=t(128);n.exports=function(n,e){var t,p,d,m,u,h=n.target,g=n.global,b=n.stat;if(t=g?o:b?o[h]||i(h,{}):(o[h]||{}).prototype)for(p in e){if(m=e[p],d=n.dontCallGetSet?(u=a(t,p))&&u.value:t[p],!c(g?p:h+(b?".":"#")+p,n.forced)&&void 0!==d){if(typeof m==typeof d)continue;l(m,d)}(n.sham||d&&d.sham)&&s(m,"sham",!0),r(t,p,m,n)}}},function(n,e,t){var o=t(2),a=o({}.toString),s=o("".slice);n.exports=function(n){return s(a(n),8,-1)}},function(n,e,t){var o=t(3),a=t(62),s=t(7),r=t(64),i=t(60),l=t(59),c=o.Symbol,p=a("wks"),d=l?c.for||c:c&&c.withoutSetter||r;n.exports=function(n){return s(p,n)||(p[n]=i&&s(c,n)?c[n]:d("Symbol."+n)),p[n]}},function(n,e,t){var o=t(157),a=t(158),s=t(159),r=t(160),i=t(161);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var o=n[e];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=a,l.prototype.get=s,l.prototype.has=r,l.prototype.set=i,n.exports=l},function(n,e,t){var o=t(72);n.exports=function(n,e){for(var t=n.length;t--;)if(o(n[t][0],e))return t;return-1}},function(n,e,t){var o=t(10)(Object,"create");n.exports=o},function(n,e,t){var o=t(179);n.exports=function(n,e){var t=n.__data__;return o(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var o=t(46);n.exports=function(n){if("string"==typeof n||o(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e,t){var o,a;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(a="function"==typeof(o=function(){var n,e,t={version:"0.2.0"},o=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function a(n,e,t){return n<e?e:n>t?t:n}function s(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(o[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=a(n,o.minimum,1),t.status=1===n?null:n;var l=t.render(!e),c=l.querySelector(o.barSelector),p=o.speed,d=o.easing;return l.offsetWidth,r((function(e){""===o.positionUsing&&(o.positionUsing=t.getPositioningCSS()),i(c,function(n,e,t){var a;return(a="translate3d"===o.positionUsing?{transform:"translate3d("+s(n)+"%,0,0)"}:"translate"===o.positionUsing?{transform:"translate("+s(n)+"%,0)"}:{"margin-left":s(n)+"%"}).transition="all "+e+"ms "+t,a}(n,p,d)),1===n?(i(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){i(l,{transition:"all "+p+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),p)}),p)):setTimeout(e,p)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),o.trickleSpeed)};return o.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*a(Math.random()*e,.1,.95)),e=a(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*o.trickleRate)},n=0,e=0,t.promise=function(o){return o&&"resolved"!==o.state()?(0===e&&t.start(),n++,e++,o.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=o.template;var a,r=e.querySelector(o.barSelector),l=n?"-100":s(t.status||0),p=document.querySelector(o.parent);return i(r,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),o.showSpinner||(a=e.querySelector(o.spinnerSelector))&&m(a),p!=document.body&&c(p,"nprogress-custom-parent"),p.appendChild(e),e},t.remove=function(){p(document.documentElement,"nprogress-busy"),p(document.querySelector(o.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&m(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var r=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),i=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var o,a=n.length,s=e.charAt(0).toUpperCase()+e.slice(1);a--;)if((o=n[a]+s)in t)return o;return e}(t))}function o(n,e,o){e=t(e),n.style[e]=o}return function(n,e){var t,a,s=arguments;if(2==s.length)for(t in e)void 0!==(a=e[t])&&e.hasOwnProperty(t)&&o(n,t,a);else o(n,s[1],s[2])}}();function l(n,e){return("string"==typeof n?n:d(n)).indexOf(" "+e+" ")>=0}function c(n,e){var t=d(n),o=t+e;l(t,e)||(n.className=o.substring(1))}function p(n,e){var t,o=d(n);l(n,e)&&(t=o.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function d(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function m(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?o.call(e,t,e,n):o)||(n.exports=a)},function(n,e,t){var o=t(9),a=String,s=TypeError;n.exports=function(n){if(o(n))return n;throw s(a(n)+" is not an object")}},function(n,e,t){var o=t(4);n.exports=!o((function(){var n=function(){}.bind();return"function"!=typeof n||n.hasOwnProperty("prototype")}))},function(n,e,t){var o=t(48),a=t(53);n.exports=function(n){return o(a(n))}},function(n,e,t){var o=t(3),a=t(1),s=function(n){return a(n)?n:void 0};n.exports=function(n,e){return arguments.length<2?s(o[n]):o[n]&&o[n][e]}},function(n,e,t){var o=t(1),a=t(113),s=TypeError;n.exports=function(n){if(o(n))return n;throw s(a(n)+" is not a function")}},function(n,e,t){var o=t(53),a=Object;n.exports=function(n){return a(o(n))}},function(n,e,t){var o=t(126);n.exports=function(n){return o(n.length)}},function(n,e,t){var o=t(29),a=Function.prototype.call;n.exports=o?a.bind(a):function(){return a.apply(a,arguments)}},function(n,e){n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e,t){var o=t(3),a=t(38),s=o["__core-js_shared__"]||a("__core-js_shared__",{});n.exports=s},function(n,e,t){var o=t(3),a=Object.defineProperty;n.exports=function(n,e){try{a(o,n,{value:e,configurable:!0,writable:!0})}catch(t){o[n]=e}return e}},function(n,e,t){var o=t(151),a=t(12),s=Object.prototype,r=s.hasOwnProperty,i=s.propertyIsEnumerable,l=o(function(){return arguments}())?o:function(n){return a(n)&&r.call(n,"callee")&&!i.call(n,"callee")};n.exports=l},function(n,e,t){var o=t(10)(t(8),"Map");n.exports=o},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var o=t(171),a=t(178),s=t(180),r=t(181),i=t(182);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var o=n[e];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=a,l.prototype.get=s,l.prototype.has=r,l.prototype.set=i,n.exports=l},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var o=t(6),a=t(46),s=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,r=/^\w*$/;n.exports=function(n,e){if(o(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!a(n))||(r.test(n)||!s.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var o=t(14),a=t(12);n.exports=function(n){return"symbol"==typeof n||a(n)&&"[object Symbol]"==o(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){var o=t(2),a=t(4),s=t(20),r=Object,i=o("".split);n.exports=a((function(){return!r("z").propertyIsEnumerable(0)}))?function(n){return"String"==s(n)?i(n,""):r(n)}:r},function(n,e){n.exports={}},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e){var t=/^\s+|\s+$/g,o=/^[-+]0x[0-9a-f]+$/i,a=/^0b[01]+$/i,s=/^0o[0-7]+$/i,r=parseInt,i="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,c=i||l||Function("return this")(),p=Object.prototype.toString,d=Math.max,m=Math.min,u=function(){return c.Date.now()};function h(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function g(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==p.call(n)}(n))return NaN;if(h(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=h(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var i=a.test(n);return i||s.test(n)?r(n.slice(2),i?2:8):o.test(n)?NaN:+n}n.exports=function(n,e,t){var o,a,s,r,i,l,c=0,p=!1,b=!1,f=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function y(e){var t=o,s=a;return o=a=void 0,c=e,r=n.apply(s,t)}function _(n){return c=n,i=setTimeout(k,e),p?y(n):r}function v(n){var t=n-l;return void 0===l||t>=e||t<0||b&&n-c>=s}function k(){var n=u();if(v(n))return x(n);i=setTimeout(k,function(n){var t=e-(n-l);return b?m(t,s-(n-c)):t}(n))}function x(n){return i=void 0,f&&o?y(n):(o=a=void 0,r)}function w(){var n=u(),t=v(n);if(o=arguments,a=this,l=n,t){if(void 0===i)return _(l);if(b)return i=setTimeout(k,e),y(l)}return void 0===i&&(i=setTimeout(k,e)),r}return e=g(e)||0,h(t)&&(p=!!t.leading,s=(b="maxWait"in t)?d(g(t.maxWait)||0,e):s,f="trailing"in t?!!t.trailing:f),w.cancel=function(){void 0!==i&&clearTimeout(i),c=0,o=l=a=i=void 0},w.flush=function(){return void 0===i?r:x(u())},w}},function(n,e,t){var o=t(5),a=t(35),s=t(109),r=t(36),i=t(30),l=t(55),c=t(7),p=t(65),d=Object.getOwnPropertyDescriptor;e.f=o?d:function(n,e){if(n=i(n),e=l(e),p)try{return d(n,e)}catch(n){}if(c(n,e))return r(!a(s.f,n,e),n[e])}},function(n,e,t){var o=t(54),a=TypeError;n.exports=function(n){if(o(n))throw a("Can't call method on "+n);return n}},function(n,e){n.exports=function(n){return null==n}},function(n,e,t){var o=t(110),a=t(57);n.exports=function(n){var e=o(n,"string");return a(e)?e:e+""}},function(n,e){var t="object"==typeof document&&document.all,o=void 0===t&&void 0!==t;n.exports={all:t,IS_HTMLDDA:o}},function(n,e,t){var o=t(31),a=t(1),s=t(58),r=t(59),i=Object;n.exports=r?function(n){return"symbol"==typeof n}:function(n){var e=o("Symbol");return a(e)&&s(e.prototype,i(n))}},function(n,e,t){var o=t(2);n.exports=o({}.isPrototypeOf)},function(n,e,t){var o=t(60);n.exports=o&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){var o=t(61),a=t(4);n.exports=!!Object.getOwnPropertySymbols&&!a((function(){var n=Symbol();return!String(n)||!(Object(n)instanceof Symbol)||!Symbol.sham&&o&&o<41}))},function(n,e,t){var o,a,s=t(3),r=t(111),i=s.process,l=s.Deno,c=i&&i.versions||l&&l.version,p=c&&c.v8;p&&(a=(o=p.split("."))[0]>0&&o[0]<4?1:+(o[0]+o[1])),!a&&r&&(!(o=r.match(/Edge\/(\d+)/))||o[1]>=74)&&(o=r.match(/Chrome\/(\d+)/))&&(a=+o[1]),n.exports=a},function(n,e,t){var o=t(63),a=t(37);(n.exports=function(n,e){return a[n]||(a[n]=void 0!==e?e:{})})("versions",[]).push({version:"3.29.0",mode:o?"pure":"global",copyright:"© 2014-2023 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.29.0/LICENSE",source:"https://github.com/zloirock/core-js"})},function(n,e){n.exports=!1},function(n,e,t){var o=t(2),a=0,s=Math.random(),r=o(1..toString);n.exports=function(n){return"Symbol("+(void 0===n?"":n)+")_"+r(++a+s,36)}},function(n,e,t){var o=t(5),a=t(4),s=t(101);n.exports=!o&&!a((function(){return 7!=Object.defineProperty(s("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){var o=t(7),a=t(121),s=t(52),r=t(13);n.exports=function(n,e,t){for(var i=a(e),l=r.f,c=s.f,p=0;p<i.length;p++){var d=i[p];o(n,d)||t&&o(t,d)||l(n,d,c(e,d))}}},function(n,e,t){var o=t(125);n.exports=function(n){var e=+n;return e!=e||0===e?0:o(e)}},function(n,e,t){var o=t(137),a=t(28),s=t(138);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=o(Object.prototype,"__proto__","set"))(t,[]),e=t instanceof Array}catch(n){}return function(t,o){return a(t),s(o),e?n(t,o):t.__proto__=o,t}}():void 0)},function(n,e){n.exports=function(n,e){for(var t=-1,o=e.length,a=n.length;++t<o;)n[a+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var o=t(22),a=t(162),s=t(163),r=t(164),i=t(165),l=t(166);function c(n){var e=this.__data__=new o(n);this.size=e.size}c.prototype.clear=a,c.prototype.delete=s,c.prototype.get=r,c.prototype.has=i,c.prototype.set=l,n.exports=c},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var o=t(14),a=t(41);n.exports=function(n){if(!a(n))return!1;var e=o(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var o=t(183),a=t(12);n.exports=function n(e,t,s,r,i){return e===t||(null==e||null==t||!a(e)&&!a(t)?e!=e&&t!=t:o(e,t,s,r,n,i))}},function(n,e,t){var o=t(77),a=t(186),s=t(78);n.exports=function(n,e,t,r,i,l){var c=1&t,p=n.length,d=e.length;if(p!=d&&!(c&&d>p))return!1;var m=l.get(n),u=l.get(e);if(m&&u)return m==e&&u==n;var h=-1,g=!0,b=2&t?new o:void 0;for(l.set(n,e),l.set(e,n);++h<p;){var f=n[h],y=e[h];if(r)var _=c?r(y,f,h,e,n,l):r(f,y,h,n,e,l);if(void 0!==_){if(_)continue;g=!1;break}if(b){if(!a(e,(function(n,e){if(!s(b,e)&&(f===n||i(f,n,t,r,l)))return b.push(e)}))){g=!1;break}}else if(f!==y&&!i(f,y,t,r,l)){g=!1;break}}return l.delete(n),l.delete(e),g}},function(n,e,t){var o=t(42),a=t(184),s=t(185);function r(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new o;++e<t;)this.add(n[e])}r.prototype.add=r.prototype.push=a,r.prototype.has=s,n.exports=r},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var o=t(196),a=t(202),s=t(83);n.exports=function(n){return s(n)?o(n):a(n)}},function(n,e,t){(function(n){var o=t(8),a=t(198),s=e&&!e.nodeType&&e,r=s&&"object"==typeof n&&n&&!n.nodeType&&n,i=r&&r.exports===s?o.Buffer:void 0,l=(i?i.isBuffer:void 0)||a;n.exports=l}).call(this,t(50)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var o=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==o||"symbol"!=o&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var o=t(199),a=t(200),s=t(201),r=s&&s.isTypedArray,i=r?a(r):o;n.exports=i},function(n,e,t){var o=t(73),a=t(44);n.exports=function(n){return null!=n&&a(n.length)&&!o(n)}},function(n,e,t){var o=t(10)(t(8),"Set");n.exports=o},function(n,e,t){var o=t(41);n.exports=function(n){return n==n&&!o(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var o=t(88),a=t(26);n.exports=function(n,e){for(var t=0,s=(e=o(e,n)).length;null!=n&&t<s;)n=n[a(e[t++])];return t&&t==s?n:void 0}},function(n,e,t){var o=t(6),a=t(45),s=t(213),r=t(216);n.exports=function(n,e){return o(n)?n:a(n,e)?[n]:s(r(n))}},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){var o=t(149),a=t(154),s=t(225),r=t(233),i=t(242),l=t(106),c=s((function(n){var e=l(n);return i(e)&&(e=void 0),r(o(n,1,i,!0),a(e,2))}));n.exports=c},function(n,e,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var o=/["'&<>]/;n.exports=function(n){var e,t=""+n,a=o.exec(t);if(!a)return t;var s="",r=0,i=0;for(r=a.index;r<t.length;r++){switch(t.charCodeAt(r)){case 34:e="&quot;";break;case 38:e="&amp;";break;case 39:e="&#39;";break;case 60:e="&lt;";break;case 62:e="&gt;";break;default:continue}i!==r&&(s+=t.substring(i,r)),i=r+1,s+=e}return i!==r?s+t.substring(i,r):s}},function(n,e,t){"use strict";
/**
 * @file Embedded JavaScript templating engine. {@link http://ejs.co}
 * @author Matthew Eernisse <mde@fleegix.org>
 * @author Tiancheng "Timothy" Gu <timothygu99@gmail.com>
 * @project EJS
 * @license {@link http://www.apache.org/licenses/LICENSE-2.0 Apache License, Version 2.0}
 */var o=t(253),a=t(254),s=t(255),r=!1,i=t(256).version,l=["delimiter","scope","context","debug","compileDebug","client","_with","rmWhitespace","strict","filename","async"],c=l.concat("cache"),p=/^\uFEFF/,d=/^[a-zA-Z_$][0-9a-zA-Z_$]*$/;function m(n,t){var a;if(t.some((function(t){return a=e.resolveInclude(n,t,!0),o.existsSync(a)})))return a}function u(n,t){var o,a=n.filename,s=arguments.length>1;if(n.cache){if(!a)throw new Error("cache option requires a filename");if(o=e.cache.get(a))return o;s||(t=g(a).toString().replace(p,""))}else if(!s){if(!a)throw new Error("Internal EJS error: no file name or template provided");t=g(a).toString().replace(p,"")}return o=e.compile(t,n),n.cache&&e.cache.set(a,o),o}function h(n,t,o){var a;if(!o){if("function"==typeof e.promiseImpl)return new e.promiseImpl((function(e,o){try{e(a=u(n)(t))}catch(n){o(n)}}));throw new Error("Please provide a callback function")}try{a=u(n)(t)}catch(n){return o(n)}o(null,a)}function g(n){return e.fileLoader(n)}function b(n,t){var a=s.shallowCopy(s.createNullProtoObjWherePossible(),t);if(a.filename=function(n,t){var a,s,r=t.views,i=/^[A-Za-z]+:\\|^\//.exec(n);if(i&&i.length)n=n.replace(/^\/*/,""),a=Array.isArray(t.root)?m(n,t.root):e.resolveInclude(n,t.root||"/",!0);else if(t.filename&&(s=e.resolveInclude(n,t.filename),o.existsSync(s)&&(a=s)),!a&&Array.isArray(r)&&(a=m(n,r)),!a&&"function"!=typeof t.includer)throw new Error('Could not find the include file "'+t.escapeFunction(n)+'"');return a}(n,a),"function"==typeof t.includer){var r=t.includer(n,a.filename);if(r&&(r.filename&&(a.filename=r.filename),r.template))return u(a,r.template)}return u(a)}function f(n,e,t,o,a){var s=e.split("\n"),r=Math.max(o-3,0),i=Math.min(s.length,o+3),l=a(t),c=s.slice(r,i).map((function(n,e){var t=e+r+1;return(t==o?" >> ":"    ")+t+"| "+n})).join("\n");throw n.path=l,n.message=(l||"ejs")+":"+o+"\n"+c+"\n\n"+n.message,n}function y(n){return n.replace(/;(\s*$)/,"$1")}function _(n,t){t=t||s.createNullProtoObjWherePossible();var o=s.createNullProtoObjWherePossible();this.templateText=n,this.mode=null,this.truncate=!1,this.currentLine=1,this.source="",o.client=t.client||!1,o.escapeFunction=t.escape||t.escapeFunction||s.escapeXML,o.compileDebug=!1!==t.compileDebug,o.debug=!!t.debug,o.filename=t.filename,o.openDelimiter=t.openDelimiter||e.openDelimiter||"<",o.closeDelimiter=t.closeDelimiter||e.closeDelimiter||">",o.delimiter=t.delimiter||e.delimiter||"%",o.strict=t.strict||!1,o.context=t.context,o.cache=t.cache||!1,o.rmWhitespace=t.rmWhitespace,o.root=t.root,o.includer=t.includer,o.outputFunctionName=t.outputFunctionName,o.localsName=t.localsName||e.localsName||"locals",o.views=t.views,o.async=t.async,o.destructuredLocals=t.destructuredLocals,o.legacyInclude=void 0===t.legacyInclude||!!t.legacyInclude,o.strict?o._with=!1:o._with=void 0===t._with||t._with,this.opts=o,this.regex=this.createRegex()}e.cache=s.cache,e.fileLoader=o.readFileSync,e.localsName="locals",e.promiseImpl=new Function("return this;")().Promise,e.resolveInclude=function(n,e,t){var o=a.dirname,s=a.extname,r=(0,a.resolve)(t?e:o(e),n);return s(n)||(r+=".ejs"),r},e.compile=function(n,e){return e&&e.scope&&(r||(console.warn("`scope` option is deprecated and will be removed in EJS 3"),r=!0),e.context||(e.context=e.scope),delete e.scope),new _(n,e).compile()},e.render=function(n,e,t){var o=e||s.createNullProtoObjWherePossible(),a=t||s.createNullProtoObjWherePossible();return 2==arguments.length&&s.shallowCopyFromList(a,o,l),u(a,n)(o)},e.renderFile=function(){var n,e,t,o=Array.prototype.slice.call(arguments),a=o.shift(),r={filename:a};return"function"==typeof arguments[arguments.length-1]&&(n=o.pop()),o.length?(e=o.shift(),o.length?s.shallowCopy(r,o.pop()):(e.settings&&(e.settings.views&&(r.views=e.settings.views),e.settings["view cache"]&&(r.cache=!0),(t=e.settings["view options"])&&s.shallowCopy(r,t)),s.shallowCopyFromList(r,e,c)),r.filename=a):e=s.createNullProtoObjWherePossible(),h(r,e,n)},e.Template=_,e.clearCache=function(){e.cache.reset()},_.modes={EVAL:"eval",ESCAPED:"escaped",RAW:"raw",COMMENT:"comment",LITERAL:"literal"},_.prototype={createRegex:function(){var n="(<%%|%%>|<%=|<%-|<%_|<%#|<%|%>|-%>|_%>)",e=s.escapeRegExpChars(this.opts.delimiter),t=s.escapeRegExpChars(this.opts.openDelimiter),o=s.escapeRegExpChars(this.opts.closeDelimiter);return n=n.replace(/%/g,e).replace(/</g,t).replace(/>/g,o),new RegExp(n)},compile:function(){var n,e,t,o=this.opts,r="",i="",l=o.escapeFunction,c=o.filename?JSON.stringify(o.filename):"undefined";if(!this.source){if(this.generateSource(),r+='  var __output = "";\n  function __append(s) { if (s !== undefined && s !== null) __output += s }\n',o.outputFunctionName){if(!d.test(o.outputFunctionName))throw new Error("outputFunctionName is not a valid JS identifier.");r+="  var "+o.outputFunctionName+" = __append;\n"}if(o.localsName&&!d.test(o.localsName))throw new Error("localsName is not a valid JS identifier.");if(o.destructuredLocals&&o.destructuredLocals.length){for(var p="  var __locals = ("+o.localsName+" || {}),\n",m=0;m<o.destructuredLocals.length;m++){var u=o.destructuredLocals[m];if(!d.test(u))throw new Error("destructuredLocals["+m+"] is not a valid JS identifier.");m>0&&(p+=",\n  "),p+=u+" = __locals."+u}r+=p+";\n"}!1!==o._with&&(r+="  with ("+o.localsName+" || {}) {\n",i+="  }\n"),i+="  return __output;\n",this.source=r+this.source+i}n=o.compileDebug?"var __line = 1\n  , __lines = "+JSON.stringify(this.templateText)+"\n  , __filename = "+c+";\ntry {\n"+this.source+"} catch (e) {\n  rethrow(e, __lines, __filename, __line, escapeFn);\n}\n":this.source,o.client&&(n="escapeFn = escapeFn || "+l.toString()+";\n"+n,o.compileDebug&&(n="rethrow = rethrow || "+f.toString()+";\n"+n)),o.strict&&(n='"use strict";\n'+n),o.debug&&console.log(n),o.compileDebug&&o.filename&&(n=n+"\n//# sourceURL="+c+"\n");try{if(o.async)try{t=new Function("return (async function(){}).constructor;")()}catch(n){throw n instanceof SyntaxError?new Error("This environment does not support async/await"):n}else t=Function;e=new t(o.localsName+", escapeFn, include, rethrow",n)}catch(n){throw n instanceof SyntaxError&&(o.filename&&(n.message+=" in "+o.filename),n.message+=" while compiling ejs\n\n",n.message+="If the above error is not helpful, you may want to try EJS-Lint:\n",n.message+="https://github.com/RyanZim/EJS-Lint",o.async||(n.message+="\n",n.message+="Or, if you meant to create an async function, pass `async: true` as an option.")),n}var h=o.client?e:function(n){return e.apply(o.context,[n||s.createNullProtoObjWherePossible(),l,function(e,t){var a=s.shallowCopy(s.createNullProtoObjWherePossible(),n);return t&&(a=s.shallowCopy(a,t)),b(e,o)(a)},f])};if(o.filename&&"function"==typeof Object.defineProperty){var g=o.filename,y=a.basename(g,a.extname(g));try{Object.defineProperty(h,"name",{value:y,writable:!1,enumerable:!1,configurable:!0})}catch(n){}}return h},generateSource:function(){this.opts.rmWhitespace&&(this.templateText=this.templateText.replace(/[\r\n]+/g,"\n").replace(/^\s+|\s+$/gm,"")),this.templateText=this.templateText.replace(/[ \t]*<%_/gm,"<%_").replace(/_%>[ \t]*/gm,"_%>");var n=this,e=this.parseTemplateText(),t=this.opts.delimiter,o=this.opts.openDelimiter,a=this.opts.closeDelimiter;e&&e.length&&e.forEach((function(s,r){var i;if(0===s.indexOf(o+t)&&0!==s.indexOf(o+t+t)&&(i=e[r+2])!=t+a&&i!="-"+t+a&&i!="_"+t+a)throw new Error('Could not find matching close tag for "'+s+'".');n.scanLine(s)}))},parseTemplateText:function(){for(var n,e=this.templateText,t=this.regex,o=t.exec(e),a=[];o;)0!==(n=o.index)&&(a.push(e.substring(0,n)),e=e.slice(n)),a.push(o[0]),e=e.slice(o[0].length),o=t.exec(e);return e&&a.push(e),a},_addOutput:function(n){if(this.truncate&&(n=n.replace(/^(?:\r\n|\r|\n)/,""),this.truncate=!1),!n)return n;n=(n=(n=(n=n.replace(/\\/g,"\\\\")).replace(/\n/g,"\\n")).replace(/\r/g,"\\r")).replace(/"/g,'\\"'),this.source+='    ; __append("'+n+'")\n'},scanLine:function(n){var e,t=this.opts.delimiter,o=this.opts.openDelimiter,a=this.opts.closeDelimiter;switch(e=n.split("\n").length-1,n){case o+t:case o+t+"_":this.mode=_.modes.EVAL;break;case o+t+"=":this.mode=_.modes.ESCAPED;break;case o+t+"-":this.mode=_.modes.RAW;break;case o+t+"#":this.mode=_.modes.COMMENT;break;case o+t+t:this.mode=_.modes.LITERAL,this.source+='    ; __append("'+n.replace(o+t+t,o+t)+'")\n';break;case t+t+a:this.mode=_.modes.LITERAL,this.source+='    ; __append("'+n.replace(t+t+a,t+a)+'")\n';break;case t+a:case"-"+t+a:case"_"+t+a:this.mode==_.modes.LITERAL&&this._addOutput(n),this.mode=null,this.truncate=0===n.indexOf("-")||0===n.indexOf("_");break;default:if(this.mode){switch(this.mode){case _.modes.EVAL:case _.modes.ESCAPED:case _.modes.RAW:n.lastIndexOf("//")>n.lastIndexOf("\n")&&(n+="\n")}switch(this.mode){case _.modes.EVAL:this.source+="    ; "+n+"\n";break;case _.modes.ESCAPED:this.source+="    ; __append(escapeFn("+y(n)+"))\n";break;case _.modes.RAW:this.source+="    ; __append("+y(n)+")\n";break;case _.modes.COMMENT:break;case _.modes.LITERAL:this._addOutput(n)}}else this._addOutput(n)}this.opts.compileDebug&&e&&(this.currentLine+=e,this.source+="    ; __line = "+this.currentLine+"\n")}},e.escapeXML=s.escapeXML,e.__express=e.renderFile,e.VERSION=i,e.name="ejs","undefined"!=typeof window&&(window.ejs=e)},function(n,e,t){"use strict";t.r(e);var o={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},a=(t(245),t(0)),s=Object(a.a)(o,(function(){return(0,this._self._c)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);e.default=s.exports},function(n,e,t){"use strict";t.r(e);var o={name:"CodeGroup",data:()=>({codeTabs:[],activeCodeTabIndex:-1}),watch:{activeCodeTabIndex(n){this.codeTabs.forEach(n=>{n.elm.classList.remove("theme-code-block__active")}),this.codeTabs[n].elm.classList.add("theme-code-block__active")}},mounted(){this.codeTabs=(this.$slots.default||[]).filter(n=>Boolean(n.componentOptions)).map((n,e)=>(""===n.componentOptions.propsData.active&&(this.activeCodeTabIndex=e),{title:n.componentOptions.propsData.title,elm:n.elm})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab(n){this.activeCodeTabIndex=n}}},a=(t(246),t(0)),s=Object(a.a)(o,(function(){var n=this,e=n._self._c;return e("div",{staticClass:"theme-code-group"},[e("div",{staticClass:"theme-code-group__nav"},[e("ul",{staticClass:"theme-code-group__ul"},n._l(n.codeTabs,(function(t,o){return e("li",{key:t.title,staticClass:"theme-code-group__li"},[e("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":o===n.activeCodeTabIndex},on:{click:function(e){return n.changeCodeTab(o)}}},[n._v("\n            "+n._s(t.title)+"\n          ")])])})),0)]),n._v(" "),n._t("default"),n._v(" "),n.codeTabs.length<1?e("pre",{staticClass:"pre-blank"},[n._v("// Make sure to add code blocks to your code group")]):n._e()],2)}),[],!1,null,"2f5f1757",null);e.default=s.exports},function(n,e){n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){var o=t(3),a=t(9),s=o.document,r=a(s)&&a(s.createElement);n.exports=function(n){return r?s.createElement(n):{}}},function(n,e,t){var o=t(5),a=t(4);n.exports=o&&a((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(n,e,t){var o=t(62),a=t(64),s=o("keys");n.exports=function(n){return s[n]||(s[n]=a(n))}},function(n,e,t){var o=t(2),a=t(7),s=t(30),r=t(123).indexOf,i=t(49),l=o([].push);n.exports=function(n,e){var t,o=s(n),c=0,p=[];for(t in o)!a(i,t)&&a(o,t)&&l(p,t);for(;e.length>c;)a(o,t=e[c++])&&(~r(p,t)||l(p,t));return p}},function(n,e,t){var o=t(19),a=t(3),s=t(135),r=t(136),i=a.WebAssembly,l=7!==Error("e",{cause:7}).cause,c=function(n,e){var t={};t[n]=r(n,e,l),o({global:!0,constructor:!0,arity:1,forced:l},t)},p=function(n,e){if(i&&i[n]){var t={};t[n]=r("WebAssembly."+n,e,l),o({target:"WebAssembly",stat:!0,constructor:!0,arity:1,forced:l},t)}};c("Error",(function(n){return function(e){return s(n,this,arguments)}})),c("EvalError",(function(n){return function(e){return s(n,this,arguments)}})),c("RangeError",(function(n){return function(e){return s(n,this,arguments)}})),c("ReferenceError",(function(n){return function(e){return s(n,this,arguments)}})),c("SyntaxError",(function(n){return function(e){return s(n,this,arguments)}})),c("TypeError",(function(n){return function(e){return s(n,this,arguments)}})),c("URIError",(function(n){return function(e){return s(n,this,arguments)}})),p("CompileError",(function(n){return function(e){return s(n,this,arguments)}})),p("LinkError",(function(n){return function(e){return s(n,this,arguments)}})),p("RuntimeError",(function(n){return function(e){return s(n,this,arguments)}}))},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){n.exports=t(259)},function(n,e,t){"use strict";var o=t(19),a=t(129).left,s=t(130),r=t(61);o({target:"Array",proto:!0,forced:!t(131)&&r>79&&r<83||!s("reduce")},{reduce:function(n){var e=arguments.length;return a(this,n,e,e>1?arguments[1]:void 0)}})},function(n,e,t){"use strict";var o={}.propertyIsEnumerable,a=Object.getOwnPropertyDescriptor,s=a&&!o.call({1:2},1);e.f=s?function(n){var e=a(this,n);return!!e&&e.enumerable}:o},function(n,e,t){var o=t(35),a=t(9),s=t(57),r=t(112),i=t(114),l=t(21),c=TypeError,p=l("toPrimitive");n.exports=function(n,e){if(!a(n)||s(n))return n;var t,l=r(n,p);if(l){if(void 0===e&&(e="default"),t=o(l,n,e),!a(t)||s(t))return t;throw c("Can't convert object to primitive value")}return void 0===e&&(e="number"),i(n,e)}},function(n,e){n.exports="undefined"!=typeof navigator&&String(navigator.userAgent)||""},function(n,e,t){var o=t(32),a=t(54);n.exports=function(n,e){var t=n[e];return a(t)?void 0:o(t)}},function(n,e){var t=String;n.exports=function(n){try{return t(n)}catch(n){return"Object"}}},function(n,e,t){var o=t(35),a=t(1),s=t(9),r=TypeError;n.exports=function(n,e){var t,i;if("string"===e&&a(t=n.toString)&&!s(i=o(t,n)))return i;if(a(t=n.valueOf)&&!s(i=o(t,n)))return i;if("string"!==e&&a(t=n.toString)&&!s(i=o(t,n)))return i;throw r("Can't convert object to primitive value")}},function(n,e,t){var o=t(1),a=t(13),s=t(116),r=t(38);n.exports=function(n,e,t,i){i||(i={});var l=i.enumerable,c=void 0!==i.name?i.name:e;if(o(t)&&s(t,c,i),i.global)l?n[e]=t:r(e,t);else{try{i.unsafe?n[e]&&(l=!0):delete n[e]}catch(n){}l?n[e]=t:a.f(n,e,{value:t,enumerable:!1,configurable:!i.nonConfigurable,writable:!i.nonWritable})}return n}},function(n,e,t){var o=t(2),a=t(4),s=t(1),r=t(7),i=t(5),l=t(117).CONFIGURABLE,c=t(118),p=t(119),d=p.enforce,m=p.get,u=String,h=Object.defineProperty,g=o("".slice),b=o("".replace),f=o([].join),y=i&&!a((function(){return 8!==h((function(){}),"length",{value:8}).length})),_=String(String).split("String"),v=n.exports=function(n,e,t){"Symbol("===g(u(e),0,7)&&(e="["+b(u(e),/^Symbol\(([^)]*)\)/,"$1")+"]"),t&&t.getter&&(e="get "+e),t&&t.setter&&(e="set "+e),(!r(n,"name")||l&&n.name!==e)&&(i?h(n,"name",{value:e,configurable:!0}):n.name=e),y&&t&&r(t,"arity")&&n.length!==t.arity&&h(n,"length",{value:t.arity});try{t&&r(t,"constructor")&&t.constructor?i&&h(n,"prototype",{writable:!1}):n.prototype&&(n.prototype=void 0)}catch(n){}var o=d(n);return r(o,"source")||(o.source=f(_,"string"==typeof e?e:"")),n};Function.prototype.toString=v((function(){return s(this)&&m(this).source||c(this)}),"toString")},function(n,e,t){var o=t(5),a=t(7),s=Function.prototype,r=o&&Object.getOwnPropertyDescriptor,i=a(s,"name"),l=i&&"something"===function(){}.name,c=i&&(!o||o&&r(s,"name").configurable);n.exports={EXISTS:i,PROPER:l,CONFIGURABLE:c}},function(n,e,t){var o=t(2),a=t(1),s=t(37),r=o(Function.toString);a(s.inspectSource)||(s.inspectSource=function(n){return r(n)}),n.exports=s.inspectSource},function(n,e,t){var o,a,s,r=t(120),i=t(3),l=t(9),c=t(15),p=t(7),d=t(37),m=t(103),u=t(49),h=i.TypeError,g=i.WeakMap;if(r||d.state){var b=d.state||(d.state=new g);b.get=b.get,b.has=b.has,b.set=b.set,o=function(n,e){if(b.has(n))throw h("Object already initialized");return e.facade=n,b.set(n,e),e},a=function(n){return b.get(n)||{}},s=function(n){return b.has(n)}}else{var f=m("state");u[f]=!0,o=function(n,e){if(p(n,f))throw h("Object already initialized");return e.facade=n,c(n,f,e),e},a=function(n){return p(n,f)?n[f]:{}},s=function(n){return p(n,f)}}n.exports={set:o,get:a,has:s,enforce:function(n){return s(n)?a(n):o(n,{})},getterFor:function(n){return function(e){var t;if(!l(e)||(t=a(e)).type!==n)throw h("Incompatible receiver, "+n+" required");return t}}}},function(n,e,t){var o=t(3),a=t(1),s=o.WeakMap;n.exports=a(s)&&/native code/.test(String(s))},function(n,e,t){var o=t(31),a=t(2),s=t(122),r=t(127),i=t(28),l=a([].concat);n.exports=o("Reflect","ownKeys")||function(n){var e=s.f(i(n)),t=r.f;return t?l(e,t(n)):e}},function(n,e,t){var o=t(104),a=t(100).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return o(n,a)}},function(n,e,t){var o=t(30),a=t(124),s=t(34),r=function(n){return function(e,t,r){var i,l=o(e),c=s(l),p=a(r,c);if(n&&t!=t){for(;c>p;)if((i=l[p++])!=i)return!0}else for(;c>p;p++)if((n||p in l)&&l[p]===t)return n||p||0;return!n&&-1}};n.exports={includes:r(!0),indexOf:r(!1)}},function(n,e,t){var o=t(67),a=Math.max,s=Math.min;n.exports=function(n,e){var t=o(n);return t<0?a(t+e,0):s(t,e)}},function(n,e){var t=Math.ceil,o=Math.floor;n.exports=Math.trunc||function(n){var e=+n;return(e>0?o:t)(e)}},function(n,e,t){var o=t(67),a=Math.min;n.exports=function(n){return n>0?a(o(n),9007199254740991):0}},function(n,e){e.f=Object.getOwnPropertySymbols},function(n,e,t){var o=t(4),a=t(1),s=/#|\.prototype\./,r=function(n,e){var t=l[i(n)];return t==p||t!=c&&(a(e)?o(e):!!e)},i=r.normalize=function(n){return String(n).replace(s,".").toLowerCase()},l=r.data={},c=r.NATIVE="N",p=r.POLYFILL="P";n.exports=r},function(n,e,t){var o=t(32),a=t(33),s=t(48),r=t(34),i=TypeError,l=function(n){return function(e,t,l,c){o(t);var p=a(e),d=s(p),m=r(p),u=n?m-1:0,h=n?-1:1;if(l<2)for(;;){if(u in d){c=d[u],u+=h;break}if(u+=h,n?u<0:m<=u)throw i("Reduce of empty array with no initial value")}for(;n?u>=0:m>u;u+=h)u in d&&(c=t(c,d[u],u,p));return c}};n.exports={left:l(!1),right:l(!0)}},function(n,e,t){"use strict";var o=t(4);n.exports=function(n,e){var t=[][n];return!!t&&o((function(){t.call(null,e||function(){return 1},1)}))}},function(n,e,t){var o=t(20);n.exports="undefined"!=typeof process&&"process"==o(process)},function(n,e,t){"use strict";var o=t(5),a=t(133),s=TypeError,r=Object.getOwnPropertyDescriptor,i=o&&!function(){if(void 0!==this)return!0;try{Object.defineProperty([],"length",{writable:!1}).length=1}catch(n){return n instanceof TypeError}}();n.exports=i?function(n,e){if(a(n)&&!r(n,"length").writable)throw s("Cannot set read only .length");return n.length=e}:function(n,e){return n.length=e}},function(n,e,t){var o=t(20);n.exports=Array.isArray||function(n){return"Array"==o(n)}},function(n,e){var t=TypeError;n.exports=function(n){if(n>9007199254740991)throw t("Maximum allowed index exceeded");return n}},function(n,e,t){var o=t(29),a=Function.prototype,s=a.apply,r=a.call;n.exports="object"==typeof Reflect&&Reflect.apply||(o?r.bind(s):function(){return r.apply(s,arguments)})},function(n,e,t){"use strict";var o=t(31),a=t(7),s=t(15),r=t(58),i=t(68),l=t(66),c=t(139),p=t(140),d=t(141),m=t(145),u=t(146),h=t(5),g=t(63);n.exports=function(n,e,t,b){var f=b?2:1,y=n.split("."),_=y[y.length-1],v=o.apply(null,y);if(v){var k=v.prototype;if(!g&&a(k,"cause")&&delete k.cause,!t)return v;var x=o("Error"),w=e((function(n,e){var t=d(b?e:n,void 0),o=b?new v(n):new v;return void 0!==t&&s(o,"message",t),u(o,w,o.stack,2),this&&r(k,this)&&p(o,this,w),arguments.length>f&&m(o,arguments[f]),o}));if(w.prototype=k,"Error"!==_?i?i(w,x):l(w,x,{name:!0}):h&&"stackTraceLimit"in v&&(c(w,v,"stackTraceLimit"),c(w,v,"prepareStackTrace")),l(w,v),!g)try{k.name!==_&&s(k,"name",_),k.constructor=w}catch(n){}return w}}},function(n,e,t){var o=t(2),a=t(32);n.exports=function(n,e,t){try{return o(a(Object.getOwnPropertyDescriptor(n,e)[t]))}catch(n){}}},function(n,e,t){var o=t(1),a=String,s=TypeError;n.exports=function(n){if("object"==typeof n||o(n))return n;throw s("Can't set "+a(n)+" as a prototype")}},function(n,e,t){var o=t(13).f;n.exports=function(n,e,t){t in n||o(n,t,{configurable:!0,get:function(){return e[t]},set:function(n){e[t]=n}})}},function(n,e,t){var o=t(1),a=t(9),s=t(68);n.exports=function(n,e,t){var r,i;return s&&o(r=e.constructor)&&r!==t&&a(i=r.prototype)&&i!==t.prototype&&s(n,i),n}},function(n,e,t){var o=t(142);n.exports=function(n,e){return void 0===n?arguments.length<2?"":e:o(n)}},function(n,e,t){var o=t(143),a=String;n.exports=function(n){if("Symbol"===o(n))throw TypeError("Cannot convert a Symbol value to a string");return a(n)}},function(n,e,t){var o=t(144),a=t(1),s=t(20),r=t(21)("toStringTag"),i=Object,l="Arguments"==s(function(){return arguments}());n.exports=o?s:function(n){var e,t,o;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=i(n),r))?t:l?s(e):"Object"==(o=s(e))&&a(e.callee)?"Arguments":o}},function(n,e,t){var o={};o[t(21)("toStringTag")]="z",n.exports="[object z]"===String(o)},function(n,e,t){var o=t(9),a=t(15);n.exports=function(n,e){o(e)&&"cause"in e&&a(n,"cause",e.cause)}},function(n,e,t){var o=t(15),a=t(147),s=t(148),r=Error.captureStackTrace;n.exports=function(n,e,t,i){s&&(r?r(n,e):o(n,"stack",a(t,i)))}},function(n,e,t){var o=t(2),a=Error,s=o("".replace),r=String(a("zxcasd").stack),i=/\n\s*at [^:]*:[^\n]*/,l=i.test(r);n.exports=function(n,e){if(l&&"string"==typeof n&&!a.prepareStackTrace)for(;e--;)n=s(n,i,"");return n}},function(n,e,t){var o=t(4),a=t(36);n.exports=!o((function(){var n=Error("a");return!("stack"in n)||(Object.defineProperty(n,"stack",a(1,7)),7!==n.stack)}))},function(n,e,t){var o=t(69),a=t(150);n.exports=function n(e,t,s,r,i){var l=-1,c=e.length;for(s||(s=a),i||(i=[]);++l<c;){var p=e[l];t>0&&s(p)?t>1?n(p,t-1,s,r,i):o(i,p):r||(i[i.length]=p)}return i}},function(n,e,t){var o=t(16),a=t(39),s=t(6),r=o?o.isConcatSpreadable:void 0;n.exports=function(n){return s(n)||a(n)||!!(r&&n&&n[r])}},function(n,e,t){var o=t(14),a=t(12);n.exports=function(n){return a(n)&&"[object Arguments]"==o(n)}},function(n,e,t){var o=t(16),a=Object.prototype,s=a.hasOwnProperty,r=a.toString,i=o?o.toStringTag:void 0;n.exports=function(n){var e=s.call(n,i),t=n[i];try{n[i]=void 0;var o=!0}catch(n){}var a=r.call(n);return o&&(e?n[i]=t:delete n[i]),a}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var o=t(155),a=t(211),s=t(47),r=t(6),i=t(222);n.exports=function(n){return"function"==typeof n?n:null==n?s:"object"==typeof n?r(n)?a(n[0],n[1]):o(n):i(n)}},function(n,e,t){var o=t(156),a=t(210),s=t(86);n.exports=function(n){var e=a(n);return 1==e.length&&e[0][2]?s(e[0][0],e[0][1]):function(t){return t===n||o(t,n,e)}}},function(n,e,t){var o=t(71),a=t(75);n.exports=function(n,e,t,s){var r=t.length,i=r,l=!s;if(null==n)return!i;for(n=Object(n);r--;){var c=t[r];if(l&&c[2]?c[1]!==n[c[0]]:!(c[0]in n))return!1}for(;++r<i;){var p=(c=t[r])[0],d=n[p],m=c[1];if(l&&c[2]){if(void 0===d&&!(p in n))return!1}else{var u=new o;if(s)var h=s(d,m,p,n,e,u);if(!(void 0===h?a(m,d,3,s,u):h))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var o=t(23),a=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=o(e,n);return!(t<0)&&(t==e.length-1?e.pop():a.call(e,t,1),--this.size,!0)}},function(n,e,t){var o=t(23);n.exports=function(n){var e=this.__data__,t=o(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var o=t(23);n.exports=function(n){return o(this.__data__,n)>-1}},function(n,e,t){var o=t(23);n.exports=function(n,e){var t=this.__data__,a=o(t,n);return a<0?(++this.size,t.push([n,e])):t[a][1]=e,this}},function(n,e,t){var o=t(22);n.exports=function(){this.__data__=new o,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var o=t(22),a=t(40),s=t(42);n.exports=function(n,e){var t=this.__data__;if(t instanceof o){var r=t.__data__;if(!a||r.length<199)return r.push([n,e]),this.size=++t.size,this;t=this.__data__=new s(r)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var o=t(73),a=t(168),s=t(41),r=t(74),i=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,p=l.toString,d=c.hasOwnProperty,m=RegExp("^"+p.call(d).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!s(n)||a(n))&&(o(n)?m:i).test(r(n))}},function(n,e,t){var o,a=t(169),s=(o=/[^.]+$/.exec(a&&a.keys&&a.keys.IE_PROTO||""))?"Symbol(src)_1."+o:"";n.exports=function(n){return!!s&&s in n}},function(n,e,t){var o=t(8)["__core-js_shared__"];n.exports=o},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var o=t(172),a=t(22),s=t(40);n.exports=function(){this.size=0,this.__data__={hash:new o,map:new(s||a),string:new o}}},function(n,e,t){var o=t(173),a=t(174),s=t(175),r=t(176),i=t(177);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var o=n[e];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=a,l.prototype.get=s,l.prototype.has=r,l.prototype.set=i,n.exports=l},function(n,e,t){var o=t(24);n.exports=function(){this.__data__=o?o(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var o=t(24),a=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(o){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return a.call(e,n)?e[n]:void 0}},function(n,e,t){var o=t(24),a=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return o?void 0!==e[n]:a.call(e,n)}},function(n,e,t){var o=t(24);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=o&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var o=t(25);n.exports=function(n){var e=o(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var o=t(25);n.exports=function(n){return o(this,n).get(n)}},function(n,e,t){var o=t(25);n.exports=function(n){return o(this,n).has(n)}},function(n,e,t){var o=t(25);n.exports=function(n,e){var t=o(this,n),a=t.size;return t.set(n,e),this.size+=t.size==a?0:1,this}},function(n,e,t){var o=t(71),a=t(76),s=t(187),r=t(190),i=t(206),l=t(6),c=t(80),p=t(82),d="[object Object]",m=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,u,h,g){var b=l(n),f=l(e),y=b?"[object Array]":i(n),_=f?"[object Array]":i(e),v=(y="[object Arguments]"==y?d:y)==d,k=(_="[object Arguments]"==_?d:_)==d,x=y==_;if(x&&c(n)){if(!c(e))return!1;b=!0,v=!1}if(x&&!v)return g||(g=new o),b||p(n)?a(n,e,t,u,h,g):s(n,e,y,t,u,h,g);if(!(1&t)){var w=v&&m.call(n,"__wrapped__"),E=k&&m.call(e,"__wrapped__");if(w||E){var A=w?n.value():n,z=E?e.value():e;return g||(g=new o),h(A,z,t,u,g)}}return!!x&&(g||(g=new o),r(n,e,t,u,h,g))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,o=null==n?0:n.length;++t<o;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var o=t(16),a=t(188),s=t(72),r=t(76),i=t(189),l=t(43),c=o?o.prototype:void 0,p=c?c.valueOf:void 0;n.exports=function(n,e,t,o,c,d,m){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!d(new a(n),new a(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return s(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var u=i;case"[object Set]":var h=1&o;if(u||(u=l),n.size!=e.size&&!h)return!1;var g=m.get(n);if(g)return g==e;o|=2,m.set(n,e);var b=r(u(n),u(e),o,c,d,m);return m.delete(n),b;case"[object Symbol]":if(p)return p.call(n)==p.call(e)}return!1}},function(n,e,t){var o=t(8).Uint8Array;n.exports=o},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,o){t[++e]=[o,n]})),t}},function(n,e,t){var o=t(191),a=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,s,r,i){var l=1&t,c=o(n),p=c.length;if(p!=o(e).length&&!l)return!1;for(var d=p;d--;){var m=c[d];if(!(l?m in e:a.call(e,m)))return!1}var u=i.get(n),h=i.get(e);if(u&&h)return u==e&&h==n;var g=!0;i.set(n,e),i.set(e,n);for(var b=l;++d<p;){var f=n[m=c[d]],y=e[m];if(s)var _=l?s(y,f,m,e,n,i):s(f,y,m,n,e,i);if(!(void 0===_?f===y||r(f,y,t,s,i):_)){g=!1;break}b||(b="constructor"==m)}if(g&&!b){var v=n.constructor,k=e.constructor;v==k||!("constructor"in n)||!("constructor"in e)||"function"==typeof v&&v instanceof v&&"function"==typeof k&&k instanceof k||(g=!1)}return i.delete(n),i.delete(e),g}},function(n,e,t){var o=t(192),a=t(193),s=t(79);n.exports=function(n){return o(n,s,a)}},function(n,e,t){var o=t(69),a=t(6);n.exports=function(n,e,t){var s=e(n);return a(n)?s:o(s,t(n))}},function(n,e,t){var o=t(194),a=t(195),s=Object.prototype.propertyIsEnumerable,r=Object.getOwnPropertySymbols,i=r?function(n){return null==n?[]:(n=Object(n),o(r(n),(function(e){return s.call(n,e)})))}:a;n.exports=i},function(n,e){n.exports=function(n,e){for(var t=-1,o=null==n?0:n.length,a=0,s=[];++t<o;){var r=n[t];e(r,t,n)&&(s[a++]=r)}return s}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var o=t(197),a=t(39),s=t(6),r=t(80),i=t(81),l=t(82),c=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=s(n),p=!t&&a(n),d=!t&&!p&&r(n),m=!t&&!p&&!d&&l(n),u=t||p||d||m,h=u?o(n.length,String):[],g=h.length;for(var b in n)!e&&!c.call(n,b)||u&&("length"==b||d&&("offset"==b||"parent"==b)||m&&("buffer"==b||"byteLength"==b||"byteOffset"==b)||i(b,g))||h.push(b);return h}},function(n,e){n.exports=function(n,e){for(var t=-1,o=Array(n);++t<n;)o[t]=e(t);return o}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var o=t(14),a=t(44),s=t(12),r={};r["[object Float32Array]"]=r["[object Float64Array]"]=r["[object Int8Array]"]=r["[object Int16Array]"]=r["[object Int32Array]"]=r["[object Uint8Array]"]=r["[object Uint8ClampedArray]"]=r["[object Uint16Array]"]=r["[object Uint32Array]"]=!0,r["[object Arguments]"]=r["[object Array]"]=r["[object ArrayBuffer]"]=r["[object Boolean]"]=r["[object DataView]"]=r["[object Date]"]=r["[object Error]"]=r["[object Function]"]=r["[object Map]"]=r["[object Number]"]=r["[object Object]"]=r["[object RegExp]"]=r["[object Set]"]=r["[object String]"]=r["[object WeakMap]"]=!1,n.exports=function(n){return s(n)&&a(n.length)&&!!r[o(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var o=t(70),a=e&&!e.nodeType&&e,s=a&&"object"==typeof n&&n&&!n.nodeType&&n,r=s&&s.exports===a&&o.process,i=function(){try{var n=s&&s.require&&s.require("util").types;return n||r&&r.binding&&r.binding("util")}catch(n){}}();n.exports=i}).call(this,t(50)(n))},function(n,e,t){var o=t(203),a=t(204),s=Object.prototype.hasOwnProperty;n.exports=function(n){if(!o(n))return a(n);var e=[];for(var t in Object(n))s.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var o=t(205)(Object.keys,Object);n.exports=o},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var o=t(207),a=t(40),s=t(208),r=t(84),i=t(209),l=t(14),c=t(74),p=c(o),d=c(a),m=c(s),u=c(r),h=c(i),g=l;(o&&"[object DataView]"!=g(new o(new ArrayBuffer(1)))||a&&"[object Map]"!=g(new a)||s&&"[object Promise]"!=g(s.resolve())||r&&"[object Set]"!=g(new r)||i&&"[object WeakMap]"!=g(new i))&&(g=function(n){var e=l(n),t="[object Object]"==e?n.constructor:void 0,o=t?c(t):"";if(o)switch(o){case p:return"[object DataView]";case d:return"[object Map]";case m:return"[object Promise]";case u:return"[object Set]";case h:return"[object WeakMap]"}return e}),n.exports=g},function(n,e,t){var o=t(10)(t(8),"DataView");n.exports=o},function(n,e,t){var o=t(10)(t(8),"Promise");n.exports=o},function(n,e,t){var o=t(10)(t(8),"WeakMap");n.exports=o},function(n,e,t){var o=t(85),a=t(79);n.exports=function(n){for(var e=a(n),t=e.length;t--;){var s=e[t],r=n[s];e[t]=[s,r,o(r)]}return e}},function(n,e,t){var o=t(75),a=t(212),s=t(219),r=t(45),i=t(85),l=t(86),c=t(26);n.exports=function(n,e){return r(n)&&i(e)?l(c(n),e):function(t){var r=a(t,n);return void 0===r&&r===e?s(t,n):o(e,r,3)}}},function(n,e,t){var o=t(87);n.exports=function(n,e,t){var a=null==n?void 0:o(n,e);return void 0===a?t:a}},function(n,e,t){var o=t(214),a=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,s=/\\(\\)?/g,r=o((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(a,(function(n,t,o,a){e.push(o?a.replace(s,"$1"):t||n)})),e}));n.exports=r},function(n,e,t){var o=t(215);n.exports=function(n){var e=o(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var o=t(42);function a(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var o=arguments,a=e?e.apply(this,o):o[0],s=t.cache;if(s.has(a))return s.get(a);var r=n.apply(this,o);return t.cache=s.set(a,r)||s,r};return t.cache=new(a.Cache||o),t}a.Cache=o,n.exports=a},function(n,e,t){var o=t(217);n.exports=function(n){return null==n?"":o(n)}},function(n,e,t){var o=t(16),a=t(218),s=t(6),r=t(46),i=o?o.prototype:void 0,l=i?i.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(s(e))return a(e,n)+"";if(r(e))return l?l.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,o=null==n?0:n.length,a=Array(o);++t<o;)a[t]=e(n[t],t,n);return a}},function(n,e,t){var o=t(220),a=t(221);n.exports=function(n,e){return null!=n&&a(n,e,o)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var o=t(88),a=t(39),s=t(6),r=t(81),i=t(44),l=t(26);n.exports=function(n,e,t){for(var c=-1,p=(e=o(e,n)).length,d=!1;++c<p;){var m=l(e[c]);if(!(d=null!=n&&t(n,m)))break;n=n[m]}return d||++c!=p?d:!!(p=null==n?0:n.length)&&i(p)&&r(m,p)&&(s(n)||a(n))}},function(n,e,t){var o=t(223),a=t(224),s=t(45),r=t(26);n.exports=function(n){return s(n)?o(r(n)):a(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var o=t(87);n.exports=function(n){return function(e){return o(e,n)}}},function(n,e,t){var o=t(47),a=t(226),s=t(228);n.exports=function(n,e){return s(a(n,e,o),n+"")}},function(n,e,t){var o=t(227),a=Math.max;n.exports=function(n,e,t){return e=a(void 0===e?n.length-1:e,0),function(){for(var s=arguments,r=-1,i=a(s.length-e,0),l=Array(i);++r<i;)l[r]=s[e+r];r=-1;for(var c=Array(e+1);++r<e;)c[r]=s[r];return c[e]=t(l),o(n,this,c)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var o=t(229),a=t(232)(o);n.exports=a},function(n,e,t){var o=t(230),a=t(231),s=t(47),r=a?function(n,e){return a(n,"toString",{configurable:!0,enumerable:!1,value:o(e),writable:!0})}:s;n.exports=r},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var o=t(10),a=function(){try{var n=o(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=a},function(n,e){var t=Date.now;n.exports=function(n){var e=0,o=0;return function(){var a=t(),s=16-(a-o);if(o=a,s>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var o=t(77),a=t(234),s=t(239),r=t(78),i=t(240),l=t(43);n.exports=function(n,e,t){var c=-1,p=a,d=n.length,m=!0,u=[],h=u;if(t)m=!1,p=s;else if(d>=200){var g=e?null:i(n);if(g)return l(g);m=!1,p=r,h=new o}else h=e?[]:u;n:for(;++c<d;){var b=n[c],f=e?e(b):b;if(b=t||0!==b?b:0,m&&f==f){for(var y=h.length;y--;)if(h[y]===f)continue n;e&&h.push(f),u.push(b)}else p(h,f,t)||(h!==u&&h.push(f),u.push(b))}return u}},function(n,e,t){var o=t(235);n.exports=function(n,e){return!!(null==n?0:n.length)&&o(n,e,0)>-1}},function(n,e,t){var o=t(236),a=t(237),s=t(238);n.exports=function(n,e,t){return e==e?s(n,e,t):o(n,a,t)}},function(n,e){n.exports=function(n,e,t,o){for(var a=n.length,s=t+(o?1:-1);o?s--:++s<a;)if(e(n[s],s,n))return s;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var o=t-1,a=n.length;++o<a;)if(n[o]===e)return o;return-1}},function(n,e){n.exports=function(n,e,t){for(var o=-1,a=null==n?0:n.length;++o<a;)if(t(e,n[o]))return!0;return!1}},function(n,e,t){var o=t(84),a=t(241),s=t(43),r=o&&1/s(new o([,-0]))[1]==1/0?function(n){return new o(n)}:a;n.exports=r},function(n,e){n.exports=function(){}},function(n,e,t){var o=t(83),a=t(12);n.exports=function(n){return a(n)&&o(n)}},function(n,e,t){},function(n,e,t){},function(n,e,t){"use strict";t(89)},function(n,e,t){"use strict";t(90)},function(n,e,t){},function(n,e,t){},function(n,e,t){"use strict";t(91)},function(n,e,t){"use strict";t(92)},function(n,e,t){var o=t(19),a=t(3),s=t(252);o({global:!0},{Reflect:{}}),s(a.Reflect,"Reflect",!0)},function(n,e,t){var o=t(13).f,a=t(7),s=t(21)("toStringTag");n.exports=function(n,e,t){n&&!t&&(n=n.prototype),n&&!a(n,s)&&o(n,s,{configurable:!0,value:e})}},function(n,e){},function(n,e){function t(n,e){for(var t=0,o=n.length-1;o>=0;o--){var a=n[o];"."===a?n.splice(o,1):".."===a?(n.splice(o,1),t++):t&&(n.splice(o,1),t--)}if(e)for(;t--;t)n.unshift("..");return n}function o(n,e){if(n.filter)return n.filter(e);for(var t=[],o=0;o<n.length;o++)e(n[o],o,n)&&t.push(n[o]);return t}e.resolve=function(){for(var n="",e=!1,a=arguments.length-1;a>=-1&&!e;a--){var s=a>=0?arguments[a]:process.cwd();if("string"!=typeof s)throw new TypeError("Arguments to path.resolve must be strings");s&&(n=s+"/"+n,e="/"===s.charAt(0))}return(e?"/":"")+(n=t(o(n.split("/"),(function(n){return!!n})),!e).join("/"))||"."},e.normalize=function(n){var s=e.isAbsolute(n),r="/"===a(n,-1);return(n=t(o(n.split("/"),(function(n){return!!n})),!s).join("/"))||s||(n="."),n&&r&&(n+="/"),(s?"/":"")+n},e.isAbsolute=function(n){return"/"===n.charAt(0)},e.join=function(){var n=Array.prototype.slice.call(arguments,0);return e.normalize(o(n,(function(n,e){if("string"!=typeof n)throw new TypeError("Arguments to path.join must be strings");return n})).join("/"))},e.relative=function(n,t){function o(n){for(var e=0;e<n.length&&""===n[e];e++);for(var t=n.length-1;t>=0&&""===n[t];t--);return e>t?[]:n.slice(e,t-e+1)}n=e.resolve(n).substr(1),t=e.resolve(t).substr(1);for(var a=o(n.split("/")),s=o(t.split("/")),r=Math.min(a.length,s.length),i=r,l=0;l<r;l++)if(a[l]!==s[l]){i=l;break}var c=[];for(l=i;l<a.length;l++)c.push("..");return(c=c.concat(s.slice(i))).join("/")},e.sep="/",e.delimiter=":",e.dirname=function(n){if("string"!=typeof n&&(n+=""),0===n.length)return".";for(var e=n.charCodeAt(0),t=47===e,o=-1,a=!0,s=n.length-1;s>=1;--s)if(47===(e=n.charCodeAt(s))){if(!a){o=s;break}}else a=!1;return-1===o?t?"/":".":t&&1===o?"/":n.slice(0,o)},e.basename=function(n,e){var t=function(n){"string"!=typeof n&&(n+="");var e,t=0,o=-1,a=!0;for(e=n.length-1;e>=0;--e)if(47===n.charCodeAt(e)){if(!a){t=e+1;break}}else-1===o&&(a=!1,o=e+1);return-1===o?"":n.slice(t,o)}(n);return e&&t.substr(-1*e.length)===e&&(t=t.substr(0,t.length-e.length)),t},e.extname=function(n){"string"!=typeof n&&(n+="");for(var e=-1,t=0,o=-1,a=!0,s=0,r=n.length-1;r>=0;--r){var i=n.charCodeAt(r);if(47!==i)-1===o&&(a=!1,o=r+1),46===i?-1===e?e=r:1!==s&&(s=1):-1!==e&&(s=-1);else if(!a){t=r+1;break}}return-1===e||-1===o||0===s||1===s&&e===o-1&&e===t+1?"":n.slice(e,o)};var a="b"==="ab".substr(-1)?function(n,e,t){return n.substr(e,t)}:function(n,e,t){return e<0&&(e=n.length+e),n.substr(e,t)}},function(n,e,t){"use strict";var o=/[|\\{}()[\]^$+*?.]/g,a=Object.prototype.hasOwnProperty,s=function(n,e){return a.apply(n,[e])};e.escapeRegExpChars=function(n){return n?String(n).replace(o,"\\$&"):""};var r={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&#34;","'":"&#39;"},i=/[&<>'"]/g;function l(n){return r[n]||n}e.escapeXML=function(n){return null==n?"":String(n).replace(i,l)},e.escapeXML.toString=function(){return Function.prototype.toString.call(this)+';\nvar _ENCODE_HTML_RULES = {\n      "&": "&amp;"\n    , "<": "&lt;"\n    , ">": "&gt;"\n    , \'"\': "&#34;"\n    , "\'": "&#39;"\n    }\n  , _MATCH_HTML = /[&<>\'"]/g;\nfunction encode_char(c) {\n  return _ENCODE_HTML_RULES[c] || c;\n};\n'},e.shallowCopy=function(n,e){if(e=e||{},null!=n)for(var t in e)s(e,t)&&"__proto__"!==t&&"constructor"!==t&&(n[t]=e[t]);return n},e.shallowCopyFromList=function(n,e,t){if(t=t||[],e=e||{},null!=n)for(var o=0;o<t.length;o++){var a=t[o];if(void 0!==e[a]){if(!s(e,a))continue;if("__proto__"===a||"constructor"===a)continue;n[a]=e[a]}}return n},e.cache={_data:{},set:function(n,e){this._data[n]=e},get:function(n){return this._data[n]},remove:function(n){delete this._data[n]},reset:function(){this._data={}}},e.hyphenToCamel=function(n){return n.replace(/-[a-z]/g,(function(n){return n[1].toUpperCase()}))},e.createNullProtoObjWherePossible="function"==typeof Object.create?function(){return Object.create(null)}:{__proto__:null}instanceof Object?function(){return{}}:function(){return{__proto__:null}}},function(n){n.exports=JSON.parse('{"name":"ejs","description":"Embedded JavaScript templates","keywords":["template","engine","ejs"],"version":"3.1.8","author":"Matthew Eernisse <mde@fleegix.org> (http://fleegix.org)","license":"Apache-2.0","bin":{"ejs":"./bin/cli.js"},"main":"./lib/ejs.js","jsdelivr":"ejs.min.js","unpkg":"ejs.min.js","repository":{"type":"git","url":"git://github.com/mde/ejs.git"},"bugs":"https://github.com/mde/ejs/issues","homepage":"https://github.com/mde/ejs","dependencies":{"jake":"^10.8.5"},"devDependencies":{"browserify":"^16.5.1","eslint":"^6.8.0","git-directory-deploy":"^1.5.1","jsdoc":"^3.6.7","lru-cache":"^4.0.1","mocha":"^7.1.1","uglify-js":"^3.3.16"},"engines":{"node":">=0.10.0"},"scripts":{"test":"mocha"}}')},function(n,e,t){"use strict";t(93)},function(n,e,t){"use strict";t(94)},function(n,e,t){"use strict";t.r(e);
/*!
 * Vue.js v2.7.14
 * (c) 2014-2022 Evan You
 * Released under the MIT License.
 */
var o=Object.freeze({}),a=Array.isArray;function s(n){return null==n}function r(n){return null!=n}function i(n){return!0===n}function l(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function c(n){return"function"==typeof n}function p(n){return null!==n&&"object"==typeof n}var d=Object.prototype.toString;function m(n){return"[object Object]"===d.call(n)}function u(n){return"[object RegExp]"===d.call(n)}function h(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function g(n){return r(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function b(n){return null==n?"":Array.isArray(n)||m(n)&&n.toString===d?JSON.stringify(n,null,2):String(n)}function f(n){var e=parseFloat(n);return isNaN(e)?n:e}function y(n,e){for(var t=Object.create(null),o=n.split(","),a=0;a<o.length;a++)t[o[a]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}y("slot,component",!0);var _=y("key,ref,slot,slot-scope,is");function v(n,e){var t=n.length;if(t){if(e===n[t-1])return void(n.length=t-1);var o=n.indexOf(e);if(o>-1)return n.splice(o,1)}}var k=Object.prototype.hasOwnProperty;function x(n,e){return k.call(n,e)}function w(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var E=/-(\w)/g,A=w((function(n){return n.replace(E,(function(n,e){return e?e.toUpperCase():""}))})),z=w((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),B=/\B([A-Z])/g,T=w((function(n){return n.replace(B,"-$1").toLowerCase()}));var S=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var o=arguments.length;return o?o>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function $(n,e){e=e||0;for(var t=n.length-e,o=new Array(t);t--;)o[t]=n[t+e];return o}function j(n,e){for(var t in e)n[t]=e[t];return n}function I(n){for(var e={},t=0;t<n.length;t++)n[t]&&j(e,n[t]);return e}function P(n,e,t){}var D=function(n,e,t){return!1},C=function(n){return n};function R(n,e){if(n===e)return!0;var t=p(n),o=p(e);if(!t||!o)return!t&&!o&&String(n)===String(e);try{var a=Array.isArray(n),s=Array.isArray(e);if(a&&s)return n.length===e.length&&n.every((function(n,t){return R(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(a||s)return!1;var r=Object.keys(n),i=Object.keys(e);return r.length===i.length&&r.every((function(t){return R(n[t],e[t])}))}catch(n){return!1}}function O(n,e){for(var t=0;t<n.length;t++)if(R(n[t],e))return t;return-1}function L(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}function M(n,e){return n===e?0===n&&1/n!=1/e:n==n||e==e}var q=["component","directive","filter"],N=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],F={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:D,isReservedAttr:D,isUnknownElement:D,getTagNamespace:P,parsePlatformTagName:C,mustUseProp:D,async:!0,_lifecycleHooks:N},U=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function G(n){var e=(n+"").charCodeAt(0);return 36===e||95===e}function H(n,e,t,o){Object.defineProperty(n,e,{value:t,enumerable:!!o,writable:!0,configurable:!0})}var V=new RegExp("[^".concat(U.source,".$_\\d]"));var Z="__proto__"in{},K="undefined"!=typeof window,W=K&&window.navigator.userAgent.toLowerCase(),Y=W&&/msie|trident/.test(W),X=W&&W.indexOf("msie 9.0")>0,Q=W&&W.indexOf("edge/")>0;W&&W.indexOf("android");var J=W&&/iphone|ipad|ipod|ios/.test(W);W&&/chrome\/\d+/.test(W),W&&/phantomjs/.test(W);var nn,en=W&&W.match(/firefox\/(\d+)/),tn={}.watch,on=!1;if(K)try{var an={};Object.defineProperty(an,"passive",{get:function(){on=!0}}),window.addEventListener("test-passive",null,an)}catch(Bl){}var sn=function(){return void 0===nn&&(nn=!K&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),nn},rn=K&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function ln(n){return"function"==typeof n&&/native code/.test(n.toString())}var cn,pn="undefined"!=typeof Symbol&&ln(Symbol)&&"undefined"!=typeof Reflect&&ln(Reflect.ownKeys);cn="undefined"!=typeof Set&&ln(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var dn=null;function mn(n){void 0===n&&(n=null),n||dn&&dn._scope.off(),dn=n,n&&n._scope.on()}var un=function(){function n(n,e,t,o,a,s,r,i){this.tag=n,this.data=e,this.children=t,this.text=o,this.elm=a,this.ns=void 0,this.context=s,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=r,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=i,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(n.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),n}(),hn=function(n){void 0===n&&(n="");var e=new un;return e.text=n,e.isComment=!0,e};function gn(n){return new un(void 0,void 0,void 0,String(n))}function bn(n){var e=new un(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}var fn=0,yn=[],_n=function(){function n(){this._pending=!1,this.id=fn++,this.subs=[]}return n.prototype.addSub=function(n){this.subs.push(n)},n.prototype.removeSub=function(n){this.subs[this.subs.indexOf(n)]=null,this._pending||(this._pending=!0,yn.push(this))},n.prototype.depend=function(e){n.target&&n.target.addDep(this)},n.prototype.notify=function(n){var e=this.subs.filter((function(n){return n}));for(var t=0,o=e.length;t<o;t++){0,e[t].update()}},n}();_n.target=null;var vn=[];function kn(n){vn.push(n),_n.target=n}function xn(){vn.pop(),_n.target=vn[vn.length-1]}var wn=Array.prototype,En=Object.create(wn);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=wn[n];H(En,n,(function(){for(var t=[],o=0;o<arguments.length;o++)t[o]=arguments[o];var a,s=e.apply(this,t),r=this.__ob__;switch(n){case"push":case"unshift":a=t;break;case"splice":a=t.slice(2)}return a&&r.observeArray(a),r.dep.notify(),s}))}));var An=Object.getOwnPropertyNames(En),zn={},Bn=!0;function Tn(n){Bn=n}var Sn={notify:P,depend:P,addSub:P,removeSub:P},$n=function(){function n(n,e,t){if(void 0===e&&(e=!1),void 0===t&&(t=!1),this.value=n,this.shallow=e,this.mock=t,this.dep=t?Sn:new _n,this.vmCount=0,H(n,"__ob__",this),a(n)){if(!t)if(Z)n.__proto__=En;else for(var o=0,s=An.length;o<s;o++){H(n,i=An[o],En[i])}e||this.observeArray(n)}else{var r=Object.keys(n);for(o=0;o<r.length;o++){var i;In(n,i=r[o],zn,void 0,e,t)}}}return n.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)jn(n[e],!1,this.mock)},n}();function jn(n,e,t){return n&&x(n,"__ob__")&&n.__ob__ instanceof $n?n.__ob__:!Bn||!t&&sn()||!a(n)&&!m(n)||!Object.isExtensible(n)||n.__v_skip||Mn(n)||n instanceof un?void 0:new $n(n,e,t)}function In(n,e,t,o,s,r){var i=new _n,l=Object.getOwnPropertyDescriptor(n,e);if(!l||!1!==l.configurable){var c=l&&l.get,p=l&&l.set;c&&!p||t!==zn&&2!==arguments.length||(t=n[e]);var d=!s&&jn(t,!1,r);return Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=c?c.call(n):t;return _n.target&&(i.depend(),d&&(d.dep.depend(),a(e)&&Cn(e))),Mn(e)&&!s?e.value:e},set:function(e){var o=c?c.call(n):t;if(M(o,e)){if(p)p.call(n,e);else{if(c)return;if(!s&&Mn(o)&&!Mn(e))return void(o.value=e);t=e}d=!s&&jn(e,!1,r),i.notify()}}}),i}}function Pn(n,e,t){if(!Ln(n)){var o=n.__ob__;return a(n)&&h(e)?(n.length=Math.max(n.length,e),n.splice(e,1,t),o&&!o.shallow&&o.mock&&jn(t,!1,!0),t):e in n&&!(e in Object.prototype)?(n[e]=t,t):n._isVue||o&&o.vmCount?t:o?(In(o.value,e,t,void 0,o.shallow,o.mock),o.dep.notify(),t):(n[e]=t,t)}}function Dn(n,e){if(a(n)&&h(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||Ln(n)||x(n,e)&&(delete n[e],t&&t.dep.notify())}}function Cn(n){for(var e=void 0,t=0,o=n.length;t<o;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),a(e)&&Cn(e)}function Rn(n){return On(n,!0),H(n,"__v_isShallow",!0),n}function On(n,e){if(!Ln(n)){jn(n,e,sn());0}}function Ln(n){return!(!n||!n.__v_isReadonly)}function Mn(n){return!(!n||!0!==n.__v_isRef)}function qn(n,e,t){Object.defineProperty(n,t,{enumerable:!0,configurable:!0,get:function(){var n=e[t];if(Mn(n))return n.value;var o=n&&n.__ob__;return o&&o.dep.depend(),n},set:function(n){var o=e[t];Mn(o)&&!Mn(n)?o.value=n:e[t]=n}})}"".concat("watcher"," callback"),"".concat("watcher"," getter"),"".concat("watcher"," cleanup");var Nn;var Fn=function(){function n(n){void 0===n&&(n=!1),this.detached=n,this.active=!0,this.effects=[],this.cleanups=[],this.parent=Nn,!n&&Nn&&(this.index=(Nn.scopes||(Nn.scopes=[])).push(this)-1)}return n.prototype.run=function(n){if(this.active){var e=Nn;try{return Nn=this,n()}finally{Nn=e}}else 0},n.prototype.on=function(){Nn=this},n.prototype.off=function(){Nn=this.parent},n.prototype.stop=function(n){if(this.active){var e=void 0,t=void 0;for(e=0,t=this.effects.length;e<t;e++)this.effects[e].teardown();for(e=0,t=this.cleanups.length;e<t;e++)this.cleanups[e]();if(this.scopes)for(e=0,t=this.scopes.length;e<t;e++)this.scopes[e].stop(!0);if(!this.detached&&this.parent&&!n){var o=this.parent.scopes.pop();o&&o!==this&&(this.parent.scopes[this.index]=o,o.index=this.index)}this.parent=void 0,this.active=!1}},n}();function Un(n){var e=n._provided,t=n.$parent&&n.$parent._provided;return t===e?n._provided=Object.create(t):e}var Gn=w((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),o="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=o?n.slice(1):n,once:t,capture:o,passive:e}}));function Hn(n,e){function t(){var n=t.fns;if(!a(n))return Be(n,null,arguments,e,"v-on handler");for(var o=n.slice(),s=0;s<o.length;s++)Be(o[s],null,arguments,e,"v-on handler")}return t.fns=n,t}function Vn(n,e,t,o,a,r){var l,c,p,d;for(l in n)c=n[l],p=e[l],d=Gn(l),s(c)||(s(p)?(s(c.fns)&&(c=n[l]=Hn(c,r)),i(d.once)&&(c=n[l]=a(d.name,c,d.capture)),t(d.name,c,d.capture,d.passive,d.params)):c!==p&&(p.fns=c,n[l]=p));for(l in e)s(n[l])&&o((d=Gn(l)).name,e[l],d.capture)}function Zn(n,e,t){var o;n instanceof un&&(n=n.data.hook||(n.data.hook={}));var a=n[e];function l(){t.apply(this,arguments),v(o.fns,l)}s(a)?o=Hn([l]):r(a.fns)&&i(a.merged)?(o=a).fns.push(l):o=Hn([a,l]),o.merged=!0,n[e]=o}function Kn(n,e,t,o,a){if(r(e)){if(x(e,t))return n[t]=e[t],a||delete e[t],!0;if(x(e,o))return n[t]=e[o],a||delete e[o],!0}return!1}function Wn(n){return l(n)?[gn(n)]:a(n)?function n(e,t){var o,c,p,d,m=[];for(o=0;o<e.length;o++)s(c=e[o])||"boolean"==typeof c||(p=m.length-1,d=m[p],a(c)?c.length>0&&(Yn((c=n(c,"".concat(t||"","_").concat(o)))[0])&&Yn(d)&&(m[p]=gn(d.text+c[0].text),c.shift()),m.push.apply(m,c)):l(c)?Yn(d)?m[p]=gn(d.text+c):""!==c&&m.push(gn(c)):Yn(c)&&Yn(d)?m[p]=gn(d.text+c.text):(i(e._isVList)&&r(c.tag)&&s(c.key)&&r(t)&&(c.key="__vlist".concat(t,"_").concat(o,"__")),m.push(c)));return m}(n):void 0}function Yn(n){return r(n)&&r(n.text)&&function(n){return!1===n}(n.isComment)}function Xn(n,e){var t,o,s,i,l=null;if(a(n)||"string"==typeof n)for(l=new Array(n.length),t=0,o=n.length;t<o;t++)l[t]=e(n[t],t);else if("number"==typeof n)for(l=new Array(n),t=0;t<n;t++)l[t]=e(t+1,t);else if(p(n))if(pn&&n[Symbol.iterator]){l=[];for(var c=n[Symbol.iterator](),d=c.next();!d.done;)l.push(e(d.value,l.length)),d=c.next()}else for(s=Object.keys(n),l=new Array(s.length),t=0,o=s.length;t<o;t++)i=s[t],l[t]=e(n[i],i,t);return r(l)||(l=[]),l._isVList=!0,l}function Qn(n,e,t,o){var a,s=this.$scopedSlots[n];s?(t=t||{},o&&(t=j(j({},o),t)),a=s(t)||(c(e)?e():e)):a=this.$slots[n]||(c(e)?e():e);var r=t&&t.slot;return r?this.$createElement("template",{slot:r},a):a}function Jn(n){return $t(this.$options,"filters",n,!0)||C}function ne(n,e){return a(n)?-1===n.indexOf(e):n!==e}function ee(n,e,t,o,a){var s=F.keyCodes[e]||t;return a&&o&&!F.keyCodes[e]?ne(a,o):s?ne(s,n):o?T(o)!==e:void 0===n}function te(n,e,t,o,s){if(t)if(p(t)){a(t)&&(t=I(t));var r=void 0,i=function(a){if("class"===a||"style"===a||_(a))r=n;else{var i=n.attrs&&n.attrs.type;r=o||F.mustUseProp(e,i,a)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var l=A(a),c=T(a);l in r||c in r||(r[a]=t[a],s&&((n.on||(n.on={}))["update:".concat(a)]=function(n){t[a]=n}))};for(var l in t)i(l)}else;return n}function oe(n,e){var t=this._staticTrees||(this._staticTrees=[]),o=t[n];return o&&!e||se(o=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,this._c,this),"__static__".concat(n),!1),o}function ae(n,e,t){return se(n,"__once__".concat(e).concat(t?"_".concat(t):""),!0),n}function se(n,e,t){if(a(n))for(var o=0;o<n.length;o++)n[o]&&"string"!=typeof n[o]&&re(n[o],"".concat(e,"_").concat(o),t);else re(n,e,t)}function re(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function ie(n,e){if(e)if(m(e)){var t=n.on=n.on?j({},n.on):{};for(var o in e){var a=t[o],s=e[o];t[o]=a?[].concat(a,s):s}}else;return n}function le(n,e,t,o){e=e||{$stable:!t};for(var s=0;s<n.length;s++){var r=n[s];a(r)?le(r,e,t):r&&(r.proxy&&(r.fn.proxy=!0),e[r.key]=r.fn)}return o&&(e.$key=o),e}function ce(n,e){for(var t=0;t<e.length;t+=2){var o=e[t];"string"==typeof o&&o&&(n[e[t]]=e[t+1])}return n}function pe(n,e){return"string"==typeof n?e+n:n}function de(n){n._o=ae,n._n=f,n._s=b,n._l=Xn,n._t=Qn,n._q=R,n._i=O,n._m=oe,n._f=Jn,n._k=ee,n._b=te,n._v=gn,n._e=hn,n._u=le,n._g=ie,n._d=ce,n._p=pe}function me(n,e){if(!n||!n.length)return{};for(var t={},o=0,a=n.length;o<a;o++){var s=n[o],r=s.data;if(r&&r.attrs&&r.attrs.slot&&delete r.attrs.slot,s.context!==e&&s.fnContext!==e||!r||null==r.slot)(t.default||(t.default=[])).push(s);else{var i=r.slot,l=t[i]||(t[i]=[]);"template"===s.tag?l.push.apply(l,s.children||[]):l.push(s)}}for(var c in t)t[c].every(ue)&&delete t[c];return t}function ue(n){return n.isComment&&!n.asyncFactory||" "===n.text}function he(n){return n.isComment&&n.asyncFactory}function ge(n,e,t,a){var s,r=Object.keys(t).length>0,i=e?!!e.$stable:!r,l=e&&e.$key;if(e){if(e._normalized)return e._normalized;if(i&&a&&a!==o&&l===a.$key&&!r&&!a.$hasNormal)return a;for(var c in s={},e)e[c]&&"$"!==c[0]&&(s[c]=be(n,t,c,e[c]))}else s={};for(var p in t)p in s||(s[p]=fe(t,p));return e&&Object.isExtensible(e)&&(e._normalized=s),H(s,"$stable",i),H(s,"$key",l),H(s,"$hasNormal",r),s}function be(n,e,t,o){var s=function(){var e=dn;mn(n);var t=arguments.length?o.apply(null,arguments):o({}),s=(t=t&&"object"==typeof t&&!a(t)?[t]:Wn(t))&&t[0];return mn(e),t&&(!s||1===t.length&&s.isComment&&!he(s))?void 0:t};return o.proxy&&Object.defineProperty(e,t,{get:s,enumerable:!0,configurable:!0}),s}function fe(n,e){return function(){return n[e]}}function ye(n){return{get attrs(){if(!n._attrsProxy){var e=n._attrsProxy={};H(e,"_v_attr_proxy",!0),_e(e,n.$attrs,o,n,"$attrs")}return n._attrsProxy},get listeners(){n._listenersProxy||_e(n._listenersProxy={},n.$listeners,o,n,"$listeners");return n._listenersProxy},get slots(){return function(n){n._slotsProxy||ke(n._slotsProxy={},n.$scopedSlots);return n._slotsProxy}(n)},emit:S(n.$emit,n),expose:function(e){e&&Object.keys(e).forEach((function(t){return qn(n,e,t)}))}}}function _e(n,e,t,o,a){var s=!1;for(var r in e)r in n?e[r]!==t[r]&&(s=!0):(s=!0,ve(n,r,o,a));for(var r in n)r in e||(s=!0,delete n[r]);return s}function ve(n,e,t,o){Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){return t[o][e]}})}function ke(n,e){for(var t in e)n[t]=e[t];for(var t in n)t in e||delete n[t]}var xe=null;function we(n,e){return(n.__esModule||pn&&"Module"===n[Symbol.toStringTag])&&(n=n.default),p(n)?e.extend(n):n}function Ee(n){if(a(n))for(var e=0;e<n.length;e++){var t=n[e];if(r(t)&&(r(t.componentOptions)||he(t)))return t}}function Ae(n,e,t,o,d,m){return(a(t)||l(t))&&(d=o,o=t,t=void 0),i(m)&&(d=2),function(n,e,t,o,l){if(r(t)&&r(t.__ob__))return hn();r(t)&&r(t.is)&&(e=t.is);if(!e)return hn();0;a(o)&&c(o[0])&&((t=t||{}).scopedSlots={default:o[0]},o.length=0);2===l?o=Wn(o):1===l&&(o=function(n){for(var e=0;e<n.length;e++)if(a(n[e]))return Array.prototype.concat.apply([],n);return n}(o));var d,m;if("string"==typeof e){var u=void 0;m=n.$vnode&&n.$vnode.ns||F.getTagNamespace(e),d=F.isReservedTag(e)?new un(F.parsePlatformTagName(e),t,o,void 0,void 0,n):t&&t.pre||!r(u=$t(n.$options,"components",e))?new un(e,t,o,void 0,void 0,n):vt(u,t,n,o,e)}else d=vt(e,t,n,o);return a(d)?d:r(d)?(r(m)&&function n(e,t,o){e.ns=t,"foreignObject"===e.tag&&(t=void 0,o=!0);if(r(e.children))for(var a=0,l=e.children.length;a<l;a++){var c=e.children[a];r(c.tag)&&(s(c.ns)||i(o)&&"svg"!==c.tag)&&n(c,t,o)}}(d,m),r(t)&&function(n){p(n.style)&&Fe(n.style);p(n.class)&&Fe(n.class)}(t),d):hn()}(n,e,t,o,d)}function ze(n,e,t){kn();try{if(e)for(var o=e;o=o.$parent;){var a=o.$options.errorCaptured;if(a)for(var s=0;s<a.length;s++)try{if(!1===a[s].call(o,n,e,t))return}catch(n){Te(n,o,"errorCaptured hook")}}Te(n,e,t)}finally{xn()}}function Be(n,e,t,o,a){var s;try{(s=t?n.apply(e,t):n.call(e))&&!s._isVue&&g(s)&&!s._handled&&(s.catch((function(n){return ze(n,o,a+" (Promise/async)")})),s._handled=!0)}catch(n){ze(n,o,a)}return s}function Te(n,e,t){if(F.errorHandler)try{return F.errorHandler.call(null,n,e,t)}catch(e){e!==n&&Se(e,null,"config.errorHandler")}Se(n,e,t)}function Se(n,e,t){if(!K||"undefined"==typeof console)throw n;console.error(n)}var $e,je=!1,Ie=[],Pe=!1;function De(){Pe=!1;var n=Ie.slice(0);Ie.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&ln(Promise)){var Ce=Promise.resolve();$e=function(){Ce.then(De),J&&setTimeout(P)},je=!0}else if(Y||"undefined"==typeof MutationObserver||!ln(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())$e="undefined"!=typeof setImmediate&&ln(setImmediate)?function(){setImmediate(De)}:function(){setTimeout(De,0)};else{var Re=1,Oe=new MutationObserver(De),Le=document.createTextNode(String(Re));Oe.observe(Le,{characterData:!0}),$e=function(){Re=(Re+1)%2,Le.data=String(Re)},je=!0}function Me(n,e){var t;if(Ie.push((function(){if(n)try{n.call(e)}catch(n){ze(n,e,"nextTick")}else t&&t(e)})),Pe||(Pe=!0,$e()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}function qe(n){return function(e,t){if(void 0===t&&(t=dn),t)return function(n,e,t){var o=n.$options;o[e]=zt(o[e],t)}(t,n,e)}}qe("beforeMount"),qe("mounted"),qe("beforeUpdate"),qe("updated"),qe("beforeDestroy"),qe("destroyed"),qe("activated"),qe("deactivated"),qe("serverPrefetch"),qe("renderTracked"),qe("renderTriggered"),qe("errorCaptured");var Ne=new cn;function Fe(n){return function n(e,t){var o,s,r=a(e);if(!r&&!p(e)||e.__v_skip||Object.isFrozen(e)||e instanceof un)return;if(e.__ob__){var i=e.__ob__.dep.id;if(t.has(i))return;t.add(i)}if(r)for(o=e.length;o--;)n(e[o],t);else if(Mn(e))n(e.value,t);else for(s=Object.keys(e),o=s.length;o--;)n(e[s[o]],t)}(n,Ne),Ne.clear(),n}var Ue,Ge=0,He=function(){function n(n,e,t,o,a){var s,r;s=this,void 0===(r=Nn&&!Nn._vm?Nn:n?n._scope:void 0)&&(r=Nn),r&&r.active&&r.effects.push(s),(this.vm=n)&&a&&(n._watcher=this),o?(this.deep=!!o.deep,this.user=!!o.user,this.lazy=!!o.lazy,this.sync=!!o.sync,this.before=o.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++Ge,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new cn,this.newDepIds=new cn,this.expression="",c(e)?this.getter=e:(this.getter=function(n){if(!V.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=P)),this.value=this.lazy?void 0:this.get()}return n.prototype.get=function(){var n;kn(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;ze(n,e,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&Fe(n),xn(),this.cleanupDeps()}return n},n.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},n.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},n.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():mt(this)},n.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||p(n)||this.deep){var e=this.value;if(this.value=n,this.user){var t='callback for watcher "'.concat(this.expression,'"');Be(this.cb,this.vm,[n,e],this.vm,t)}else this.cb.call(this.vm,n,e)}}},n.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},n.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},n.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&v(this.vm._scope.effects,this),this.active){for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},n}();function Ve(n,e){Ue.$on(n,e)}function Ze(n,e){Ue.$off(n,e)}function Ke(n,e){var t=Ue;return function o(){var a=e.apply(null,arguments);null!==a&&t.$off(n,o)}}function We(n,e,t){Ue=n,Vn(e,t||{},Ve,Ze,Ke,n),Ue=void 0}var Ye=null;function Xe(n){var e=Ye;return Ye=n,function(){Ye=e}}function Qe(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function Je(n,e){if(e){if(n._directInactive=!1,Qe(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)Je(n.$children[t]);nt(n,"activated")}}function nt(n,e,t,o){void 0===o&&(o=!0),kn();var a=dn;o&&mn(n);var s=n.$options[e],r="".concat(e," hook");if(s)for(var i=0,l=s.length;i<l;i++)Be(s[i],n,t||null,n,r);n._hasHookEvent&&n.$emit("hook:"+e),o&&mn(a),xn()}var et=[],tt=[],ot={},at=!1,st=!1,rt=0;var it=0,lt=Date.now;if(K&&!Y){var ct=window.performance;ct&&"function"==typeof ct.now&&lt()>document.createEvent("Event").timeStamp&&(lt=function(){return ct.now()})}var pt=function(n,e){if(n.post){if(!e.post)return 1}else if(e.post)return-1;return n.id-e.id};function dt(){var n,e;for(it=lt(),st=!0,et.sort(pt),rt=0;rt<et.length;rt++)(n=et[rt]).before&&n.before(),e=n.id,ot[e]=null,n.run();var t=tt.slice(),o=et.slice();rt=et.length=tt.length=0,ot={},at=st=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,Je(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],o=t.vm;o&&o._watcher===t&&o._isMounted&&!o._isDestroyed&&nt(o,"updated")}}(o),function(){for(var n=0;n<yn.length;n++){var e=yn[n];e.subs=e.subs.filter((function(n){return n})),e._pending=!1}yn.length=0}(),rn&&F.devtools&&rn.emit("flush")}function mt(n){var e=n.id;if(null==ot[e]&&(n!==_n.target||!n.noRecurse)){if(ot[e]=!0,st){for(var t=et.length-1;t>rt&&et[t].id>n.id;)t--;et.splice(t+1,0,n)}else et.push(n);at||(at=!0,Me(dt))}}function ut(n,e){if(n){for(var t=Object.create(null),o=pn?Reflect.ownKeys(n):Object.keys(n),a=0;a<o.length;a++){var s=o[a];if("__ob__"!==s){var r=n[s].from;if(r in e._provided)t[s]=e._provided[r];else if("default"in n[s]){var i=n[s].default;t[s]=c(i)?i.call(e):i}else 0}}return t}}function ht(n,e,t,s,r){var l,c=this,p=r.options;x(s,"_uid")?(l=Object.create(s))._original=s:(l=s,s=s._original);var d=i(p._compiled),m=!d;this.data=n,this.props=e,this.children=t,this.parent=s,this.listeners=n.on||o,this.injections=ut(p.inject,s),this.slots=function(){return c.$slots||ge(s,n.scopedSlots,c.$slots=me(t,s)),c.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return ge(s,n.scopedSlots,this.slots())}}),d&&(this.$options=p,this.$slots=this.slots(),this.$scopedSlots=ge(s,n.scopedSlots,this.$slots)),p._scopeId?this._c=function(n,e,t,o){var r=Ae(l,n,e,t,o,m);return r&&!a(r)&&(r.fnScopeId=p._scopeId,r.fnContext=s),r}:this._c=function(n,e,t,o){return Ae(l,n,e,t,o,m)}}function gt(n,e,t,o,a){var s=bn(n);return s.fnContext=t,s.fnOptions=o,e.slot&&((s.data||(s.data={})).slot=e.slot),s}function bt(n,e){for(var t in e)n[A(t)]=e[t]}function ft(n){return n.name||n.__name||n._componentTag}de(ht.prototype);var yt={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;yt.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},o=n.data.inlineTemplate;r(o)&&(t.render=o.render,t.staticRenderFns=o.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,Ye)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,a,s){var r=a.data.scopedSlots,i=n.$scopedSlots,l=!!(r&&!r.$stable||i!==o&&!i.$stable||r&&n.$scopedSlots.$key!==r.$key||!r&&n.$scopedSlots.$key),c=!!(s||n.$options._renderChildren||l),p=n.$vnode;n.$options._parentVnode=a,n.$vnode=a,n._vnode&&(n._vnode.parent=a),n.$options._renderChildren=s;var d=a.data.attrs||o;n._attrsProxy&&_e(n._attrsProxy,d,p.data&&p.data.attrs||o,n,"$attrs")&&(c=!0),n.$attrs=d,t=t||o;var m=n.$options._parentListeners;if(n._listenersProxy&&_e(n._listenersProxy,t,m||o,n,"$listeners"),n.$listeners=n.$options._parentListeners=t,We(n,t,m),e&&n.$options.props){Tn(!1);for(var u=n._props,h=n.$options._propKeys||[],g=0;g<h.length;g++){var b=h[g],f=n.$options.props;u[b]=jt(b,f,e,n)}Tn(!0),n.$options.propsData=e}c&&(n.$slots=me(s,a.context),n.$forceUpdate())}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,o=n.componentInstance;o._isMounted||(o._isMounted=!0,nt(o,"mounted")),n.data.keepAlive&&(t._isMounted?((e=o)._inactive=!1,tt.push(e)):Je(o,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(!(t&&(e._directInactive=!0,Qe(e))||e._inactive)){e._inactive=!0;for(var o=0;o<e.$children.length;o++)n(e.$children[o]);nt(e,"deactivated")}}(e,!0):e.$destroy())}},_t=Object.keys(yt);function vt(n,e,t,l,c){if(!s(n)){var d=t.$options._base;if(p(n)&&(n=d.extend(n)),"function"==typeof n){var m;if(s(n.cid)&&void 0===(n=function(n,e){if(i(n.error)&&r(n.errorComp))return n.errorComp;if(r(n.resolved))return n.resolved;var t=xe;if(t&&r(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t),i(n.loading)&&r(n.loadingComp))return n.loadingComp;if(t&&!r(n.owners)){var o=n.owners=[t],a=!0,l=null,c=null;t.$on("hook:destroyed",(function(){return v(o,t)}));var d=function(n){for(var e=0,t=o.length;e<t;e++)o[e].$forceUpdate();n&&(o.length=0,null!==l&&(clearTimeout(l),l=null),null!==c&&(clearTimeout(c),c=null))},m=L((function(t){n.resolved=we(t,e),a?o.length=0:d(!0)})),u=L((function(e){r(n.errorComp)&&(n.error=!0,d(!0))})),h=n(m,u);return p(h)&&(g(h)?s(n.resolved)&&h.then(m,u):g(h.component)&&(h.component.then(m,u),r(h.error)&&(n.errorComp=we(h.error,e)),r(h.loading)&&(n.loadingComp=we(h.loading,e),0===h.delay?n.loading=!0:l=setTimeout((function(){l=null,s(n.resolved)&&s(n.error)&&(n.loading=!0,d(!1))}),h.delay||200)),r(h.timeout)&&(c=setTimeout((function(){c=null,s(n.resolved)&&u(null)}),h.timeout)))),a=!1,n.loading?n.loadingComp:n.resolved}}(m=n,d)))return function(n,e,t,o,a){var s=hn();return s.asyncFactory=n,s.asyncMeta={data:e,context:t,children:o,tag:a},s}(m,e,t,l,c);e=e||{},Ht(n),r(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",o=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var s=e.on||(e.on={}),i=s[o],l=e.model.callback;r(i)?(a(i)?-1===i.indexOf(l):i!==l)&&(s[o]=[l].concat(i)):s[o]=l}(n.options,e);var u=function(n,e,t){var o=e.options.props;if(!s(o)){var a={},i=n.attrs,l=n.props;if(r(i)||r(l))for(var c in o){var p=T(c);Kn(a,l,c,p,!0)||Kn(a,i,c,p,!1)}return a}}(e,n);if(i(n.options.functional))return function(n,e,t,s,i){var l=n.options,c={},p=l.props;if(r(p))for(var d in p)c[d]=jt(d,p,e||o);else r(t.attrs)&&bt(c,t.attrs),r(t.props)&&bt(c,t.props);var m=new ht(t,c,i,s,n),u=l.render.call(null,m._c,m);if(u instanceof un)return gt(u,t,m.parent,l,m);if(a(u)){for(var h=Wn(u)||[],g=new Array(h.length),b=0;b<h.length;b++)g[b]=gt(h[b],t,m.parent,l,m);return g}}(n,u,e,t,l);var h=e.on;if(e.on=e.nativeOn,i(n.options.abstract)){var b=e.slot;e={},b&&(e.slot=b)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<_t.length;t++){var o=_t[t],a=e[o],s=yt[o];a===s||a&&a._merged||(e[o]=a?kt(s,a):s)}}(e);var f=ft(n.options)||c;return new un("vue-component-".concat(n.cid).concat(f?"-".concat(f):""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:u,listeners:h,tag:c,children:l},m)}}}function kt(n,e){var t=function(t,o){n(t,o),e(t,o)};return t._merged=!0,t}var xt=P,wt=F.optionMergeStrategies;function Et(n,e,t){if(void 0===t&&(t=!0),!e)return n;for(var o,a,s,r=pn?Reflect.ownKeys(e):Object.keys(e),i=0;i<r.length;i++)"__ob__"!==(o=r[i])&&(a=n[o],s=e[o],t&&x(n,o)?a!==s&&m(a)&&m(s)&&Et(a,s):Pn(n,o,s));return n}function At(n,e,t){return t?function(){var o=c(e)?e.call(t,t):e,a=c(n)?n.call(t,t):n;return o?Et(o,a):a}:e?n?function(){return Et(c(e)?e.call(this,this):e,c(n)?n.call(this,this):n)}:e:n}function zt(n,e){var t=e?n?n.concat(e):a(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function Bt(n,e,t,o){var a=Object.create(n||null);return e?j(a,e):a}wt.data=function(n,e,t){return t?At(n,e,t):e&&"function"!=typeof e?n:At(n,e)},N.forEach((function(n){wt[n]=zt})),q.forEach((function(n){wt[n+"s"]=Bt})),wt.watch=function(n,e,t,o){if(n===tn&&(n=void 0),e===tn&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var s={};for(var r in j(s,n),e){var i=s[r],l=e[r];i&&!a(i)&&(i=[i]),s[r]=i?i.concat(l):a(l)?l:[l]}return s},wt.props=wt.methods=wt.inject=wt.computed=function(n,e,t,o){if(!n)return e;var a=Object.create(null);return j(a,n),e&&j(a,e),a},wt.provide=function(n,e){return n?function(){var t=Object.create(null);return Et(t,c(n)?n.call(this):n),e&&Et(t,c(e)?e.call(this):e,!1),t}:e};var Tt=function(n,e){return void 0===e?n:e};function St(n,e,t){if(c(e)&&(e=e.options),function(n,e){var t=n.props;if(t){var o,s,r={};if(a(t))for(o=t.length;o--;)"string"==typeof(s=t[o])&&(r[A(s)]={type:null});else if(m(t))for(var i in t)s=t[i],r[A(i)]=m(s)?s:{type:s};else 0;n.props=r}}(e),function(n,e){var t=n.inject;if(t){var o=n.inject={};if(a(t))for(var s=0;s<t.length;s++)o[t[s]]={from:t[s]};else if(m(t))for(var r in t){var i=t[r];o[r]=m(i)?j({from:r},i):{from:i}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var o=e[t];c(o)&&(e[t]={bind:o,update:o})}}(e),!e._base&&(e.extends&&(n=St(n,e.extends,t)),e.mixins))for(var o=0,s=e.mixins.length;o<s;o++)n=St(n,e.mixins[o],t);var r,i={};for(r in n)l(r);for(r in e)x(n,r)||l(r);function l(o){var a=wt[o]||Tt;i[o]=a(n[o],e[o],t,o)}return i}function $t(n,e,t,o){if("string"==typeof t){var a=n[e];if(x(a,t))return a[t];var s=A(t);if(x(a,s))return a[s];var r=z(s);return x(a,r)?a[r]:a[t]||a[s]||a[r]}}function jt(n,e,t,o){var a=e[n],s=!x(t,n),r=t[n],i=Ct(Boolean,a.type);if(i>-1)if(s&&!x(a,"default"))r=!1;else if(""===r||r===T(n)){var l=Ct(String,a.type);(l<0||i<l)&&(r=!0)}if(void 0===r){r=function(n,e,t){if(!x(e,"default"))return;var o=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return c(o)&&"Function"!==Pt(e.type)?o.call(n):o}(o,a,n);var p=Bn;Tn(!0),jn(r),Tn(p)}return r}var It=/^\s*function (\w+)/;function Pt(n){var e=n&&n.toString().match(It);return e?e[1]:""}function Dt(n,e){return Pt(n)===Pt(e)}function Ct(n,e){if(!a(e))return Dt(e,n)?0:-1;for(var t=0,o=e.length;t<o;t++)if(Dt(e[t],n))return t;return-1}var Rt={enumerable:!0,configurable:!0,get:P,set:P};function Ot(n,e,t){Rt.get=function(){return this[e][t]},Rt.set=function(n){this[e][t]=n},Object.defineProperty(n,t,Rt)}function Lt(n){var e=n.$options;if(e.props&&function(n,e){var t=n.$options.propsData||{},o=n._props=Rn({}),a=n.$options._propKeys=[];n.$parent&&Tn(!1);var s=function(s){a.push(s);var r=jt(s,e,t,n);In(o,s,r),s in n||Ot(n,"_props",s)};for(var r in e)s(r);Tn(!0)}(n,e.props),function(n){var e=n.$options,t=e.setup;if(t){var o=n._setupContext=ye(n);mn(n),kn();var a=Be(t,null,[n._props||Rn({}),o],n,"setup");if(xn(),mn(),c(a))e.render=a;else if(p(a))if(n._setupState=a,a.__sfc){var s=n._setupProxy={};for(var r in a)"__sfc"!==r&&qn(s,a,r)}else for(var r in a)G(r)||qn(n,a,r);else 0}}(n),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?P:S(e[t],n)}(n,e.methods),e.data)!function(n){var e=n.$options.data;m(e=n._data=c(e)?function(n,e){kn();try{return n.call(e,e)}catch(n){return ze(n,e,"data()"),{}}finally{xn()}}(e,n):e||{})||(e={});var t=Object.keys(e),o=n.$options.props,a=(n.$options.methods,t.length);for(;a--;){var s=t[a];0,o&&x(o,s)||G(s)||Ot(n,"_data",s)}var r=jn(e);r&&r.vmCount++}(n);else{var t=jn(n._data={});t&&t.vmCount++}e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),o=sn();for(var a in e){var s=e[a],r=c(s)?s:s.get;0,o||(t[a]=new He(n,r||P,P,Mt)),a in n||qt(n,a,s)}}(n,e.computed),e.watch&&e.watch!==tn&&function(n,e){for(var t in e){var o=e[t];if(a(o))for(var s=0;s<o.length;s++)Ut(n,t,o[s]);else Ut(n,t,o)}}(n,e.watch)}var Mt={lazy:!0};function qt(n,e,t){var o=!sn();c(t)?(Rt.get=o?Nt(e):Ft(t),Rt.set=P):(Rt.get=t.get?o&&!1!==t.cache?Nt(e):Ft(t.get):P,Rt.set=t.set||P),Object.defineProperty(n,e,Rt)}function Nt(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),_n.target&&e.depend(),e.value}}function Ft(n){return function(){return n.call(this,this)}}function Ut(n,e,t,o){return m(t)&&(o=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,o)}var Gt=0;function Ht(n){var e=n.options;if(n.super){var t=Ht(n.super);if(t!==n.superOptions){n.superOptions=t;var o=function(n){var e,t=n.options,o=n.sealedOptions;for(var a in t)t[a]!==o[a]&&(e||(e={}),e[a]=t[a]);return e}(n);o&&j(n.extendOptions,o),(e=n.options=St(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function Vt(n){this._init(n)}function Zt(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,o=t.cid,a=n._Ctor||(n._Ctor={});if(a[o])return a[o];var s=ft(n)||ft(t.options);var r=function(n){this._init(n)};return(r.prototype=Object.create(t.prototype)).constructor=r,r.cid=e++,r.options=St(t.options,n),r.super=t,r.options.props&&function(n){var e=n.options.props;for(var t in e)Ot(n.prototype,"_props",t)}(r),r.options.computed&&function(n){var e=n.options.computed;for(var t in e)qt(n.prototype,t,e[t])}(r),r.extend=t.extend,r.mixin=t.mixin,r.use=t.use,q.forEach((function(n){r[n]=t[n]})),s&&(r.options.components[s]=r),r.superOptions=t.options,r.extendOptions=n,r.sealedOptions=j({},r.options),a[o]=r,r}}function Kt(n){return n&&(ft(n.Ctor.options)||n.tag)}function Wt(n,e){return a(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!u(n)&&n.test(e)}function Yt(n,e){var t=n.cache,o=n.keys,a=n._vnode;for(var s in t){var r=t[s];if(r){var i=r.name;i&&!e(i)&&Xt(t,s,o,a)}}}function Xt(n,e,t,o){var a=n[e];!a||o&&a.tag===o.tag||a.componentInstance.$destroy(),n[e]=null,v(t,e)}!function(n){n.prototype._init=function(n){var e=this;e._uid=Gt++,e._isVue=!0,e.__v_skip=!0,e._scope=new Fn(!0),e._scope._vm=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),o=e._parentVnode;t.parent=e.parent,t._parentVnode=o;var a=o.componentOptions;t.propsData=a.propsData,t._parentListeners=a.listeners,t._renderChildren=a.children,t._componentTag=a.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=St(Ht(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._provided=t?t._provided:Object.create(null),n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&We(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,a=t&&t.context;n.$slots=me(e._renderChildren,a),n.$scopedSlots=t?ge(n.$parent,t.data.scopedSlots,n.$slots):o,n._c=function(e,t,o,a){return Ae(n,e,t,o,a,!1)},n.$createElement=function(e,t,o,a){return Ae(n,e,t,o,a,!0)};var s=t&&t.data;In(n,"$attrs",s&&s.attrs||o,null,!0),In(n,"$listeners",e._parentListeners||o,null,!0)}(e),nt(e,"beforeCreate",void 0,!1),function(n){var e=ut(n.$options.inject,n);e&&(Tn(!1),Object.keys(e).forEach((function(t){In(n,t,e[t])})),Tn(!0))}(e),Lt(e),function(n){var e=n.$options.provide;if(e){var t=c(e)?e.call(n):e;if(!p(t))return;for(var o=Un(n),a=pn?Reflect.ownKeys(t):Object.keys(t),s=0;s<a.length;s++){var r=a[s];Object.defineProperty(o,r,Object.getOwnPropertyDescriptor(t,r))}}}(e),nt(e,"created"),e.$options.el&&e.$mount(e.$options.el)}}(Vt),function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=Pn,n.prototype.$delete=Dn,n.prototype.$watch=function(n,e,t){if(m(e))return Ut(this,n,e,t);(t=t||{}).user=!0;var o=new He(this,n,e,t);if(t.immediate){var a='callback for immediate watcher "'.concat(o.expression,'"');kn(),Be(e,this,[o.value],this,a),xn()}return function(){o.teardown()}}}(Vt),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var o=this;if(a(n))for(var s=0,r=n.length;s<r;s++)o.$on(n[s],t);else(o._events[n]||(o._events[n]=[])).push(t),e.test(n)&&(o._hasHookEvent=!0);return o},n.prototype.$once=function(n,e){var t=this;function o(){t.$off(n,o),e.apply(t,arguments)}return o.fn=e,t.$on(n,o),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(a(n)){for(var o=0,s=n.length;o<s;o++)t.$off(n[o],e);return t}var r,i=t._events[n];if(!i)return t;if(!e)return t._events[n]=null,t;for(var l=i.length;l--;)if((r=i[l])===e||r.fn===e){i.splice(l,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?$(t):t;for(var o=$(arguments,1),a='event handler for "'.concat(n,'"'),s=0,r=t.length;s<r;s++)Be(t[s],e,o,e,a)}return e}}(Vt),function(n){n.prototype._update=function(n,e){var t=this,o=t.$el,a=t._vnode,s=Xe(t);t._vnode=n,t.$el=a?t.__patch__(a,n):t.__patch__(t.$el,n,e,!1),s(),o&&(o.__vue__=null),t.$el&&(t.$el.__vue__=t);for(var r=t;r&&r.$vnode&&r.$parent&&r.$vnode===r.$parent._vnode;)r.$parent.$el=r.$el,r=r.$parent},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){nt(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||v(e.$children,n),n._scope.stop(),n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),nt(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(Vt),function(n){de(n.prototype),n.prototype.$nextTick=function(n){return Me(n,this)},n.prototype._render=function(){var n,e=this,t=e.$options,o=t.render,s=t._parentVnode;s&&e._isMounted&&(e.$scopedSlots=ge(e.$parent,s.data.scopedSlots,e.$slots,e.$scopedSlots),e._slotsProxy&&ke(e._slotsProxy,e.$scopedSlots)),e.$vnode=s;try{mn(e),xe=e,n=o.call(e._renderProxy,e.$createElement)}catch(t){ze(t,e,"render"),n=e._vnode}finally{xe=null,mn()}return a(n)&&1===n.length&&(n=n[0]),n instanceof un||(n=hn()),n.parent=s,n}}(Vt);var Qt=[String,RegExp,Array],Jt={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Qt,exclude:Qt,max:[String,Number]},methods:{cacheVNode:function(){var n=this.cache,e=this.keys,t=this.vnodeToCache,o=this.keyToCache;if(t){var a=t.tag,s=t.componentInstance,r=t.componentOptions;n[o]={name:Kt(r),tag:a,componentInstance:s},e.push(o),this.max&&e.length>parseInt(this.max)&&Xt(n,e[0],e,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)Xt(this.cache,n,this.keys)},mounted:function(){var n=this;this.cacheVNode(),this.$watch("include",(function(e){Yt(n,(function(n){return Wt(e,n)}))})),this.$watch("exclude",(function(e){Yt(n,(function(n){return!Wt(e,n)}))}))},updated:function(){this.cacheVNode()},render:function(){var n=this.$slots.default,e=Ee(n),t=e&&e.componentOptions;if(t){var o=Kt(t),a=this.include,s=this.exclude;if(a&&(!o||!Wt(a,o))||s&&o&&Wt(s,o))return e;var r=this.cache,i=this.keys,l=null==e.key?t.Ctor.cid+(t.tag?"::".concat(t.tag):""):e.key;r[l]?(e.componentInstance=r[l].componentInstance,v(i,l),i.push(l)):(this.vnodeToCache=e,this.keyToCache=l),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return F}};Object.defineProperty(n,"config",e),n.util={warn:xt,extend:j,mergeOptions:St,defineReactive:In},n.set=Pn,n.delete=Dn,n.nextTick=Me,n.observable=function(n){return jn(n),n},n.options=Object.create(null),q.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,j(n.options.components,Jt),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=$(arguments,1);return t.unshift(this),c(n.install)?n.install.apply(n,t):c(n)&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=St(this.options,n),this}}(n),Zt(n),function(n){q.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&m(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&c(t)&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(Vt),Object.defineProperty(Vt.prototype,"$isServer",{get:sn}),Object.defineProperty(Vt.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Vt,"FunctionalRenderContext",{value:ht}),Vt.version="2.7.14";var no=y("style,class"),eo=y("input,textarea,option,select,progress"),to=y("contenteditable,draggable,spellcheck"),oo=y("events,caret,typing,plaintext-only"),ao=y("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),so="http://www.w3.org/1999/xlink",ro=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},io=function(n){return ro(n)?n.slice(6,n.length):""},lo=function(n){return null==n||!1===n};function co(n){for(var e=n.data,t=n,o=n;r(o.componentInstance);)(o=o.componentInstance._vnode)&&o.data&&(e=po(o.data,e));for(;r(t=t.parent);)t&&t.data&&(e=po(e,t.data));return function(n,e){if(r(n)||r(e))return mo(n,uo(e));return""}(e.staticClass,e.class)}function po(n,e){return{staticClass:mo(n.staticClass,e.staticClass),class:r(n.class)?[n.class,e.class]:e.class}}function mo(n,e){return n?e?n+" "+e:n:e||""}function uo(n){return Array.isArray(n)?function(n){for(var e,t="",o=0,a=n.length;o<a;o++)r(e=uo(n[o]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):p(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var ho={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},go=y("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),bo=y("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),fo=function(n){return go(n)||bo(n)};var yo=Object.create(null);var _o=y("text,number,password,search,email,tel,url");var vo=Object.freeze({__proto__:null,createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(ho[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),ko={create:function(n,e){xo(e)},update:function(n,e){n.data.ref!==e.data.ref&&(xo(n,!0),xo(e))},destroy:function(n){xo(n,!0)}};function xo(n,e){var t=n.data.ref;if(r(t)){var o=n.context,s=n.componentInstance||n.elm,i=e?null:s,l=e?void 0:s;if(c(t))Be(t,o,[i],o,"template ref function");else{var p=n.data.refInFor,d="string"==typeof t||"number"==typeof t,m=Mn(t),u=o.$refs;if(d||m)if(p){var h=d?u[t]:t.value;e?a(h)&&v(h,s):a(h)?h.includes(s)||h.push(s):d?(u[t]=[s],wo(o,t,u[t])):t.value=[s]}else if(d){if(e&&u[t]!==s)return;u[t]=l,wo(o,t,i)}else if(m){if(e&&t.value!==s)return;t.value=i}else 0}}}function wo(n,e,t){var o=n._setupState;o&&x(o,e)&&(Mn(o[e])?o[e].value=t:o[e]=t)}var Eo=new un("",{},[]),Ao=["create","activate","update","remove","destroy"];function zo(n,e){return n.key===e.key&&n.asyncFactory===e.asyncFactory&&(n.tag===e.tag&&n.isComment===e.isComment&&r(n.data)===r(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,o=r(t=n.data)&&r(t=t.attrs)&&t.type,a=r(t=e.data)&&r(t=t.attrs)&&t.type;return o===a||_o(o)&&_o(a)}(n,e)||i(n.isAsyncPlaceholder)&&s(e.asyncFactory.error))}function Bo(n,e,t){var o,a,s={};for(o=e;o<=t;++o)r(a=n[o].key)&&(s[a]=o);return s}var To={create:So,update:So,destroy:function(n){So(n,Eo)}};function So(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,o,a,s=n===Eo,r=e===Eo,i=jo(n.data.directives,n.context),l=jo(e.data.directives,e.context),c=[],p=[];for(t in l)o=i[t],a=l[t],o?(a.oldValue=o.value,a.oldArg=o.arg,Po(a,"update",e,n),a.def&&a.def.componentUpdated&&p.push(a)):(Po(a,"bind",e,n),a.def&&a.def.inserted&&c.push(a));if(c.length){var d=function(){for(var t=0;t<c.length;t++)Po(c[t],"inserted",e,n)};s?Zn(e,"insert",d):d()}p.length&&Zn(e,"postpatch",(function(){for(var t=0;t<p.length;t++)Po(p[t],"componentUpdated",e,n)}));if(!s)for(t in i)l[t]||Po(i[t],"unbind",n,n,r)}(n,e)}var $o=Object.create(null);function jo(n,e){var t,o,a=Object.create(null);if(!n)return a;for(t=0;t<n.length;t++){if((o=n[t]).modifiers||(o.modifiers=$o),a[Io(o)]=o,e._setupState&&e._setupState.__sfc){var s=o.def||$t(e,"_setupState","v-"+o.name);o.def="function"==typeof s?{bind:s,update:s}:s}o.def=o.def||$t(e.$options,"directives",o.name)}return a}function Io(n){return n.rawName||"".concat(n.name,".").concat(Object.keys(n.modifiers||{}).join("."))}function Po(n,e,t,o,a){var s=n.def&&n.def[e];if(s)try{s(t.elm,n,t,o,a)}catch(o){ze(o,t.context,"directive ".concat(n.name," ").concat(e," hook"))}}var Do=[ko,To];function Co(n,e){var t=e.componentOptions;if(!(r(t)&&!1===t.Ctor.options.inheritAttrs||s(n.data.attrs)&&s(e.data.attrs))){var o,a,l=e.elm,c=n.data.attrs||{},p=e.data.attrs||{};for(o in(r(p.__ob__)||i(p._v_attr_proxy))&&(p=e.data.attrs=j({},p)),p)a=p[o],c[o]!==a&&Ro(l,o,a,e.data.pre);for(o in(Y||Q)&&p.value!==c.value&&Ro(l,"value",p.value),c)s(p[o])&&(ro(o)?l.removeAttributeNS(so,io(o)):to(o)||l.removeAttribute(o))}}function Ro(n,e,t,o){o||n.tagName.indexOf("-")>-1?Oo(n,e,t):ao(e)?lo(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):to(e)?n.setAttribute(e,function(n,e){return lo(e)||"false"===e?"false":"contenteditable"===n&&oo(e)?e:"true"}(e,t)):ro(e)?lo(t)?n.removeAttributeNS(so,io(e)):n.setAttributeNS(so,e,t):Oo(n,e,t)}function Oo(n,e,t){if(lo(t))n.removeAttribute(e);else{if(Y&&!X&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var o=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",o)};n.addEventListener("input",o),n.__ieph=!0}n.setAttribute(e,t)}}var Lo={create:Co,update:Co};function Mo(n,e){var t=e.elm,o=e.data,a=n.data;if(!(s(o.staticClass)&&s(o.class)&&(s(a)||s(a.staticClass)&&s(a.class)))){var i=co(e),l=t._transitionClasses;r(l)&&(i=mo(i,uo(l))),i!==t._prevClass&&(t.setAttribute("class",i),t._prevClass=i)}}var qo,No={create:Mo,update:Mo};function Fo(n,e,t){var o=qo;return function a(){var s=e.apply(null,arguments);null!==s&&Ho(n,a,t,o)}}var Uo=je&&!(en&&Number(en[1])<=53);function Go(n,e,t,o){if(Uo){var a=it,s=e;e=s._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=a||n.timeStamp<=0||n.target.ownerDocument!==document)return s.apply(this,arguments)}}qo.addEventListener(n,e,on?{capture:t,passive:o}:t)}function Ho(n,e,t,o){(o||qo).removeEventListener(n,e._wrapper||e,t)}function Vo(n,e){if(!s(n.data.on)||!s(e.data.on)){var t=e.data.on||{},o=n.data.on||{};qo=e.elm||n.elm,function(n){if(r(n.__r)){var e=Y?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}r(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),Vn(t,o,Go,Ho,Fo,e.context),qo=void 0}}var Zo,Ko={create:Vo,update:Vo,destroy:function(n){return Vo(n,Eo)}};function Wo(n,e){if(!s(n.data.domProps)||!s(e.data.domProps)){var t,o,a=e.elm,l=n.data.domProps||{},c=e.data.domProps||{};for(t in(r(c.__ob__)||i(c._v_attr_proxy))&&(c=e.data.domProps=j({},c)),l)t in c||(a[t]="");for(t in c){if(o=c[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),o===l[t])continue;1===a.childNodes.length&&a.removeChild(a.childNodes[0])}if("value"===t&&"PROGRESS"!==a.tagName){a._value=o;var p=s(o)?"":String(o);Yo(a,p)&&(a.value=p)}else if("innerHTML"===t&&bo(a.tagName)&&s(a.innerHTML)){(Zo=Zo||document.createElement("div")).innerHTML="<svg>".concat(o,"</svg>");for(var d=Zo.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;d.firstChild;)a.appendChild(d.firstChild)}else if(o!==l[t])try{a[t]=o}catch(n){}}}}function Yo(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,o=n._vModifiers;if(r(o)){if(o.number)return f(t)!==f(e);if(o.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var Xo={create:Wo,update:Wo},Qo=w((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var o=n.split(t);o.length>1&&(e[o[0].trim()]=o[1].trim())}})),e}));function Jo(n){var e=na(n.style);return n.staticStyle?j(n.staticStyle,e):e}function na(n){return Array.isArray(n)?I(n):"string"==typeof n?Qo(n):n}var ea,ta=/^--/,oa=/\s*!important$/,aa=function(n,e,t){if(ta.test(e))n.style.setProperty(e,t);else if(oa.test(t))n.style.setProperty(T(e),t.replace(oa,""),"important");else{var o=ra(e);if(Array.isArray(t))for(var a=0,s=t.length;a<s;a++)n.style[o]=t[a];else n.style[o]=t}},sa=["Webkit","Moz","ms"],ra=w((function(n){if(ea=ea||document.createElement("div").style,"filter"!==(n=A(n))&&n in ea)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<sa.length;t++){var o=sa[t]+e;if(o in ea)return o}}));function ia(n,e){var t=e.data,o=n.data;if(!(s(t.staticStyle)&&s(t.style)&&s(o.staticStyle)&&s(o.style))){var a,i,l=e.elm,c=o.staticStyle,p=o.normalizedStyle||o.style||{},d=c||p,m=na(e.data.style)||{};e.data.normalizedStyle=r(m.__ob__)?j({},m):m;var u=function(n,e){var t,o={};if(e)for(var a=n;a.componentInstance;)(a=a.componentInstance._vnode)&&a.data&&(t=Jo(a.data))&&j(o,t);(t=Jo(n.data))&&j(o,t);for(var s=n;s=s.parent;)s.data&&(t=Jo(s.data))&&j(o,t);return o}(e,!0);for(i in d)s(u[i])&&aa(l,i,"");for(i in u)(a=u[i])!==d[i]&&aa(l,i,null==a?"":a)}}var la={create:ia,update:ia},ca=/\s+/;function pa(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(ca).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" ".concat(n.getAttribute("class")||""," ");t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function da(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(ca).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" ".concat(n.getAttribute("class")||""," "),o=" "+e+" ";t.indexOf(o)>=0;)t=t.replace(o," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function ma(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&j(e,ua(n.name||"v")),j(e,n),e}return"string"==typeof n?ua(n):void 0}}var ua=w((function(n){return{enterClass:"".concat(n,"-enter"),enterToClass:"".concat(n,"-enter-to"),enterActiveClass:"".concat(n,"-enter-active"),leaveClass:"".concat(n,"-leave"),leaveToClass:"".concat(n,"-leave-to"),leaveActiveClass:"".concat(n,"-leave-active")}})),ha=K&&!X,ga="transition",ba="transitionend",fa="animation",ya="animationend";ha&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(ga="WebkitTransition",ba="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(fa="WebkitAnimation",ya="webkitAnimationEnd"));var _a=K?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function va(n){_a((function(){_a(n)}))}function ka(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),pa(n,e))}function xa(n,e){n._transitionClasses&&v(n._transitionClasses,e),da(n,e)}function wa(n,e,t){var o=Aa(n,e),a=o.type,s=o.timeout,r=o.propCount;if(!a)return t();var i="transition"===a?ba:ya,l=0,c=function(){n.removeEventListener(i,p),t()},p=function(e){e.target===n&&++l>=r&&c()};setTimeout((function(){l<r&&c()}),s+1),n.addEventListener(i,p)}var Ea=/\b(transform|all)(,|$)/;function Aa(n,e){var t,o=window.getComputedStyle(n),a=(o[ga+"Delay"]||"").split(", "),s=(o[ga+"Duration"]||"").split(", "),r=za(a,s),i=(o[fa+"Delay"]||"").split(", "),l=(o[fa+"Duration"]||"").split(", "),c=za(i,l),p=0,d=0;return"transition"===e?r>0&&(t="transition",p=r,d=s.length):"animation"===e?c>0&&(t="animation",p=c,d=l.length):d=(t=(p=Math.max(r,c))>0?r>c?"transition":"animation":null)?"transition"===t?s.length:l.length:0,{type:t,timeout:p,propCount:d,hasTransform:"transition"===t&&Ea.test(o[ga+"Property"])}}function za(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return Ba(e)+Ba(n[t])})))}function Ba(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function Ta(n,e){var t=n.elm;r(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var o=ma(n.data.transition);if(!s(o)&&!r(t._enterCb)&&1===t.nodeType){for(var a=o.css,i=o.type,l=o.enterClass,d=o.enterToClass,m=o.enterActiveClass,u=o.appearClass,h=o.appearToClass,g=o.appearActiveClass,b=o.beforeEnter,y=o.enter,_=o.afterEnter,v=o.enterCancelled,k=o.beforeAppear,x=o.appear,w=o.afterAppear,E=o.appearCancelled,A=o.duration,z=Ye,B=Ye.$vnode;B&&B.parent;)z=B.context,B=B.parent;var T=!z._isMounted||!n.isRootInsert;if(!T||x||""===x){var S=T&&u?u:l,$=T&&g?g:m,j=T&&h?h:d,I=T&&k||b,P=T&&c(x)?x:y,D=T&&w||_,C=T&&E||v,R=f(p(A)?A.enter:A);0;var O=!1!==a&&!X,M=ja(P),q=t._enterCb=L((function(){O&&(xa(t,j),xa(t,$)),q.cancelled?(O&&xa(t,S),C&&C(t)):D&&D(t),t._enterCb=null}));n.data.show||Zn(n,"insert",(function(){var e=t.parentNode,o=e&&e._pending&&e._pending[n.key];o&&o.tag===n.tag&&o.elm._leaveCb&&o.elm._leaveCb(),P&&P(t,q)})),I&&I(t),O&&(ka(t,S),ka(t,$),va((function(){xa(t,S),q.cancelled||(ka(t,j),M||($a(R)?setTimeout(q,R):wa(t,i,q)))}))),n.data.show&&(e&&e(),P&&P(t,q)),O||M||q()}}}function Sa(n,e){var t=n.elm;r(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var o=ma(n.data.transition);if(s(o)||1!==t.nodeType)return e();if(!r(t._leaveCb)){var a=o.css,i=o.type,l=o.leaveClass,c=o.leaveToClass,d=o.leaveActiveClass,m=o.beforeLeave,u=o.leave,h=o.afterLeave,g=o.leaveCancelled,b=o.delayLeave,y=o.duration,_=!1!==a&&!X,v=ja(u),k=f(p(y)?y.leave:y);0;var x=t._leaveCb=L((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),_&&(xa(t,c),xa(t,d)),x.cancelled?(_&&xa(t,l),g&&g(t)):(e(),h&&h(t)),t._leaveCb=null}));b?b(w):w()}function w(){x.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),m&&m(t),_&&(ka(t,l),ka(t,d),va((function(){xa(t,l),x.cancelled||(ka(t,c),v||($a(k)?setTimeout(x,k):wa(t,i,x)))}))),u&&u(t,x),_||v||x())}}function $a(n){return"number"==typeof n&&!isNaN(n)}function ja(n){if(s(n))return!1;var e=n.fns;return r(e)?ja(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function Ia(n,e){!0!==e.data.show&&Ta(e)}var Pa=function(n){var e,t,o={},c=n.modules,p=n.nodeOps;for(e=0;e<Ao.length;++e)for(o[Ao[e]]=[],t=0;t<c.length;++t)r(c[t][Ao[e]])&&o[Ao[e]].push(c[t][Ao[e]]);function d(n){var e=p.parentNode(n);r(e)&&p.removeChild(e,n)}function m(n,e,t,a,s,l,c){if(r(n.elm)&&r(l)&&(n=l[c]=bn(n)),n.isRootInsert=!s,!function(n,e,t,a){var s=n.data;if(r(s)){var l=r(n.componentInstance)&&s.keepAlive;if(r(s=s.hook)&&r(s=s.init)&&s(n,!1),r(n.componentInstance))return u(n,e),h(t,n.elm,a),i(l)&&function(n,e,t,a){var s,i=n;for(;i.componentInstance;)if(i=i.componentInstance._vnode,r(s=i.data)&&r(s=s.transition)){for(s=0;s<o.activate.length;++s)o.activate[s](Eo,i);e.push(i);break}h(t,n.elm,a)}(n,e,t,a),!0}}(n,e,t,a)){var d=n.data,m=n.children,b=n.tag;r(b)?(n.elm=n.ns?p.createElementNS(n.ns,b):p.createElement(b,n),_(n),g(n,m,e),r(d)&&f(n,e),h(t,n.elm,a)):i(n.isComment)?(n.elm=p.createComment(n.text),h(t,n.elm,a)):(n.elm=p.createTextNode(n.text),h(t,n.elm,a))}}function u(n,e){r(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,b(n)?(f(n,e),_(n)):(xo(n),e.push(n))}function h(n,e,t){r(n)&&(r(t)?p.parentNode(t)===n&&p.insertBefore(n,e,t):p.appendChild(n,e))}function g(n,e,t){if(a(e)){0;for(var o=0;o<e.length;++o)m(e[o],t,n.elm,null,!0,e,o)}else l(n.text)&&p.appendChild(n.elm,p.createTextNode(String(n.text)))}function b(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return r(n.tag)}function f(n,t){for(var a=0;a<o.create.length;++a)o.create[a](Eo,n);r(e=n.data.hook)&&(r(e.create)&&e.create(Eo,n),r(e.insert)&&t.push(n))}function _(n){var e;if(r(e=n.fnScopeId))p.setStyleScope(n.elm,e);else for(var t=n;t;)r(e=t.context)&&r(e=e.$options._scopeId)&&p.setStyleScope(n.elm,e),t=t.parent;r(e=Ye)&&e!==n.context&&e!==n.fnContext&&r(e=e.$options._scopeId)&&p.setStyleScope(n.elm,e)}function v(n,e,t,o,a,s){for(;o<=a;++o)m(t[o],s,n,e,!1,t,o)}function k(n){var e,t,a=n.data;if(r(a))for(r(e=a.hook)&&r(e=e.destroy)&&e(n),e=0;e<o.destroy.length;++e)o.destroy[e](n);if(r(e=n.children))for(t=0;t<n.children.length;++t)k(n.children[t])}function x(n,e,t){for(;e<=t;++e){var o=n[e];r(o)&&(r(o.tag)?(w(o),k(o)):d(o.elm))}}function w(n,e){if(r(e)||r(n.data)){var t,a=o.remove.length+1;for(r(e)?e.listeners+=a:e=function(n,e){function t(){0==--t.listeners&&d(n)}return t.listeners=e,t}(n.elm,a),r(t=n.componentInstance)&&r(t=t._vnode)&&r(t.data)&&w(t,e),t=0;t<o.remove.length;++t)o.remove[t](n,e);r(t=n.data.hook)&&r(t=t.remove)?t(n,e):e()}else d(n.elm)}function E(n,e,t,o){for(var a=t;a<o;a++){var s=e[a];if(r(s)&&zo(n,s))return a}}function A(n,e,t,a,l,c){if(n!==e){r(e.elm)&&r(a)&&(e=a[l]=bn(e));var d=e.elm=n.elm;if(i(n.isAsyncPlaceholder))r(e.asyncFactory.resolved)?T(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(i(e.isStatic)&&i(n.isStatic)&&e.key===n.key&&(i(e.isCloned)||i(e.isOnce)))e.componentInstance=n.componentInstance;else{var u,h=e.data;r(h)&&r(u=h.hook)&&r(u=u.prepatch)&&u(n,e);var g=n.children,f=e.children;if(r(h)&&b(e)){for(u=0;u<o.update.length;++u)o.update[u](n,e);r(u=h.hook)&&r(u=u.update)&&u(n,e)}s(e.text)?r(g)&&r(f)?g!==f&&function(n,e,t,o,a){var i,l,c,d=0,u=0,h=e.length-1,g=e[0],b=e[h],f=t.length-1,y=t[0],_=t[f],k=!a;for(0;d<=h&&u<=f;)s(g)?g=e[++d]:s(b)?b=e[--h]:zo(g,y)?(A(g,y,o,t,u),g=e[++d],y=t[++u]):zo(b,_)?(A(b,_,o,t,f),b=e[--h],_=t[--f]):zo(g,_)?(A(g,_,o,t,f),k&&p.insertBefore(n,g.elm,p.nextSibling(b.elm)),g=e[++d],_=t[--f]):zo(b,y)?(A(b,y,o,t,u),k&&p.insertBefore(n,b.elm,g.elm),b=e[--h],y=t[++u]):(s(i)&&(i=Bo(e,d,h)),s(l=r(y.key)?i[y.key]:E(y,e,d,h))?m(y,o,n,g.elm,!1,t,u):zo(c=e[l],y)?(A(c,y,o,t,u),e[l]=void 0,k&&p.insertBefore(n,c.elm,g.elm)):m(y,o,n,g.elm,!1,t,u),y=t[++u]);d>h?v(n,s(t[f+1])?null:t[f+1].elm,t,u,f,o):u>f&&x(e,d,h)}(d,g,f,t,c):r(f)?(r(n.text)&&p.setTextContent(d,""),v(d,null,f,0,f.length-1,t)):r(g)?x(g,0,g.length-1):r(n.text)&&p.setTextContent(d,""):n.text!==e.text&&p.setTextContent(d,e.text),r(h)&&r(u=h.hook)&&r(u=u.postpatch)&&u(n,e)}}}function z(n,e,t){if(i(t)&&r(n.parent))n.parent.data.pendingInsert=e;else for(var o=0;o<e.length;++o)e[o].data.hook.insert(e[o])}var B=y("attrs,class,staticClass,staticStyle,key");function T(n,e,t,o){var a,s=e.tag,l=e.data,c=e.children;if(o=o||l&&l.pre,e.elm=n,i(e.isComment)&&r(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(r(l)&&(r(a=l.hook)&&r(a=a.init)&&a(e,!0),r(a=e.componentInstance)))return u(e,t),!0;if(r(s)){if(r(c))if(n.hasChildNodes())if(r(a=l)&&r(a=a.domProps)&&r(a=a.innerHTML)){if(a!==n.innerHTML)return!1}else{for(var p=!0,d=n.firstChild,m=0;m<c.length;m++){if(!d||!T(d,c[m],t,o)){p=!1;break}d=d.nextSibling}if(!p||d)return!1}else g(e,c,t);if(r(l)){var h=!1;for(var b in l)if(!B(b)){h=!0,f(e,t);break}!h&&l.class&&Fe(l.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,a){if(!s(e)){var l,c=!1,d=[];if(s(n))c=!0,m(e,d);else{var u=r(n.nodeType);if(!u&&zo(n,e))A(n,e,d,null,null,a);else{if(u){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),i(t)&&T(n,e,d))return z(e,d,!0),n;l=n,n=new un(p.tagName(l).toLowerCase(),{},[],void 0,l)}var h=n.elm,g=p.parentNode(h);if(m(e,d,h._leaveCb?null:g,p.nextSibling(h)),r(e.parent))for(var f=e.parent,y=b(e);f;){for(var _=0;_<o.destroy.length;++_)o.destroy[_](f);if(f.elm=e.elm,y){for(var v=0;v<o.create.length;++v)o.create[v](Eo,f);var w=f.data.hook.insert;if(w.merged)for(var E=1;E<w.fns.length;E++)w.fns[E]()}else xo(f);f=f.parent}r(g)?x([n],0,0):r(n.tag)&&k(n)}}return z(e,d,c),e.elm}r(n)&&k(n)}}({nodeOps:vo,modules:[Lo,No,Ko,Xo,la,K?{create:Ia,activate:Ia,remove:function(n,e){!0!==n.data.show?Sa(n,e):e()}}:{}].concat(Do)});X&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&Na(n,"input")}));var Da={inserted:function(n,e,t,o){"select"===t.tag?(o.elm&&!o.elm._vOptions?Zn(t,"postpatch",(function(){Da.componentUpdated(n,e,t)})):Ca(n,e,t.context),n._vOptions=[].map.call(n.options,La)):("textarea"===t.tag||_o(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",Ma),n.addEventListener("compositionend",qa),n.addEventListener("change",qa),X&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){Ca(n,e,t.context);var o=n._vOptions,a=n._vOptions=[].map.call(n.options,La);if(a.some((function(n,e){return!R(n,o[e])})))(n.multiple?e.value.some((function(n){return Oa(n,a)})):e.value!==e.oldValue&&Oa(e.value,a))&&Na(n,"change")}}};function Ca(n,e,t){Ra(n,e,t),(Y||Q)&&setTimeout((function(){Ra(n,e,t)}),0)}function Ra(n,e,t){var o=e.value,a=n.multiple;if(!a||Array.isArray(o)){for(var s,r,i=0,l=n.options.length;i<l;i++)if(r=n.options[i],a)s=O(o,La(r))>-1,r.selected!==s&&(r.selected=s);else if(R(La(r),o))return void(n.selectedIndex!==i&&(n.selectedIndex=i));a||(n.selectedIndex=-1)}}function Oa(n,e){return e.every((function(e){return!R(e,n)}))}function La(n){return"_value"in n?n._value:n.value}function Ma(n){n.target.composing=!0}function qa(n){n.target.composing&&(n.target.composing=!1,Na(n.target,"input"))}function Na(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function Fa(n){return!n.componentInstance||n.data&&n.data.transition?n:Fa(n.componentInstance._vnode)}var Ua={model:Da,show:{bind:function(n,e,t){var o=e.value,a=(t=Fa(t)).data&&t.data.transition,s=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;o&&a?(t.data.show=!0,Ta(t,(function(){n.style.display=s}))):n.style.display=o?s:"none"},update:function(n,e,t){var o=e.value;!o!=!e.oldValue&&((t=Fa(t)).data&&t.data.transition?(t.data.show=!0,o?Ta(t,(function(){n.style.display=n.__vOriginalDisplay})):Sa(t,(function(){n.style.display="none"}))):n.style.display=o?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,o,a){a||(n.style.display=n.__vOriginalDisplay)}}},Ga={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function Ha(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?Ha(Ee(e.children)):n}function Va(n){var e={},t=n.$options;for(var o in t.propsData)e[o]=n[o];var a=t._parentListeners;for(var o in a)e[A(o)]=a[o];return e}function Za(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var Ka=function(n){return n.tag||he(n)},Wa=function(n){return"show"===n.name},Ya={name:"transition",props:Ga,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(Ka)).length){0;var o=this.mode;0;var a=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return a;var s=Ha(a);if(!s)return a;if(this._leaving)return Za(n,a);var r="__transition-".concat(this._uid,"-");s.key=null==s.key?s.isComment?r+"comment":r+s.tag:l(s.key)?0===String(s.key).indexOf(r)?s.key:r+s.key:s.key;var i=(s.data||(s.data={})).transition=Va(this),c=this._vnode,p=Ha(c);if(s.data.directives&&s.data.directives.some(Wa)&&(s.data.show=!0),p&&p.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(s,p)&&!he(p)&&(!p.componentInstance||!p.componentInstance._vnode.isComment)){var d=p.data.transition=j({},i);if("out-in"===o)return this._leaving=!0,Zn(d,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),Za(n,a);if("in-out"===o){if(he(s))return c;var m,u=function(){m()};Zn(i,"afterEnter",u),Zn(i,"enterCancelled",u),Zn(d,"delayLeave",(function(n){m=n}))}}return a}}},Xa=j({tag:String,moveClass:String},Ga);function Qa(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function Ja(n){n.data.newPos=n.elm.getBoundingClientRect()}function ns(n){var e=n.data.pos,t=n.data.newPos,o=e.left-t.left,a=e.top-t.top;if(o||a){n.data.moved=!0;var s=n.elm.style;s.transform=s.WebkitTransform="translate(".concat(o,"px,").concat(a,"px)"),s.transitionDuration="0s"}}delete Xa.mode;var es={Transition:Ya,TransitionGroup:{props:Xa,beforeMount:function(){var n=this,e=this._update;this._update=function(t,o){var a=Xe(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,a(),e.call(n,t,o)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),o=this.prevChildren=this.children,a=this.$slots.default||[],s=this.children=[],r=Va(this),i=0;i<a.length;i++){if((p=a[i]).tag)if(null!=p.key&&0!==String(p.key).indexOf("__vlist"))s.push(p),t[p.key]=p,(p.data||(p.data={})).transition=r;else;}if(o){var l=[],c=[];for(i=0;i<o.length;i++){var p;(p=o[i]).data.transition=r,p.data.pos=p.elm.getBoundingClientRect(),t[p.key]?l.push(p):c.push(p)}this.kept=n(e,null,l),this.removed=c}return n(e,null,s)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(Qa),n.forEach(Ja),n.forEach(ns),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,o=t.style;ka(t,e),o.transform=o.WebkitTransform=o.transitionDuration="",t.addEventListener(ba,t._moveCb=function n(o){o&&o.target!==t||o&&!/transform$/.test(o.propertyName)||(t.removeEventListener(ba,n),t._moveCb=null,xa(t,e))})}})))},methods:{hasMove:function(n,e){if(!ha)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){da(t,n)})),pa(t,e),t.style.display="none",this.$el.appendChild(t);var o=Aa(t);return this.$el.removeChild(t),this._hasMove=o.hasTransform}}}};function ts(n,e){for(var t in e)n[t]=e[t];return n}Vt.config.mustUseProp=function(n,e,t){return"value"===t&&eo(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},Vt.config.isReservedTag=fo,Vt.config.isReservedAttr=no,Vt.config.getTagNamespace=function(n){return bo(n)?"svg":"math"===n?"math":void 0},Vt.config.isUnknownElement=function(n){if(!K)return!0;if(fo(n))return!1;if(n=n.toLowerCase(),null!=yo[n])return yo[n];var e=document.createElement(n);return n.indexOf("-")>-1?yo[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:yo[n]=/HTMLUnknownElement/.test(e.toString())},j(Vt.options.directives,Ua),j(Vt.options.components,es),Vt.prototype.__patch__=K?Pa:P,Vt.prototype.$mount=function(n,e){return function(n,e,t){var o;n.$el=e,n.$options.render||(n.$options.render=hn),nt(n,"beforeMount"),o=function(){n._update(n._render(),t)},new He(n,o,P,{before:function(){n._isMounted&&!n._isDestroyed&&nt(n,"beforeUpdate")}},!0),t=!1;var a=n._preWatchers;if(a)for(var s=0;s<a.length;s++)a[s].run();return null==n.$vnode&&(n._isMounted=!0,nt(n,"mounted")),n}(this,n=n&&K?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},K&&setTimeout((function(){F.devtools&&rn&&rn.emit("init",Vt)}),0);var os=/[!'()*]/g,as=function(n){return"%"+n.charCodeAt(0).toString(16)},ss=/%2C/g,rs=function(n){return encodeURIComponent(n).replace(os,as).replace(ss,",")};function is(n){try{return decodeURIComponent(n)}catch(n){0}return n}var ls=function(n){return null==n||"object"==typeof n?n:String(n)};function cs(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),o=is(t.shift()),a=t.length>0?is(t.join("=")):null;void 0===e[o]?e[o]=a:Array.isArray(e[o])?e[o].push(a):e[o]=[e[o],a]})),e):e}function ps(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return rs(e);if(Array.isArray(t)){var o=[];return t.forEach((function(n){void 0!==n&&(null===n?o.push(rs(e)):o.push(rs(e)+"="+rs(n)))})),o.join("&")}return rs(e)+"="+rs(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var ds=/\/?$/;function ms(n,e,t,o){var a=o&&o.options.stringifyQuery,s=e.query||{};try{s=us(s)}catch(n){}var r={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:s,params:e.params||{},fullPath:bs(e,a),matched:n?gs(n):[]};return t&&(r.redirectedFrom=bs(t,a)),Object.freeze(r)}function us(n){if(Array.isArray(n))return n.map(us);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=us(n[t]);return e}return n}var hs=ms(null,{path:"/"});function gs(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function bs(n,e){var t=n.path,o=n.query;void 0===o&&(o={});var a=n.hash;return void 0===a&&(a=""),(t||"/")+(e||ps)(o)+a}function fs(n,e,t){return e===hs?n===e:!!e&&(n.path&&e.path?n.path.replace(ds,"")===e.path.replace(ds,"")&&(t||n.hash===e.hash&&ys(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&ys(n.query,e.query)&&ys(n.params,e.params))))}function ys(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),o=Object.keys(e).sort();return t.length===o.length&&t.every((function(t,a){var s=n[t];if(o[a]!==t)return!1;var r=e[t];return null==s||null==r?s===r:"object"==typeof s&&"object"==typeof r?ys(s,r):String(s)===String(r)}))}function _s(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var o in t.instances){var a=t.instances[o],s=t.enteredCbs[o];if(a&&s){delete t.enteredCbs[o];for(var r=0;r<s.length;r++)a._isBeingDestroyed||s[r](a)}}}}var vs={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,o=e.children,a=e.parent,s=e.data;s.routerView=!0;for(var r=a.$createElement,i=t.name,l=a.$route,c=a._routerViewCache||(a._routerViewCache={}),p=0,d=!1;a&&a._routerRoot!==a;){var m=a.$vnode?a.$vnode.data:{};m.routerView&&p++,m.keepAlive&&a._directInactive&&a._inactive&&(d=!0),a=a.$parent}if(s.routerViewDepth=p,d){var u=c[i],h=u&&u.component;return h?(u.configProps&&ks(h,s,u.route,u.configProps),r(h,s,o)):r()}var g=l.matched[p],b=g&&g.components[i];if(!g||!b)return c[i]=null,r();c[i]={component:b},s.registerRouteInstance=function(n,e){var t=g.instances[i];(e&&t!==n||!e&&t===n)&&(g.instances[i]=e)},(s.hook||(s.hook={})).prepatch=function(n,e){g.instances[i]=e.componentInstance},s.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==g.instances[i]&&(g.instances[i]=n.componentInstance),_s(l)};var f=g.props&&g.props[i];return f&&(ts(c[i],{route:l,configProps:f}),ks(b,s,l,f)),r(b,s,o)}};function ks(n,e,t,o){var a=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,o);if(a){a=e.props=ts({},a);var s=e.attrs=e.attrs||{};for(var r in a)n.props&&r in n.props||(s[r]=a[r],delete a[r])}}function xs(n,e,t){var o=n.charAt(0);if("/"===o)return n;if("?"===o||"#"===o)return e+n;var a=e.split("/");t&&a[a.length-1]||a.pop();for(var s=n.replace(/^\//,"").split("/"),r=0;r<s.length;r++){var i=s[r];".."===i?a.pop():"."!==i&&a.push(i)}return""!==a[0]&&a.unshift(""),a.join("/")}function ws(n){return n.replace(/\/(?:\s*\/)+/g,"/")}var Es=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},As=Ms,zs=js,Bs=function(n,e){return Ps(js(n,e),e)},Ts=Ps,Ss=Ls,$s=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function js(n,e){for(var t,o=[],a=0,s=0,r="",i=e&&e.delimiter||"/";null!=(t=$s.exec(n));){var l=t[0],c=t[1],p=t.index;if(r+=n.slice(s,p),s=p+l.length,c)r+=c[1];else{var d=n[s],m=t[2],u=t[3],h=t[4],g=t[5],b=t[6],f=t[7];r&&(o.push(r),r="");var y=null!=m&&null!=d&&d!==m,_="+"===b||"*"===b,v="?"===b||"*"===b,k=t[2]||i,x=h||g;o.push({name:u||a++,prefix:m||"",delimiter:k,optional:v,repeat:_,partial:y,asterisk:!!f,pattern:x?Cs(x):f?".*":"[^"+Ds(k)+"]+?"})}}return s<n.length&&(r+=n.substr(s)),r&&o.push(r),o}function Is(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function Ps(n,e){for(var t=new Array(n.length),o=0;o<n.length;o++)"object"==typeof n[o]&&(t[o]=new RegExp("^(?:"+n[o].pattern+")$",Os(e)));return function(e,o){for(var a="",s=e||{},r=(o||{}).pretty?Is:encodeURIComponent,i=0;i<n.length;i++){var l=n[i];if("string"!=typeof l){var c,p=s[l.name];if(null==p){if(l.optional){l.partial&&(a+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(Es(p)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(p)+"`");if(0===p.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var d=0;d<p.length;d++){if(c=r(p[d]),!t[i].test(c))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(c)+"`");a+=(0===d?l.prefix:l.delimiter)+c}}else{if(c=l.asterisk?encodeURI(p).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):r(p),!t[i].test(c))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+c+'"');a+=l.prefix+c}}else a+=l}return a}}function Ds(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function Cs(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function Rs(n,e){return n.keys=e,n}function Os(n){return n&&n.sensitive?"":"i"}function Ls(n,e,t){Es(e)||(t=e||t,e=[]);for(var o=(t=t||{}).strict,a=!1!==t.end,s="",r=0;r<n.length;r++){var i=n[r];if("string"==typeof i)s+=Ds(i);else{var l=Ds(i.prefix),c="(?:"+i.pattern+")";e.push(i),i.repeat&&(c+="(?:"+l+c+")*"),s+=c=i.optional?i.partial?l+"("+c+")?":"(?:"+l+"("+c+"))?":l+"("+c+")"}}var p=Ds(t.delimiter||"/"),d=s.slice(-p.length)===p;return o||(s=(d?s.slice(0,-p.length):s)+"(?:"+p+"(?=$))?"),s+=a?"$":o&&d?"":"(?="+p+"|$)",Rs(new RegExp("^"+s,Os(t)),e)}function Ms(n,e,t){return Es(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var o=0;o<t.length;o++)e.push({name:o,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return Rs(n,e)}(n,e):Es(n)?function(n,e,t){for(var o=[],a=0;a<n.length;a++)o.push(Ms(n[a],e,t).source);return Rs(new RegExp("(?:"+o.join("|")+")",Os(t)),e)}(n,e,t):function(n,e,t){return Ls(js(n,t),e,t)}(n,e,t)}As.parse=zs,As.compile=Bs,As.tokensToFunction=Ts,As.tokensToRegExp=Ss;var qs=Object.create(null);function Ns(n,e,t){e=e||{};try{var o=qs[n]||(qs[n]=As.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),o(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function Fs(n,e,t,o){var a="string"==typeof n?{path:n}:n;if(a._normalized)return a;if(a.name){var s=(a=ts({},n)).params;return s&&"object"==typeof s&&(a.params=ts({},s)),a}if(!a.path&&a.params&&e){(a=ts({},a))._normalized=!0;var r=ts(ts({},e.params),a.params);if(e.name)a.name=e.name,a.params=r;else if(e.matched.length){var i=e.matched[e.matched.length-1].path;a.path=Ns(i,r,e.path)}else 0;return a}var l=function(n){var e="",t="",o=n.indexOf("#");o>=0&&(e=n.slice(o),n=n.slice(0,o));var a=n.indexOf("?");return a>=0&&(t=n.slice(a+1),n=n.slice(0,a)),{path:n,query:t,hash:e}}(a.path||""),c=e&&e.path||"/",p=l.path?xs(l.path,c,t||a.append):c,d=function(n,e,t){void 0===e&&(e={});var o,a=t||cs;try{o=a(n||"")}catch(n){o={}}for(var s in e){var r=e[s];o[s]=Array.isArray(r)?r.map(ls):ls(r)}return o}(l.query,a.query,o&&o.options.parseQuery),m=a.hash||l.hash;return m&&"#"!==m.charAt(0)&&(m="#"+m),{_normalized:!0,path:p,query:d,hash:m}}var Us,Gs=function(){},Hs={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,o=this.$route,a=t.resolve(this.to,o,this.append),s=a.location,r=a.route,i=a.href,l={},c=t.options.linkActiveClass,p=t.options.linkExactActiveClass,d=null==c?"router-link-active":c,m=null==p?"router-link-exact-active":p,u=null==this.activeClass?d:this.activeClass,h=null==this.exactActiveClass?m:this.exactActiveClass,g=r.redirectedFrom?ms(null,Fs(r.redirectedFrom),null,t):r;l[h]=fs(o,g,this.exactPath),l[u]=this.exact||this.exactPath?l[h]:function(n,e){return 0===n.path.replace(ds,"/").indexOf(e.path.replace(ds,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(o,g);var b=l[h]?this.ariaCurrentValue:null,f=function(n){Vs(n)&&(e.replace?t.replace(s,Gs):t.push(s,Gs))},y={click:Vs};Array.isArray(this.event)?this.event.forEach((function(n){y[n]=f})):y[this.event]=f;var _={class:l},v=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:i,route:r,navigate:f,isActive:l[u],isExactActive:l[h]});if(v){if(1===v.length)return v[0];if(v.length>1||!v.length)return 0===v.length?n():n("span",{},v)}if("a"===this.tag)_.on=y,_.attrs={href:i,"aria-current":b};else{var k=function n(e){var t;if(e)for(var o=0;o<e.length;o++){if("a"===(t=e[o]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(k){k.isStatic=!1;var x=k.data=ts({},k.data);for(var w in x.on=x.on||{},x.on){var E=x.on[w];w in y&&(x.on[w]=Array.isArray(E)?E:[E])}for(var A in y)A in x.on?x.on[A].push(y[A]):x.on[A]=f;var z=k.data.attrs=ts({},k.data.attrs);z.href=i,z["aria-current"]=b}else _.on=y}return n(this.tag,_,this.$slots.default)}};function Vs(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var Zs="undefined"!=typeof window;function Ks(n,e,t,o,a){var s=e||[],r=t||Object.create(null),i=o||Object.create(null);n.forEach((function(n){!function n(e,t,o,a,s,r){var i=a.path,l=a.name;0;var c=a.pathToRegexpOptions||{},p=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return ws(e.path+"/"+n)}(i,s,c.strict);"boolean"==typeof a.caseSensitive&&(c.sensitive=a.caseSensitive);var d={path:p,regex:Ws(p,c),components:a.components||{default:a.component},alias:a.alias?"string"==typeof a.alias?[a.alias]:a.alias:[],instances:{},enteredCbs:{},name:l,parent:s,matchAs:r,redirect:a.redirect,beforeEnter:a.beforeEnter,meta:a.meta||{},props:null==a.props?{}:a.components?a.props:{default:a.props}};a.children&&a.children.forEach((function(a){var s=r?ws(r+"/"+a.path):void 0;n(e,t,o,a,d,s)}));t[d.path]||(e.push(d.path),t[d.path]=d);if(void 0!==a.alias)for(var m=Array.isArray(a.alias)?a.alias:[a.alias],u=0;u<m.length;++u){0;var h={path:m[u],children:a.children};n(e,t,o,h,s,d.path||"/")}l&&(o[l]||(o[l]=d))}(s,r,i,n,a)}));for(var l=0,c=s.length;l<c;l++)"*"===s[l]&&(s.push(s.splice(l,1)[0]),c--,l--);return{pathList:s,pathMap:r,nameMap:i}}function Ws(n,e){return As(n,[],e)}function Ys(n,e){var t=Ks(n),o=t.pathList,a=t.pathMap,s=t.nameMap;function r(n,t,r){var i=Fs(n,t,!1,e),c=i.name;if(c){var p=s[c];if(!p)return l(null,i);var d=p.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof i.params&&(i.params={}),t&&"object"==typeof t.params)for(var m in t.params)!(m in i.params)&&d.indexOf(m)>-1&&(i.params[m]=t.params[m]);return i.path=Ns(p.path,i.params),l(p,i,r)}if(i.path){i.params={};for(var u=0;u<o.length;u++){var h=o[u],g=a[h];if(Xs(g.regex,i.path,i.params))return l(g,i,r)}}return l(null,i)}function i(n,t){var o=n.redirect,a="function"==typeof o?o(ms(n,t,null,e)):o;if("string"==typeof a&&(a={path:a}),!a||"object"!=typeof a)return l(null,t);var i=a,c=i.name,p=i.path,d=t.query,m=t.hash,u=t.params;if(d=i.hasOwnProperty("query")?i.query:d,m=i.hasOwnProperty("hash")?i.hash:m,u=i.hasOwnProperty("params")?i.params:u,c){s[c];return r({_normalized:!0,name:c,query:d,hash:m,params:u},void 0,t)}if(p){var h=function(n,e){return xs(n,e.parent?e.parent.path:"/",!0)}(p,n);return r({_normalized:!0,path:Ns(h,u),query:d,hash:m},void 0,t)}return l(null,t)}function l(n,t,o){return n&&n.redirect?i(n,o||t):n&&n.matchAs?function(n,e,t){var o=r({_normalized:!0,path:Ns(t,e.params)});if(o){var a=o.matched,s=a[a.length-1];return e.params=o.params,l(s,e)}return l(null,e)}(0,t,n.matchAs):ms(n,t,o,e)}return{match:r,addRoute:function(n,e){var t="object"!=typeof n?s[n]:void 0;Ks([e||n],o,a,s,t),t&&t.alias.length&&Ks(t.alias.map((function(n){return{path:n,children:[e]}})),o,a,s,t)},getRoutes:function(){return o.map((function(n){return a[n]}))},addRoutes:function(n){Ks(n,o,a,s)}}}function Xs(n,e,t){var o=e.match(n);if(!o)return!1;if(!t)return!0;for(var a=1,s=o.length;a<s;++a){var r=n.keys[a-1];r&&(t[r.name||"pathMatch"]="string"==typeof o[a]?is(o[a]):o[a])}return!0}var Qs=Zs&&window.performance&&window.performance.now?window.performance:Date;function Js(){return Qs.now().toFixed(3)}var nr=Js();function er(){return nr}function tr(n){return nr=n}var or=Object.create(null);function ar(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=ts({},window.history.state);return t.key=er(),window.history.replaceState(t,"",e),window.addEventListener("popstate",ir),function(){window.removeEventListener("popstate",ir)}}function sr(n,e,t,o){if(n.app){var a=n.options.scrollBehavior;a&&n.app.$nextTick((function(){var s=function(){var n=er();if(n)return or[n]}(),r=a.call(n,e,t,o?s:null);r&&("function"==typeof r.then?r.then((function(n){mr(n,s)})).catch((function(n){0})):mr(r,s))}))}}function rr(){var n=er();n&&(or[n]={x:window.pageXOffset,y:window.pageYOffset})}function ir(n){rr(),n.state&&n.state.key&&tr(n.state.key)}function lr(n){return pr(n.x)||pr(n.y)}function cr(n){return{x:pr(n.x)?n.x:window.pageXOffset,y:pr(n.y)?n.y:window.pageYOffset}}function pr(n){return"number"==typeof n}var dr=/^#\d/;function mr(n,e){var t,o="object"==typeof n;if(o&&"string"==typeof n.selector){var a=dr.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(a){var s=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),o=n.getBoundingClientRect();return{x:o.left-t.left-e.x,y:o.top-t.top-e.y}}(a,s={x:pr((t=s).x)?t.x:0,y:pr(t.y)?t.y:0})}else lr(n)&&(e=cr(n))}else o&&lr(n)&&(e=cr(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var ur,hr=Zs&&((-1===(ur=window.navigator.userAgent).indexOf("Android 2.")&&-1===ur.indexOf("Android 4.0")||-1===ur.indexOf("Mobile Safari")||-1!==ur.indexOf("Chrome")||-1!==ur.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function gr(n,e){rr();var t=window.history;try{if(e){var o=ts({},t.state);o.key=er(),t.replaceState(o,"",n)}else t.pushState({key:tr(Js())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function br(n){gr(n,!0)}var fr={redirected:2,aborted:4,cancelled:8,duplicated:16};function yr(n,e){return vr(n,e,fr.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return kr.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function _r(n,e){return vr(n,e,fr.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function vr(n,e,t,o){var a=new Error(o);return a._isRouter=!0,a.from=n,a.to=e,a.type=t,a}var kr=["params","query","hash"];function xr(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function wr(n,e){return xr(n)&&n._isRouter&&(null==e||n.type===e)}function Er(n,e,t){var o=function(a){a>=n.length?t():n[a]?e(n[a],(function(){o(a+1)})):o(a+1)};o(0)}function Ar(n){return function(e,t,o){var a=!1,s=0,r=null;zr(n,(function(n,e,t,i){if("function"==typeof n&&void 0===n.cid){a=!0,s++;var l,c=Sr((function(e){var a;((a=e).__esModule||Tr&&"Module"===a[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:Us.extend(e),t.components[i]=e,--s<=0&&o()})),p=Sr((function(n){var e="Failed to resolve async component "+i+": "+n;r||(r=xr(n)?n:new Error(e),o(r))}));try{l=n(c,p)}catch(n){p(n)}if(l)if("function"==typeof l.then)l.then(c,p);else{var d=l.component;d&&"function"==typeof d.then&&d.then(c,p)}}})),a||o()}}function zr(n,e){return Br(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function Br(n){return Array.prototype.concat.apply([],n)}var Tr="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function Sr(n){var e=!1;return function(){for(var t=[],o=arguments.length;o--;)t[o]=arguments[o];if(!e)return e=!0,n.apply(this,t)}}var $r=function(n,e){this.router=n,this.base=function(n){if(!n)if(Zs){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=hs,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function jr(n,e,t,o){var a=zr(n,(function(n,o,a,s){var r=function(n,e){"function"!=typeof n&&(n=Us.extend(n));return n.options[e]}(n,e);if(r)return Array.isArray(r)?r.map((function(n){return t(n,o,a,s)})):t(r,o,a,s)}));return Br(o?a.reverse():a)}function Ir(n,e){if(e)return function(){return n.apply(e,arguments)}}$r.prototype.listen=function(n){this.cb=n},$r.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},$r.prototype.onError=function(n){this.errorCbs.push(n)},$r.prototype.transitionTo=function(n,e,t){var o,a=this;try{o=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var s=this.current;this.confirmTransition(o,(function(){a.updateRoute(o),e&&e(o),a.ensureURL(),a.router.afterHooks.forEach((function(n){n&&n(o,s)})),a.ready||(a.ready=!0,a.readyCbs.forEach((function(n){n(o)})))}),(function(n){t&&t(n),n&&!a.ready&&(wr(n,fr.redirected)&&s===hs||(a.ready=!0,a.readyErrorCbs.forEach((function(e){e(n)}))))}))},$r.prototype.confirmTransition=function(n,e,t){var o=this,a=this.current;this.pending=n;var s,r,i=function(n){!wr(n)&&xr(n)&&(o.errorCbs.length?o.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},l=n.matched.length-1,c=a.matched.length-1;if(fs(n,a)&&l===c&&n.matched[l]===a.matched[c])return this.ensureURL(),n.hash&&sr(this.router,a,n,!1),i(((r=vr(s=a,n,fr.duplicated,'Avoided redundant navigation to current location: "'+s.fullPath+'".')).name="NavigationDuplicated",r));var p=function(n,e){var t,o=Math.max(n.length,e.length);for(t=0;t<o&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),d=p.updated,m=p.deactivated,u=p.activated,h=[].concat(function(n){return jr(n,"beforeRouteLeave",Ir,!0)}(m),this.router.beforeHooks,function(n){return jr(n,"beforeRouteUpdate",Ir)}(d),u.map((function(n){return n.beforeEnter})),Ar(u)),g=function(e,t){if(o.pending!==n)return i(_r(a,n));try{e(n,a,(function(e){!1===e?(o.ensureURL(!0),i(function(n,e){return vr(n,e,fr.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(a,n))):xr(e)?(o.ensureURL(!0),i(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(i(yr(a,n)),"object"==typeof e&&e.replace?o.replace(e):o.push(e)):t(e)}))}catch(n){i(n)}};Er(h,g,(function(){Er(function(n){return jr(n,"beforeRouteEnter",(function(n,e,t,o){return function(n,e,t){return function(o,a,s){return n(o,a,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),s(n)}))}}(n,t,o)}))}(u).concat(o.router.resolveHooks),g,(function(){if(o.pending!==n)return i(_r(a,n));o.pending=null,e(n),o.router.app&&o.router.app.$nextTick((function(){_s(n)}))}))}))},$r.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},$r.prototype.setupListeners=function(){},$r.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=hs,this.pending=null};var Pr=function(n){function e(e,t){n.call(this,e,t),this._startLocation=Dr(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,o=hr&&t;o&&this.listeners.push(ar());var a=function(){var t=n.current,a=Dr(n.base);n.current===hs&&a===n._startLocation||n.transitionTo(a,(function(n){o&&sr(e,n,t,!0)}))};window.addEventListener("popstate",a),this.listeners.push((function(){window.removeEventListener("popstate",a)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var o=this,a=this.current;this.transitionTo(n,(function(n){gr(ws(o.base+n.fullPath)),sr(o.router,n,a,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var o=this,a=this.current;this.transitionTo(n,(function(n){br(ws(o.base+n.fullPath)),sr(o.router,n,a,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(Dr(this.base)!==this.current.fullPath){var e=ws(this.base+this.current.fullPath);n?gr(e):br(e)}},e.prototype.getCurrentLocation=function(){return Dr(this.base)},e}($r);function Dr(n){var e=window.location.pathname,t=e.toLowerCase(),o=n.toLowerCase();return!n||t!==o&&0!==t.indexOf(ws(o+"/"))||(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var Cr=function(n){function e(e,t,o){n.call(this,e,t),o&&function(n){var e=Dr(n);if(!/^\/#/.test(e))return window.location.replace(ws(n+"/#"+e)),!0}(this.base)||Rr()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=hr&&e;t&&this.listeners.push(ar());var o=function(){var e=n.current;Rr()&&n.transitionTo(Or(),(function(o){t&&sr(n.router,o,e,!0),hr||qr(o.fullPath)}))},a=hr?"popstate":"hashchange";window.addEventListener(a,o),this.listeners.push((function(){window.removeEventListener(a,o)}))}},e.prototype.push=function(n,e,t){var o=this,a=this.current;this.transitionTo(n,(function(n){Mr(n.fullPath),sr(o.router,n,a,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var o=this,a=this.current;this.transitionTo(n,(function(n){qr(n.fullPath),sr(o.router,n,a,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;Or()!==e&&(n?Mr(e):qr(e))},e.prototype.getCurrentLocation=function(){return Or()},e}($r);function Rr(){var n=Or();return"/"===n.charAt(0)||(qr("/"+n),!1)}function Or(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function Lr(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function Mr(n){hr?gr(Lr(n)):window.location.hash=n}function qr(n){hr?br(Lr(n)):window.location.replace(Lr(n))}var Nr=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var o=this;this.transitionTo(n,(function(n){o.stack=o.stack.slice(0,o.index+1).concat(n),o.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var o=this;this.transitionTo(n,(function(n){o.stack=o.stack.slice(0,o.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var o=this.stack[t];this.confirmTransition(o,(function(){var n=e.current;e.index=t,e.updateRoute(o),e.router.afterHooks.forEach((function(e){e&&e(o,n)}))}),(function(n){wr(n,fr.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}($r),Fr=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Ys(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!hr&&!1!==n.fallback,this.fallback&&(e="hash"),Zs||(e="abstract"),this.mode=e,e){case"history":this.history=new Pr(this,n.base);break;case"hash":this.history=new Cr(this,n.base,this.fallback);break;case"abstract":this.history=new Nr(this,n.base);break;default:0}},Ur={currentRoute:{configurable:!0}};Fr.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},Ur.currentRoute.get=function(){return this.history&&this.history.current},Fr.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof Pr||t instanceof Cr){var o=function(n){t.setupListeners(),function(n){var o=t.current,a=e.options.scrollBehavior;hr&&a&&"fullPath"in n&&sr(e,n,o,!1)}(n)};t.transitionTo(t.getCurrentLocation(),o,o)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},Fr.prototype.beforeEach=function(n){return Hr(this.beforeHooks,n)},Fr.prototype.beforeResolve=function(n){return Hr(this.resolveHooks,n)},Fr.prototype.afterEach=function(n){return Hr(this.afterHooks,n)},Fr.prototype.onReady=function(n,e){this.history.onReady(n,e)},Fr.prototype.onError=function(n){this.history.onError(n)},Fr.prototype.push=function(n,e,t){var o=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){o.history.push(n,e,t)}));this.history.push(n,e,t)},Fr.prototype.replace=function(n,e,t){var o=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){o.history.replace(n,e,t)}));this.history.replace(n,e,t)},Fr.prototype.go=function(n){this.history.go(n)},Fr.prototype.back=function(){this.go(-1)},Fr.prototype.forward=function(){this.go(1)},Fr.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},Fr.prototype.resolve=function(n,e,t){var o=Fs(n,e=e||this.history.current,t,this),a=this.match(o,e),s=a.redirectedFrom||a.fullPath;return{location:o,route:a,href:function(n,e,t){var o="hash"===t?"#"+e:e;return n?ws(n+"/"+o):o}(this.history.base,s,this.mode),normalizedTo:o,resolved:a}},Fr.prototype.getRoutes=function(){return this.matcher.getRoutes()},Fr.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==hs&&this.history.transitionTo(this.history.getCurrentLocation())},Fr.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==hs&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(Fr.prototype,Ur);var Gr=Fr;function Hr(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}Fr.install=function n(e){if(!n.installed||Us!==e){n.installed=!0,Us=e;var t=function(n){return void 0!==n},o=function(n,e){var o=n.$options._parentVnode;t(o)&&t(o=o.data)&&t(o=o.registerRouteInstance)&&o(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,o(this,this)},destroyed:function(){o(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",vs),e.component("RouterLink",Hs);var a=e.config.optionMergeStrategies;a.beforeRouteEnter=a.beforeRouteLeave=a.beforeRouteUpdate=a.created}},Fr.version="3.6.5",Fr.isNavigationFailure=wr,Fr.NavigationFailureType=fr,Fr.START_LOCATION=hs,Zs&&window.Vue&&window.Vue.use(Fr);t(108);t(18),t(105);var Vr={NotFound:()=>Promise.all([t.e(0),t.e(6)]).then(t.bind(null,353)),Layout:()=>Promise.all([t.e(0),t.e(2)]).then(t.bind(null,351))},Zr={"v-12697179":()=>t.e(9).then(t.bind(null,356)),"v-f6d557ce":()=>t.e(10).then(t.bind(null,357)),"v-460f57d9":()=>t.e(12).then(t.bind(null,358)),"v-538f7059":()=>t.e(13).then(t.bind(null,359)),"v-e8898224":()=>t.e(14).then(t.bind(null,360)),"v-5bfdcf08":()=>t.e(15).then(t.bind(null,361)),"v-125e2d39":()=>t.e(11).then(t.bind(null,362)),"v-390cfdc4":()=>t.e(16).then(t.bind(null,363)),"v-d4660b6a":()=>t.e(17).then(t.bind(null,364)),"v-00ab4999":()=>t.e(19).then(t.bind(null,365)),"v-508a552b":()=>t.e(18).then(t.bind(null,366)),"v-06859f1a":()=>t.e(20).then(t.bind(null,367)),"v-c48b57a8":()=>t.e(22).then(t.bind(null,368)),"v-6d0e3699":()=>t.e(23).then(t.bind(null,369)),"v-ba6e35ce":()=>t.e(24).then(t.bind(null,370)),"v-3ae46072":()=>t.e(21).then(t.bind(null,371)),"v-799b0d32":()=>t.e(25).then(t.bind(null,372)),"v-f1cf5a38":()=>t.e(27).then(t.bind(null,373)),"v-3e101fea":()=>t.e(28).then(t.bind(null,374)),"v-7c1d5117":()=>t.e(26).then(t.bind(null,375)),"v-d172e978":()=>t.e(29).then(t.bind(null,376)),"v-21c554e7":()=>t.e(31).then(t.bind(null,377)),"v-7a7009dc":()=>t.e(32).then(t.bind(null,378)),"v-7492cf63":()=>t.e(30).then(t.bind(null,379)),"v-9ffde008":()=>t.e(33).then(t.bind(null,380)),"v-16ca57ce":()=>t.e(34).then(t.bind(null,381)),"v-9c028ada":()=>t.e(35).then(t.bind(null,382)),"v-3a928a43":()=>t.e(36).then(t.bind(null,383)),"v-6bdef9fc":()=>t.e(37).then(t.bind(null,384)),"v-f0715856":()=>t.e(38).then(t.bind(null,385)),"v-23f7a879":()=>t.e(39).then(t.bind(null,386)),"v-6f8ad9d9":()=>t.e(40).then(t.bind(null,387)),"v-0d3f68c1":()=>t.e(41).then(t.bind(null,388)),"v-d692e0c2":()=>t.e(42).then(t.bind(null,389)),"v-4e6cb445":()=>t.e(43).then(t.bind(null,390)),"v-12346ac4":()=>t.e(44).then(t.bind(null,391)),"v-5a9f7a7c":()=>t.e(45).then(t.bind(null,392)),"v-4b66e9c6":()=>t.e(46).then(t.bind(null,393)),"v-dd6a2a08":()=>t.e(47).then(t.bind(null,394)),"v-b8800ef2":()=>t.e(48).then(t.bind(null,395)),"v-9066c126":()=>t.e(49).then(t.bind(null,396)),"v-5aabcc3a":()=>t.e(51).then(t.bind(null,397)),"v-099c9ca4":()=>t.e(50).then(t.bind(null,398)),"v-5436895e":()=>t.e(52).then(t.bind(null,399)),"v-5cac2a3b":()=>t.e(53).then(t.bind(null,400)),"v-228a9bce":()=>t.e(54).then(t.bind(null,401)),"v-4c3a43d2":()=>t.e(55).then(t.bind(null,402)),"v-765df57e":()=>t.e(56).then(t.bind(null,403)),"v-e52dd084":()=>t.e(57).then(t.bind(null,404)),"v-e2ff91da":()=>t.e(58).then(t.bind(null,405)),"v-df98990e":()=>t.e(59).then(t.bind(null,406)),"v-303fed36":()=>t.e(60).then(t.bind(null,407)),"v-e138df4c":()=>t.e(61).then(t.bind(null,408)),"v-19ff8e6a":()=>t.e(62).then(t.bind(null,409)),"v-24ef5f44":()=>t.e(63).then(t.bind(null,410)),"v-403a5ccb":()=>t.e(64).then(t.bind(null,411)),"v-0542148a":()=>t.e(65).then(t.bind(null,412)),"v-7246ae2c":()=>t.e(66).then(t.bind(null,413)),"v-0580956a":()=>t.e(67).then(t.bind(null,414)),"v-71ab6464":()=>t.e(68).then(t.bind(null,415)),"v-2e07d1a0":()=>t.e(69).then(t.bind(null,416)),"v-6c2001b3":()=>t.e(70).then(t.bind(null,417)),"v-6ba31c1d":()=>t.e(71).then(t.bind(null,418)),"v-a840ca1e":()=>t.e(72).then(t.bind(null,419)),"v-8d36e636":()=>t.e(73).then(t.bind(null,420)),"v-08986d40":()=>t.e(74).then(t.bind(null,421)),"v-38a8ed18":()=>t.e(75).then(t.bind(null,422)),"v-45252678":()=>t.e(76).then(t.bind(null,423)),"v-c86447e2":()=>t.e(77).then(t.bind(null,424)),"v-1ae891a8":()=>t.e(78).then(t.bind(null,425)),"v-0aaf2f47":()=>t.e(79).then(t.bind(null,426)),"v-11090b27":()=>t.e(80).then(t.bind(null,427)),"v-2f584880":()=>t.e(81).then(t.bind(null,428)),"v-52cf205e":()=>t.e(82).then(t.bind(null,429)),"v-44ab5907":()=>t.e(83).then(t.bind(null,430)),"v-e8e0b9ea":()=>t.e(84).then(t.bind(null,431)),"v-ba2529e4":()=>t.e(85).then(t.bind(null,432)),"v-2c290406":()=>t.e(86).then(t.bind(null,433)),"v-78111152":()=>t.e(87).then(t.bind(null,434)),"v-c1c1c24e":()=>t.e(88).then(t.bind(null,435)),"v-0cc13236":()=>t.e(89).then(t.bind(null,436)),"v-0df2a669":()=>t.e(90).then(t.bind(null,437)),"v-74be9f5b":()=>t.e(91).then(t.bind(null,438)),"v-38030407":()=>t.e(92).then(t.bind(null,439)),"v-0576e07d":()=>t.e(93).then(t.bind(null,440)),"v-8488e3a8":()=>t.e(94).then(t.bind(null,441)),"v-515e9f9a":()=>t.e(95).then(t.bind(null,442)),"v-66a7ae24":()=>t.e(98).then(t.bind(null,443)),"v-084ed8e0":()=>t.e(97).then(t.bind(null,444)),"v-9198a8e6":()=>t.e(99).then(t.bind(null,445)),"v-28967f21":()=>t.e(96).then(t.bind(null,446)),"v-44bb153f":()=>t.e(101).then(t.bind(null,447)),"v-0cf4e9e4":()=>t.e(100).then(t.bind(null,448)),"v-c30db28e":()=>t.e(102).then(t.bind(null,449)),"v-477589d8":()=>t.e(103).then(t.bind(null,450)),"v-0bebf3bd":()=>t.e(105).then(t.bind(null,451)),"v-ed9b282c":()=>t.e(104).then(t.bind(null,452)),"v-0a29d12e":()=>t.e(106).then(t.bind(null,453)),"v-09ffdab8":()=>t.e(107).then(t.bind(null,454)),"v-b2ae320e":()=>t.e(110).then(t.bind(null,455)),"v-1ebf1e36":()=>t.e(108).then(t.bind(null,456)),"v-447429d0":()=>t.e(111).then(t.bind(null,457)),"v-64021198":()=>t.e(112).then(t.bind(null,458)),"v-c4242b76":()=>t.e(113).then(t.bind(null,459)),"v-6feefc5e":()=>t.e(114).then(t.bind(null,460)),"v-28dc669e":()=>t.e(115).then(t.bind(null,461)),"v-65a31f9a":()=>t.e(117).then(t.bind(null,462)),"v-db6593ce":()=>t.e(109).then(t.bind(null,463)),"v-3ed70d81":()=>t.e(116).then(t.bind(null,464)),"v-573ab138":()=>t.e(118).then(t.bind(null,465)),"v-20b39df5":()=>t.e(119).then(t.bind(null,466)),"v-0f41bfdd":()=>t.e(120).then(t.bind(null,467)),"v-28471c42":()=>t.e(121).then(t.bind(null,468)),"v-2e1ddaa3":()=>t.e(122).then(t.bind(null,469)),"v-10b832a2":()=>t.e(125).then(t.bind(null,470)),"v-282498df":()=>t.e(126).then(t.bind(null,471)),"v-7b122aa1":()=>t.e(124).then(t.bind(null,472)),"v-787b413e":()=>t.e(123).then(t.bind(null,473)),"v-68775ace":()=>t.e(127).then(t.bind(null,474)),"v-3bd0d01e":()=>t.e(129).then(t.bind(null,475)),"v-1955a3de":()=>t.e(128).then(t.bind(null,476)),"v-8105e1aa":()=>t.e(130).then(t.bind(null,477)),"v-c5ed0f8c":()=>t.e(131).then(t.bind(null,478)),"v-20df7df7":()=>t.e(132).then(t.bind(null,479)),"v-4008dcfe":()=>t.e(133).then(t.bind(null,480)),"v-6f6532a7":()=>t.e(134).then(t.bind(null,481)),"v-003c987e":()=>t.e(135).then(t.bind(null,482)),"v-403d9ae8":()=>t.e(139).then(t.bind(null,483)),"v-75545c56":()=>t.e(136).then(t.bind(null,484)),"v-17a259f2":()=>t.e(137).then(t.bind(null,485)),"v-50528ad2":()=>t.e(138).then(t.bind(null,486)),"v-65b5a670":()=>t.e(140).then(t.bind(null,487)),"v-8923d930":()=>t.e(141).then(t.bind(null,488)),"v-7e9c1840":()=>t.e(142).then(t.bind(null,489))};function Kr(n){const e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}const Wr=/-(\w)/g,Yr=Kr(n=>n.replace(Wr,(n,e)=>e?e.toUpperCase():"")),Xr=/\B([A-Z])/g,Qr=Kr(n=>n.replace(Xr,"-$1").toLowerCase()),Jr=Kr(n=>n.charAt(0).toUpperCase()+n.slice(1));function ni(n,e){if(!e)return;if(n(e))return n(e);return e.includes("-")?n(Jr(Yr(e))):n(Jr(e))||n(Qr(e))}const ei=Object.assign({},Vr,Zr),ti=n=>ei[n],oi=n=>Zr[n],ai=n=>Vr[n],si=n=>Vt.component(n);function ri(n){return ni(oi,n)}function ii(n){return ni(ai,n)}function li(n){return ni(ti,n)}function ci(n){return ni(si,n)}function pi(...n){return Promise.all(n.filter(n=>n).map(async n=>{if(!ci(n)&&li(n)){const e=await li(n)();Vt.component(n,e.default)}}))}function di(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}var mi=t(95),ui=t.n(mi),hi=t(96),gi=t.n(hi),bi={created(){if(this.siteMeta=this.$site.headTags.filter(([n])=>"meta"===n).map(([n,e])=>e),this.$ssrContext){const e=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(n=e)?n.map(n=>{let e="<meta";return Object.keys(n).forEach(t=>{e+=` ${t}="${gi()(n[t])}"`}),e+">"}).join("\n    "):"",this.$ssrContext.canonicalLink=yi(this.$canonicalUrl)}var n},mounted(){this.currentMetaTags=[...document.querySelectorAll("meta")],this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta(){document.title=this.$title,document.documentElement.lang=this.$lang;const n=this.getMergedMetaTags();this.currentMetaTags=_i(n,this.currentMetaTags)},getMergedMetaTags(){const n=this.$page.frontmatter.meta||[];return ui()([{name:"description",content:this.$description}],n,this.siteMeta,vi)},updateCanonicalLink(){fi(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",yi(this.$canonicalUrl))}},watch:{$page(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy(){_i(null,this.currentMetaTags),fi()}};function fi(){const n=document.querySelector("link[rel='canonical']");n&&n.remove()}function yi(n=""){return n?`<link href="${n}" rel="canonical" />`:""}function _i(n,e){if(e&&[...e].filter(n=>n.parentNode===document.head).forEach(n=>document.head.removeChild(n)),n)return n.map(n=>{const e=document.createElement("meta");return Object.keys(n).forEach(t=>{e.setAttribute(t,n[t])}),document.head.appendChild(e),e})}function vi(n){for(const e of["name","property","itemprop"])if(n.hasOwnProperty(e))return n[e]+e;return JSON.stringify(n)}var ki=t(51),xi={mounted(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(ki)()((function(){this.setActiveHash()}),300),setActiveHash(){const n=[].slice.call(document.querySelectorAll(".sidebar-link")),e=[].slice.call(document.querySelectorAll(".header-anchor")).filter(e=>n.some(n=>n.hash===e.hash)),t=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),o=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),a=window.innerHeight+t;for(let n=0;n<e.length;n++){const s=e[n],r=e[n+1],i=0===n&&0===t||t>=s.parentElement.offsetTop+10&&(!r||t<r.parentElement.offsetTop-10),l=decodeURIComponent(this.$route.hash);if(i&&l!==decodeURIComponent(s.hash)){const t=s;if(a===o)for(let t=n+1;t<e.length;t++)if(l===decodeURIComponent(e[t].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(t.hash),()=>{this.$nextTick(()=>{this.$vuepress.$set("disableScrollBehavior",!1)})})}}}},beforeDestroy(){window.removeEventListener("scroll",this.onScroll)}},wi=t(27),Ei=t.n(wi),Ai={mounted(){Ei.a.configure({showSpinner:!1}),this.$router.beforeEach((n,e,t)=>{n.path===e.path||Vt.component(n.name)||Ei.a.start(),t()}),this.$router.afterEach(()=>{Ei.a.done(),this.isSidebarOpen=!1})}};t(243),t(244);class zi{constructor(){this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}show({text:n="",duration:e=3e3}){let t=document.createElement("div");t.className="message move-in",t.innerHTML=`\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">${n}</div>\n    `,this.containerEl.appendChild(t),e>0&&setTimeout(()=>{this.close(t)},e)}close(n){n.className=n.className.replace("move-in",""),n.className+="move-out",n.addEventListener("animationend",()=>{n.remove()})}}var Bi={mounted(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy(){setTimeout(()=>{(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach(n=>{document.querySelectorAll(n).forEach(this.generateCopyButton)})},1e3)},generateCopyButton(n){if(n.classList.contains("codecopy-enabled"))return;const e=document.createElement("i");e.className="code-copy",e.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',e.title="Copy to clipboard",e.addEventListener("click",()=>{this.copyToClipboard(n.innerText)}),n.appendChild(e),n.classList.add("codecopy-enabled")},copyToClipboard(n){const e=document.createElement("textarea");e.value=n,e.setAttribute("readonly",""),e.style.position="absolute",e.style.left="-9999px",document.body.appendChild(e);const t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);e.select(),document.execCommand("copy");(new zi).show({text:"复制成功 🎉",duration:1e3}),document.body.removeChild(e),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}};!function(n,e){void 0===e&&(e={});var t=e.insertAt;if(n&&"undefined"!=typeof document){var o=document.head||document.getElementsByTagName("head")[0],a=document.createElement("style");a.type="text/css","top"===t&&o.firstChild?o.insertBefore(a,o.firstChild):o.appendChild(a),a.styleSheet?a.styleSheet.cssText=n:a.appendChild(document.createTextNode(n))}}("@media (max-width: 1000px) {\n  .vuepress-plugin-demo-block__h_code {\n    display: none;\n  }\n  .vuepress-plugin-demo-block__app {\n    margin-left: auto !important;\n    margin-right: auto !important;\n  }\n}\n.vuepress-plugin-demo-block__wrapper {\n  margin-top: 10px;\n  border: 1px solid #ebebeb;\n  border-radius: 4px;\n  transition: all 0.2s;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display {\n  height: 400px;\n  display: flex;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__app {\n  width: 300px;\n  border: 1px solid #ebebeb;\n  box-shadow: 1px 1px 3px #ebebeb;\n  margin-right: 5px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code {\n  flex: 1;\n  overflow: auto;\n  height: 100%;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code > pre {\n  overflow: visible;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  max-height: 400px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper div {\n  box-sizing: border-box;\n}\n.vuepress-plugin-demo-block__wrapper:hover {\n  box-shadow: 0 0 11px rgba(33, 33, 33, 0.2);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code {\n  overflow: hidden;\n  height: 0;\n  padding: 0 !important;\n  background-color: #282c34;\n  border-radius: 0 !important;\n  transition: height 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code pre {\n  margin: 0 !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  padding: 20px;\n  border-bottom: 1px solid #ebebeb;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer {\n  position: relative;\n  text-align: center;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__codepen {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__expand::before {\n  border-top: none;\n  border-right: 6px solid transparent;\n  border-bottom: 6px solid #ccc;\n  border-left: 6px solid transparent;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__codepen,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand span,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand::before {\n  border-top-color: #3eaf7c !important;\n  border-bottom-color: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover svg {\n  fill: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand-text {\n  transition: all 0.5s;\n  opacity: 0;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:nth-last-child(2) {\n  right: 50px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:last-child {\n  right: 10px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button {\n  border-color: transparent;\n  background-color: transparent;\n  font-size: 14px;\n  color: #3eaf7c;\n  cursor: pointer;\n  outline: none;\n  margin: 0;\n  width: 46px;\n  position: relative;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::before {\n  content: attr(data-tip);\n  white-space: nowrap;\n  position: absolute;\n  top: -30px;\n  left: 50%;\n  color: #eee;\n  line-height: 1;\n  z-index: 1000;\n  border-radius: 4px;\n  padding: 6px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  background-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::after {\n  content: '' !important;\n  display: block;\n  position: absolute;\n  left: 50%;\n  top: -5px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  border: 5px solid transparent;\n  border-top-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button svg {\n  width: 34px;\n  height: 20px;\n  fill: #ccc;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__codepen {\n  position: absolute;\n  top: 10px;\n  transition: all 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand {\n  position: relative;\n  width: 100px;\n  height: 40px;\n  margin: 0;\n  color: #3eaf7c;\n  font-size: 14px;\n  background-color: transparent;\n  border-color: transparent;\n  outline: none;\n  transition: all 0.5s;\n  cursor: pointer;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand::before {\n  content: \"\";\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  width: 0;\n  height: 0;\n  border-top: 6px solid #ccc;\n  border-right: 6px solid transparent;\n  border-left: 6px solid transparent;\n  -webkit-transform: translate(-50%, -50%);\n          transform: translate(-50%, -50%);\n}\n");var Ti={jsLib:[],cssLib:[],jsfiddle:!0,codepen:!0,codepenLayout:"left",codepenJsProcessor:"babel",codepenEditors:"101",horizontal:!1,vue:"https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js",react:"https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js",reactDOM:"https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"},Si={},$i=function(n){return'<div id="app">\n'.concat(n,"\n</div>")},ji=function(n){return window.$VUEPRESS_DEMO_BLOCK&&void 0!==window.$VUEPRESS_DEMO_BLOCK[n]?window.$VUEPRESS_DEMO_BLOCK[n]:Ti[n]},Ii=function n(e,t,o){var a=document.createElement(e);return t&&Object.keys(t).forEach((function(n){if(n.indexOf("data"))a[n]=t[n];else{var e=n.replace("data","");a.dataset[e]=t[n]}})),o&&o.forEach((function(e){var t=e.tag,o=e.attrs,s=e.children;a.appendChild(n(t,o,s))})),a},Pi=function(n,e,t){var o,a=(o=n.querySelectorAll(".".concat(e)),Array.prototype.slice.call(o));return 1!==a.length||t?a:a[0]},Di=function(n,e){var t,o,a=n.match(/<style>([\s\S]+)<\/style>/),s=n.match(/<template>([\s\S]+)<\/template>/),r=n.match(/<script>([\s\S]+)<\/script>/),i={css:a&&a[1].replace(/^\n|\n$/g,""),html:s&&s[1].replace(/^\n|\n$/g,""),js:r&&r[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};i.htmlTpl=$i(i.html),i.jsTpl=(t=i.js,o=t.replace(/export\s+default\s*?\{\n*/,"").replace(/\n*\}\s*$/,"").trim(),"new Vue({\n  el: '#app',\n  ".concat(o,"\n})")),i.script=function(n,e){var t=n.split(/export\s+default/),o="(function() {".concat(t[0]," ; return ").concat(t[1],"})()"),a=window.Babel?window.Babel.transform(o,{presets:["es2015"]}).code:o,s=[eval][0](a);return s.template=e,s}(i.js,i.html);var l=ji("vue");return i.jsLib.unshift(l),i},Ci=function(n,e){var t,o=n.match(/<style>([\s\S]+)<\/style>/),a=n.match(/<html>([\s\S]+)<\/html>/),s=n.match(/<script>([\s\S]+)<\/script>/),r={css:o&&o[1].replace(/^\n|\n$/g,""),html:a&&a[1].replace(/^\n|\n$/g,""),js:s&&s[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};return r.htmlTpl=r.html,r.jsTpl=r.js,r.script=(t=r.js,window.Babel?window.Babel.transform(t,{presets:["es2015"]}).code:t),r},Ri=function(n){return n=n.replace("export default ","").replace(/App\.__style__(\s*)=(\s*)`([\s\S]*)?`/,""),n+='ReactDOM.render(React.createElement(App), document.getElementById("app"))'};function Oi(){var n=Pi(document,"vuepress-plugin-demo-block__wrapper",!0);n.length?n.forEach((function(n){if("true"!==n.dataset.created){n.style.display="block";var e=Pi(n,"vuepress-plugin-demo-block__code"),t=Pi(n,"vuepress-plugin-demo-block__display"),o=Pi(n,"vuepress-plugin-demo-block__footer"),a=Pi(t,"vuepress-plugin-demo-block__app"),s=decodeURIComponent(n.dataset.code),r=decodeURIComponent(n.dataset.config),i=decodeURIComponent(n.dataset.type);r=r?JSON.parse(r):{};var l=e.querySelector("div").clientHeight,c="react"===i?function(n,e){var t=(0,window.Babel.transform)(n,{presets:["es2015","react"]}).code,o="(function(exports){var module={};module.exports=exports;".concat(t,";return module.exports.__esModule?module.exports.default:module.exports;})({})"),a=new Function("return ".concat(o))(),s={js:a,css:a.__style__||"",jsLib:e.jsLib||[],cssLib:e.cssLib||[],jsTpl:Ri(n),htmlTpl:$i("")},r=ji("react"),i=ji("reactDOM");return s.jsLib.unshift(r,i),s}(s,r):"vanilla"===i?Ci(s,r):Di(s,r),p=Ii("button",{className:"".concat("vuepress-plugin-demo-block__expand")});if(o.appendChild(p),p.addEventListener("click",Li.bind(null,p,l,e,o)),ji("jsfiddle")&&o.appendChild(function(n){var e=n.css,t=n.htmlTpl,o=n.jsTpl,a=n.jsLib,s=n.cssLib,r=a.concat(s).concat(ji("cssLib")).concat(ji("jsLib")).join(",");return Ii("form",{className:"vuepress-plugin-demo-block__jsfiddle",target:"_blank",action:"https://jsfiddle.net/api/post/library/pure/",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"css",value:e}},{tag:"input",attrs:{type:"hidden",name:"html",value:t}},{tag:"input",attrs:{type:"hidden",name:"js",value:o}},{tag:"input",attrs:{type:"hidden",name:"panel_js",value:3}},{tag:"input",attrs:{type:"hidden",name:"wrap",value:1}},{tag:"input",attrs:{type:"hidden",name:"resources",value:r}},{tag:"button",attrs:{type:"submit",className:"vuepress-plugin-demo-block__button",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088289967" class="icon" style="" viewBox="0 0 1170 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1952" xmlns:xlink="http://www.w3.org/1999/xlink" width="228.515625" height="200"><defs><style type="text/css"></style></defs><path d="M1028.571429 441.142857q63.428571 26.285714 102.571428 83.142857T1170.285714 650.857143q0 93.714286-67.428571 160.285714T940 877.714286q-2.285714 0-6.571429-0.285715t-6-0.285714H232q-97.142857-5.714286-164.571429-71.714286T0 645.142857q0-62.857143 31.428571-116t84-84q-6.857143-22.285714-6.857142-46.857143 0-65.714286 46.857142-112t113.714286-46.285714q54.285714 0 98.285714 33.142857 42.857143-88 127.142858-141.714286t186.571428-53.714285q94.857143 0 174.857143 46T982.571429 248.571429t46.571428 172q0 3.428571-0.285714 10.285714t-0.285714 10.285714zM267.428571 593.142857q0 69.714286 48 110.285714t118.857143 40.571429q78.285714 0 137.142857-56.571429-9.142857-11.428571-27.142857-32.285714T519.428571 626.285714q-38.285714 37.142857-82.285714 37.142857-31.428571 0-53.428571-19.142857T361.714286 594.285714q0-30.285714 22-49.714285t52.285714-19.428572q25.142857 0 48.285714 12t41.714286 31.428572 37.142857 42.857142 39.428572 46.857143 44 42.857143 55.428571 31.428572 69.428571 12q69.142857 0 116.857143-40.857143T936 594.857143q0-69.142857-48-109.714286t-118.285714-40.571428q-81.714286 0-137.714286 55.428571l53.142857 61.714286q37.714286-36.571429 81.142857-36.571429 29.714286 0 52.571429 18.857143t22.857143 48q0 32.571429-21.142857 52.285714t-53.714286 19.714286q-24.571429 0-47.142857-12t-41.142857-31.428571-37.428572-42.857143-39.714286-46.857143-44.285714-42.857143-55.142857-31.428571T434.285714 444.571429q-69.714286 0-118.285714 40.285714T267.428571 593.142857z" p-id="1953"></path></svg>',datatip:"JSFiddle"}}])}(c)),ji("codepen")&&o.appendChild(function(n){var e=n.css,t=n.htmlTpl,o=n.jsTpl,a=n.jsLib,s=n.cssLib,r=JSON.stringify({css:e,html:t,js:o,js_external:a.concat(ji("jsLib")).join(";"),css_external:s.concat(ji("cssLib")).join(";"),layout:ji("codepenLayout"),js_pre_processor:ji("codepenJsProcessor"),editors:ji("codepenEditors")});return Ii("form",{className:"vuepress-plugin-demo-block__codepen",target:"_blank",action:"https://codepen.io/pen/define",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"data",value:r}},{tag:"button",attrs:{type:"submit",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088271207" class="icon" style="" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1737" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><defs><style type="text/css"></style></defs><path d="M123.428571 668l344.571429 229.714286v-205.142857L277.142857 565.142857z m-35.428571-82.285714l110.285714-73.714286-110.285714-73.714286v147.428572z m468 312l344.571429-229.714286-153.714286-102.857143-190.857143 127.428572v205.142857z m-44-281.714286l155.428571-104-155.428571-104-155.428571 104zM277.142857 458.857143l190.857143-127.428572V126.285714L123.428571 356z m548.571429 53.142857l110.285714 73.714286V438.285714z m-78.857143-53.142857l153.714286-102.857143-344.571429-229.714286v205.142857z m277.142857-102.857143v312q0 23.428571-19.428571 36.571429l-468 312q-12 7.428571-24.571429 7.428571t-24.571429-7.428571L19.428571 704.571429q-19.428571-13.142857-19.428571-36.571429V356q0-23.428571 19.428571-36.571429L487.428571 7.428571q12-7.428571 24.571429-7.428571t24.571429 7.428571l468 312q19.428571 13.142857 19.428571 36.571429z" p-id="1738"></path></svg>',className:"vuepress-plugin-demo-block__button",datatip:"Codepen"}}])}(c)),void 0!==r.horizontal?r.horizontal:ji("horizontal")){n.classList.add("vuepress-plugin-demo-block__horizontal");var d=e.firstChild.cloneNode(!0);d.classList.add("vuepress-plugin-demo-block__h_code"),t.appendChild(d)}if(c.css&&function(n){if(!Si[n]){var e=Ii("style",{innerHTML:n});document.body.appendChild(e),Si[n]=!0}}(c.css),"react"===i)ReactDOM.render(React.createElement(c.js),a);else if("vue"===i){var m=(new(Vue.extend(c.script))).$mount();a.appendChild(m.$el)}else"vanilla"===i&&(a.innerHTML=c.html,new Function("return (function(){".concat(c.script,"})()"))());n.dataset.created="true"}})):setTimeout((function(n){Oi()}),300)}function Li(n,e,t,o){var a="1"!==n.dataset.isExpand;t.style.height=a?"".concat(e,"px"):0,a?o.classList.add("vuepress-plugin-demo-block__show-link"):o.classList.remove("vuepress-plugin-demo-block__show-link"),n.dataset.isExpand=a?"1":"0"}var Mi={mounted:function(){window.$VUEPRESS_DEMO_BLOCK={jsfiddle:!1,codepen:!0,horizontal:!1},Oi()},updated:function(){Oi()}},qi="auto",Ni="zoom-in",Fi="zoom-out",Ui="grab",Gi="move";function Hi(n,e,t){var o=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],a={passive:!1};o?n.addEventListener(e,t,a):n.removeEventListener(e,t,a)}function Vi(n,e){if(n){var t=new Image;t.onload=function(){e&&e(t)},t.src=n}}function Zi(n){return n.dataset.original?n.dataset.original:"A"===n.parentNode.tagName?n.parentNode.getAttribute("href"):null}function Ki(n,e,t){!function(n){var e=Wi,t=Yi;if(n.transition){var o=n.transition;delete n.transition,n[e]=o}if(n.transform){var a=n.transform;delete n.transform,n[t]=a}}(e);var o=n.style,a={};for(var s in e)t&&(a[s]=o[s]||""),o[s]=e[s];return a}var Wi="transition",Yi="transform",Xi="transform",Qi="transitionend";var Ji=function(){},nl={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Ji,onClose:Ji,onGrab:Ji,onMove:Ji,onRelease:Ji,onBeforeOpen:Ji,onBeforeClose:Ji,onBeforeGrab:Ji,onBeforeRelease:Ji,onImageLoading:Ji,onImageLoaded:Ji},el={init:function(n){var e,t;e=this,t=n,Object.getOwnPropertyNames(Object.getPrototypeOf(e)).forEach((function(n){e[n]=e[n].bind(t)}))},click:function(n){if(n.preventDefault(),ol(n))return window.open(this.target.srcOriginal||n.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(n.currentTarget)},scroll:function(){var n=document.documentElement||document.body.parentNode||document.body,e=window.pageXOffset||n.scrollLeft,t=window.pageYOffset||n.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:e,y:t});var o=this.lastScrollPosition.x-e,a=this.lastScrollPosition.y-t,s=this.options.scrollThreshold;(Math.abs(a)>=s||Math.abs(o)>=s)&&(this.lastScrollPosition=null,this.close())},keydown:function(n){(function(n){return"Escape"===(n.key||n.code)||27===n.keyCode})(n)&&(this.released?this.close():this.release(this.close))},mousedown:function(n){if(tl(n)&&!ol(n)){n.preventDefault();var e=n.clientX,t=n.clientY;this.pressTimer=setTimeout(function(){this.grab(e,t)}.bind(this),200)}},mousemove:function(n){this.released||this.move(n.clientX,n.clientY)},mouseup:function(n){tl(n)&&!ol(n)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(n){n.preventDefault();var e=n.touches[0],t=e.clientX,o=e.clientY;this.pressTimer=setTimeout(function(){this.grab(t,o)}.bind(this),200)},touchmove:function(n){if(!this.released){var e=n.touches[0],t=e.clientX,o=e.clientY;this.move(t,o)}},touchend:function(n){(function(n){n.targetTouches.length})(n)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function tl(n){return 0===n.button}function ol(n){return n.metaKey||n.ctrlKey}var al={init:function(n){this.el=document.createElement("div"),this.instance=n,this.parent=document.body,Ki(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(n.options),Hi(this.el,"click",n.handler.clickOverlay.bind(n))},updateStyle:function(n){Ki(this.el,{zIndex:n.zIndex,backgroundColor:n.bgColor,transition:"opacity\n        "+n.transitionDuration+"s\n        "+n.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},sl="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},rl=function(){function n(n,e){for(var t=0;t<e.length;t++){var o=e[t];o.enumerable=o.enumerable||!1,o.configurable=!0,"value"in o&&(o.writable=!0),Object.defineProperty(n,o.key,o)}}return function(e,t,o){return t&&n(e.prototype,t),o&&n(e,o),e}}(),il=Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var o in t)Object.prototype.hasOwnProperty.call(t,o)&&(n[o]=t[o])}return n},ll={init:function(n,e){this.el=n,this.instance=e,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=Zi(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var n=this.instance.options,e=n.zIndex,t=n.enableGrab,o=n.transitionDuration,a=n.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:e+1,cursor:t?Ui:Fi,transition:Xi+"\n        "+o+"s\n        "+a,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=Ki(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,Ki(this.el,{transform:"none"})},grab:function(n,e,t){var o=cl(),a=o.x-n,s=o.y-e;Ki(this.el,{cursor:Gi,transform:"translate3d(\n        "+(this.translate.x+a)+"px, "+(this.translate.y+s)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(n,e,t){var o=cl(),a=o.x-n,s=o.y-e;Ki(this.el,{transition:Xi,transform:"translate3d(\n        "+(this.translate.x+a)+"px, "+(this.translate.y+s)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){Ki(this.el,this.styleClose)},restoreOpenStyle:function(){Ki(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var n=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var e=this.el.cloneNode(!1);e.setAttribute("src",this.srcOriginal),e.style.position="fixed",e.style.visibility="hidden",n.appendChild(e),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),n.removeChild(e)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var n=cl(),e=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:n.x-e,y:n.y-t}},calculateScale:function(){var n=this.el.dataset,e=n.zoomingHeight,t=n.zoomingWidth,o=this.instance.options,a=o.customSize,s=o.scaleBase;if(!a&&e&&t)return{x:t/this.rect.width,y:e/this.rect.height};if(a&&"object"===(void 0===a?"undefined":sl(a)))return{x:a.width/this.rect.width,y:a.height/this.rect.height};var r=this.rect.width/2,i=this.rect.height/2,l=cl(),c={x:l.x-r,y:l.y-i},p=c.x/r,d=c.y/i,m=s+Math.min(p,d);if(a&&"string"==typeof a){var u=t||this.el.naturalWidth,h=e||this.el.naturalHeight,g=parseFloat(a)*u/(100*this.rect.width),b=parseFloat(a)*h/(100*this.rect.height);if(m>g||m>b)return{x:g,y:b}}return{x:m,y:m}}};function cl(){var n=document.documentElement;return{x:Math.min(n.clientWidth,window.innerWidth)/2,y:Math.min(n.clientHeight,window.innerHeight)/2}}function pl(n,e,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(o){Hi(n,o,e[o],t)}))}var dl=function(){function n(e){!function(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}(this,n),this.target=Object.create(ll),this.overlay=Object.create(al),this.handler=Object.create(el),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=il({},nl,e),this.overlay.init(this),this.handler.init(this)}return rl(n,[{key:"listen",value:function(n){if("string"==typeof n)for(var e=document.querySelectorAll(n),t=e.length;t--;)this.listen(e[t]);else"IMG"===n.tagName&&(n.style.cursor=Ni,Hi(n,"click",this.handler.click),this.options.preloadImage&&Vi(Zi(n)));return this}},{key:"config",value:function(n){return n?(il(this.options,n),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(n){var e=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var o="string"==typeof n?document.querySelector(n):n;if("IMG"===o.tagName){if(this.options.onBeforeOpen(o),this.target.init(o,this),!this.options.preloadImage){var a=this.target.srcOriginal;null!=a&&(this.options.onImageLoading(o),Vi(a,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),Hi(document,"scroll",this.handler.scroll),Hi(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&Hi(window,"resize",this.handler.resizeWindow);var s=function n(){Hi(o,Qi,n,!1),e.lock=!1,e.target.upgradeSource(),e.options.enableGrab&&pl(document,e.handler,!0),t(o)};return Hi(o,Qi,s),this}}}},{key:"close",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=qi,this.overlay.fadeOut(),this.target.zoomOut(),Hi(document,"scroll",this.handler.scroll,!1),Hi(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&Hi(window,"resize",this.handler.resizeWindow,!1);var o=function o(){Hi(t,Qi,o,!1),n.shown=!1,n.lock=!1,n.target.downgradeSource(),n.options.enableGrab&&pl(document,n.handler,!1),n.target.restoreCloseStyle(),n.overlay.remove(),e(t)};return Hi(t,Qi,o),this}}},{key:"grab",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,o=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var a=this.target.el;this.options.onBeforeGrab(a),this.released=!1,this.target.grab(n,e,t);var s=function n(){Hi(a,Qi,n,!1),o(a)};return Hi(a,Qi,s),this}}},{key:"move",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,o=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=Gi,this.target.move(n,e,t);var a=this.target.el,s=function n(){Hi(a,Qi,n,!1),o(a)};return Hi(a,Qi,s),this}}},{key:"release",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=qi,this.target.restoreOpenStyle();var o=function o(){Hi(t,Qi,o,!1),n.lock=!1,n.released=!0,e(t)};return Hi(t,Qi,o),this}}}]),n}();const ml=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),ul=Number("500");class hl{constructor(){this.instance=new dl(ml)}update(n=".theme-vdoing-content img:not(.no-zoom)"){"undefined"!=typeof window&&this.instance.listen(n)}updateDelay(n=".theme-vdoing-content img:not(.no-zoom)",e=ul){setTimeout(()=>this.update(n),e)}}var gl=[bi,xi,Ai,Bi,Mi,{watch:{"$page.path"(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted(){this.$vuepress.zooming=new hl,this.$vuepress.zooming.updateDelay()}}],bl={name:"GlobalLayout",computed:{layout(){const n=this.getLayout();return di("layout",n),Vt.component(n)}},methods:{getLayout(){if(this.$page.path){const n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},fl=t(0),yl=Object(fl.a)(bl,(function(){return(0,this._self._c)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(n,e,t){switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),n[e].push(...t);break;default:throw new Error("Unknown option name.")}}(yl,"mixins",gl);const _l=[{name:"v-12697179",path:"/ansible/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-12697179").then(t)}},{path:"/ansible/index.html",redirect:"/ansible/"},{path:"/00.目录页/01.ansible.html",redirect:"/ansible/"},{name:"v-f6d557ce",path:"/k8s/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-f6d557ce").then(t)}},{path:"/k8s/index.html",redirect:"/k8s/"},{path:"/00.目录页/02.k8s.html",redirect:"/k8s/"},{name:"v-460f57d9",path:"/jenkins/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-460f57d9").then(t)}},{path:"/jenkins/index.html",redirect:"/jenkins/"},{path:"/00.目录页/04.jenkins.html",redirect:"/jenkins/"},{name:"v-538f7059",path:"/suibi/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-538f7059").then(t)}},{path:"/suibi/index.html",redirect:"/suibi/"},{path:"/00.目录页/05.随笔.html",redirect:"/suibi/"},{name:"v-e8898224",path:"/mianshi/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-e8898224").then(t)}},{path:"/mianshi/index.html",redirect:"/mianshi/"},{path:"/00.目录页/06.面试.html",redirect:"/mianshi/"},{name:"v-5bfdcf08",path:"/gongju/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-5bfdcf08").then(t)}},{path:"/gongju/index.html",redirect:"/gongju/"},{path:"/00.目录页/07.工具.html",redirect:"/gongju/"},{name:"v-125e2d39",path:"/elk/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-125e2d39").then(t)}},{path:"/elk/index.html",redirect:"/elk/"},{path:"/00.目录页/03.elk.html",redirect:"/elk/"},{name:"v-390cfdc4",path:"/life/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-390cfdc4").then(t)}},{path:"/life/index.html",redirect:"/life/"},{path:"/00.目录页/08.生活.html",redirect:"/life/"},{name:"v-d4660b6a",path:"/code/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-d4660b6a").then(t)}},{path:"/code/index.html",redirect:"/code/"},{path:"/00.目录页/09.编程.html",redirect:"/code/"},{name:"v-00ab4999",path:"/shell/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-00ab4999").then(t)}},{path:"/shell/index.html",redirect:"/shell/"},{path:"/00.目录页/11.shell.html",redirect:"/shell/"},{name:"v-508a552b",path:"/python/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-508a552b").then(t)}},{path:"/python/index.html",redirect:"/python/"},{path:"/00.目录页/10.python.html",redirect:"/python/"},{name:"v-06859f1a",path:"/topic/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-06859f1a").then(t)}},{path:"/topic/index.html",redirect:"/topic/"},{path:"/00.目录页/12.专题.html",redirect:"/topic/"},{name:"v-c48b57a8",path:"/middleware/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-c48b57a8").then(t)}},{path:"/middleware/index.html",redirect:"/middleware/"},{path:"/00.目录页/14.中间件.html",redirect:"/middleware/"},{name:"v-6d0e3699",path:"/linux/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-6d0e3699").then(t)}},{path:"/linux/index.html",redirect:"/linux/"},{path:"/00.目录页/15.linux.html",redirect:"/linux/"},{name:"v-ba6e35ce",path:"/windows/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-ba6e35ce").then(t)}},{path:"/windows/index.html",redirect:"/windows/"},{path:"/00.目录页/16.windows.html",redirect:"/windows/"},{name:"v-3ae46072",path:"/ops/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-3ae46072").then(t)}},{path:"/ops/index.html",redirect:"/ops/"},{path:"/00.目录页/13.运维.html",redirect:"/ops/"},{name:"v-799b0d32",path:"/network/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-799b0d32").then(t)}},{path:"/network/index.html",redirect:"/network/"},{path:"/00.目录页/17.网络.html",redirect:"/network/"},{name:"v-f1cf5a38",path:"/storage/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-f1cf5a38").then(t)}},{path:"/storage/index.html",redirect:"/storage/"},{path:"/00.目录页/19.存储.html",redirect:"/storage/"},{name:"v-3e101fea",path:"/firewalld/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-3e101fea").then(t)}},{path:"/firewalld/index.html",redirect:"/firewalld/"},{path:"/00.目录页/20.防火墙.html",redirect:"/firewalld/"},{name:"v-7c1d5117",path:"/safety/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-7c1d5117").then(t)}},{path:"/safety/index.html",redirect:"/safety/"},{path:"/00.目录页/18.安全.html",redirect:"/safety/"},{name:"v-d172e978",path:"/db/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-d172e978").then(t)}},{path:"/db/index.html",redirect:"/db/"},{path:"/00.目录页/21.数据库.html",redirect:"/db/"},{name:"v-21c554e7",path:"/docker/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-21c554e7").then(t)}},{path:"/docker/index.html",redirect:"/docker/"},{path:"/00.目录页/23.docker.html",redirect:"/docker/"},{name:"v-7a7009dc",path:"/tool/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-7a7009dc").then(t)}},{path:"/tool/index.html",redirect:"/tool/"},{path:"/00.目录页/24.运维工具.html",redirect:"/tool/"},{name:"v-7492cf63",path:"/sys/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-7492cf63").then(t)}},{path:"/sys/index.html",redirect:"/sys/"},{path:"/00.目录页/22.系统.html",redirect:"/sys/"},{name:"v-9ffde008",path:"/monitor/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-9ffde008").then(t)}},{path:"/monitor/index.html",redirect:"/monitor/"},{path:"/00.目录页/25.监控.html",redirect:"/monitor/"},{name:"v-16ca57ce",path:"/other/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-16ca57ce").then(t)}},{path:"/other/index.html",redirect:"/other/"},{path:"/00.目录页/26.other.html",redirect:"/other/"},{name:"v-9c028ada",path:"/pages/12c5da01/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-9c028ada").then(t)}},{path:"/pages/12c5da01/index.html",redirect:"/pages/12c5da01/"},{path:"/01.专题/01.ansible系列文章/01.ansible入门.html",redirect:"/pages/12c5da01/"},{name:"v-3a928a43",path:"/pages/f8f66c/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-3a928a43").then(t)}},{path:"/pages/f8f66c/index.html",redirect:"/pages/f8f66c/"},{path:"/01.专题/01.ansible系列文章/02.anisble批量安装node_exporter.html",redirect:"/pages/f8f66c/"},{name:"v-6bdef9fc",path:"/pages/d740bc/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-6bdef9fc").then(t)}},{path:"/pages/d740bc/index.html",redirect:"/pages/d740bc/"},{path:"/01.专题/01.ansible系列文章/03.ansible-playbook中的变量.html",redirect:"/pages/d740bc/"},{name:"v-f0715856",path:"/pages/a06506/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-f0715856").then(t)}},{path:"/pages/a06506/index.html",redirect:"/pages/a06506/"},{path:"/01.专题/01.ansible系列文章/04.Ansible性能优化——提升ansible执行效率.html",redirect:"/pages/a06506/"},{name:"v-23f7a879",path:"/pages/847542/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-23f7a879").then(t)}},{path:"/pages/847542/index.html",redirect:"/pages/847542/"},{path:"/01.专题/01.ansible系列文章/05.ansible之roles简单使用.html",redirect:"/pages/847542/"},{name:"v-6f8ad9d9",path:"/pages/4189b8/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-6f8ad9d9").then(t)}},{path:"/pages/4189b8/index.html",redirect:"/pages/4189b8/"},{path:"/01.专题/01.ansible系列文章/06.ansible中template简单使用.html",redirect:"/pages/4189b8/"},{name:"v-0d3f68c1",path:"/pages/14eda9/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-0d3f68c1").then(t)}},{path:"/pages/14eda9/index.html",redirect:"/pages/14eda9/"},{path:"/01.专题/01.ansible系列文章/07.Ansible中的playbook 详解.html",redirect:"/pages/14eda9/"},{name:"v-d692e0c2",path:"/pages/7eca6b/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-d692e0c2").then(t)}},{path:"/pages/7eca6b/index.html",redirect:"/pages/7eca6b/"},{path:"/01.专题/01.ansible系列文章/08.ansible-playbook编排使用tips.html",redirect:"/pages/7eca6b/"},{name:"v-4e6cb445",path:"/pages/4fbba1/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-4e6cb445").then(t)}},{path:"/pages/4fbba1/index.html",redirect:"/pages/4fbba1/"},{path:"/01.专题/01.ansible系列文章/09.ansible优秀案例.html",redirect:"/pages/4fbba1/"},{name:"v-12346ac4",path:"/pages/fc6bfb/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-12346ac4").then(t)}},{path:"/pages/fc6bfb/index.html",redirect:"/pages/fc6bfb/"},{path:"/01.专题/02.k8s/01.快速部署k8s集群.html",redirect:"/pages/fc6bfb/"},{name:"v-5a9f7a7c",path:"/pages/98c9f5/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-5a9f7a7c").then(t)}},{path:"/pages/98c9f5/index.html",redirect:"/pages/98c9f5/"},{path:"/01.专题/03.elk/01.elk安装.html",redirect:"/pages/98c9f5/"},{name:"v-4b66e9c6",path:"/pages/bbe108/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-4b66e9c6").then(t)}},{path:"/pages/bbe108/index.html",redirect:"/pages/bbe108/"},{path:"/01.专题/03.elk/02.docker-compose安装elk.html",redirect:"/pages/bbe108/"},{name:"v-dd6a2a08",path:"/pages/ed9e8d/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-dd6a2a08").then(t)}},{path:"/pages/ed9e8d/index.html",redirect:"/pages/ed9e8d/"},{path:"/01.专题/03.elk/03.filebeat-log相关配置指南.html",redirect:"/pages/ed9e8d/"},{name:"v-b8800ef2",path:"/pages/ce2b89/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-b8800ef2").then(t)}},{path:"/pages/ce2b89/index.html",redirect:"/pages/ce2b89/"},{path:"/01.专题/04.jenkins/01.jenkins容器安装.html",redirect:"/pages/ce2b89/"},{name:"v-9066c126",path:"/pages/a9ff44/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-9066c126").then(t)}},{path:"/pages/a9ff44/index.html",redirect:"/pages/a9ff44/"},{path:"/01.专题/04.jenkins/02.jenkins流水线部署.html",redirect:"/pages/a9ff44/"},{name:"v-5aabcc3a",path:"/pages/0cd089/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-5aabcc3a").then(t)}},{path:"/pages/0cd089/index.html",redirect:"/pages/0cd089/"},{path:"/02.生活/01.随笔/02.如果面试时大家都说真话.html",redirect:"/pages/0cd089/"},{name:"v-099c9ca4",path:"/pages/aa6ada/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-099c9ca4").then(t)}},{path:"/pages/aa6ada/index.html",redirect:"/pages/aa6ada/"},{path:"/02.生活/01.随笔/01.男人心智成熟的九大表现.html",redirect:"/pages/aa6ada/"},{name:"v-5436895e",path:"/pages/7b0831/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-5436895e").then(t)}},{path:"/pages/7b0831/index.html",redirect:"/pages/7b0831/"},{path:"/02.生活/01.随笔/03.这四个故事小段，够你受用一生.html",redirect:"/pages/7b0831/"},{name:"v-5cac2a3b",path:"/pages/e7e74b/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-5cac2a3b").then(t)}},{path:"/pages/e7e74b/index.html",redirect:"/pages/e7e74b/"},{path:"/02.生活/01.随笔/04.小说活着摘录.html",redirect:"/pages/e7e74b/"},{name:"v-228a9bce",path:"/pages/58b3c0/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-228a9bce").then(t)}},{path:"/pages/58b3c0/index.html",redirect:"/pages/58b3c0/"},{path:"/02.生活/01.随笔/05.人生格言.html",redirect:"/pages/58b3c0/"},{name:"v-4c3a43d2",path:"/pages/f6c9c9/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-4c3a43d2").then(t)}},{path:"/pages/f6c9c9/index.html",redirect:"/pages/f6c9c9/"},{path:"/02.生活/02.面试/01.运维10道基础面试题.html",redirect:"/pages/f6c9c9/"},{name:"v-765df57e",path:"/pages/a2e381/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-765df57e").then(t)}},{path:"/pages/a2e381/index.html",redirect:"/pages/a2e381/"},{path:"/02.生活/02.面试/02.http状态码.html",redirect:"/pages/a2e381/"},{name:"v-e52dd084",path:"/pages/9245d9/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-e52dd084").then(t)}},{path:"/pages/9245d9/index.html",redirect:"/pages/9245d9/"},{path:"/02.生活/02.面试/03.高级运维工程需要掌握的技能.html",redirect:"/pages/9245d9/"},{name:"v-e2ff91da",path:"/pages/45fa3c/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-e2ff91da").then(t)}},{path:"/pages/45fa3c/index.html",redirect:"/pages/45fa3c/"},{path:"/02.生活/03.工具/01.vpn/01.企业级openvpn搭建.html",redirect:"/pages/45fa3c/"},{name:"v-df98990e",path:"/pages/e9bb88/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-df98990e").then(t)}},{path:"/pages/e9bb88/index.html",redirect:"/pages/e9bb88/"},{path:"/02.生活/03.工具/02.翻墙教程/01.实现V2Ray通过CloudFlare自选IP加速.html",redirect:"/pages/e9bb88/"},{name:"v-303fed36",path:"/pages/4ad301/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-303fed36").then(t)}},{path:"/pages/4ad301/index.html",redirect:"/pages/4ad301/"},{path:"/02.生活/03.工具/03.mac电脑常用软件.html",redirect:"/pages/4ad301/"},{name:"v-e138df4c",path:"/pages/8994e9/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-e138df4c").then(t)}},{path:"/pages/8994e9/index.html",redirect:"/pages/8994e9/"},{path:"/02.生活/03.工具/04.个人工具链接.html",redirect:"/pages/8994e9/"},{name:"v-19ff8e6a",path:"/pages/468f43/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-19ff8e6a").then(t)}},{path:"/pages/468f43/index.html",redirect:"/pages/468f43/"},{path:"/02.生活/03.工具/05.vuepress配置artalk.html",redirect:"/pages/468f43/"},{name:"v-24ef5f44",path:"/pages/112008/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-24ef5f44").then(t)}},{path:"/pages/112008/index.html",redirect:"/pages/112008/"},{path:"/02.生活/03.工具/06.windows目录实时同步工具.html",redirect:"/pages/112008/"},{name:"v-403a5ccb",path:"/pages/f8da74/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-403a5ccb").then(t)}},{path:"/pages/f8da74/index.html",redirect:"/pages/f8da74/"},{path:"/03.编程/01.python/01.监控目录或文件变化.html",redirect:"/pages/f8da74/"},{name:"v-0542148a",path:"/pages/f659cd/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-0542148a").then(t)}},{path:"/pages/f659cd/index.html",redirect:"/pages/f659cd/"},{path:"/03.编程/01.python/02.批量更改文件.html",redirect:"/pages/f659cd/"},{name:"v-7246ae2c",path:"/pages/a109ec/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-7246ae2c").then(t)}},{path:"/pages/a109ec/index.html",redirect:"/pages/a109ec/"},{path:"/03.编程/01.python/03.python引用数据库.html",redirect:"/pages/a109ec/"},{name:"v-0580956a",path:"/pages/457fb7/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-0580956a").then(t)}},{path:"/pages/457fb7/index.html",redirect:"/pages/457fb7/"},{path:"/03.编程/01.python/04.python3给防火墙添加放行.html",redirect:"/pages/457fb7/"},{name:"v-71ab6464",path:"/pages/d0fcb5/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-71ab6464").then(t)}},{path:"/pages/d0fcb5/index.html",redirect:"/pages/d0fcb5/"},{path:"/03.编程/01.python/05.python生成部署脚本.html",redirect:"/pages/d0fcb5/"},{name:"v-2e07d1a0",path:"/pages/8d0b76/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-2e07d1a0").then(t)}},{path:"/pages/8d0b76/index.html",redirect:"/pages/8d0b76/"},{path:"/03.编程/01.python/06.python将多个文件内容输出到一个文件中.html",redirect:"/pages/8d0b76/"},{name:"v-6c2001b3",path:"/pages/cae02d/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-6c2001b3").then(t)}},{path:"/pages/cae02d/index.html",redirect:"/pages/cae02d/"},{path:"/03.编程/02.shell/01.进程pid判断脚本.html",redirect:"/pages/cae02d/"},{name:"v-6ba31c1d",path:"/pages/3b937e/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-6ba31c1d").then(t)}},{path:"/pages/3b937e/index.html",redirect:"/pages/3b937e/"},{path:"/03.编程/02.shell/02.日志切割脚本.html",redirect:"/pages/3b937e/"},{name:"v-a840ca1e",path:"/pages/69b699/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-a840ca1e").then(t)}},{path:"/pages/69b699/index.html",redirect:"/pages/69b699/"},{path:"/03.编程/02.shell/03.设置跳板机脚本.html",redirect:"/pages/69b699/"},{name:"v-8d36e636",path:"/pages/fe0782/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-8d36e636").then(t)}},{path:"/pages/fe0782/index.html",redirect:"/pages/fe0782/"},{path:"/03.编程/02.shell/04.编写启动、停止、重启的脚本.html",redirect:"/pages/fe0782/"},{name:"v-08986d40",path:"/pages/93dcc7/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-08986d40").then(t)}},{path:"/pages/93dcc7/index.html",redirect:"/pages/93dcc7/"},{path:"/03.编程/02.shell/05.mysql数据库备份的三种方式.html",redirect:"/pages/93dcc7/"},{name:"v-38a8ed18",path:"/pages/2019f8/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-38a8ed18").then(t)}},{path:"/pages/2019f8/index.html",redirect:"/pages/2019f8/"},{path:"/03.编程/02.shell/06.jenkins编译服务脚本.html",redirect:"/pages/2019f8/"},{name:"v-45252678",path:"/pages/fd3678/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-45252678").then(t)}},{path:"/pages/fd3678/index.html",redirect:"/pages/fd3678/"},{path:"/03.编程/02.shell/07.app编译脚本.html",redirect:"/pages/fd3678/"},{name:"v-c86447e2",path:"/pages/8fdac1/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-c86447e2").then(t)}},{path:"/pages/8fdac1/index.html",redirect:"/pages/8fdac1/"},{path:"/03.编程/02.shell/08.常用shell脚本.html",redirect:"/pages/8fdac1/"},{name:"v-1ae891a8",path:"/pages/f5c376/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-1ae891a8").then(t)}},{path:"/pages/f5c376/index.html",redirect:"/pages/f5c376/"},{path:"/03.编程/02.shell/09.字符串的截取拼接.html",redirect:"/pages/f5c376/"},{name:"v-0aaf2f47",path:"/pages/665355/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-0aaf2f47").then(t)}},{path:"/pages/665355/index.html",redirect:"/pages/665355/"},{path:"/03.编程/02.shell/10.shell基础.html",redirect:"/pages/665355/"},{name:"v-11090b27",path:"/pages/98cac7/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-11090b27").then(t)}},{path:"/pages/98cac7/index.html",redirect:"/pages/98cac7/"},{path:"/04.运维/01.linux/01.rsync/01.rsync用法及参数详解.html",redirect:"/pages/98cac7/"},{name:"v-2f584880",path:"/pages/c7f1bc/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-2f584880").then(t)}},{path:"/pages/c7f1bc/index.html",redirect:"/pages/c7f1bc/"},{path:"/04.运维/01.linux/01.rsync/02.rsync服务实现推送，拉取.html",redirect:"/pages/c7f1bc/"},{name:"v-52cf205e",path:"/pages/78c801/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-52cf205e").then(t)}},{path:"/pages/78c801/index.html",redirect:"/pages/78c801/"},{path:"/04.运维/01.linux/02.dns/01.centos7搭建dns,bind配置.html",redirect:"/pages/78c801/"},{name:"v-44ab5907",path:"/pages/4baba0/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-44ab5907").then(t)}},{path:"/pages/4baba0/index.html",redirect:"/pages/4baba0/"},{path:"/04.运维/01.linux/03.sed、awk、grep、find四剑客/01.sed命令在文本每行,行尾或行首添加字符.html",redirect:"/pages/4baba0/"},{name:"v-e8e0b9ea",path:"/pages/5f261c/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-e8e0b9ea").then(t)}},{path:"/pages/5f261c/index.html",redirect:"/pages/5f261c/"},{path:"/04.运维/01.linux/04.LVM管理.html",redirect:"/pages/5f261c/"},{name:"v-ba2529e4",path:"/pages/0bac05/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-ba2529e4").then(t)}},{path:"/pages/0bac05/index.html",redirect:"/pages/0bac05/"},{path:"/04.运维/01.linux/05.sudo权限规划.html",redirect:"/pages/0bac05/"},{name:"v-2c290406",path:"/pages/c17655/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-2c290406").then(t)}},{path:"/pages/c17655/index.html",redirect:"/pages/c17655/"},{path:"/04.运维/01.linux/06.linux修改网卡为eth0的两种方法.html",redirect:"/pages/c17655/"},{name:"v-78111152",path:"/pages/a8c469/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-78111152").then(t)}},{path:"/pages/a8c469/index.html",redirect:"/pages/a8c469/"},{path:"/04.运维/01.linux/07.Logrotate入门了解及生产实践.html",redirect:"/pages/a8c469/"},{name:"v-c1c1c24e",path:"/pages/18b06c/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-c1c1c24e").then(t)}},{path:"/pages/18b06c/index.html",redirect:"/pages/18b06c/"},{path:"/04.运维/02.windows/01.windows支持多用户远程登录.html",redirect:"/pages/18b06c/"},{name:"v-0cc13236",path:"/pages/adbe78/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-0cc13236").then(t)}},{path:"/pages/adbe78/index.html",redirect:"/pages/adbe78/"},{path:"/04.运维/02.windows/02.windows应用服务部署脚本.html",redirect:"/pages/adbe78/"},{name:"v-0df2a669",path:"/pages/5ed327/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-0df2a669").then(t)}},{path:"/pages/5ed327/index.html",redirect:"/pages/5ed327/"},{path:"/04.运维/03.中间件/01.nginx/01.nginx配置教程.html",redirect:"/pages/5ed327/"},{name:"v-74be9f5b",path:"/pages/df9ea8/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-74be9f5b").then(t)}},{path:"/pages/df9ea8/index.html",redirect:"/pages/df9ea8/"},{path:"/04.运维/03.中间件/01.nginx/02.nginx常用配置.html",redirect:"/pages/df9ea8/"},{name:"v-38030407",path:"/pages/98b071/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-38030407").then(t)}},{path:"/pages/98b071/index.html",redirect:"/pages/98b071/"},{path:"/04.运维/03.中间件/02.kafka/01.kafka介绍和常见操作.html",redirect:"/pages/98b071/"},{name:"v-0576e07d",path:"/pages/43f361/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-0576e07d").then(t)}},{path:"/pages/43f361/index.html",redirect:"/pages/43f361/"},{path:"/04.运维/03.中间件/02.kafka/02.docker-compose安装kafka集群.html",redirect:"/pages/43f361/"},{name:"v-8488e3a8",path:"/pages/b73d79/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-8488e3a8").then(t)}},{path:"/pages/b73d79/index.html",redirect:"/pages/b73d79/"},{path:"/04.运维/03.中间件/02.kafka/03.kafka工作原理.html",redirect:"/pages/b73d79/"},{name:"v-515e9f9a",path:"/pages/fd6cf8/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-515e9f9a").then(t)}},{path:"/pages/fd6cf8/index.html",redirect:"/pages/fd6cf8/"},{path:"/04.运维/03.中间件/03.apollo部署.html",redirect:"/pages/fd6cf8/"},{name:"v-66a7ae24",path:"/pages/38ce88/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-66a7ae24").then(t)}},{path:"/pages/38ce88/index.html",redirect:"/pages/38ce88/"},{path:"/04.运维/04.网络/03.网络工具.html",redirect:"/pages/38ce88/"},{name:"v-084ed8e0",path:"/pages/5933ee/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-084ed8e0").then(t)}},{path:"/pages/5933ee/index.html",redirect:"/pages/5933ee/"},{path:"/04.运维/04.网络/02.网络代理.html",redirect:"/pages/5933ee/"},{name:"v-9198a8e6",path:"/pages/7f4bdd/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-9198a8e6").then(t)}},{path:"/pages/7f4bdd/index.html",redirect:"/pages/7f4bdd/"},{path:"/04.运维/05.安全/01.docker部署openvas.html",redirect:"/pages/7f4bdd/"},{name:"v-28967f21",path:"/pages/2f613d/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-28967f21").then(t)}},{path:"/pages/2f613d/index.html",redirect:"/pages/2f613d/"},{path:"/04.运维/04.网络/01.抓包工具 tcpdump 用法说明.html",redirect:"/pages/2f613d/"},{name:"v-44bb153f",path:"/pages/3913e5/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-44bb153f").then(t)}},{path:"/pages/3913e5/index.html",redirect:"/pages/3913e5/"},{path:"/04.运维/05.安全/03.ssh安全.html",redirect:"/pages/3913e5/"},{name:"v-0cf4e9e4",path:"/pages/fcba46/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-0cf4e9e4").then(t)}},{path:"/pages/fcba46/index.html",redirect:"/pages/fcba46/"},{path:"/04.运维/05.安全/02.白帽子讲web安全.html",redirect:"/pages/fcba46/"},{name:"v-c30db28e",path:"/pages/8dcd54/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-c30db28e").then(t)}},{path:"/pages/8dcd54/index.html",redirect:"/pages/8dcd54/"},{path:"/04.运维/05.安全/04.系统安全及应用基础.html",redirect:"/pages/8dcd54/"},{name:"v-477589d8",path:"/pages/988870/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-477589d8").then(t)}},{path:"/pages/988870/index.html",redirect:"/pages/988870/"},{path:"/04.运维/06.存储/01.fastdfs.html",redirect:"/pages/988870/"},{name:"v-0bebf3bd",path:"/pages/88a4de/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-0bebf3bd").then(t)}},{path:"/pages/88a4de/index.html",redirect:"/pages/88a4de/"},{path:"/04.运维/06.存储/03.ceph/01.部署ceph集群 Nautilus版.html",redirect:"/pages/88a4de/"},{name:"v-ed9b282c",path:"/pages/70e7ad/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-ed9b282c").then(t)}},{path:"/pages/70e7ad/index.html",redirect:"/pages/70e7ad/"},{path:"/04.运维/06.存储/02.glusterfs.html",redirect:"/pages/70e7ad/"},{name:"v-0a29d12e",path:"/pages/ce46b6/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-0a29d12e").then(t)}},{path:"/pages/ce46b6/index.html",redirect:"/pages/ce46b6/"},{path:"/04.运维/07.防火墙/01.centos7下配置firewalld实现nat转发软路由.html",redirect:"/pages/ce46b6/"},{name:"v-09ffdab8",path:"/pages/1f5460/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-09ffdab8").then(t)}},{path:"/pages/1f5460/index.html",redirect:"/pages/1f5460/"},{path:"/04.运维/08.数据库/01.mysql/01.数据库二进制安装.html",redirect:"/pages/1f5460/"},{name:"v-b2ae320e",path:"/pages/e977f3/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-b2ae320e").then(t)}},{path:"/pages/e977f3/index.html",redirect:"/pages/e977f3/"},{path:"/04.运维/08.数据库/03.oracle/01.oracle数据库安装.html",redirect:"/pages/e977f3/"},{name:"v-1ebf1e36",path:"/pages/598be1/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-1ebf1e36").then(t)}},{path:"/pages/598be1/index.html",redirect:"/pages/598be1/"},{path:"/04.运维/08.数据库/02.mongodb/01.mongodb相关概念.html",redirect:"/pages/598be1/"},{name:"v-447429d0",path:"/pages/26f193/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-447429d0").then(t)}},{path:"/pages/26f193/index.html",redirect:"/pages/26f193/"},{path:"/04.运维/09.系统/01.vmware/01.服务器虚拟化VMware ESXI搭建集群.html",redirect:"/pages/26f193/"},{name:"v-64021198",path:"/pages/affcf2/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-64021198").then(t)}},{path:"/pages/affcf2/index.html",redirect:"/pages/affcf2/"},{path:"/04.运维/09.系统/02.ftp/01.proftpd环境部署.html",redirect:"/pages/affcf2/"},{name:"v-c4242b76",path:"/pages/af4fce/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-c4242b76").then(t)}},{path:"/pages/af4fce/index.html",redirect:"/pages/af4fce/"},{path:"/04.运维/09.系统/03.nexus/01.nexus安装.html",redirect:"/pages/af4fce/"},{name:"v-6feefc5e",path:"/pages/b851fe/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-6feefc5e").then(t)}},{path:"/pages/b851fe/index.html",redirect:"/pages/b851fe/"},{path:"/04.运维/09.系统/03.nexus/02.使用nexus3配置npm私有仓库.html",redirect:"/pages/b851fe/"},{name:"v-28dc669e",path:"/pages/12e16a/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-28dc669e").then(t)}},{path:"/pages/12e16a/index.html",redirect:"/pages/12e16a/"},{path:"/04.运维/09.系统/03.nexus/03.使用nexus3配置maven私有仓库.html",redirect:"/pages/12e16a/"},{name:"v-65a31f9a",path:"/pages/217e32/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-65a31f9a").then(t)}},{path:"/pages/217e32/index.html",redirect:"/pages/217e32/"},{path:"/04.运维/10.docker/02.docker和docker-compose安装.html",redirect:"/pages/217e32/"},{name:"v-db6593ce",path:"/pages/359db9/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-db6593ce").then(t)}},{path:"/pages/359db9/index.html",redirect:"/pages/359db9/"},{path:"/04.运维/08.数据库/02.mongodb/02.mongodb副本集.html",redirect:"/pages/359db9/"},{name:"v-3ed70d81",path:"/pages/83a393/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-3ed70d81").then(t)}},{path:"/pages/83a393/index.html",redirect:"/pages/83a393/"},{path:"/04.运维/10.docker/01.Docker构建镜像.html",redirect:"/pages/83a393/"},{name:"v-573ab138",path:"/pages/8884ac/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-573ab138").then(t)}},{path:"/pages/8884ac/index.html",redirect:"/pages/8884ac/"},{path:"/04.运维/10.docker/03.如何选择docker基础镜像.html",redirect:"/pages/8884ac/"},{name:"v-20b39df5",path:"/pages/382a6c/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-20b39df5").then(t)}},{path:"/pages/382a6c/index.html",redirect:"/pages/382a6c/"},{path:"/04.运维/10.docker/04.常见的dockerfile汇总.html",redirect:"/pages/382a6c/"},{name:"v-0f41bfdd",path:"/pages/6e3cb9/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-0f41bfdd").then(t)}},{path:"/pages/6e3cb9/index.html",redirect:"/pages/6e3cb9/"},{path:"/04.运维/10.docker/05.基于官方php7-2-34镜像构建生产可用镜像.html",redirect:"/pages/6e3cb9/"},{name:"v-28471c42",path:"/pages/767641/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-28471c42").then(t)}},{path:"/pages/767641/index.html",redirect:"/pages/767641/"},{path:"/04.运维/11.other/01.debian中科大替换教程.html",redirect:"/pages/767641/"},{name:"v-2e1ddaa3",path:"/pages/aa9eee/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-2e1ddaa3").then(t)}},{path:"/pages/aa9eee/index.html",redirect:"/pages/aa9eee/"},{path:"/04.运维/11.other/02.Alpine安装php各种扩展.html",redirect:"/pages/aa9eee/"},{name:"v-10b832a2",path:"/pages/e0085e/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-10b832a2").then(t)}},{path:"/pages/e0085e/index.html",redirect:"/pages/e0085e/"},{path:"/04.运维/12.监控/01.zabbix/03.docker文件安装zabbix5.html",redirect:"/pages/e0085e/"},{name:"v-282498df",path:"/pages/a4e3ce/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-282498df").then(t)}},{path:"/pages/a4e3ce/index.html",redirect:"/pages/a4e3ce/"},{path:"/04.运维/12.监控/01.zabbix/04.zabbix配置钉钉告警.html",redirect:"/pages/a4e3ce/"},{name:"v-7b122aa1",path:"/pages/a0a3ae/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-7b122aa1").then(t)}},{path:"/pages/a0a3ae/index.html",redirect:"/pages/a0a3ae/"},{path:"/04.运维/12.监控/01.zabbix/02.zabbix添加端口和进程监控.html",redirect:"/pages/a0a3ae/"},{name:"v-787b413e",path:"/pages/3dac52/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-787b413e").then(t)}},{path:"/pages/3dac52/index.html",redirect:"/pages/3dac52/"},{path:"/04.运维/12.监控/01.zabbix/01.zabbix添加证书监控.html",redirect:"/pages/3dac52/"},{name:"v-68775ace",path:"/pages/334bec/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-68775ace").then(t)}},{path:"/pages/334bec/index.html",redirect:"/pages/334bec/"},{path:"/04.运维/12.监控/01.zabbix/05.zabbix添加日志监控.html",redirect:"/pages/334bec/"},{name:"v-3bd0d01e",path:"/pages/269839/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-3bd0d01e").then(t)}},{path:"/pages/269839/index.html",redirect:"/pages/269839/"},{path:"/04.运维/12.监控/01.zabbix/07.zabbix监控windows进程.html",redirect:"/pages/269839/"},{name:"v-1955a3de",path:"/pages/20f905/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-1955a3de").then(t)}},{path:"/pages/20f905/index.html",redirect:"/pages/20f905/"},{path:"/04.运维/12.监控/01.zabbix/06.zabbix添加进程pid监控.html",redirect:"/pages/20f905/"},{name:"v-8105e1aa",path:"/pages/b70eb8/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-8105e1aa").then(t)}},{path:"/pages/b70eb8/index.html",redirect:"/pages/b70eb8/"},{path:"/04.运维/12.监控/01.zabbix/08.zabbix添加web监控.html",redirect:"/pages/b70eb8/"},{name:"v-c5ed0f8c",path:"/pages/723e7e/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-c5ed0f8c").then(t)}},{path:"/pages/723e7e/index.html",redirect:"/pages/723e7e/"},{path:"/04.运维/12.监控/01.zabbix/09.centos7编译安装zabbix proxy端.html",redirect:"/pages/723e7e/"},{name:"v-20df7df7",path:"/pages/ef347e/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-20df7df7").then(t)}},{path:"/pages/ef347e/index.html",redirect:"/pages/ef347e/"},{path:"/04.运维/12.监控/01.zabbix/10.rpm安装zabbix proxy过程简记.html",redirect:"/pages/ef347e/"},{name:"v-4008dcfe",path:"/pages/40794d/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-4008dcfe").then(t)}},{path:"/pages/40794d/index.html",redirect:"/pages/40794d/"},{path:"/04.运维/12.监控/02.prometheus/01.prometheus监控、告警与存储.html",redirect:"/pages/40794d/"},{name:"v-6f6532a7",path:"/pages/712f5c/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-6f6532a7").then(t)}},{path:"/pages/712f5c/index.html",redirect:"/pages/712f5c/"},{path:"/04.运维/12.监控/02.prometheus/02.使用docker-compose搭建promethes+grafana监控系统.html",redirect:"/pages/712f5c/"},{name:"v-003c987e",path:"/pages/22b836/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-003c987e").then(t)}},{path:"/pages/22b836/index.html",redirect:"/pages/22b836/"},{path:"/04.运维/12.监控/02.prometheus/03.prometheus添加钉钉消息告警.html",redirect:"/pages/22b836/"},{name:"v-403d9ae8",path:"/archives/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-403d9ae8").then(t)}},{path:"/archives/index.html",redirect:"/archives/"},{path:"/@pages/archivesPage.html",redirect:"/archives/"},{name:"v-75545c56",path:"/pages/96c6ce/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-75545c56").then(t)}},{path:"/pages/96c6ce/index.html",redirect:"/pages/96c6ce/"},{path:"/04.运维/12.监控/02.prometheus/04.alertmanager实现某个时间段静默某些告警项.html",redirect:"/pages/96c6ce/"},{name:"v-17a259f2",path:"/about/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-17a259f2").then(t)}},{path:"/about/index.html",redirect:"/about/"},{path:"/05.关于/01.关于.html",redirect:"/about/"},{name:"v-50528ad2",path:"/friends/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-50528ad2").then(t)}},{path:"/friends/index.html",redirect:"/friends/"},{path:"/06.友链/01.友链.html",redirect:"/friends/"},{name:"v-65b5a670",path:"/categories/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-65b5a670").then(t)}},{path:"/categories/index.html",redirect:"/categories/"},{path:"/@pages/categoriesPage.html",redirect:"/categories/"},{name:"v-8923d930",path:"/tags/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-8923d930").then(t)}},{path:"/tags/index.html",redirect:"/tags/"},{path:"/@pages/tagsPage.html",redirect:"/tags/"},{name:"v-7e9c1840",path:"/",component:yl,beforeEnter:(n,e,t)=>{pi("Layout","v-7e9c1840").then(t)}},{path:"/index.html",redirect:"/"},{path:"*",component:yl}],vl={title:"章工运维",description:"好好学习，天天向上",base:"/",headTags:[["link",{rel:"icon",href:"/img/favicon.ico"}],["meta",{name:"keywords",content:"分享生活,分享技术,zpj,jenkins,ansible,k8s,监控"}],["meta",{name:"theme-color",content:"#11a8cd"}],["meta",{name:"referrer",content:"no-referrer-when-downgrade"}],["link",{rel:"stylesheet",href:"https://at.alicdn.com/t/font_3114978_qe0b39no76.css"}],["script",{language:"javascript",type:"text/javascript",src:"/js/pgmanor-self.js"}],["link",{rel:"alternate",type:"application/rss+xml",href:"https://blog.zzppjj.top/rss.xml",title:"章工运维 RSS Feed"}],["link",{rel:"alternate",type:"application/atom+xml",href:"https://blog.zzppjj.top/feed.atom",title:"章工运维 Atom Feed"}],["link",{rel:"alternate",type:"application/json",href:"https://blog.zzppjj.top/feed.json",title:"章工运维 JSON Feed"}]],pages:[{title:"ansible",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.专题/01.ansible系列文章",description:"ansible"}},title:"ansible",date:"2022-12-15T10:37:52.000Z",permalink:"/ansible/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"ansible"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/01.ansible.html"},{property:"og:type",content:"article"},{property:"og:title",content:"ansible"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/01.ansible.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T10:37:52.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"ansible"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/01.ansible.html",relativePath:"00.目录页/01.ansible.md",key:"v-12697179",path:"/ansible/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"k8s",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.专题/02.k8s",description:"k8s"}},title:"k8s",date:"2022-12-15T14:40:52.000Z",permalink:"/k8s/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"k8s"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/02.k8s.html"},{property:"og:type",content:"article"},{property:"og:title",content:"k8s"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/02.k8s.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:40:52.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"k8s"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/02.k8s.html",relativePath:"00.目录页/02.k8s.md",key:"v-f6d557ce",path:"/k8s/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"jenkins",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.专题/06.jenkins",description:"jenkins"}},title:"jenkins",date:"2022-12-15T14:43:23.000Z",permalink:"/jenkins/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"jenkins"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/04.jenkins.html"},{property:"og:type",content:"article"},{property:"og:title",content:"jenkins"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/04.jenkins.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"jenkins"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/04.jenkins.html",relativePath:"00.目录页/04.jenkins.md",key:"v-460f57d9",path:"/jenkins/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"随笔",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"02.生活/01.随笔",description:"随笔"}},title:"随笔",date:"2022-12-15T14:43:23.000Z",permalink:"/suibi/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"随笔"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/05.%E9%9A%8F%E7%AC%94.html"},{property:"og:type",content:"article"},{property:"og:title",content:"随笔"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/05.%E9%9A%8F%E7%AC%94.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"随笔"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/05.%E9%9A%8F%E7%AC%94.html",relativePath:"00.目录页/05.随笔.md",key:"v-538f7059",path:"/suibi/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"面试",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"02.生活/02.面试",description:"面试"}},title:"面试",date:"2022-12-15T14:43:23.000Z",permalink:"/mianshi/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"面试"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/06.%E9%9D%A2%E8%AF%95.html"},{property:"og:type",content:"article"},{property:"og:title",content:"面试"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/06.%E9%9D%A2%E8%AF%95.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"面试"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/06.%E9%9D%A2%E8%AF%95.html",relativePath:"00.目录页/06.面试.md",key:"v-e8898224",path:"/mianshi/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"工具",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"02.生活/03.工具",description:"工具"}},title:"工具",date:"2022-12-15T14:43:23.000Z",permalink:"/gongju/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"工具"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/07.%E5%B7%A5%E5%85%B7.html"},{property:"og:type",content:"article"},{property:"og:title",content:"工具"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/07.%E5%B7%A5%E5%85%B7.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"工具"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/07.%E5%B7%A5%E5%85%B7.html",relativePath:"00.目录页/07.工具.md",key:"v-5bfdcf08",path:"/gongju/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"elk",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.专题/03.elk",description:"elk"}},title:"elk",date:"2022-12-15T10:41:32.000Z",permalink:"/elk/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"elk"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/03.elk.html"},{property:"og:type",content:"article"},{property:"og:title",content:"elk"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/03.elk.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T10:41:32.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"elk"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/03.elk.html",relativePath:"00.目录页/03.elk.md",key:"v-125e2d39",path:"/elk/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"生活",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"02.生活",description:"生活"}},title:"生活",date:"2022-12-15T14:43:23.000Z",permalink:"/life/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"生活"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/08.%E7%94%9F%E6%B4%BB.html"},{property:"og:type",content:"article"},{property:"og:title",content:"生活"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/08.%E7%94%9F%E6%B4%BB.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"生活"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/08.%E7%94%9F%E6%B4%BB.html",relativePath:"00.目录页/08.生活.md",key:"v-390cfdc4",path:"/life/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"编程",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"03.编程",description:"编程"}},title:"编程",date:"2022-12-15T14:43:23.000Z",permalink:"/code/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"编程"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/09.%E7%BC%96%E7%A8%8B.html"},{property:"og:type",content:"article"},{property:"og:title",content:"编程"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/09.%E7%BC%96%E7%A8%8B.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"编程"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/09.%E7%BC%96%E7%A8%8B.html",relativePath:"00.目录页/09.编程.md",key:"v-d4660b6a",path:"/code/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"shell",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"03.编程/02.shell",description:"shell"}},title:"shell",date:"2022-12-15T14:43:23.000Z",permalink:"/shell/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"shell"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/11.shell.html"},{property:"og:type",content:"article"},{property:"og:title",content:"shell"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/11.shell.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"shell"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/11.shell.html",relativePath:"00.目录页/11.shell.md",key:"v-00ab4999",path:"/shell/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"python",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"03.编程/01.python",description:"python"}},title:"python",date:"2022-12-15T14:43:23.000Z",permalink:"/python/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"python"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/10.python.html"},{property:"og:type",content:"article"},{property:"og:title",content:"python"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/10.python.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"python"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/10.python.html",relativePath:"00.目录页/10.python.md",key:"v-508a552b",path:"/python/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"专题",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"01.专题",description:"专题"}},title:"专题",date:"2022-12-15T14:43:23.000Z",permalink:"/topic/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"专题"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/12.%E4%B8%93%E9%A2%98.html"},{property:"og:type",content:"article"},{property:"og:title",content:"专题"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/12.%E4%B8%93%E9%A2%98.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"专题"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/12.%E4%B8%93%E9%A2%98.html",relativePath:"00.目录页/12.专题.md",key:"v-06859f1a",path:"/topic/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"中间件",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/03.中间件",description:"中间件"}},title:"中间件",date:"2022-12-15T14:43:23.000Z",permalink:"/middleware/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"中间件"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/14.%E4%B8%AD%E9%97%B4%E4%BB%B6.html"},{property:"og:type",content:"article"},{property:"og:title",content:"中间件"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/14.%E4%B8%AD%E9%97%B4%E4%BB%B6.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"中间件"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/14.%E4%B8%AD%E9%97%B4%E4%BB%B6.html",relativePath:"00.目录页/14.中间件.md",key:"v-c48b57a8",path:"/middleware/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"linux",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/01.linux",description:"linux"}},title:"linux",date:"2022-12-15T14:43:23.000Z",permalink:"/linux/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"linux"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/15.linux.html"},{property:"og:type",content:"article"},{property:"og:title",content:"linux"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/15.linux.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"linux"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/15.linux.html",relativePath:"00.目录页/15.linux.md",key:"v-6d0e3699",path:"/linux/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"windows",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/02.windows",description:"windows"}},title:"windows",date:"2022-12-15T14:43:23.000Z",permalink:"/windows/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"windows"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/16.windows.html"},{property:"og:type",content:"article"},{property:"og:title",content:"windows"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/16.windows.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"windows"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/16.windows.html",relativePath:"00.目录页/16.windows.md",key:"v-ba6e35ce",path:"/windows/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"运维",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维",description:"运维"}},title:"运维",date:"2022-12-15T14:43:23.000Z",permalink:"/ops/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"运维"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/13.%E8%BF%90%E7%BB%B4.html"},{property:"og:type",content:"article"},{property:"og:title",content:"运维"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/13.%E8%BF%90%E7%BB%B4.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"运维"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/13.%E8%BF%90%E7%BB%B4.html",relativePath:"00.目录页/13.运维.md",key:"v-3ae46072",path:"/ops/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"网络",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/04.网络",description:"网络"}},title:"网络",date:"2022-12-15T14:43:23.000Z",permalink:"/network/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"网络"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/17.%E7%BD%91%E7%BB%9C.html"},{property:"og:type",content:"article"},{property:"og:title",content:"网络"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/17.%E7%BD%91%E7%BB%9C.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"网络"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/17.%E7%BD%91%E7%BB%9C.html",relativePath:"00.目录页/17.网络.md",key:"v-799b0d32",path:"/network/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"存储",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/06.存储",description:"存储"}},title:"存储",date:"2022-12-15T14:43:23.000Z",permalink:"/storage/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"存储"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/19.%E5%AD%98%E5%82%A8.html"},{property:"og:type",content:"article"},{property:"og:title",content:"存储"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/19.%E5%AD%98%E5%82%A8.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"存储"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/19.%E5%AD%98%E5%82%A8.html",relativePath:"00.目录页/19.存储.md",key:"v-f1cf5a38",path:"/storage/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"防火墙",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/07.防火墙",description:"防火墙"}},title:"防火墙",date:"2022-12-15T14:43:23.000Z",permalink:"/firewalld/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"防火墙"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/20.%E9%98%B2%E7%81%AB%E5%A2%99.html"},{property:"og:type",content:"article"},{property:"og:title",content:"防火墙"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/20.%E9%98%B2%E7%81%AB%E5%A2%99.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"防火墙"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/20.%E9%98%B2%E7%81%AB%E5%A2%99.html",relativePath:"00.目录页/20.防火墙.md",key:"v-3e101fea",path:"/firewalld/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"安全",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/05.安全",description:"安全"}},title:"安全",date:"2022-12-15T14:43:23.000Z",permalink:"/safety/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"安全"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/18.%E5%AE%89%E5%85%A8.html"},{property:"og:type",content:"article"},{property:"og:title",content:"安全"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/18.%E5%AE%89%E5%85%A8.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"安全"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/18.%E5%AE%89%E5%85%A8.html",relativePath:"00.目录页/18.安全.md",key:"v-7c1d5117",path:"/safety/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"数据库",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/08.数据库",description:"数据库"}},title:"数据库",date:"2022-12-15T14:43:23.000Z",permalink:"/db/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"数据库"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/21.%E6%95%B0%E6%8D%AE%E5%BA%93.html"},{property:"og:type",content:"article"},{property:"og:title",content:"数据库"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/21.%E6%95%B0%E6%8D%AE%E5%BA%93.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"数据库"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/21.%E6%95%B0%E6%8D%AE%E5%BA%93.html",relativePath:"00.目录页/21.数据库.md",key:"v-d172e978",path:"/db/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"docker",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/10.docker",description:"docker"}},title:"docker",date:"2022-12-15T14:43:23.000Z",permalink:"/docker/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"docker"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/23.docker.html"},{property:"og:type",content:"article"},{property:"og:title",content:"docker"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/23.docker.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"docker"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/23.docker.html",relativePath:"00.目录页/23.docker.md",key:"v-21c554e7",path:"/docker/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"运维工具",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"02.生活/03.工具",description:"工具"}},title:"运维工具",date:"2022-12-15T14:43:23.000Z",permalink:"/tool/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"运维工具"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/24.%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7.html"},{property:"og:type",content:"article"},{property:"og:title",content:"运维工具"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/24.%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"运维工具"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/24.%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7.html",relativePath:"00.目录页/24.运维工具.md",key:"v-7a7009dc",path:"/tool/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"系统",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/09.系统",description:"系统"}},title:"系统",date:"2022-12-15T14:43:23.000Z",permalink:"/sys/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"系统"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/22.%E7%B3%BB%E7%BB%9F.html"},{property:"og:type",content:"article"},{property:"og:title",content:"系统"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/22.%E7%B3%BB%E7%BB%9F.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"系统"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/22.%E7%B3%BB%E7%BB%9F.html",relativePath:"00.目录页/22.系统.md",key:"v-7492cf63",path:"/sys/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"监控",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/12.监控",description:"监控"}},title:"监控",date:"2022-12-15T14:43:23.000Z",permalink:"/monitor/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"监控"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/25.%E7%9B%91%E6%8E%A7.html"},{property:"og:type",content:"article"},{property:"og:title",content:"监控"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/25.%E7%9B%91%E6%8E%A7.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"监控"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/25.%E7%9B%91%E6%8E%A7.html",relativePath:"00.目录页/25.监控.md",key:"v-9ffde008",path:"/monitor/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"other",frontmatter:{pageComponent:{name:"Catalogue",data:{path:"04.运维/11.other",description:"other"}},title:"other",date:"2022-12-15T14:43:23.000Z",permalink:"/other/",categories:["目录页"],tags:[null],readingShow:"top",description:"::: center",meta:[{name:"twitter:title",content:"other"},{name:"twitter:description",content:"::: center"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/26.other.html"},{property:"og:type",content:"article"},{property:"og:title",content:"other"},{property:"og:description",content:"::: center"},{property:"og:url",content:"https://blog.zzppjj.top/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/26.other.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:43:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"other"},{itemprop:"description",content:"::: center"}]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/26.other.html",relativePath:"00.目录页/26.other.md",key:"v-16ca57ce",path:"/other/",headers:[{level:2,title:"坚持，坚持，再坚持",slug:"坚持-坚持-再坚持",normalizedTitle:"坚持，坚持，再坚持",charIndex:2}],headersStr:"坚持，坚持，再坚持",content:"# 坚持，坚持，再坚持\n\n学习，记录，分享",normalizedContent:"# 坚持，坚持，再坚持\n\n学习，记录，分享",charsets:{cjk:!0}},{title:"ansible入门",frontmatter:{title:"ansible入门",date:null,permalink:"/pages/12c5da01/",categories:["系列专题","ansible"],tags:["ansible"],description:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14199445.html",readingShow:"top",meta:[{name:"image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/Dingtalk_20221209141135.jpg"},{name:"twitter:title",content:"ansible入门"},{name:"twitter:description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14199445.html"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/Dingtalk_20221209141135.jpg"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/01.ansible%E5%85%A5%E9%97%A8.html"},{property:"og:type",content:"article"},{property:"og:title",content:"ansible入门"},{property:"og:description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14199445.html"},{property:"og:image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/Dingtalk_20221209141135.jpg"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/01.ansible%E5%85%A5%E9%97%A8.html"},{property:"og:site_name",content:"zpj"},{property:"article:tag",content:"ansible"},{itemprop:"name",content:"ansible入门"},{itemprop:"description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14199445.html"},{itemprop:"image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/Dingtalk_20221209141135.jpg"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/01.ansible%E5%85%A5%E9%97%A8.html",relativePath:"01.专题/01.ansible系列文章/01.ansible入门.md",key:"v-9c028ada",path:"/pages/12c5da01/",headers:[{level:3,title:"一、Ansible基础概述",slug:"一、ansible基础概述",normalizedTitle:"一、ansible基础概述",charIndex:79},{level:4,title:"1.1 什么是Ansible",slug:"_1-1-什么是ansible",normalizedTitle:"1.1 什么是ansible",charIndex:98},{level:4,title:"1.2 Ansible 可以完成哪些功能",slug:"_1-2-ansible-可以完成哪些功能",normalizedTitle:"1.2 ansible 可以完成哪些功能",charIndex:118},{level:4,title:"1.3 Ansible特点",slug:"_1-3-ansible特点",normalizedTitle:"1.3 ansible特点",charIndex:144},{level:4,title:"1.4 Ansible的架构中的控制节点、被控制节点、inventory、ad-hoc、playbook、连接协议这些是什么？",slug:"_1-4-ansible的架构中的控制节点、被控制节点、inventory、ad-hoc、playbook、连接协议这些是什么",normalizedTitle:"1.4 ansible的架构中的控制节点、被控制节点、inventory、ad-hoc、playbook、连接协议这些是什么？",charIndex:163},{level:3,title:"二、Ansible安装配置",slug:"二、ansible安装配置",normalizedTitle:"二、ansible安装配置",charIndex:230},{level:4,title:"2.1 ansible安装",slug:"_2-1-ansible安装",normalizedTitle:"2.1 ansible安装",charIndex:249},{level:3,title:"三、Ansible inventory",slug:"三、ansible-inventory",normalizedTitle:"三、ansible inventory",charIndex:266},{level:4,title:"3.1 场景一：基于密码连接",slug:"_3-1-场景一-基于密码连接",normalizedTitle:"3.1 场景一：基于密码连接",charIndex:291},{level:4,title:"3.2 场景二：基于秘钥连接",slug:"_3-2-场景二-基于秘钥连接",normalizedTitle:"3.2 场景二：基于秘钥连接",charIndex:311},{level:4,title:"3.3 场景三：主机组使用方式",slug:"_3-3-场景三-主机组使用方式",normalizedTitle:"3.3 场景三：主机组使用方式",charIndex:331},{level:4,title:"3.4 列出每个主机组下面的主机情况",slug:"_3-4-列出每个主机组下面的主机情况",normalizedTitle:"3.4 列出每个主机组下面的主机情况",charIndex:352},{level:3,title:"四、Ansible ad-hoc",slug:"四、ansible-ad-hoc",normalizedTitle:"四、ansible ad-hoc",charIndex:374},{level:4,title:"4.1 什么是ad-hoc",slug:"_4-1-什么是ad-hoc",normalizedTitle:"4.1 什么是ad-hoc",charIndex:396},{level:4,title:"4.2 ad-hocm模式的使用场景",slug:"_4-2-ad-hocm模式的使用场景",normalizedTitle:"4.2 ad-hocm模式的使用场景",charIndex:415},{level:4,title:"4.3 ad-hoc模式的命令使用",slug:"_4-3-ad-hoc模式的命令使用",normalizedTitle:"4.3 ad-hoc模式的命令使用",charIndex:439},{level:4,title:"4.4 使用ad-doc指定一次远程命令，注意观察返回结果的颜色",slug:"_4-4-使用ad-doc指定一次远程命令-注意观察返回结果的颜色",normalizedTitle:"4.4 使用ad-doc指定一次远程命令，注意观察返回结果的颜色",charIndex:462},{level:4,title:"4.5 ad-hoc常用的模块",slug:"_4-5-ad-hoc常用的模块",normalizedTitle:"4.5 ad-hoc常用的模块",charIndex:500},{level:4,title:"4.6 使用过程中需要先了解anisble-doc帮助手册",slug:"_4-6-使用过程中需要先了解anisble-doc帮助手册",normalizedTitle:"4.6 使用过程中需要先了解anisble-doc帮助手册",charIndex:521},{level:3,title:"五、ansibel 模块",slug:"五、ansibel-模块",normalizedTitle:"五、ansibel 模块",charIndex:554},{level:4,title:"5.1 yum模块",slug:"_5-1-yum模块",normalizedTitle:"5.1 yum模块",charIndex:572},{level:4,title:"5.2 copy模块",slug:"_5-2-copy模块",normalizedTitle:"5.2 copy模块",charIndex:587},{level:4,title:"5.3 file模块",slug:"_5-3-file模块",normalizedTitle:"5.3 file模块",charIndex:603},{level:4,title:"5.4 get_url模块",slug:"_5-4-get-url模块",normalizedTitle:"5.4 get_url模块",charIndex:619},{level:4,title:"5.5 service模块",slug:"_5-5-service模块",normalizedTitle:"5.5 service模块",charIndex:638},{level:4,title:"5.6 group模块",slug:"_5-6-group模块",normalizedTitle:"5.6 group模块",charIndex:657},{level:4,title:"5.7 user模块",slug:"_5-7-user模块",normalizedTitle:"5.7 user模块",charIndex:674},{level:4,title:"5.8 cron模块",slug:"_5-8-cron模块",normalizedTitle:"5.8 cron模块",charIndex:690},{level:4,title:"5.9 mount",slug:"_5-9-mount",normalizedTitle:"5.9 mount",charIndex:706},{level:4,title:"5.10 firewalld",slug:"_5-10-firewalld",normalizedTitle:"5.10 firewalld",charIndex:721},{level:3,title:"六、Ansible playbook",slug:"六、ansible-playbook",normalizedTitle:"六、ansible playbook",charIndex:739},{level:4,title:"6.1 什么是playbook",slug:"_6-1-什么是playbook",normalizedTitle:"6.1 什么是playbook",charIndex:763},{level:4,title:"6.3 Ansible playbook书写格式",slug:"_6-3-ansible-playbook书写格式",normalizedTitle:"6.3 ansible playbook书写格式",charIndex:820},{level:3,title:"七、变量",slug:"七、变量",normalizedTitle:"七、变量",charIndex:848},{level:4,title:"7.1 变量概述",slug:"_7-1-变量概述",normalizedTitle:"7.1 变量概述",charIndex:858},{level:4,title:"7.2 变量定义",slug:"_7-2-变量定义",normalizedTitle:"7.2 变量定义",charIndex:872},{level:4,title:"7.3 变量注册",slug:"_7-3-变量注册",normalizedTitle:"7.3 变量注册",charIndex:886},{level:4,title:"7.4 facts变量",slug:"_7-4-facts变量",normalizedTitle:"7.4 facts变量",charIndex:900},{level:3,title:"八、Ansible Task 任务控制",slug:"八、ansible-task-任务控制",normalizedTitle:"八、ansible task 任务控制",charIndex:915},{level:4,title:"8.1 piaybook条件语句",slug:"_8-1-piaybook条件语句",normalizedTitle:"8.1 piaybook条件语句",charIndex:940},{level:5,title:"案例一：根据不同操作系统，安装相同的软件包",slug:"案例一-根据不同操作系统-安装相同的软件包",normalizedTitle:"案例一：根据不同操作系统，安装相同的软件包",charIndex:964},{level:5,title:"案例二：为所有web主机名的添加nginx仓库，其余的都跳过添加",slug:"案例二-为所有web主机名的添加nginx仓库-其余的都跳过添加",normalizedTitle:"案例二：为所有web主机名的添加nginx仓库，其余的都跳过添加",charIndex:993},{level:5,title:"案例三：根据命令执行的结果进行判断",slug:"案例三-根据命令执行的结果进行判断",normalizedTitle:"案例三：根据命令执行的结果进行判断",charIndex:1033},{level:4,title:"8.2 playbook循环语句",slug:"_8-2-playbook循环语句",normalizedTitle:"8.2 playbook循环语句",charIndex:1056},{level:5,title:"案例一： 使用循环启动多个服务",slug:"案例一-使用循环启动多个服务",normalizedTitle:"案例一： 使用循环启动多个服务",charIndex:1080},{level:5,title:"案例二： 使用字典批量创建用户",slug:"案例二-使用字典批量创建用户",normalizedTitle:"案例二： 使用字典批量创建用户",charIndex:1103},{level:5,title:"案例三： 使用变量字典循环的方式拷贝文件",slug:"案例三-使用变量字典循环的方式拷贝文件",normalizedTitle:"案例三： 使用变量字典循环的方式拷贝文件",charIndex:1126},{level:4,title:"8.3 playbook handlers",slug:"_8-3-playbook-handlers",normalizedTitle:"8.3 playbook handlers",charIndex:1152},{level:5,title:"案例：playbook安装nginx",slug:"案例-playbook安装nginx",normalizedTitle:"案例：playbook安装nginx",charIndex:1181},{level:4,title:"8.4 playbook tag",slug:"_8-4-playbook-tag",normalizedTitle:"8.4 playbook tag",charIndex:1205},{level:6,title:"案例：",slug:"案例",normalizedTitle:"案例：",charIndex:1181},{level:4,title:"8.5 playbook include",slug:"_8-5-playbook-include",normalizedTitle:"8.5 playbook include",charIndex:1238},{level:6,title:"案例：",slug:"案例-2",normalizedTitle:"案例：",charIndex:1181},{level:4,title:"8.6 ignore_errors忽略错误",slug:"_8-6-ignore-errors忽略错误",normalizedTitle:"8.6 ignore_errors忽略错误",charIndex:1275},{level:4,title:"8.7 playbook错误处理",slug:"_8-7-playbook错误处理",normalizedTitle:"8.7 playbook错误处理",charIndex:1302},{level:5,title:"案例一：task执行失败强制调用handlers",slug:"案例一-task执行失败强制调用handlers",normalizedTitle:"案例一：task执行失败强制调用handlers",charIndex:1326},{level:5,title:"案例二： 关闭change的状态（确定不会对被控端主机进行任何的修改和变更）",slug:"案例二-关闭change的状态-确定不会对被控端主机进行任何的修改和变更",normalizedTitle:"案例二： 关闭change的状态（确定不会对被控端主机进行任何的修改和变更）",charIndex:1358},{level:5,title:"案例三： 使用changed_when检查tasksrre任务返回结果",slug:"案例三-使用changed-when检查tasksrre任务返回结果",normalizedTitle:"案例三： 使用changed_when检查tasksrre任务返回结果",charIndex:1404}],headersStr:"一、Ansible基础概述 1.1 什么是Ansible 1.2 Ansible 可以完成哪些功能 1.3 Ansible特点 1.4 Ansible的架构中的控制节点、被控制节点、inventory、ad-hoc、playbook、连接协议这些是什么？ 二、Ansible安装配置 2.1 ansible安装 三、Ansible inventory 3.1 场景一：基于密码连接 3.2 场景二：基于秘钥连接 3.3 场景三：主机组使用方式 3.4 列出每个主机组下面的主机情况 四、Ansible ad-hoc 4.1 什么是ad-hoc 4.2 ad-hocm模式的使用场景 4.3 ad-hoc模式的命令使用 4.4 使用ad-doc指定一次远程命令，注意观察返回结果的颜色 4.5 ad-hoc常用的模块 4.6 使用过程中需要先了解anisble-doc帮助手册 五、ansibel 模块 5.1 yum模块 5.2 copy模块 5.3 file模块 5.4 get_url模块 5.5 service模块 5.6 group模块 5.7 user模块 5.8 cron模块 5.9 mount 5.10 firewalld 六、Ansible playbook 6.1 什么是playbook 6.3 Ansible playbook书写格式 七、变量 7.1 变量概述 7.2 变量定义 7.3 变量注册 7.4 facts变量 八、Ansible Task 任务控制 8.1 piaybook条件语句 案例一：根据不同操作系统，安装相同的软件包 案例二：为所有web主机名的添加nginx仓库，其余的都跳过添加 案例三：根据命令执行的结果进行判断 8.2 playbook循环语句 案例一： 使用循环启动多个服务 案例二： 使用字典批量创建用户 案例三： 使用变量字典循环的方式拷贝文件 8.3 playbook handlers 案例：playbook安装nginx 8.4 playbook tag 案例： 8.5 playbook include 案例： 8.6 ignore_errors忽略错误 8.7 playbook错误处理 案例一：task执行失败强制调用handlers 案例二： 关闭change的状态（确定不会对被控端主机进行任何的修改和变更） 案例三： 使用changed_when检查tasksrre任务返回结果",content:'原文链接：https://www.cnblogs.com/lvzhenjiang/p/14199445.html\n\n\n# ansible入门\n\n目录\n\n * 一、Ansible基础概述\n   * 1.1 什么是Ansible\n   * 1.2 Ansible 可以完成哪些功能\n   * 1.3 Ansible特点\n   * 1.4 Ansible的架构中的控制节点、被控制节点、inventory、ad-hoc、playbook、连接协议这些是什么？\n * 二、Ansible安装配置\n   * 2.1 ansible安装\n * 三、Ansible inventory\n   * 3.1 场景一：基于密码连接\n   * 3.2 场景二：基于秘钥连接\n   * 3.3 场景三：主机组使用方式\n   * 3.4 列出每个主机组下面的主机情况\n * 四、Ansible ad-hoc\n   * 4.1 什么是ad-hoc\n   * 4.2 ad-hocm模式的使用场景\n   * 4.3 ad-hoc模式的命令使用\n   * 4.4 使用ad-doc指定一次远程命令，注意观察返回结果的颜色\n   * 4.5 ad-hoc常用的模块\n   * 4.6 使用过程中需要先了解anisble-doc帮助手册\n * 五、ansibel 模块\n   * 5.1 yum模块\n   * 5.2 copy模块\n   * 5.3 file模块\n   * 5.4 get_url模块\n   * 5.5 service模块\n   * 5.6 group模块\n   * 5.7 user模块\n   * 5.8 cron模块\n   * 5.9 mount\n   * 5.10 firewalld\n * 六、Ansible playbook\n   * 6.1 什么是playbook\n   * 6.2 Ansible playbook与ad-hoc的关系\n   * 6.3 Ansible playbook书写格式\n * 七、变量\n   * 7.1 变量概述\n   * 7.2 变量定义\n   * 7.3 变量注册\n   * 7.4 facts变量\n * 八、Ansible Task 任务控制\n   * 8.1 piaybook条件语句\n     * 案例一：根据不同操作系统，安装相同的软件包\n     * 案例二：为所有web主机名的添加nginx仓库，其余的都跳过添加\n     * 案例三：根据命令执行的结果进行判断\n   * 8.2 playbook循环语句\n     * 案例一： 使用循环启动多个服务\n     * 案例二： 使用字典批量创建用户\n     * 案例三： 使用变量字典循环的方式拷贝文件\n   * 8.3 playbook handlers\n     * 案例：playbook安装nginx\n   * 8.4 playbook tag\n     * 案例：\n   * 8.5 playbook include\n     * 案例：\n   * 8.6 ignore_errors忽略错误\n   * 8.7 playbook错误处理\n     * 案例一：task执行失败强制调用handlers\n     * 案例二： 关闭change的状态（确定不会对被控端主机进行任何的修改和变更）\n     * 案例三： 使用changed_when检查tasksrre任务返回结果\n\n\n# 一、Ansible基础概述\n\n# 1.1 什么是Ansible\n\nAnsible是一个IT自动化的配置管理工具，自动化主要体现在Ansible集成了丰富模块、丰富的功能组件，可以通过一个命令完成一系列的操作。进而减少我们重复性的工作和维护成本，以提高工作效率！\n\n# 1.2 Ansible 可以完成哪些功能\n\n * 批量执行远程命令，可以对N多台主机同时进行命令的执行；\n * 批量配置软件服务，可以进行自动化的方式配置和管理服务；\n * 实现软件开发功能，Jumpserver底层使用ansible来实现的自动化管理；\n * 编排高级的IT任务，Ansible的Playbook是一门编程语言，可以用来描绘一套IT架构；\n\n# 1.3 Ansible特点\n\n * 容易学习，无代理模式，不像saltstack既要学习客户端与服务端，还要学习客户端与服务端中间通信协议；\n * 操作灵活，体现在Ansible有较多的模块，提供了丰富的功能、playbook则提供类似于编程语言的复杂功能；\n * 简单复用，体现在Ansible一个命令可以完成很多事情；\n * 安全可靠，因为Ansible使用了SSH协议进行通信，既稳定也安全；\n * 移植性高，可以将写好的playbook拷贝任意机器进行执行；\n\n# 1.4 Ansible的架构中的控制节点、被控制节点、inventory、ad-hoc、playbook、连接协议这些是什么？\n\n\n\n\n# 二、Ansible安装配置\n\n# 2.1 ansible安装\n\n$ wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo\n$ yum install ansible -y\n$ ansible --version\nansible 2.9.10\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = [u\'/root/.ansible/plugins/modules\', u\'/usr/share/ansible/plugins/modules\']\n  ansible python module location = /usr/lib/python2.7/site-packages/ansible\n  executable location = /usr/bin/ansible\n  python version = 2.7.5 (default, Nov 20 2015, 02:00:19) [GCC 4.8.5 20150623 (Red Hat 4.8.5-4)]\n\n$ ansible <host-pattern> [option]\n--version: # ansible版本信息\n-v：显示详细信息\n-i：主机清单文件路径，默认是/etc/ansibel/hosts\n-m：使用的模块的名称，默认使用command模块\n-a：使用的模块参数，模块的具体工作\n-k：提示输入ssh密码，而不使用基于ssh的秘钥认证\n-C：模拟执行测试，但不会真的执行\n-T：指定命令的超时时间\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\nansible配置文件优先级顺序：\n\n * 最先查找$ANSIBLE_CONFIG变量；\n * 其次查找当前目录下ansible.cfg\n * 然后查找用户家目录下的.ansible.cfg\n * 最后查找/etc/ansible/ansible.cfg（默认）\n\n\n# 三、Ansible inventory\n\nInventory文件中填写需要被管理主机与主机组信息（逻辑上定义）。默认Inventory文件在/etc/ansible/hosts。当然也可以自定义，然后使用-i指定Inventory文件位置：示例：\n\n# 3.1 场景一：基于密码连接\n\n$ cat /etc/ansible/hosts\n\n# 方法一\n[webservers]\n192.168.100.225 ansible_ssh_port=22 ansible_ssh_user=root ansible_ssh_pass=\'123456\'\n192.168.100.226 ansible_ssh_port=22 ansible_ssh_user=root ansible_ssh_pass=\'123456\'\n\n# 方法二\n[webservers]\nweb[1:2].lzj.com ansible_ssh_pass=\'123456\'\n\n# 方法三\n[webservers]\nweb[1:2].lzj.com\n[webservers：vars]\nansible_ssh_pass=\'123456\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 3.2 场景二：基于秘钥连接\n\n$ ssh-keygen\n$ ssh-copy-id 192.168.100.225\n$ ssh-copy-id 192.168.100.226\n$ mkdir project1/ && cd project1/\n$ vim hosts\n[web]\n192.168.100.225\n192.168.100.226\n$ ansible web -m ping -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 3.3 场景三：主机组使用方式\n\n[lbservers]\n192.168.100.221\n192.168.100.222\n\n[webservers]\n192.168.100.223\n192.168.100.224\n\n[servers:children]    # 定义servers组包含两个子组（lbservers、webservers）\nlbservers\nwebservers\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# 3.4 列出每个主机组下面的主机情况\n\n$ ansible all -i hosts --list-hosts\n  hosts (2):\n    192.168.100.225\n    192.168.100.226\n$ ansible web -i hosts --list-hosts\n  hosts (2):\n    192.168.100.225\n    192.168.100.226\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 四、Ansible ad-hoc\n\n# 4.1 什么是ad-hoc\n\nad-hoc就是临时命令，执行完成后就结束，并不会保存！\n\n# 4.2 ad-hocm模式的使用场景\n\n比如在多台机器上查看某个进程是否启动，或拷贝指定文件到本地，等等！\n\n# 4.3 ad-hoc模式的命令使用\n\n命令格式   ANSIBLE   ZPJ    -M     COMMAND   -A     \'DF -H\'\n格式说明   命令        主机名称   指定模块   模块名称      模块动作   具体命令\n\n# 4.4 使用ad-doc指定一次远程命令，注意观察返回结果的颜色\n\n * 绿色：表示被管理主机没有被修改；\n * 黄色：表示被管理端主机发现变更；\n * 红色：表示出现了故障，注意查看提示；\n\n# 4.5 ad-hoc常用的模块\n\ncommand            # 指定shell命令（不支持管道等特殊字符）\nshell            # 执行shell命令\nscripts            # 执行shell脚本\nyum_repository    # 配置yum仓库\nyum                # 安装软件\ncopy            # 变更配置文件\nfile            # 建立目录或文件\nservice            # 启动、停止服务\nmount            # 挂载设备\ncron            # 定时任务\nfirewalld        # 防火墙\nget_url            # 下载软件\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 4.6 使用过程中需要先了解anisble-doc帮助手册\n\n$ ansible-doc -l        # 查看所有模块说明\n$ ansible-doc copy        # 表示指定模块方法\n$ ansible-doc -s copy    # 表示指定模块参数\n\n\n1\n2\n3\n\n\n\n# 五、ansibel 模块\n\n# 5.1 yum模块\n\n# 示例一： 安装当前最新的Apache软件，如果存在则更新\n$ ansible web -m yum -a "name=httpd state=latest" -i hosts\n\n# 示例二： 安装当前最新的Apache软件，通过epel安装\n$ ansible web -m yum -a "name=httpd state=present enablerepo=epel" -i hosts\n\n# 示例三： 通过公网URL安装rpm软件\n$ ansible web -m yum -a "name=https://mirrors4.tuna.tsinghua.edu.cn/zabbix/zabbix/4.0/rhel/7/x86_64/zabbix-agent-4.0.0-2.el7.x86_64.rpm state=present" -i hosts\n\n# 示例四：更新所有的软件包，但排除和kernel相关的\n$ ansible web -m yum -a "name="*" state=present exclude="kernel*" -i hosts\n\n# 示例五： 删除Apache软件\n$  ansible web -m yum -a "name=httpd state=absent" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 5.2 copy模块\n\n# 示例一： 将本地的httpd.conf文件Listen 端口修改为9999，然后推送到远程主机\n$ sed -i \'s/Listen 80/Listen 9999/g\' httpd.conf\n$ ansible web -m copy -a "src=httpd.conf dest=/etc/httpd/conf/httpd.conf owner=root group=root mode-644" -i hosts\n\n# 示例二： 将本地的httpd.conf文件Listen端口修改为9988，然后推送到远端，检查远端是否存在上次备份的文件\n$ sed -i \'s/Listen 9999/Listen 9988/g\' httpd.conf\n$ ansible web -m copy -a "src=httpd.conf dest=/etc/httpd/conf/httpd.conf owner=root group=root mode=644 backup=yes" -i hosts\n\n# 示例三： 往远程的主机文件写入内容\n$ ansible web -m copy -a "content=123456........ dest=/var/www/html/index.html" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 5.3 file模块\n\n# 示例一： 创建文件，并设置属主、属组、权限\n$ ansible web -m file -a "path=/var/www/html/t1.html state=touch owner=apache group=apache mode=644" -i hosts\n\n# 示例二： 创建目录，并设置属主、属组、权限\n$ ansible web -m file -a "path=/var/www/html/dd state=directory owner=apache group=apache mode=755" -i hosts\n# 示例三：递归授权目录的方式\n$ ansible web -m file -a "path=/var/www/html state=directory owner=apache group=apache recurse=yes" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 5.4 get_url模块\n\n# 示例一： 下载互联网的软件到本地\n$ ansible web -m get_url -a "url=http://192.168.99.181:12138/document/Ansible.docx dest=/root/" -i hosts\n\n# 示例二： 下载互联网文件并进行mds校验\n$ md5sum Ansible.docx      # 获取文件md5值\n7b1a6fe7af7252005987e16ce64e1e1a  Ansible.docx\n$ ansible web -m get_url -a "url=http://192.168.99.181:12138/document/Ansible.docx dest=/root/ checksum=md5:7b1a6fe7af7252005987e16ce64e1e1a" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 5.5 service模块\n\n# 示例一： 启动httpd服务\n$ ansible web -m service -a "name=httpd state=started" -i hosts\n\n# 示例二： 重载httpd服务\n$ ansible web -m service -a "name=httpd state=reloaded" -i hosts\n\n# 示例三： 重启httpd服务\n$ ansible web -m service -a "name=httpd state=restarted" -i hosts\n\n# 示例四： 停止httpd服务\n$ ansible web -m service -a "name=httpd state=stopped" -i hosts\n\n# 示例五： 启动httpd服务，并加入开机自启\n$ ansible web -m service -a "name=httpd state=started enabled=yes" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 5.6 group模块\n\n# 示例一： 创建news基本组，指定gid为9999\n$ ansible web -m group -a "name=news gid=9999 state=present"\n -i hosts\n\n# 示例二： 创建http系统组，指定gid为8888\n$ ansible web -m group -a "name=http gid=8888  state=present\n system=yes" -i hosts\n\n# 示例三： 删除test基本组\n$ ansible web -m group -a "name=test state=absent" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 5.7 user模块\n\n# 示例一： 创建job用户，uid是1040，主要组是adm\n$ ansible web -m user -a "name=job uid=1040 group=adm" -i hosts\n\n# 示例二： 创建joh用户，登录shell是/sbin/nologin，追加bin、sys两个组\n$ ansible web -m user -a "name=joh groups=bin,sys shell=/sbin/nologin" -i hosts\n\n# 示例三： 创建jsm用户，为其添加123作为登录密码，并创建家目录\n$ ansible localhost -m debug -a "msg={{ \'123\' | password_hash(\'sha512\',\'salt\') }}"\n# 获取123加密后的字符串\n$6$salt$jkHSO0tOjmLW0S1NFlw5veSIDRAVsiQQMTrkOKy4xdCCLPNIsHhZkIRlzfzIvKyXeGdOfCBoW1wJZPLyQ9Qx/1\n\n$ ansible web -m user -a \'name=jsm password=$6$salt$jkHSO0tOjmLW0S1NFlw5veSIDRAVsiQQMTrkOKy4xdCCLPNIsHhZkIRlzfzIvKyXeGdOfCBoW1wJZPLyQ9Qx/1 create_home=yes\' -i hosts\n\n# 示例四： 移除job用户\n$ ansible web -m user -a "name=job state=absent remove=yes" -i hosts\n# remove=yes删除普通用户家目录\n\n# 示例五： 创建http用户，并为该用户创建2048字节的私钥，存放在~/http/.ssh/id_rsa\n$ ansible web -m user -a "name=http generate_ssh_key=yes ssh_key_bits=2048 ssh_key_file=.ssh/id_rsa" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 5.8 cron模块\n\n# 示例一： 添加定时任务，每分钟执行一次ls\n$ ansible web -m cron -a "name=job1 job=\'ls > /dev/null\'" -i hosts\n\n# 示例二： 添加定时任务，每天的凌晨2点和凌晨5点执行一次ls\n$ ansible web -m cron -a "name=job2 job=\'ls > /dev/null\' minute=0 hour=2,5" -i hosts\n\n# 示例三： 关闭定时任务，使定时任务失败\n$ ansible web -m cron -a "name=job2 job=\'ls > /dev/null\' minute=0 hour=2,5 disabled=yes" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 5.9 mount\n\n# 环境准备：\n$ ansible localhost -m yum -a "name=nfs-utils state=present" \n$ ansible localhost -m file -a "path=/ops state=directory"\n$ ansible localhost -m copy -a \'content="/ops 192.168.100.1/24(rw,sync)" dest=/etc/exports\'\n$ ansible localhost -m service -a "name=nfs state=restarted"\n\n# 示例一： 挂载nfs存储到本地的/opt目录，并实现开机自动挂载\n$ ansible web -m mount -a "src=192.168.99.181:/ops path=/opt fstype=nfs opts=defaults state=mounted" -i hosts\n\n# 示例二： 临时卸载nfs的挂载，但不会清理/etc/fstab\n$ ansible web -m mount -a "src=192.168.99.181:/ops path=/opt fstype=nfs opts=defaults state=unmounted" -i hosts\n\n# 示例三： 永久卸载ns的挂载，清理/etc/fstab\n$ ansible web -m mount -a "src=192.168.99.181:/ops path=/opt fstype=nfs opts=defaults state=absent" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 5.10 firewalld\n\n# 示例一： 永久放行https的流量，只有重启才生效\n$ ansible web -m firewalld -a "zone=public service=https permanent=yes state=enabled" -i hosts\n\n# 示例二： 永久放行8081端口的流量，只有重启才生效\n$ ansible web -m firewalld -a "zone=public port=8081/tcp permanent=yes state=enabled" -i hosts\n\n# 示例三： 放行8080-8090的所有的tcp端口流量，临时和永久都生效\n$ ansible web -m firewalld -a "zone=public port=8080-8090/tcp immediate=yes permanent=yes state=enabled" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 六、Ansible playbook\n\n# 6.1 什么是playbook\n\n * playbook：定义了一个文本文件，以.yml为后缀结尾；\n * play：定义的是主机的角色；\n * task：定义的是具体执行的任务；\n\n总结：playbook是由一个多个多个play组成，一个play可以包含多个task任务。可以理解为：使用不同的模块来共同完成一件事情！\n\n---\n#### 6.2 Ansible playbook与ad-hoc的关系\n\n\n * playbook是对ad-hoc的一种编排方式；\n * playbook可以持久运行，而ad-hoc只能临时运行；\n * playbook适合复杂的任务，而ad-hoc适合做快速简单的任务；\n * playbook能控制任务执行的先后顺序；\n\n# 6.3 Ansible playbook书写格式\n\nplaybook是由yaml语法书写，结构清晰、可读性强。所以必须掌握yaml基础语法！\n\n语法    概述\n缩进    YAML使用固定的缩进风格表示层级结构，每个缩进由两个空格组成，不能使用ta键\n冒号    以冒号结尾的除外，其他所有冒号后面所有必须有空格\n短横线   表示列表项，使用一个短横杠加一个空格。多个项使用同样的缩进级别作为统一列表\n\n\n# 七、变量\n\n# 7.1 变量概述\n\n变量提供了便捷的方式来管理ansible项目中的动态值，比如：nginx-1.12，可能后期会反复的使用这个版本的值，那么如果将此值设置为变量，后续使用和修改都将变得非常方便。这样可以简化项目的创建和维护。\n\n那么在ansible中定义变量分为以下三种方式：\n\n * 通过命令行进行变量定义；\n * 在play文件中进行定义变量；\n * 通过Inventory在主机组或单个主机中设置变量；\n\n如果定义的变量出现重复，且造成冲突，怎么办？谁说了算？\n\n# 7.2 变量定义\n\n1、在playbook的文件开头通过vars关键字进行变量定义\n\n- hosts: web\n  vars:\n    - web_packages: httpd\n    - ftp_packages: vsftpd\n\n  tasks:\n    - name: Install Rpm Packages "{{ web_packages }}" "{{ ftp_packages }}"\n      yum:\n        name:\n          - "{{ web_packages }}"\n          - "{{ ftp_packages }}"\n        state: present\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n2、可以在playbook中使用vars_files指定文件作为变量文件，好处就是其他的playbook也可以调用\n\n# 1）准备一个变量的文件，建议使用yml格式\n$ cat vars.yml\nweb_packages: httpd\nftp_packages: vsftpd\n\n# 2）准备playbook进行调用\n$ cat test.yml\n- hosts: web\n  vars_files:\n    - vars.yml\n  tasks:\n    - name: Install Rpm Packages "{{ web_packages }}" "{{ ftp_packages }}"\n      yum:\n        name:\n          - "{{ web_packages }}"\n          - "{{ ftp_packages }}"\n        state: present\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n3、在Inventory主机清单中定义变量，但是注意：主机变量优先级高于主机组变量\n\n# 1）在Inventory主机清单中对主机组进行定义变量\n$ cat hosts\n[web]\n192.168.100.225\n192.168.100.226\n\n[web:vars]\nfilename=group_vars\n\n# 2）playbook中直接调用变量\n$ cat test.yml\n- hosts: web\n  tasks:\n    - name: Create File\n      file:\n        name: /tmp/{{ filename}}\n        state: touch\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n4、官方建议是在ansible项目目录中创建两个额外的变量目录，分别是host_vars和group_vars\n\n * 测试group_vars定义变量方式\n\n$ cat hosts\n[web]\n192.168.100.225\n192.168.100.226\n\n# 1）在当前的项目目录中创建两个变量的目录\n$ mkdir {group,host}_vars\n\n# 2）在group_vars目录中创建一个文件，文件名与Inventory清单中的主机组名称要保持一致\n$ cat group_vars/web\nweb_packages: wget\nftp_packages: tree\n\n# 3）编写playbook，只需在playbook中使用变量即可\n$ cat test.yml\n- hosts: web\n  tasks:\n    - name: Install Rpm Packages "{{ web_packages }}" "{{ ftp_packages }}"\n      yum:\n        name:\n          - "{{ web_packages }}"\n          - "{{ ftp_packages }}"\n        state: present\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n测试test组能否使用web组定义的变量，无法使用！但是系统提供了特殊的all组，也就是说group_vars目录下创建一个all文件，定义变量对所有的主机都生效！\n\n * 测试host_vars定义变量方式\n\n# 1）在host_vars目录创建一个文件，文件名与Inventory清单中的主机名称要保持一致，如果是IP地址，则创建相同的IP\n$ cat hosts\n[web]\n192.168.100.225\n192.168.100.226\n\n# 2）在host_vars目录中创建文件，给192.168.100.225主机定义变量\n$ cat host_vars/192.168.100.225\nweb_packages: zlib-static\nftp_packages: zmap\n\n# 3）转别一个playbook文件调用host主机变量\n$ cat test.yml\n- hosts: 192.168.100.225\n  tasks:\n    - name: Install Rpm Packages "{{ web_packages }}" "{{ ftp_packages }}"\n      yum:\n        name:\n          - "{{ web_packages }}"\n          - "{{ ftp_packages }}"\n        state: present\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n5、通过命令行-extra-vars或-e外置穿餐设定变量\n\n# 1）准备playbook文件\n$ cat test.yml\n- hosts: "{{ host }}"\n  tasks:\n    - name: Install Rpm Packages "{{ web_packages }}" "{{ ftp_packages }}"\n      yum:\n        name:\n          - "{{ web_packages }}"\n          - "{{ ftp_packages }}"\n        state: present\n\n# 2）执行playbook时进行变量的传递\n$ ansible-playbook -i hosts test.yml -e "host=web"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n6、变量的优先级顺序：\n\n命令行变量——>paly中的vars_files——>paly中的vars——>host_vars——>group_vars/all\n\n# 7.3 变量注册\n\nregister关键字可以将某个task任务结果存储至变量中，最后使用debug输出变量内容，可以用于后续排查故障！\n\n- hosts: all\n  tasks:\n    - name: system status\n      shell: netstat -lntp\n      register: System_Status\n\n    - name: System Status\n      debug: msg="{{ System_Status.stdout_lines }}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 7.4 facts变量\n\nAnsible facts是在被管理主机上通过ansible自动采集发现的变量。facts包含每台特定的主机信息。比如：被控端主机的主机名、IP地址、系统版本、CPU数量、内存状态、磁盘状态等等。\n\n默认情况的facts变量名都已经预先定义好了，只需要采集被控端的信息，然后传递至facts变量即可！\n\n $ ansible web -m setup -i hosts > facts.txt\n # 获取所有的facts变量\n\n\n1\n2\n\n\nfacts变量使用场景：\n\n * 通过facts变量检查被控端主机硬件CPU信息，从而生成不同的Nginx配置文件；\n\n * 通过facts变量检查被控端主机名称信息，从而生成不同的zabbix配置文件；\n\n * 通过facts变量检查被控端主机内存状态信息，从而生成不同的memcached配置文件；\n   \n   ………………\n\n1、facts基本用法：比如获取被控端的主机名和IP地址，然后通过debug输出\n\n- hosts: all\n  tasks:\n    - name: OutPut variables ansible facts\n      debug:\n        msg: this default IPv4 address "{{ ansible_fqdn }}" is "{{ ansible_default_ipv4.address }}"\n\n\n1\n2\n3\n4\n5\n\n\n2、facts开启后会影响Ansible主机的性能，如果没有采集被控端主机需求可选择关闭\n\n- hosts: web\n  gather_facts: no    # 关闭信息采集\n\n\n1\n2\n\n\n\n# 八、Ansible Task 任务控制\n\n# 8.1 piaybook条件语句\n\n判断在Ansible任务中的使用频率非常高，比如yum模块可以检测软件包是否已经被安装，而在这个过程中我们不用做太多的人工干预。但是也有部分任务需要进行判断。\n\n# 案例一：根据不同操作系统，安装相同的软件包\n\n- hosts: web\n  tasks:\n    - name: Centos Install Httpd\n      yum:\n        name: httpd\n        state: present\n      when: ( ansible_distribution == "CentOS" )\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 案例二：为所有web主机名的添加nginx仓库，其余的都跳过添加\n\n- hosts: all\n  tasks:\n    - name: Create YUM Repo\n      yum_repository:\n        name: ansible_nginx\n        description: ansible_test\n        baseurl: http://nginx.org/packages/centos/$releasever/$basearch/\n        enabled: yes\n        gpgcheck: no\n      when: ( ansible_fqdn is match("web*") )\n\n# 当然when也可以使用and与or方式\n# when： ( ansible_fqdn is match("web*") ) or\n#         ( ansible_fqdn is match("lb*") )\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 案例三：根据命令执行的结果进行判断\n\n通过register将命令执行结果保存至变量，然后通过when语句进行判断\n\n- hosts: all\n  tasks:\n    - name: Check Httpd Server\n      command: systemctl is-active httpd\n      ignore_errors: yes        # 忽略错误\n      register: check_httpd\n\n    - name: Httpd Restart        # 如果check_httpd执行命令结果等于0，则重启httpd,否则跳过\n      service:\n        name: httpd\n        state: restarted\n      when: check_httpd.rc == 0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 8.2 playbook循环语句\n\n有时候我们写playbook的时候会发现了很多task都要重复引用某个模块，比如一次启动10个服务，或者一个拷贝10个文件。如果安装传统的写法最少要写10次，这样会显得playbook很臃肿。如果使用循环的方式来编写playbokk，这样可以减少重复使用某个模块！\n\n# 案例一： 使用循环启动多个服务\n\n- hosts: all\n  tasks:\n    - name: Service Start\n      service: \n        name: "{{ item }}"\n        state: started\n      with_items:\n        - httpd\n        - sshd\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 案例二： 使用字典批量创建用户\n\n- hosts: all\n  tasks:\n    - name: Add Users\n      user: \n        name: "{{ item.name }}"\n        group: "{{item.group }}"\n        state: present\n      with_items:\n        - { name: \'test01\',group: \'bin\' }\n        - { name: \'test02\',group: \'root\' }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 案例三： 使用变量字典循环的方式拷贝文件\n\n- hosts: all\n  tasks:\n    - name: Copy File\n      copy:\n        src: "{{ item.src }}"\n        dest: "{{ item.dest }}"\n        mode: "{{ item.mode }}"\n      with_items:\n        - { src: \'rsync.conf\' , dest: \'/tmp/rsync.conf\' , mode: 644 }\n        - { src: \'rsync.pass\' , dest: \'/tmp/rsync.pass\' , mode: 600 }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n高级写法(变量套变量)：\n\n- hosts: all\n  vars:\n    - Path: /tmp\n  tasks:\n    - name: Copy File\n      copy:\n        src: "{{ item.src }}"\n        dest: "{{ item.dest }}"\n        mode: "{{ item.mode }}"\n      with_items:\n        - { src: \'rsync.conf\' , dest: \'{{ Path }}/rsync.conf\' , mode: 644 }\n        - { src: \'rsync.pass\' , dest: \'{{ Path }}/rsync.pass\' , mode: 600 }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 8.3 playbook handlers\n\nhandlers是一个触发器，也是一个tasks，只不过是一个特殊的tasks，它是需要被tasks触发才会执行。只要配置文件发生变更，则会触发handlers执行重启服务操作，如果配置文件不发生任何变化，则不会重启！ notify监控——>通知——>handlers触发\n\n# 案例：playbook安装nginx\n\n- hosts: web\n  tasks:\n  - name: Instll Nginx\n    yum: \n      name: nginx\n      state: present\n\n  - name: Copy Nignx.conf\n    template:\n      src: nginx.conf.j2\n      dest: /etc/nginx/nginx.conf\n    notify: Restart Nignx Service\n\n  handlers:\n    - name: Restart Nignx Service\n      service:\n        name: nginx\n        state: restarted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nhandlers注意事项：\n\n * 无论多少个task通知相同的handlers，handlers仅会在所有task结束后执行一次；\n * 只有task发生变化后才会通知handlers，没有改变则不会触发handlers；\n * 不能使用handlers代替task，因为handlers是一个特殊的task；\n\n# 8.4 playbook tag\n\n默认情况下，Ansible在执行一个playbook时，会执行playbook中的定义的所有任务。Ansible的标签（Tags）功能可以给单独任务甚至整个playbook打上tag标签，然后利用tag标签指定要运行的个别任务，或跳过个别的任务！\n\n# 案例：\n\n- hosts: web\n  tasks:\n  - name: Instll Nginx\n    yum: \n      name: nginx\n      state: present\n    tags: install nginx            # 添加tag标签\n\n  - name: Copy Nignx.conf\n    template:\n      src: nginx.conf.j2\n      dest: /etc/nginx/nginx.conf\n    notify: Restart Nignx Service\n\n  - name: Start Nginx \n    service:\n      name: nginx\n      state: started\n    tags: start nginx          # 添加tag标签\n\n  handlers:\n    - name: Restart Nignx Service\n      service:\n        name: nginx\n        state: restarted\n$ ansible-playbook -i hosts test.yml  -t \'install nginx\'\n# 仅仅执行playbook中的某个tag标签\n\n$ $ ansible-playbook -i hosts test.yml  --skip-tags \'install nginx\'\n# 跳过playbook中的特定的tag标签\n\n$ ansible-playbook -i hosts test.yml  -t \'install nginx\' -t \'start nginx\'\n# 指定多个tag标签\n\n$ ansible-playbook -i hosts test.yml  --skip-tags \'install nginx\' --skip-tags \'start nginx\'\n# 跳过多个tag标签\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n# 8.5 playbook include\n\n有时发现大量的playbook内容要重复编写，与tasks之间贡呢需相互调用才能完成各自贡呢。playbook庞大到维护困难，这是就需要使用include！\n\n# 案例：\n\n$ cat restart_httpd.yml \n- name: Restart Httpd Service\n  service:\n    name: httpd\n    state: restarted\n\n\n1\n2\n3\n4\n5\n\n\nA Project的yaml文件：\n\n- hosts: web\n  tasks:\n    - name: A Project command\n      command: echo "A"\n    - name: Restart Httpd\n      include: restart_httpd.yml\n\n\n1\n2\n3\n4\n5\n6\n\n\nA Project的yaml文件：\n\n- hosts: web\n  tasks:\n    - name: B Project command\n      command: echo "B"\n    - name: Restart Httpd\n      include: restart_httpd.yml\n\n\n1\n2\n3\n4\n5\n6\n\n\n高级用法：\n\n一次执行的多个playbook文件！\n\n$ cat tasks_total.yml \n- import_playbook: test.yml\n- import_playbook: test01.yml\n\n$ ansible-playbook -i hosts  tasks_total.yml\n\n\n1\n2\n3\n4\n5\n\n\n# 8.6 ignore_errors忽略错误\n\n- hosts: web\n  tasks:\n    - name: Command\n      command: /bin/false\n      ignore_errors: yes\n\n    - name: Create File\n      file:\n        path: /tmp/file\n        state: touch\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 8.7 playbook错误处理\n\n通常情况下，当task失败后，play将会终止，任何在前面已经被tasks notify的handlers都不会被执行。如果在play中设置force_handlers，被通知的handlers就会被强制执行。\n\n# 案例一：task执行失败强制调用handlers\n\n- hosts: web\n  force_handlers: yes     # 强制调用handlers\n\n  tasks:\n    - name: Touch File\n      file:\n        path: /tmp/test\n        state: touch\n      notify: Restart Httpd Service\n\n    - name: Install Packages\n      yum: \n        name: sb\n        state: present\n\n  handlers:\n    - name: Restart Httpd Service\n      service:\n        name: httpd\n        state: restarted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 案例二： 关闭change的状态（确定不会对被控端主机进行任何的修改和变更）\n\n- hosts: web\n  tasks:\n    - name: Install Httpd Server\n      yum: \n        name: httpd\n        state: present\n    - name: Service Httpd Started\n      service:\n        name: httpd\n        state: started\n    - name: Check Httpd Server\n      shell: ps aux | grep httpd\n      register: check_httpd\n      changed_when: false\n    - name: OutPut Variables\n      debug:\n        msg: "{{ check_httpd.stdout_lines }}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 案例三： 使用changed_when检查tasksrre任务返回结果\n\n- hosts: web\n  tasks:\n    - name: Configure Nginx Server\n      template:\n        src: nginx.conf.j2\n        dest: /etc/nginx/nginx.conf\n      notify: Restart Nginx Server\n\n    - name: Check Httpd\n      shell: /usr/sbin/nginx -t\n      register: nginx_check\n      changed_when:\n        - (nginx_check.stdout.find(\'successful\'))\n        - false\n\n    - name: Start Httpd Server\n      service:\n        name: nginx\n        state: started\n        enabled: yes\n\n    - handlers: Restart Httpd Server\n      service:\n        name: nginx\n        state: restarted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n',normalizedContent:'原文链接：https://www.cnblogs.com/lvzhenjiang/p/14199445.html\n\n\n# ansible入门\n\n目录\n\n * 一、ansible基础概述\n   * 1.1 什么是ansible\n   * 1.2 ansible 可以完成哪些功能\n   * 1.3 ansible特点\n   * 1.4 ansible的架构中的控制节点、被控制节点、inventory、ad-hoc、playbook、连接协议这些是什么？\n * 二、ansible安装配置\n   * 2.1 ansible安装\n * 三、ansible inventory\n   * 3.1 场景一：基于密码连接\n   * 3.2 场景二：基于秘钥连接\n   * 3.3 场景三：主机组使用方式\n   * 3.4 列出每个主机组下面的主机情况\n * 四、ansible ad-hoc\n   * 4.1 什么是ad-hoc\n   * 4.2 ad-hocm模式的使用场景\n   * 4.3 ad-hoc模式的命令使用\n   * 4.4 使用ad-doc指定一次远程命令，注意观察返回结果的颜色\n   * 4.5 ad-hoc常用的模块\n   * 4.6 使用过程中需要先了解anisble-doc帮助手册\n * 五、ansibel 模块\n   * 5.1 yum模块\n   * 5.2 copy模块\n   * 5.3 file模块\n   * 5.4 get_url模块\n   * 5.5 service模块\n   * 5.6 group模块\n   * 5.7 user模块\n   * 5.8 cron模块\n   * 5.9 mount\n   * 5.10 firewalld\n * 六、ansible playbook\n   * 6.1 什么是playbook\n   * 6.2 ansible playbook与ad-hoc的关系\n   * 6.3 ansible playbook书写格式\n * 七、变量\n   * 7.1 变量概述\n   * 7.2 变量定义\n   * 7.3 变量注册\n   * 7.4 facts变量\n * 八、ansible task 任务控制\n   * 8.1 piaybook条件语句\n     * 案例一：根据不同操作系统，安装相同的软件包\n     * 案例二：为所有web主机名的添加nginx仓库，其余的都跳过添加\n     * 案例三：根据命令执行的结果进行判断\n   * 8.2 playbook循环语句\n     * 案例一： 使用循环启动多个服务\n     * 案例二： 使用字典批量创建用户\n     * 案例三： 使用变量字典循环的方式拷贝文件\n   * 8.3 playbook handlers\n     * 案例：playbook安装nginx\n   * 8.4 playbook tag\n     * 案例：\n   * 8.5 playbook include\n     * 案例：\n   * 8.6 ignore_errors忽略错误\n   * 8.7 playbook错误处理\n     * 案例一：task执行失败强制调用handlers\n     * 案例二： 关闭change的状态（确定不会对被控端主机进行任何的修改和变更）\n     * 案例三： 使用changed_when检查tasksrre任务返回结果\n\n\n# 一、ansible基础概述\n\n# 1.1 什么是ansible\n\nansible是一个it自动化的配置管理工具，自动化主要体现在ansible集成了丰富模块、丰富的功能组件，可以通过一个命令完成一系列的操作。进而减少我们重复性的工作和维护成本，以提高工作效率！\n\n# 1.2 ansible 可以完成哪些功能\n\n * 批量执行远程命令，可以对n多台主机同时进行命令的执行；\n * 批量配置软件服务，可以进行自动化的方式配置和管理服务；\n * 实现软件开发功能，jumpserver底层使用ansible来实现的自动化管理；\n * 编排高级的it任务，ansible的playbook是一门编程语言，可以用来描绘一套it架构；\n\n# 1.3 ansible特点\n\n * 容易学习，无代理模式，不像saltstack既要学习客户端与服务端，还要学习客户端与服务端中间通信协议；\n * 操作灵活，体现在ansible有较多的模块，提供了丰富的功能、playbook则提供类似于编程语言的复杂功能；\n * 简单复用，体现在ansible一个命令可以完成很多事情；\n * 安全可靠，因为ansible使用了ssh协议进行通信，既稳定也安全；\n * 移植性高，可以将写好的playbook拷贝任意机器进行执行；\n\n# 1.4 ansible的架构中的控制节点、被控制节点、inventory、ad-hoc、playbook、连接协议这些是什么？\n\n\n\n\n# 二、ansible安装配置\n\n# 2.1 ansible安装\n\n$ wget -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo\n$ yum install ansible -y\n$ ansible --version\nansible 2.9.10\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = [u\'/root/.ansible/plugins/modules\', u\'/usr/share/ansible/plugins/modules\']\n  ansible python module location = /usr/lib/python2.7/site-packages/ansible\n  executable location = /usr/bin/ansible\n  python version = 2.7.5 (default, nov 20 2015, 02:00:19) [gcc 4.8.5 20150623 (red hat 4.8.5-4)]\n\n$ ansible <host-pattern> [option]\n--version: # ansible版本信息\n-v：显示详细信息\n-i：主机清单文件路径，默认是/etc/ansibel/hosts\n-m：使用的模块的名称，默认使用command模块\n-a：使用的模块参数，模块的具体工作\n-k：提示输入ssh密码，而不使用基于ssh的秘钥认证\n-c：模拟执行测试，但不会真的执行\n-t：指定命令的超时时间\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\nansible配置文件优先级顺序：\n\n * 最先查找$ansible_config变量；\n * 其次查找当前目录下ansible.cfg\n * 然后查找用户家目录下的.ansible.cfg\n * 最后查找/etc/ansible/ansible.cfg（默认）\n\n\n# 三、ansible inventory\n\ninventory文件中填写需要被管理主机与主机组信息（逻辑上定义）。默认inventory文件在/etc/ansible/hosts。当然也可以自定义，然后使用-i指定inventory文件位置：示例：\n\n# 3.1 场景一：基于密码连接\n\n$ cat /etc/ansible/hosts\n\n# 方法一\n[webservers]\n192.168.100.225 ansible_ssh_port=22 ansible_ssh_user=root ansible_ssh_pass=\'123456\'\n192.168.100.226 ansible_ssh_port=22 ansible_ssh_user=root ansible_ssh_pass=\'123456\'\n\n# 方法二\n[webservers]\nweb[1:2].lzj.com ansible_ssh_pass=\'123456\'\n\n# 方法三\n[webservers]\nweb[1:2].lzj.com\n[webservers：vars]\nansible_ssh_pass=\'123456\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 3.2 场景二：基于秘钥连接\n\n$ ssh-keygen\n$ ssh-copy-id 192.168.100.225\n$ ssh-copy-id 192.168.100.226\n$ mkdir project1/ && cd project1/\n$ vim hosts\n[web]\n192.168.100.225\n192.168.100.226\n$ ansible web -m ping -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 3.3 场景三：主机组使用方式\n\n[lbservers]\n192.168.100.221\n192.168.100.222\n\n[webservers]\n192.168.100.223\n192.168.100.224\n\n[servers:children]    # 定义servers组包含两个子组（lbservers、webservers）\nlbservers\nwebservers\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# 3.4 列出每个主机组下面的主机情况\n\n$ ansible all -i hosts --list-hosts\n  hosts (2):\n    192.168.100.225\n    192.168.100.226\n$ ansible web -i hosts --list-hosts\n  hosts (2):\n    192.168.100.225\n    192.168.100.226\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 四、ansible ad-hoc\n\n# 4.1 什么是ad-hoc\n\nad-hoc就是临时命令，执行完成后就结束，并不会保存！\n\n# 4.2 ad-hocm模式的使用场景\n\n比如在多台机器上查看某个进程是否启动，或拷贝指定文件到本地，等等！\n\n# 4.3 ad-hoc模式的命令使用\n\n命令格式   ansible   zpj    -m     command   -a     \'df -h\'\n格式说明   命令        主机名称   指定模块   模块名称      模块动作   具体命令\n\n# 4.4 使用ad-doc指定一次远程命令，注意观察返回结果的颜色\n\n * 绿色：表示被管理主机没有被修改；\n * 黄色：表示被管理端主机发现变更；\n * 红色：表示出现了故障，注意查看提示；\n\n# 4.5 ad-hoc常用的模块\n\ncommand            # 指定shell命令（不支持管道等特殊字符）\nshell            # 执行shell命令\nscripts            # 执行shell脚本\nyum_repository    # 配置yum仓库\nyum                # 安装软件\ncopy            # 变更配置文件\nfile            # 建立目录或文件\nservice            # 启动、停止服务\nmount            # 挂载设备\ncron            # 定时任务\nfirewalld        # 防火墙\nget_url            # 下载软件\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 4.6 使用过程中需要先了解anisble-doc帮助手册\n\n$ ansible-doc -l        # 查看所有模块说明\n$ ansible-doc copy        # 表示指定模块方法\n$ ansible-doc -s copy    # 表示指定模块参数\n\n\n1\n2\n3\n\n\n\n# 五、ansibel 模块\n\n# 5.1 yum模块\n\n# 示例一： 安装当前最新的apache软件，如果存在则更新\n$ ansible web -m yum -a "name=httpd state=latest" -i hosts\n\n# 示例二： 安装当前最新的apache软件，通过epel安装\n$ ansible web -m yum -a "name=httpd state=present enablerepo=epel" -i hosts\n\n# 示例三： 通过公网url安装rpm软件\n$ ansible web -m yum -a "name=https://mirrors4.tuna.tsinghua.edu.cn/zabbix/zabbix/4.0/rhel/7/x86_64/zabbix-agent-4.0.0-2.el7.x86_64.rpm state=present" -i hosts\n\n# 示例四：更新所有的软件包，但排除和kernel相关的\n$ ansible web -m yum -a "name="*" state=present exclude="kernel*" -i hosts\n\n# 示例五： 删除apache软件\n$  ansible web -m yum -a "name=httpd state=absent" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 5.2 copy模块\n\n# 示例一： 将本地的httpd.conf文件listen 端口修改为9999，然后推送到远程主机\n$ sed -i \'s/listen 80/listen 9999/g\' httpd.conf\n$ ansible web -m copy -a "src=httpd.conf dest=/etc/httpd/conf/httpd.conf owner=root group=root mode-644" -i hosts\n\n# 示例二： 将本地的httpd.conf文件listen端口修改为9988，然后推送到远端，检查远端是否存在上次备份的文件\n$ sed -i \'s/listen 9999/listen 9988/g\' httpd.conf\n$ ansible web -m copy -a "src=httpd.conf dest=/etc/httpd/conf/httpd.conf owner=root group=root mode=644 backup=yes" -i hosts\n\n# 示例三： 往远程的主机文件写入内容\n$ ansible web -m copy -a "content=123456........ dest=/var/www/html/index.html" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 5.3 file模块\n\n# 示例一： 创建文件，并设置属主、属组、权限\n$ ansible web -m file -a "path=/var/www/html/t1.html state=touch owner=apache group=apache mode=644" -i hosts\n\n# 示例二： 创建目录，并设置属主、属组、权限\n$ ansible web -m file -a "path=/var/www/html/dd state=directory owner=apache group=apache mode=755" -i hosts\n# 示例三：递归授权目录的方式\n$ ansible web -m file -a "path=/var/www/html state=directory owner=apache group=apache recurse=yes" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 5.4 get_url模块\n\n# 示例一： 下载互联网的软件到本地\n$ ansible web -m get_url -a "url=http://192.168.99.181:12138/document/ansible.docx dest=/root/" -i hosts\n\n# 示例二： 下载互联网文件并进行mds校验\n$ md5sum ansible.docx      # 获取文件md5值\n7b1a6fe7af7252005987e16ce64e1e1a  ansible.docx\n$ ansible web -m get_url -a "url=http://192.168.99.181:12138/document/ansible.docx dest=/root/ checksum=md5:7b1a6fe7af7252005987e16ce64e1e1a" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 5.5 service模块\n\n# 示例一： 启动httpd服务\n$ ansible web -m service -a "name=httpd state=started" -i hosts\n\n# 示例二： 重载httpd服务\n$ ansible web -m service -a "name=httpd state=reloaded" -i hosts\n\n# 示例三： 重启httpd服务\n$ ansible web -m service -a "name=httpd state=restarted" -i hosts\n\n# 示例四： 停止httpd服务\n$ ansible web -m service -a "name=httpd state=stopped" -i hosts\n\n# 示例五： 启动httpd服务，并加入开机自启\n$ ansible web -m service -a "name=httpd state=started enabled=yes" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 5.6 group模块\n\n# 示例一： 创建news基本组，指定gid为9999\n$ ansible web -m group -a "name=news gid=9999 state=present"\n -i hosts\n\n# 示例二： 创建http系统组，指定gid为8888\n$ ansible web -m group -a "name=http gid=8888  state=present\n system=yes" -i hosts\n\n# 示例三： 删除test基本组\n$ ansible web -m group -a "name=test state=absent" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 5.7 user模块\n\n# 示例一： 创建job用户，uid是1040，主要组是adm\n$ ansible web -m user -a "name=job uid=1040 group=adm" -i hosts\n\n# 示例二： 创建joh用户，登录shell是/sbin/nologin，追加bin、sys两个组\n$ ansible web -m user -a "name=joh groups=bin,sys shell=/sbin/nologin" -i hosts\n\n# 示例三： 创建jsm用户，为其添加123作为登录密码，并创建家目录\n$ ansible localhost -m debug -a "msg={{ \'123\' | password_hash(\'sha512\',\'salt\') }}"\n# 获取123加密后的字符串\n$6$salt$jkhso0tojmlw0s1nflw5vesidravsiqqmtrkoky4xdcclpnishhzkirlzfzivkyxegdofcbow1wjzplyq9qx/1\n\n$ ansible web -m user -a \'name=jsm password=$6$salt$jkhso0tojmlw0s1nflw5vesidravsiqqmtrkoky4xdcclpnishhzkirlzfzivkyxegdofcbow1wjzplyq9qx/1 create_home=yes\' -i hosts\n\n# 示例四： 移除job用户\n$ ansible web -m user -a "name=job state=absent remove=yes" -i hosts\n# remove=yes删除普通用户家目录\n\n# 示例五： 创建http用户，并为该用户创建2048字节的私钥，存放在~/http/.ssh/id_rsa\n$ ansible web -m user -a "name=http generate_ssh_key=yes ssh_key_bits=2048 ssh_key_file=.ssh/id_rsa" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 5.8 cron模块\n\n# 示例一： 添加定时任务，每分钟执行一次ls\n$ ansible web -m cron -a "name=job1 job=\'ls > /dev/null\'" -i hosts\n\n# 示例二： 添加定时任务，每天的凌晨2点和凌晨5点执行一次ls\n$ ansible web -m cron -a "name=job2 job=\'ls > /dev/null\' minute=0 hour=2,5" -i hosts\n\n# 示例三： 关闭定时任务，使定时任务失败\n$ ansible web -m cron -a "name=job2 job=\'ls > /dev/null\' minute=0 hour=2,5 disabled=yes" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 5.9 mount\n\n# 环境准备：\n$ ansible localhost -m yum -a "name=nfs-utils state=present" \n$ ansible localhost -m file -a "path=/ops state=directory"\n$ ansible localhost -m copy -a \'content="/ops 192.168.100.1/24(rw,sync)" dest=/etc/exports\'\n$ ansible localhost -m service -a "name=nfs state=restarted"\n\n# 示例一： 挂载nfs存储到本地的/opt目录，并实现开机自动挂载\n$ ansible web -m mount -a "src=192.168.99.181:/ops path=/opt fstype=nfs opts=defaults state=mounted" -i hosts\n\n# 示例二： 临时卸载nfs的挂载，但不会清理/etc/fstab\n$ ansible web -m mount -a "src=192.168.99.181:/ops path=/opt fstype=nfs opts=defaults state=unmounted" -i hosts\n\n# 示例三： 永久卸载ns的挂载，清理/etc/fstab\n$ ansible web -m mount -a "src=192.168.99.181:/ops path=/opt fstype=nfs opts=defaults state=absent" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 5.10 firewalld\n\n# 示例一： 永久放行https的流量，只有重启才生效\n$ ansible web -m firewalld -a "zone=public service=https permanent=yes state=enabled" -i hosts\n\n# 示例二： 永久放行8081端口的流量，只有重启才生效\n$ ansible web -m firewalld -a "zone=public port=8081/tcp permanent=yes state=enabled" -i hosts\n\n# 示例三： 放行8080-8090的所有的tcp端口流量，临时和永久都生效\n$ ansible web -m firewalld -a "zone=public port=8080-8090/tcp immediate=yes permanent=yes state=enabled" -i hosts\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 六、ansible playbook\n\n# 6.1 什么是playbook\n\n * playbook：定义了一个文本文件，以.yml为后缀结尾；\n * play：定义的是主机的角色；\n * task：定义的是具体执行的任务；\n\n总结：playbook是由一个多个多个play组成，一个play可以包含多个task任务。可以理解为：使用不同的模块来共同完成一件事情！\n\n---\n#### 6.2 ansible playbook与ad-hoc的关系\n\n\n * playbook是对ad-hoc的一种编排方式；\n * playbook可以持久运行，而ad-hoc只能临时运行；\n * playbook适合复杂的任务，而ad-hoc适合做快速简单的任务；\n * playbook能控制任务执行的先后顺序；\n\n# 6.3 ansible playbook书写格式\n\nplaybook是由yaml语法书写，结构清晰、可读性强。所以必须掌握yaml基础语法！\n\n语法    概述\n缩进    yaml使用固定的缩进风格表示层级结构，每个缩进由两个空格组成，不能使用ta键\n冒号    以冒号结尾的除外，其他所有冒号后面所有必须有空格\n短横线   表示列表项，使用一个短横杠加一个空格。多个项使用同样的缩进级别作为统一列表\n\n\n# 七、变量\n\n# 7.1 变量概述\n\n变量提供了便捷的方式来管理ansible项目中的动态值，比如：nginx-1.12，可能后期会反复的使用这个版本的值，那么如果将此值设置为变量，后续使用和修改都将变得非常方便。这样可以简化项目的创建和维护。\n\n那么在ansible中定义变量分为以下三种方式：\n\n * 通过命令行进行变量定义；\n * 在play文件中进行定义变量；\n * 通过inventory在主机组或单个主机中设置变量；\n\n如果定义的变量出现重复，且造成冲突，怎么办？谁说了算？\n\n# 7.2 变量定义\n\n1、在playbook的文件开头通过vars关键字进行变量定义\n\n- hosts: web\n  vars:\n    - web_packages: httpd\n    - ftp_packages: vsftpd\n\n  tasks:\n    - name: install rpm packages "{{ web_packages }}" "{{ ftp_packages }}"\n      yum:\n        name:\n          - "{{ web_packages }}"\n          - "{{ ftp_packages }}"\n        state: present\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n2、可以在playbook中使用vars_files指定文件作为变量文件，好处就是其他的playbook也可以调用\n\n# 1）准备一个变量的文件，建议使用yml格式\n$ cat vars.yml\nweb_packages: httpd\nftp_packages: vsftpd\n\n# 2）准备playbook进行调用\n$ cat test.yml\n- hosts: web\n  vars_files:\n    - vars.yml\n  tasks:\n    - name: install rpm packages "{{ web_packages }}" "{{ ftp_packages }}"\n      yum:\n        name:\n          - "{{ web_packages }}"\n          - "{{ ftp_packages }}"\n        state: present\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n3、在inventory主机清单中定义变量，但是注意：主机变量优先级高于主机组变量\n\n# 1）在inventory主机清单中对主机组进行定义变量\n$ cat hosts\n[web]\n192.168.100.225\n192.168.100.226\n\n[web:vars]\nfilename=group_vars\n\n# 2）playbook中直接调用变量\n$ cat test.yml\n- hosts: web\n  tasks:\n    - name: create file\n      file:\n        name: /tmp/{{ filename}}\n        state: touch\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n4、官方建议是在ansible项目目录中创建两个额外的变量目录，分别是host_vars和group_vars\n\n * 测试group_vars定义变量方式\n\n$ cat hosts\n[web]\n192.168.100.225\n192.168.100.226\n\n# 1）在当前的项目目录中创建两个变量的目录\n$ mkdir {group,host}_vars\n\n# 2）在group_vars目录中创建一个文件，文件名与inventory清单中的主机组名称要保持一致\n$ cat group_vars/web\nweb_packages: wget\nftp_packages: tree\n\n# 3）编写playbook，只需在playbook中使用变量即可\n$ cat test.yml\n- hosts: web\n  tasks:\n    - name: install rpm packages "{{ web_packages }}" "{{ ftp_packages }}"\n      yum:\n        name:\n          - "{{ web_packages }}"\n          - "{{ ftp_packages }}"\n        state: present\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n测试test组能否使用web组定义的变量，无法使用！但是系统提供了特殊的all组，也就是说group_vars目录下创建一个all文件，定义变量对所有的主机都生效！\n\n * 测试host_vars定义变量方式\n\n# 1）在host_vars目录创建一个文件，文件名与inventory清单中的主机名称要保持一致，如果是ip地址，则创建相同的ip\n$ cat hosts\n[web]\n192.168.100.225\n192.168.100.226\n\n# 2）在host_vars目录中创建文件，给192.168.100.225主机定义变量\n$ cat host_vars/192.168.100.225\nweb_packages: zlib-static\nftp_packages: zmap\n\n# 3）转别一个playbook文件调用host主机变量\n$ cat test.yml\n- hosts: 192.168.100.225\n  tasks:\n    - name: install rpm packages "{{ web_packages }}" "{{ ftp_packages }}"\n      yum:\n        name:\n          - "{{ web_packages }}"\n          - "{{ ftp_packages }}"\n        state: present\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n5、通过命令行-extra-vars或-e外置穿餐设定变量\n\n# 1）准备playbook文件\n$ cat test.yml\n- hosts: "{{ host }}"\n  tasks:\n    - name: install rpm packages "{{ web_packages }}" "{{ ftp_packages }}"\n      yum:\n        name:\n          - "{{ web_packages }}"\n          - "{{ ftp_packages }}"\n        state: present\n\n# 2）执行playbook时进行变量的传递\n$ ansible-playbook -i hosts test.yml -e "host=web"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n6、变量的优先级顺序：\n\n命令行变量——>paly中的vars_files——>paly中的vars——>host_vars——>group_vars/all\n\n# 7.3 变量注册\n\nregister关键字可以将某个task任务结果存储至变量中，最后使用debug输出变量内容，可以用于后续排查故障！\n\n- hosts: all\n  tasks:\n    - name: system status\n      shell: netstat -lntp\n      register: system_status\n\n    - name: system status\n      debug: msg="{{ system_status.stdout_lines }}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 7.4 facts变量\n\nansible facts是在被管理主机上通过ansible自动采集发现的变量。facts包含每台特定的主机信息。比如：被控端主机的主机名、ip地址、系统版本、cpu数量、内存状态、磁盘状态等等。\n\n默认情况的facts变量名都已经预先定义好了，只需要采集被控端的信息，然后传递至facts变量即可！\n\n $ ansible web -m setup -i hosts > facts.txt\n # 获取所有的facts变量\n\n\n1\n2\n\n\nfacts变量使用场景：\n\n * 通过facts变量检查被控端主机硬件cpu信息，从而生成不同的nginx配置文件；\n\n * 通过facts变量检查被控端主机名称信息，从而生成不同的zabbix配置文件；\n\n * 通过facts变量检查被控端主机内存状态信息，从而生成不同的memcached配置文件；\n   \n   ………………\n\n1、facts基本用法：比如获取被控端的主机名和ip地址，然后通过debug输出\n\n- hosts: all\n  tasks:\n    - name: output variables ansible facts\n      debug:\n        msg: this default ipv4 address "{{ ansible_fqdn }}" is "{{ ansible_default_ipv4.address }}"\n\n\n1\n2\n3\n4\n5\n\n\n2、facts开启后会影响ansible主机的性能，如果没有采集被控端主机需求可选择关闭\n\n- hosts: web\n  gather_facts: no    # 关闭信息采集\n\n\n1\n2\n\n\n\n# 八、ansible task 任务控制\n\n# 8.1 piaybook条件语句\n\n判断在ansible任务中的使用频率非常高，比如yum模块可以检测软件包是否已经被安装，而在这个过程中我们不用做太多的人工干预。但是也有部分任务需要进行判断。\n\n# 案例一：根据不同操作系统，安装相同的软件包\n\n- hosts: web\n  tasks:\n    - name: centos install httpd\n      yum:\n        name: httpd\n        state: present\n      when: ( ansible_distribution == "centos" )\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 案例二：为所有web主机名的添加nginx仓库，其余的都跳过添加\n\n- hosts: all\n  tasks:\n    - name: create yum repo\n      yum_repository:\n        name: ansible_nginx\n        description: ansible_test\n        baseurl: http://nginx.org/packages/centos/$releasever/$basearch/\n        enabled: yes\n        gpgcheck: no\n      when: ( ansible_fqdn is match("web*") )\n\n# 当然when也可以使用and与or方式\n# when： ( ansible_fqdn is match("web*") ) or\n#         ( ansible_fqdn is match("lb*") )\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 案例三：根据命令执行的结果进行判断\n\n通过register将命令执行结果保存至变量，然后通过when语句进行判断\n\n- hosts: all\n  tasks:\n    - name: check httpd server\n      command: systemctl is-active httpd\n      ignore_errors: yes        # 忽略错误\n      register: check_httpd\n\n    - name: httpd restart        # 如果check_httpd执行命令结果等于0，则重启httpd,否则跳过\n      service:\n        name: httpd\n        state: restarted\n      when: check_httpd.rc == 0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 8.2 playbook循环语句\n\n有时候我们写playbook的时候会发现了很多task都要重复引用某个模块，比如一次启动10个服务，或者一个拷贝10个文件。如果安装传统的写法最少要写10次，这样会显得playbook很臃肿。如果使用循环的方式来编写playbokk，这样可以减少重复使用某个模块！\n\n# 案例一： 使用循环启动多个服务\n\n- hosts: all\n  tasks:\n    - name: service start\n      service: \n        name: "{{ item }}"\n        state: started\n      with_items:\n        - httpd\n        - sshd\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 案例二： 使用字典批量创建用户\n\n- hosts: all\n  tasks:\n    - name: add users\n      user: \n        name: "{{ item.name }}"\n        group: "{{item.group }}"\n        state: present\n      with_items:\n        - { name: \'test01\',group: \'bin\' }\n        - { name: \'test02\',group: \'root\' }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 案例三： 使用变量字典循环的方式拷贝文件\n\n- hosts: all\n  tasks:\n    - name: copy file\n      copy:\n        src: "{{ item.src }}"\n        dest: "{{ item.dest }}"\n        mode: "{{ item.mode }}"\n      with_items:\n        - { src: \'rsync.conf\' , dest: \'/tmp/rsync.conf\' , mode: 644 }\n        - { src: \'rsync.pass\' , dest: \'/tmp/rsync.pass\' , mode: 600 }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n高级写法(变量套变量)：\n\n- hosts: all\n  vars:\n    - path: /tmp\n  tasks:\n    - name: copy file\n      copy:\n        src: "{{ item.src }}"\n        dest: "{{ item.dest }}"\n        mode: "{{ item.mode }}"\n      with_items:\n        - { src: \'rsync.conf\' , dest: \'{{ path }}/rsync.conf\' , mode: 644 }\n        - { src: \'rsync.pass\' , dest: \'{{ path }}/rsync.pass\' , mode: 600 }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 8.3 playbook handlers\n\nhandlers是一个触发器，也是一个tasks，只不过是一个特殊的tasks，它是需要被tasks触发才会执行。只要配置文件发生变更，则会触发handlers执行重启服务操作，如果配置文件不发生任何变化，则不会重启！ notify监控——>通知——>handlers触发\n\n# 案例：playbook安装nginx\n\n- hosts: web\n  tasks:\n  - name: instll nginx\n    yum: \n      name: nginx\n      state: present\n\n  - name: copy nignx.conf\n    template:\n      src: nginx.conf.j2\n      dest: /etc/nginx/nginx.conf\n    notify: restart nignx service\n\n  handlers:\n    - name: restart nignx service\n      service:\n        name: nginx\n        state: restarted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nhandlers注意事项：\n\n * 无论多少个task通知相同的handlers，handlers仅会在所有task结束后执行一次；\n * 只有task发生变化后才会通知handlers，没有改变则不会触发handlers；\n * 不能使用handlers代替task，因为handlers是一个特殊的task；\n\n# 8.4 playbook tag\n\n默认情况下，ansible在执行一个playbook时，会执行playbook中的定义的所有任务。ansible的标签（tags）功能可以给单独任务甚至整个playbook打上tag标签，然后利用tag标签指定要运行的个别任务，或跳过个别的任务！\n\n# 案例：\n\n- hosts: web\n  tasks:\n  - name: instll nginx\n    yum: \n      name: nginx\n      state: present\n    tags: install nginx            # 添加tag标签\n\n  - name: copy nignx.conf\n    template:\n      src: nginx.conf.j2\n      dest: /etc/nginx/nginx.conf\n    notify: restart nignx service\n\n  - name: start nginx \n    service:\n      name: nginx\n      state: started\n    tags: start nginx          # 添加tag标签\n\n  handlers:\n    - name: restart nignx service\n      service:\n        name: nginx\n        state: restarted\n$ ansible-playbook -i hosts test.yml  -t \'install nginx\'\n# 仅仅执行playbook中的某个tag标签\n\n$ $ ansible-playbook -i hosts test.yml  --skip-tags \'install nginx\'\n# 跳过playbook中的特定的tag标签\n\n$ ansible-playbook -i hosts test.yml  -t \'install nginx\' -t \'start nginx\'\n# 指定多个tag标签\n\n$ ansible-playbook -i hosts test.yml  --skip-tags \'install nginx\' --skip-tags \'start nginx\'\n# 跳过多个tag标签\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n# 8.5 playbook include\n\n有时发现大量的playbook内容要重复编写，与tasks之间贡呢需相互调用才能完成各自贡呢。playbook庞大到维护困难，这是就需要使用include！\n\n# 案例：\n\n$ cat restart_httpd.yml \n- name: restart httpd service\n  service:\n    name: httpd\n    state: restarted\n\n\n1\n2\n3\n4\n5\n\n\na project的yaml文件：\n\n- hosts: web\n  tasks:\n    - name: a project command\n      command: echo "a"\n    - name: restart httpd\n      include: restart_httpd.yml\n\n\n1\n2\n3\n4\n5\n6\n\n\na project的yaml文件：\n\n- hosts: web\n  tasks:\n    - name: b project command\n      command: echo "b"\n    - name: restart httpd\n      include: restart_httpd.yml\n\n\n1\n2\n3\n4\n5\n6\n\n\n高级用法：\n\n一次执行的多个playbook文件！\n\n$ cat tasks_total.yml \n- import_playbook: test.yml\n- import_playbook: test01.yml\n\n$ ansible-playbook -i hosts  tasks_total.yml\n\n\n1\n2\n3\n4\n5\n\n\n# 8.6 ignore_errors忽略错误\n\n- hosts: web\n  tasks:\n    - name: command\n      command: /bin/false\n      ignore_errors: yes\n\n    - name: create file\n      file:\n        path: /tmp/file\n        state: touch\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 8.7 playbook错误处理\n\n通常情况下，当task失败后，play将会终止，任何在前面已经被tasks notify的handlers都不会被执行。如果在play中设置force_handlers，被通知的handlers就会被强制执行。\n\n# 案例一：task执行失败强制调用handlers\n\n- hosts: web\n  force_handlers: yes     # 强制调用handlers\n\n  tasks:\n    - name: touch file\n      file:\n        path: /tmp/test\n        state: touch\n      notify: restart httpd service\n\n    - name: install packages\n      yum: \n        name: sb\n        state: present\n\n  handlers:\n    - name: restart httpd service\n      service:\n        name: httpd\n        state: restarted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 案例二： 关闭change的状态（确定不会对被控端主机进行任何的修改和变更）\n\n- hosts: web\n  tasks:\n    - name: install httpd server\n      yum: \n        name: httpd\n        state: present\n    - name: service httpd started\n      service:\n        name: httpd\n        state: started\n    - name: check httpd server\n      shell: ps aux | grep httpd\n      register: check_httpd\n      changed_when: false\n    - name: output variables\n      debug:\n        msg: "{{ check_httpd.stdout_lines }}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 案例三： 使用changed_when检查tasksrre任务返回结果\n\n- hosts: web\n  tasks:\n    - name: configure nginx server\n      template:\n        src: nginx.conf.j2\n        dest: /etc/nginx/nginx.conf\n      notify: restart nginx server\n\n    - name: check httpd\n      shell: /usr/sbin/nginx -t\n      register: nginx_check\n      changed_when:\n        - (nginx_check.stdout.find(\'successful\'))\n        - false\n\n    - name: start httpd server\n      service:\n        name: nginx\n        state: started\n        enabled: yes\n\n    - handlers: restart httpd server\n      service:\n        name: nginx\n        state: restarted\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n',charsets:{cjk:!0}},{title:"anisble批量安装node_exporter",frontmatter:{title:"anisble批量安装node_exporter",categories:["系列专题","ansible"],tags:["ansible"],date:"2022-12-09T20:53:14.000Z",permalink:"/pages/f8f66c/",readingShow:"top",description:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14374243.html",meta:[{name:"twitter:title",content:"anisble批量安装node_exporter"},{name:"twitter:description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14374243.html"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/02.anisble%E6%89%B9%E9%87%8F%E5%AE%89%E8%A3%85node_exporter.html"},{property:"og:type",content:"article"},{property:"og:title",content:"anisble批量安装node_exporter"},{property:"og:description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14374243.html"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/02.anisble%E6%89%B9%E9%87%8F%E5%AE%89%E8%A3%85node_exporter.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:53:14.000Z"},{property:"article:tag",content:"ansible"},{itemprop:"name",content:"anisble批量安装node_exporter"},{itemprop:"description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14374243.html"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/02.anisble%E6%89%B9%E9%87%8F%E5%AE%89%E8%A3%85node_exporter.html",relativePath:"01.专题/01.ansible系列文章/02.anisble批量安装node_exporter.md",key:"v-3a928a43",path:"/pages/f8f66c/",headers:[{level:3,title:"一、目录结构",slug:"一、目录结构",normalizedTitle:"一、目录结构",charIndex:94},{level:3,title:"二、playbook文件",slug:"二、playbook文件",normalizedTitle:"二、playbook文件",charIndex:104},{level:3,title:"三、服务文件",slug:"三、服务文件",normalizedTitle:"三、服务文件",charIndex:120},{level:3,title:"四、任务文件",slug:"四、任务文件",normalizedTitle:"四、任务文件",charIndex:130}],headersStr:"一、目录结构 二、playbook文件 三、服务文件 四、任务文件",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14374243.html\n\n\n# anisble批量安装node_exporter\n\n目录\n\n * 一、目录结构\n * 二、playbook文件\n * 三、服务文件\n * 四、任务文件\n\n\n# 一、目录结构\n\n$ tree .\n.\n|-- hosts\n|-- node_exporter\n|   |-- files\n|   |   |-- node_exporter-1.0.1.linux-amd64.tar.gz\n|   |   `-- node_exporter.service\n|   `-- tasks\n|       `-- main.yml\n`-- node_exporter.yml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 二、playbook文件\n\n$ cat node_exporter.yml \n#!/usr/bin/env ansible-playbook\n- hosts: all\n  remote_user: root\n  gather_facts: false\n  roles:\n  - role: node_exporter\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 三、服务文件\n\n$ cat node_exporter/files/node_exporter.service \n[Unit]\nDescription=Prometheus node_exporter\nRequires=network.target remote-fs.target\nAfter=network.target remote-fs.target\n\n[Service]\nType=simple\nUser=root\nGroup=root\nExecStart=/usr/local/node_exporter/node_exporter --web.listen-address=0.0.0.0:9100\nExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\nRestart=on-failure\nRestartSec=5s\n\n[Install]\nWantedBy=multi-user.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 四、任务文件\n\n$ cat node_exporter/tasks/main.yml \n- name: 安装node_exporter\n  unarchive: \n    src: node_exporter-1.0.1.linux-amd64.tar.gz\n    dest: /usr/local/\n\n- name: 创建软链接\n  file:\n    src: /usr/local/node_exporter-1.0.1.linux-amd64\n    dest: /usr/local/node_exporter\n    state: link\n\n- name: 添加node_exporter服务\n  copy: \n    src: node_exporter.service\n    dest: /usr/lib/systemd/system/\n\n- name: daemon-reload\n  systemd: \n    daemon_reload: yes\n\n- name: 设置开机自动启动\n  systemd: \n    name: node_exporter\n    state: started\n    enabled: True\n\n- name: 确定端口在监听\n  wait_for:\n    host: 0.0.0.0\n    port: 9100\n    delay: 2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************",normalizedContent:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14374243.html\n\n\n# anisble批量安装node_exporter\n\n目录\n\n * 一、目录结构\n * 二、playbook文件\n * 三、服务文件\n * 四、任务文件\n\n\n# 一、目录结构\n\n$ tree .\n.\n|-- hosts\n|-- node_exporter\n|   |-- files\n|   |   |-- node_exporter-1.0.1.linux-amd64.tar.gz\n|   |   `-- node_exporter.service\n|   `-- tasks\n|       `-- main.yml\n`-- node_exporter.yml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 二、playbook文件\n\n$ cat node_exporter.yml \n#!/usr/bin/env ansible-playbook\n- hosts: all\n  remote_user: root\n  gather_facts: false\n  roles:\n  - role: node_exporter\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 三、服务文件\n\n$ cat node_exporter/files/node_exporter.service \n[unit]\ndescription=prometheus node_exporter\nrequires=network.target remote-fs.target\nafter=network.target remote-fs.target\n\n[service]\ntype=simple\nuser=root\ngroup=root\nexecstart=/usr/local/node_exporter/node_exporter --web.listen-address=0.0.0.0:9100\nexecreload=/bin/kill -hup $mainpid\nkillmode=process\nrestart=on-failure\nrestartsec=5s\n\n[install]\nwantedby=multi-user.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 四、任务文件\n\n$ cat node_exporter/tasks/main.yml \n- name: 安装node_exporter\n  unarchive: \n    src: node_exporter-1.0.1.linux-amd64.tar.gz\n    dest: /usr/local/\n\n- name: 创建软链接\n  file:\n    src: /usr/local/node_exporter-1.0.1.linux-amd64\n    dest: /usr/local/node_exporter\n    state: link\n\n- name: 添加node_exporter服务\n  copy: \n    src: node_exporter.service\n    dest: /usr/lib/systemd/system/\n\n- name: daemon-reload\n  systemd: \n    daemon_reload: yes\n\n- name: 设置开机自动启动\n  systemd: \n    name: node_exporter\n    state: started\n    enabled: true\n\n- name: 确定端口在监听\n  wait_for:\n    host: 0.0.0.0\n    port: 9100\n    delay: 2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************",charsets:{cjk:!0}},{title:"ansible-playbook中的变量",frontmatter:{title:"ansible-playbook中的变量",categories:["系列专题","ansible"],tags:["ansible"],date:"2022-12-09T20:53:14.000Z",permalink:"/pages/d740bc/",readingShow:"top",description:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14385777.html",meta:[{name:"twitter:title",content:"ansible-playbook中的变量"},{name:"twitter:description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14385777.html"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/03.ansible-playbook%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F.html"},{property:"og:type",content:"article"},{property:"og:title",content:"ansible-playbook中的变量"},{property:"og:description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14385777.html"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/03.ansible-playbook%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:53:14.000Z"},{property:"article:tag",content:"ansible"},{itemprop:"name",content:"ansible-playbook中的变量"},{itemprop:"description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14385777.html"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/03.ansible-playbook%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F.html",relativePath:"01.专题/01.ansible系列文章/03.ansible-playbook中的变量.md",key:"v-6bdef9fc",path:"/pages/d740bc/",headers:[{level:3,title:"一、变量的优先级",slug:"一、变量的优先级",normalizedTitle:"一、变量的优先级",charIndex:90},{level:4,title:"1.1 YAML陷阱",slug:"_1-1-yaml陷阱",normalizedTitle:"1.1 yaml陷阱",charIndex:104},{level:3,title:"二、 Ansbile-playbook变量配置方法",slug:"二、-ansbile-playbook变量配置方法",normalizedTitle:"二、 ansbile-playbook变量配置方法",charIndex:118},{level:4,title:"2.1 在inventory主机清单文件中定义变量",slug:"_2-1-在inventory主机清单文件中定义变量",normalizedTitle:"2.1 在inventory主机清单文件中定义变量",charIndex:149},{level:4,title:"2.2 通过hostvars和groupvars目录来定义变量",slug:"_2-2-通过host-vars和group-vars目录来定义变量",normalizedTitle:"2.2 通过hostvars和groupvars目录来定义变量",charIndex:null},{level:4,title:"2.3 通过var_files定义变量",slug:"_2-3-通过var-files定义变量",normalizedTitle:"2.3 通过var_files定义变量",charIndex:219},{level:4,title:"2.4 通过vars_prompt交互式传入变量",slug:"_2-4-通过vars-prompt交互式传入变量",normalizedTitle:"2.4 通过vars_prompt交互式传入变量",charIndex:244},{level:4,title:"2.5 通过ansible-playbook命令行定义变量！即参数传入变量",slug:"_2-5-通过ansible-playbook命令行定义变量-即参数传入变量",normalizedTitle:"2.5 通过ansible-playbook命令行定义变量！即参数传入变量",charIndex:274},{level:4,title:"2.6 在playbook剧本中定义变量",slug:"_2-6-在playbook剧本中定义变量",normalizedTitle:"2.6 在playbook剧本中定义变量",charIndex:317},{level:4,title:"2.7 通过roles角色定义变量",slug:"_2-7-通过roles角色定义变量",normalizedTitle:"2.7 通过roles角色定义变量",charIndex:343},{level:4,title:"2.8 使用Facts获取的信息",slug:"_2-8-使用facts获取的信息",normalizedTitle:"2.8 使用facts获取的信息",charIndex:366},{level:4,title:"2.9 register注册变量",slug:"_2-9-register注册变量",normalizedTitle:"2.9 register注册变量",charIndex:388},{level:4,title:"2.10 hostvars 变量",slug:"_2-10-hostvars-变量",normalizedTitle:"2.10 hostvars 变量",charIndex:410},{level:4,title:"2.11 列表变量、循环变量、字典变量",slug:"_2-11-列表变量、循环变量、字典变量",normalizedTitle:"2.11 列表变量、循环变量、字典变量",charIndex:432}],headersStr:"一、变量的优先级 1.1 YAML陷阱 二、 Ansbile-playbook变量配置方法 2.1 在inventory主机清单文件中定义变量 2.2 通过hostvars和groupvars目录来定义变量 2.3 通过var_files定义变量 2.4 通过vars_prompt交互式传入变量 2.5 通过ansible-playbook命令行定义变量！即参数传入变量 2.6 在playbook剧本中定义变量 2.7 通过roles角色定义变量 2.8 使用Facts获取的信息 2.9 register注册变量 2.10 hostvars 变量 2.11 列表变量、循环变量、字典变量",content:'原文链接：https://www.cnblogs.com/lvzhenjiang/p/14385777.html\n\n\n# ansible playbook中的变量\n\n目录\n\n * 一、变量的优先级\n   * 1.1 YAML陷阱\n * 二、 Ansbile-playbook变量配置方法\n   * 2.1 在inventory主机清单文件中定义变量\n   * 2.2 通过host_vars和group_vars目录来定义变量\n   * 2.3 通过var_files定义变量\n   * 2.4 通过vars_prompt交互式传入变量\n   * 2.5 通过ansible-playbook命令行定义变量！即参数传入变量\n   * 2.6 在playbook剧本中定义变量\n   * 2.7 通过roles角色定义变量\n   * 2.8 使用Facts获取的信息\n   * 2.9 register注册变量\n   * 2.10 hostvars 变量\n   * 2.11 列表变量、循环变量、字典变量\n\n\n# 一、变量的优先级\n\n * extra vars变量（在命令行中使用 -e）；优先级最高；\n * 在inventory中定义的连接变量（比如ansible_ssh_user）；优先级第二；\n * 大多数的其他变量（命令行转换，play中的变量，include的变量，role的变量等）；优先级第三；\n * 在inventory定义的其他变量；优先级第四；\n * 有系统发现的facts；优先级第五；\n * "role默认变量"，这个是最默认的值，很容易丧失优先权。优先级最小；\n\n另外：在inventory清单列表里定义的变量：单个主机定义的变量优先级高于主机组定义的变量 经过实验，ansible使用inventory定义变量的优先级顺序从高到低为： host_vars下定义变量 ---\x3e inventory中单个主机定义变量 ---\x3e group_vars下定义变量 ---\x3e inventory中组定义变量\n\n# 1.1 YAML陷阱\n\nYAML语法要求如果值以{{ foo }}开头的话，那么就需要将整行用双引号包起来，这是为了确认你不是想声明一个YAML字典。 如下面配置是不行的！！！\n\n---\n- hosts: app_servers\n  vars:\n    app_path: {{ base_path }}/data/web\n\n\n1\n2\n3\n4\n\n\n应该改成下面这样：\n\n---\n- hosts: app_servers\n  vars:\n    app_path: "{{ base_path }}/data/web"\n\n\n1\n2\n3\n4\n\n\n\n# 二、 Ansbile-playbook变量配置方法\n\n# 2.1 在inventory主机清单文件中定义变量\n\n可以直接定义在主机清单文件/etc/ansible/hosts中，表明该变量只对对应的主机或者组有效，对其余的主机和组无效。\n\n示例：\n\n$ egrep -v "^#|^$" /etc/ansible/hosts \n10.4.7.101 key=20180101\n10.4.7.102 key="niubility"\n\n$ vim ansi.yml \n---\n- hosts: all\n  gather_facts: False\n  tasks:\n    - name: haha\n      debug: msg="the {{ inventory_hostname }} value is {{ key }}"\n\n# 执行结果（注意inventory_hostname代表inventory列表列表里被控节点的主机名）：\n\n$ ansible-playbook ansi.yml \n\nPLAY [all] **************************************************************************************************************************************\n\nTASK [haha] *************************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "the 10.4.7.101 value is 20180101"\n}\nok: [10.4.7.102] => {\n    "msg": "the 10.4.7.102 value is niubility"\n}\n\nPLAY RECAP **************************************************************************************************************************************\n10.4.7.101                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n10.4.7.102                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 2.2 通过host_vars和group_vars目录来定义变量\n\n/etc/ansible/目录是linux系统上ansible默认的配置文件目录（Mac系统上的话，其默认配置目录是在/usr/local/etc/ansible/），在该目录下创建host_vars和group_vars两个目录用来存放定义变量的文件。\n\n针对单个主机的变量\n\n$ cat /etc/ansible/host_vars/10.4.7.101\n---\nuser: root\npass: root@123\n\n\n1\n2\n3\n4\n\n\n针对test组的变量\n\n$ cat /etc/ansible/group_vars/test\n---\nuser: work\npass: work@123\n\n\n1\n2\n3\n4\n\n\n在inventory清单列表文件里，单个主机定义的变量优先级高于主机组定义的变量\n\n# 2.3 通过var_files定义变量\n\n$ cat vars.yml \n---\nkey: jiayou\n\n$ cat bo.yml \n---\n- hosts: all\n  gather_facts: False\n  vars_files:\n      - vars.yml\n  tasks:\n    - name: display\n      debug: msg="the {{ inventory_hostname }} valus is {{ key }}"\n\n$ ansible-playbook bo.yml\n\nPLAY [all] **************************************************************************************************************************************\n\nTASK [display] **********************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "the 10.4.7.101 valus is jiayou"\n}\nok: [10.4.7.102] => {\n    "msg": "the 10.4.7.102 valus is jiayou"\n}\n\nPLAY RECAP **************************************************************************************************************************************\n10.4.7.101                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n10.4.7.102                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 2.4 通过vars_prompt交互式传入变量\n\n在playbook中定义vars_prompt的变量名和交互式提示信息，就可以实现在运行playbook时，通过交互的传入变量值。 private字段：用来定义交互时是否回显输入的值，默认private为yes； default字段：用来定义变量的默认值。\n\n$ cat prom.yml \n---\n- hosts: test\n  remote_user: root\n  vars_prompt:\n      - name: "var1"\n        prompt: "please input you name"\n        private: no\n      - name: "var2"\n        prompt: "please input you age"\n        private: yes\n        default: 18\n  tasks:\n      - name: display var1\n        debug: msg="your name of var1 is {{ var1 }}"\n      - name: display var2\n        debug: msg="you age of var2 is {{ var2 }}"\n\n$ ansible-playbook prom.yml \nplease input you name: lvzhenjiang    # 把输入的内容传递给变量var1。输入的值显示出来！\nplease input you age [18]:           # playbook中定义默认值是18，如果不输入便是18，但是输入的值不显示出来！比如这里输入的23\n\nPLAY [test] *************************************************************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\nTASK [display var1] *****************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "your name of var1 is lvzhenjiang"\n}\n\nTASK [display var2] *****************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "you age of var2 is 23"\n}\n\nPLAY RECAP **************************************************************************************************************************************\n10.4.7.101                 : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n# 2.5 通过ansible-playbook命令行定义变量！即参数传入变量\n\n除了vars_prompt和vars_files，也可以通过Ansible命令行发送变量。如果想要编写一个通用的发布playbook时则特别有用！你可以传递应用的版本以便部署。例如下面命令（注意： --extra-vars 相等于 -e）\n\n$ cat exap.yml \n---\n- hosts: \'{{hosts}}\'\n  remote_user: \'{{user}}\'\n  tasks:\n    - name: "一个测试"\n      debug: msg="your hosts is {{hosts}}, user is {{user}}"\n\n$  ansible-playbook exap.yml -e "hosts=test user=root" \n[WARNING]: Found variable using reserved name: hosts\n\nPLAY [test] *************************************************************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\nTASK [一个测试] *************************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "your hosts is test, user is root"\n}\n\nPLAY RECAP **************************************************************************************************************************************\n10.4.7.101                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n也可以将参数放在文件里面进行传递（注意命令行里要是用"@文件名"）：\n\n# 同样使用上面的例子\n$ cat anhui.yml \n---\nhosts: test\nuser: root\n\n$ ansible-playbook exap.yml -e "@anhui.yml"\n[WARNING]: Found variable using reserved name: hosts\n\nPLAY [test] *************************************************************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\nTASK [一个测试] *************************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "your hosts is test, user is root"\n}\n\nPLAY RECAP **************************************************************************************************************************************\n10.4.7.101                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 2.6 在playbook剧本中定义变量\n\n在playbook中定义变量需要用到Ansible的vars模块，可以将所有需要用到的变量统一在vars模块下定义，定义格式需要遵循YAML语言格式：\n\n语法格式：\n\nvars:\n  - var1: value1\n  - var2: value2\n  - var3: value3\n  - ....: .....\n\n\n1\n2\n3\n4\n5\n\n\n示例如下：\n\n$ cat playbook.yml \n---\n- hosts: test\n  remote_user: root\n  vars:\n    - dir1: /root/Ansible\n    - dir2: /root/Ansible/test1\n    - dir3: /root/Ansible/test2\n  tasks:\n    - name: Create New Folder\n      file: name={{ dir1 }} state=directory\n    - name: Create New Folder\n      file: name={{ dir2 }} state=directory\n    - name: Create New Folder\n      file: name={{ dir3 }} state=directory\n\n$ ansible-playbook playbook.yml \n\nPLAY [test] *************************************************************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\nTASK [Create New Folder] ************************************************************************************************************************\nchanged: [10.4.7.101]\n\nTASK [Create New Folder] ************************************************************************************************************************\nchanged: [10.4.7.101]\n\nTASK [Create New Folder] ************************************************************************************************************************\nchanged: [10.4.7.101]\n\nPLAY RECAP **************************************************************************************************************************************\n10.4.7.101                 : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n# 2.7 通过roles角色定义变量\n\n在Ansible的roles中定义变量，需要将变量及值的键值对形式写到roles的vars目录下的main.yml文件中，同样适用YAML语言格式，格式如下：\n\nvar1: value1\nvar2: value2\nvar3: value3\n\n\n1\n2\n3\n\n\n但是请注意：通过Roles定义的变量只适用于当前roles。\n\n# roles目录结构\n$ tree .\n.\n├── hosts\n├── playbook.yml\n└── test\n    ├── files\n    ├── tasks\n    │   └── main.yml\n    ├── templates\n    └── vars\n        └── main.yml\n\n5 directories, 4 files\n\n$ cat test/tasks/main.yml \n- name: create directory\n  file: name={{ dir }} state=directory\n- name: Get IP Address\n  shell: echo `{{ cmd }}` >> {{ dir }}/{{ file }}\n\n$ cat test/vars/main.yml \ncmd: hostname -I\n\n$ cat playbook.yml \n---\n- hosts: test\n  remote_user: root\n  roles:\n    - test\n\n$ cat hosts \n[test]\n10.4.7.101 dir=/root/node2\n10.4.7.102 dir=/root/node1\n\n[node1]\n10.4.7.100\n\n[test:vars]\nfile=hostname.txt\n\n$ ansible-playbook -i hosts  playbook.yml \n\nPLAY [test] *************************************************************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************************************************\nok: [10.4.7.101]\nok: [10.4.7.102]\n\nTASK [test : create directory] ******************************************************************************************************************\nok: [10.4.7.101]\nok: [10.4.7.102]\n\nTASK [test : Get IP Address] ********************************************************************************************************************\nchanged: [10.4.7.102]\nchanged: [10.4.7.101]\n\nPLAY RECAP **************************************************************************************************************************************\n10.4.7.101                 : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n10.4.7.102                 : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n# 2.8 使用Facts获取的信息\n\n还有其它地方可以获取变量, 这些变量是自动发现的，而不是用户自己设置的。Facts通过访问远程系统获取相应的信息，一个很好的例子就是远程主机的IP地址或者操作系统是什么。\n\n$ ansible test -m setup\n# 使用以下命令可以查看哪些信息是可用的（test是上面在/etc/ansible/hosts列表文件中配置的主机群组）\n\n$ ansible test -m setup|grep "ansible_python_version"\n        "ansible_python_version": "2.7.5", \n# 在playbook中这样引用上面被控制主机的python版本: {{ ansible_python_version }}\n\n$ ansible test -m setup|grep "ansible_nodename"\n        "ansible_nodename": "template", \n# 可以在playbook中这样引用上面被控制主机的主机名: {{ ansible_nodename }}\n\n$ ansible test -m setup | grep "ansible_hostname"\n        "ansible_hostname": "template",\n# 被控制主机的主机名变量还可以是: {{ ansible_hostname }}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n如果关闭Facts，可以大大提高ansible的执行速度 ，关闭方法如下：\n\n$ cat anhui.yml\n---\n- hosts: test\n  gather_facts: no\n\n\n1\n2\n3\n4\n\n\n# 2.9 register注册变量\n\n变量的另一个主要用途是在运行命令时，把命令结果存储到一个变量中，不同模块的执行结果是不同的。运行playbook时使用-v选项可以看到可能的结果值，ansible执行任务的结果值可以保存在变量中，以便稍后使用它。register方式主要用于在task之间传递变量。\n\n$ cat /etc/ansible/hosts\n[test]\n10.4.7.101\n10.4.7.102\n\n$ cat register.yml\n---\n- hosts: test\n  remote_user: root\n  tasks:\n      - name: register bo_test\n        shell: hostname -I\n        register: info\n      - name: display info\n        debug: msg="this host ip is {{ info[\'stdout\'] }}"\n\n$ ansible-playbook register.yml\n\nPLAY [test] *************************************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************************\nok: [10.4.7.102]\nok: [10.4.7.101]\n\nTASK [register bo_test] *************************************************************************************************\nchanged: [10.4.7.102]\nchanged: [10.4.7.101]\n\nTASK [display info] *****************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "this host ip is 10.4.7.101 "\n}\nok: [10.4.7.102] => {\n    "msg": "this host ip is 10.4.7.102 "\n}\n\nPLAY RECAP **************************************************************************************************************\n10.4.7.101                 : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.4.7.102                 : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n# 2.10 hostvars 变量\n\n该变量用于引用其他主机上收集的facts中的数据，或者引用其他主机的主机变量、主机组变量。即从一台远程主机获取另一台远程主机的变量。\n\n$ cat /etc/ansible/hosts\n[test]\n10.4.7.101 addr=beijing\n10.4.7.102 user=shibo age=39\n\n$ cat test.yml\n---\n- hosts: test\n  remote_user: root\n  gather_facts: False\n  tasks:\n    - name: this is test1\n      debug: msg="She is come from {{ hostvars[\'10.4.7.101\'][\'addr\'] }}"\n    - name: this is test2\n      debug: msg="I am {{ hostvars[\'10.4.7.102\'][\'user\'] }}, and age is {{ hostvars[\'10.4.7.102\'][\'age\'] }}"\n\n$ ansible-playbook test.yml\n\nPLAY [test] *************************************************************************************************************\n\nTASK [this is test1] ****************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "She is come from beijing"\n}\nok: [10.4.7.102] => {\n    "msg": "She is come from beijing"\n}\n\nTASK [this is test2] ****************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "I am shibo, and age is 39"\n}\nok: [10.4.7.102] => {\n    "msg": "I am shibo, and age is 39"\n}\n\nPLAY RECAP **************************************************************************************************************\n10.4.7.101                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.4.7.102                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n# 2.11 列表变量、循环变量、字典变量\n\n1）ansible的变量不仅可以是单个的值，也可以为列表，即ansible传列表作为变量\n\n$ cat test.yml\n---\n- hosts: test\n  remote_user: root\n  gather_facts: False\n  vars:\n    - list: [1,2,3]\n  tasks:\n    - name: echo\n      debug: msg="{{ list }}"\n\n$ ansible-playbook test.yml\n\nPLAY [test] *************************************************************************************************************\n\nTASK [echo] *************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": [\n        1,\n        2,\n        3\n    ]\n}\nok: [10.4.7.102] => {\n    "msg": [\n        1,\n        2,\n        3\n    ]\n}\n\nPLAY RECAP **************************************************************************************************************\n10.4.7.101                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.4.7.102                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n2）循环列表\n\n结合循环，这个特性就变得很有用；以参数传递列表给playbook，不用在playbook中固定循环的次数与内容。\n\n$ cat test.yml\n---\n- hosts: 10.4.7.101\n  remote_user: root\n  gather_facts: False\n  vars:\n    - list: [1,2,3]\n  tasks:\n    - name: this is loop\n      debug: msg="{{ item }}"\n      with_items: \'{{list}}\'\n\n$ ansible-playbook test.yml\n\nPLAY [10.4.7.101] *******************************************************************************************************\n\nTASK [this is loop] *****************************************************************************************************\nok: [10.4.7.101] => (item=1) => {\n    "msg": 1\n}\nok: [10.4.7.101] => (item=2) => {\n    "msg": 2\n}\nok: [10.4.7.101] => (item=3) => {\n    "msg": 3\n}\n\nPLAY RECAP **************************************************************************************************************\n10.4.7.101                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************',normalizedContent:'原文链接：https://www.cnblogs.com/lvzhenjiang/p/14385777.html\n\n\n# ansible playbook中的变量\n\n目录\n\n * 一、变量的优先级\n   * 1.1 yaml陷阱\n * 二、 ansbile-playbook变量配置方法\n   * 2.1 在inventory主机清单文件中定义变量\n   * 2.2 通过host_vars和group_vars目录来定义变量\n   * 2.3 通过var_files定义变量\n   * 2.4 通过vars_prompt交互式传入变量\n   * 2.5 通过ansible-playbook命令行定义变量！即参数传入变量\n   * 2.6 在playbook剧本中定义变量\n   * 2.7 通过roles角色定义变量\n   * 2.8 使用facts获取的信息\n   * 2.9 register注册变量\n   * 2.10 hostvars 变量\n   * 2.11 列表变量、循环变量、字典变量\n\n\n# 一、变量的优先级\n\n * extra vars变量（在命令行中使用 -e）；优先级最高；\n * 在inventory中定义的连接变量（比如ansible_ssh_user）；优先级第二；\n * 大多数的其他变量（命令行转换，play中的变量，include的变量，role的变量等）；优先级第三；\n * 在inventory定义的其他变量；优先级第四；\n * 有系统发现的facts；优先级第五；\n * "role默认变量"，这个是最默认的值，很容易丧失优先权。优先级最小；\n\n另外：在inventory清单列表里定义的变量：单个主机定义的变量优先级高于主机组定义的变量 经过实验，ansible使用inventory定义变量的优先级顺序从高到低为： host_vars下定义变量 ---\x3e inventory中单个主机定义变量 ---\x3e group_vars下定义变量 ---\x3e inventory中组定义变量\n\n# 1.1 yaml陷阱\n\nyaml语法要求如果值以{{ foo }}开头的话，那么就需要将整行用双引号包起来，这是为了确认你不是想声明一个yaml字典。 如下面配置是不行的！！！\n\n---\n- hosts: app_servers\n  vars:\n    app_path: {{ base_path }}/data/web\n\n\n1\n2\n3\n4\n\n\n应该改成下面这样：\n\n---\n- hosts: app_servers\n  vars:\n    app_path: "{{ base_path }}/data/web"\n\n\n1\n2\n3\n4\n\n\n\n# 二、 ansbile-playbook变量配置方法\n\n# 2.1 在inventory主机清单文件中定义变量\n\n可以直接定义在主机清单文件/etc/ansible/hosts中，表明该变量只对对应的主机或者组有效，对其余的主机和组无效。\n\n示例：\n\n$ egrep -v "^#|^$" /etc/ansible/hosts \n10.4.7.101 key=20180101\n10.4.7.102 key="niubility"\n\n$ vim ansi.yml \n---\n- hosts: all\n  gather_facts: false\n  tasks:\n    - name: haha\n      debug: msg="the {{ inventory_hostname }} value is {{ key }}"\n\n# 执行结果（注意inventory_hostname代表inventory列表列表里被控节点的主机名）：\n\n$ ansible-playbook ansi.yml \n\nplay [all] **************************************************************************************************************************************\n\ntask [haha] *************************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "the 10.4.7.101 value is 20180101"\n}\nok: [10.4.7.102] => {\n    "msg": "the 10.4.7.102 value is niubility"\n}\n\nplay recap **************************************************************************************************************************************\n10.4.7.101                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n10.4.7.102                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 2.2 通过host_vars和group_vars目录来定义变量\n\n/etc/ansible/目录是linux系统上ansible默认的配置文件目录（mac系统上的话，其默认配置目录是在/usr/local/etc/ansible/），在该目录下创建host_vars和group_vars两个目录用来存放定义变量的文件。\n\n针对单个主机的变量\n\n$ cat /etc/ansible/host_vars/10.4.7.101\n---\nuser: root\npass: root@123\n\n\n1\n2\n3\n4\n\n\n针对test组的变量\n\n$ cat /etc/ansible/group_vars/test\n---\nuser: work\npass: work@123\n\n\n1\n2\n3\n4\n\n\n在inventory清单列表文件里，单个主机定义的变量优先级高于主机组定义的变量\n\n# 2.3 通过var_files定义变量\n\n$ cat vars.yml \n---\nkey: jiayou\n\n$ cat bo.yml \n---\n- hosts: all\n  gather_facts: false\n  vars_files:\n      - vars.yml\n  tasks:\n    - name: display\n      debug: msg="the {{ inventory_hostname }} valus is {{ key }}"\n\n$ ansible-playbook bo.yml\n\nplay [all] **************************************************************************************************************************************\n\ntask [display] **********************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "the 10.4.7.101 valus is jiayou"\n}\nok: [10.4.7.102] => {\n    "msg": "the 10.4.7.102 valus is jiayou"\n}\n\nplay recap **************************************************************************************************************************************\n10.4.7.101                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n10.4.7.102                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 2.4 通过vars_prompt交互式传入变量\n\n在playbook中定义vars_prompt的变量名和交互式提示信息，就可以实现在运行playbook时，通过交互的传入变量值。 private字段：用来定义交互时是否回显输入的值，默认private为yes； default字段：用来定义变量的默认值。\n\n$ cat prom.yml \n---\n- hosts: test\n  remote_user: root\n  vars_prompt:\n      - name: "var1"\n        prompt: "please input you name"\n        private: no\n      - name: "var2"\n        prompt: "please input you age"\n        private: yes\n        default: 18\n  tasks:\n      - name: display var1\n        debug: msg="your name of var1 is {{ var1 }}"\n      - name: display var2\n        debug: msg="you age of var2 is {{ var2 }}"\n\n$ ansible-playbook prom.yml \nplease input you name: lvzhenjiang    # 把输入的内容传递给变量var1。输入的值显示出来！\nplease input you age [18]:           # playbook中定义默认值是18，如果不输入便是18，但是输入的值不显示出来！比如这里输入的23\n\nplay [test] *************************************************************************************************************************************\n\ntask [gathering facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\ntask [display var1] *****************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "your name of var1 is lvzhenjiang"\n}\n\ntask [display var2] *****************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "you age of var2 is 23"\n}\n\nplay recap **************************************************************************************************************************************\n10.4.7.101                 : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n# 2.5 通过ansible-playbook命令行定义变量！即参数传入变量\n\n除了vars_prompt和vars_files，也可以通过ansible命令行发送变量。如果想要编写一个通用的发布playbook时则特别有用！你可以传递应用的版本以便部署。例如下面命令（注意： --extra-vars 相等于 -e）\n\n$ cat exap.yml \n---\n- hosts: \'{{hosts}}\'\n  remote_user: \'{{user}}\'\n  tasks:\n    - name: "一个测试"\n      debug: msg="your hosts is {{hosts}}, user is {{user}}"\n\n$  ansible-playbook exap.yml -e "hosts=test user=root" \n[warning]: found variable using reserved name: hosts\n\nplay [test] *************************************************************************************************************************************\n\ntask [gathering facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\ntask [一个测试] *************************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "your hosts is test, user is root"\n}\n\nplay recap **************************************************************************************************************************************\n10.4.7.101                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n也可以将参数放在文件里面进行传递（注意命令行里要是用"@文件名"）：\n\n# 同样使用上面的例子\n$ cat anhui.yml \n---\nhosts: test\nuser: root\n\n$ ansible-playbook exap.yml -e "@anhui.yml"\n[warning]: found variable using reserved name: hosts\n\nplay [test] *************************************************************************************************************************************\n\ntask [gathering facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\ntask [一个测试] *************************************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "your hosts is test, user is root"\n}\n\nplay recap **************************************************************************************************************************************\n10.4.7.101                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 2.6 在playbook剧本中定义变量\n\n在playbook中定义变量需要用到ansible的vars模块，可以将所有需要用到的变量统一在vars模块下定义，定义格式需要遵循yaml语言格式：\n\n语法格式：\n\nvars:\n  - var1: value1\n  - var2: value2\n  - var3: value3\n  - ....: .....\n\n\n1\n2\n3\n4\n5\n\n\n示例如下：\n\n$ cat playbook.yml \n---\n- hosts: test\n  remote_user: root\n  vars:\n    - dir1: /root/ansible\n    - dir2: /root/ansible/test1\n    - dir3: /root/ansible/test2\n  tasks:\n    - name: create new folder\n      file: name={{ dir1 }} state=directory\n    - name: create new folder\n      file: name={{ dir2 }} state=directory\n    - name: create new folder\n      file: name={{ dir3 }} state=directory\n\n$ ansible-playbook playbook.yml \n\nplay [test] *************************************************************************************************************************************\n\ntask [gathering facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\ntask [create new folder] ************************************************************************************************************************\nchanged: [10.4.7.101]\n\ntask [create new folder] ************************************************************************************************************************\nchanged: [10.4.7.101]\n\ntask [create new folder] ************************************************************************************************************************\nchanged: [10.4.7.101]\n\nplay recap **************************************************************************************************************************************\n10.4.7.101                 : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n# 2.7 通过roles角色定义变量\n\n在ansible的roles中定义变量，需要将变量及值的键值对形式写到roles的vars目录下的main.yml文件中，同样适用yaml语言格式，格式如下：\n\nvar1: value1\nvar2: value2\nvar3: value3\n\n\n1\n2\n3\n\n\n但是请注意：通过roles定义的变量只适用于当前roles。\n\n# roles目录结构\n$ tree .\n.\n├── hosts\n├── playbook.yml\n└── test\n    ├── files\n    ├── tasks\n    │   └── main.yml\n    ├── templates\n    └── vars\n        └── main.yml\n\n5 directories, 4 files\n\n$ cat test/tasks/main.yml \n- name: create directory\n  file: name={{ dir }} state=directory\n- name: get ip address\n  shell: echo `{{ cmd }}` >> {{ dir }}/{{ file }}\n\n$ cat test/vars/main.yml \ncmd: hostname -i\n\n$ cat playbook.yml \n---\n- hosts: test\n  remote_user: root\n  roles:\n    - test\n\n$ cat hosts \n[test]\n10.4.7.101 dir=/root/node2\n10.4.7.102 dir=/root/node1\n\n[node1]\n10.4.7.100\n\n[test:vars]\nfile=hostname.txt\n\n$ ansible-playbook -i hosts  playbook.yml \n\nplay [test] *************************************************************************************************************************************\n\ntask [gathering facts] **************************************************************************************************************************\nok: [10.4.7.101]\nok: [10.4.7.102]\n\ntask [test : create directory] ******************************************************************************************************************\nok: [10.4.7.101]\nok: [10.4.7.102]\n\ntask [test : get ip address] ********************************************************************************************************************\nchanged: [10.4.7.102]\nchanged: [10.4.7.101]\n\nplay recap **************************************************************************************************************************************\n10.4.7.101                 : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n10.4.7.102                 : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n# 2.8 使用facts获取的信息\n\n还有其它地方可以获取变量, 这些变量是自动发现的，而不是用户自己设置的。facts通过访问远程系统获取相应的信息，一个很好的例子就是远程主机的ip地址或者操作系统是什么。\n\n$ ansible test -m setup\n# 使用以下命令可以查看哪些信息是可用的（test是上面在/etc/ansible/hosts列表文件中配置的主机群组）\n\n$ ansible test -m setup|grep "ansible_python_version"\n        "ansible_python_version": "2.7.5", \n# 在playbook中这样引用上面被控制主机的python版本: {{ ansible_python_version }}\n\n$ ansible test -m setup|grep "ansible_nodename"\n        "ansible_nodename": "template", \n# 可以在playbook中这样引用上面被控制主机的主机名: {{ ansible_nodename }}\n\n$ ansible test -m setup | grep "ansible_hostname"\n        "ansible_hostname": "template",\n# 被控制主机的主机名变量还可以是: {{ ansible_hostname }}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n如果关闭facts，可以大大提高ansible的执行速度 ，关闭方法如下：\n\n$ cat anhui.yml\n---\n- hosts: test\n  gather_facts: no\n\n\n1\n2\n3\n4\n\n\n# 2.9 register注册变量\n\n变量的另一个主要用途是在运行命令时，把命令结果存储到一个变量中，不同模块的执行结果是不同的。运行playbook时使用-v选项可以看到可能的结果值，ansible执行任务的结果值可以保存在变量中，以便稍后使用它。register方式主要用于在task之间传递变量。\n\n$ cat /etc/ansible/hosts\n[test]\n10.4.7.101\n10.4.7.102\n\n$ cat register.yml\n---\n- hosts: test\n  remote_user: root\n  tasks:\n      - name: register bo_test\n        shell: hostname -i\n        register: info\n      - name: display info\n        debug: msg="this host ip is {{ info[\'stdout\'] }}"\n\n$ ansible-playbook register.yml\n\nplay [test] *************************************************************************************************************\n\ntask [gathering facts] **************************************************************************************************\nok: [10.4.7.102]\nok: [10.4.7.101]\n\ntask [register bo_test] *************************************************************************************************\nchanged: [10.4.7.102]\nchanged: [10.4.7.101]\n\ntask [display info] *****************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "this host ip is 10.4.7.101 "\n}\nok: [10.4.7.102] => {\n    "msg": "this host ip is 10.4.7.102 "\n}\n\nplay recap **************************************************************************************************************\n10.4.7.101                 : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.4.7.102                 : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n# 2.10 hostvars 变量\n\n该变量用于引用其他主机上收集的facts中的数据，或者引用其他主机的主机变量、主机组变量。即从一台远程主机获取另一台远程主机的变量。\n\n$ cat /etc/ansible/hosts\n[test]\n10.4.7.101 addr=beijing\n10.4.7.102 user=shibo age=39\n\n$ cat test.yml\n---\n- hosts: test\n  remote_user: root\n  gather_facts: false\n  tasks:\n    - name: this is test1\n      debug: msg="she is come from {{ hostvars[\'10.4.7.101\'][\'addr\'] }}"\n    - name: this is test2\n      debug: msg="i am {{ hostvars[\'10.4.7.102\'][\'user\'] }}, and age is {{ hostvars[\'10.4.7.102\'][\'age\'] }}"\n\n$ ansible-playbook test.yml\n\nplay [test] *************************************************************************************************************\n\ntask [this is test1] ****************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "she is come from beijing"\n}\nok: [10.4.7.102] => {\n    "msg": "she is come from beijing"\n}\n\ntask [this is test2] ****************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": "i am shibo, and age is 39"\n}\nok: [10.4.7.102] => {\n    "msg": "i am shibo, and age is 39"\n}\n\nplay recap **************************************************************************************************************\n10.4.7.101                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.4.7.102                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n# 2.11 列表变量、循环变量、字典变量\n\n1）ansible的变量不仅可以是单个的值，也可以为列表，即ansible传列表作为变量\n\n$ cat test.yml\n---\n- hosts: test\n  remote_user: root\n  gather_facts: false\n  vars:\n    - list: [1,2,3]\n  tasks:\n    - name: echo\n      debug: msg="{{ list }}"\n\n$ ansible-playbook test.yml\n\nplay [test] *************************************************************************************************************\n\ntask [echo] *************************************************************************************************************\nok: [10.4.7.101] => {\n    "msg": [\n        1,\n        2,\n        3\n    ]\n}\nok: [10.4.7.102] => {\n    "msg": [\n        1,\n        2,\n        3\n    ]\n}\n\nplay recap **************************************************************************************************************\n10.4.7.101                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.4.7.102                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n2）循环列表\n\n结合循环，这个特性就变得很有用；以参数传递列表给playbook，不用在playbook中固定循环的次数与内容。\n\n$ cat test.yml\n---\n- hosts: 10.4.7.101\n  remote_user: root\n  gather_facts: false\n  vars:\n    - list: [1,2,3]\n  tasks:\n    - name: this is loop\n      debug: msg="{{ item }}"\n      with_items: \'{{list}}\'\n\n$ ansible-playbook test.yml\n\nplay [10.4.7.101] *******************************************************************************************************\n\ntask [this is loop] *****************************************************************************************************\nok: [10.4.7.101] => (item=1) => {\n    "msg": 1\n}\nok: [10.4.7.101] => (item=2) => {\n    "msg": 2\n}\nok: [10.4.7.101] => (item=3) => {\n    "msg": 3\n}\n\nplay recap **************************************************************************************************************\n10.4.7.101                 : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************',charsets:{cjk:!0}},{title:"Ansible性能优化——提升ansible执行效率",frontmatter:{title:"Ansible性能优化——提升ansible执行效率",categories:["ansible"],tags:["ansible"],date:"2022-12-09T20:53:07.000Z",permalink:"/pages/a06506/",readingShow:"top",description:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14386197.html",meta:[{name:"twitter:title",content:"Ansible性能优化——提升ansible执行效率"},{name:"twitter:description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14386197.html"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/04.Ansible%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E2%80%94%E2%80%94%E6%8F%90%E5%8D%87ansible%E6%89%A7%E8%A1%8C%E6%95%88%E7%8E%87.html"},{property:"og:type",content:"article"},{property:"og:title",content:"Ansible性能优化——提升ansible执行效率"},{property:"og:description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14386197.html"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/04.Ansible%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E2%80%94%E2%80%94%E6%8F%90%E5%8D%87ansible%E6%89%A7%E8%A1%8C%E6%95%88%E7%8E%87.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:53:07.000Z"},{property:"article:tag",content:"ansible"},{itemprop:"name",content:"Ansible性能优化——提升ansible执行效率"},{itemprop:"description",content:"原文链接：https://www.cnblogs.com/lvzhenjiang/p/14386197.html"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/04.Ansible%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E2%80%94%E2%80%94%E6%8F%90%E5%8D%87ansible%E6%89%A7%E8%A1%8C%E6%95%88%E7%8E%87.html",relativePath:"01.专题/01.ansible系列文章/04.Ansible性能优化——提升ansible执行效率.md",key:"v-f0715856",path:"/pages/a06506/",headers:[{level:3,title:"三、开启SSH长连接",slug:"三、开启ssh长连接",normalizedTitle:"三、开启ssh长连接",charIndex:348},{level:3,title:"三、设置facts缓存",slug:"三、设置facts缓存",normalizedTitle:"三、设置facts缓存",charIndex:426},{level:4,title:"3.1 使用json文件缓存",slug:"_3-1-使用json文件缓存",normalizedTitle:"3.1 使用json文件缓存",charIndex:443},{level:4,title:"3.2 使用redis存储facts文件需安装redis，还需要安装python库",slug:"_3-2-使用redis存储facts文件需安装redis-还需要安装python库",normalizedTitle:"3.2 使用redis存储facts文件需安装redis，还需要安装python库",charIndex:531},{level:3,title:"四、Ansible取消交互",slug:"四、ansible取消交互",normalizedTitle:"四、ansible取消交互",charIndex:671},{level:3,title:"五、Ansible的-t选项，提高ansible执行效率",slug:"五、ansible的-t选项-提高ansible执行效率",normalizedTitle:"五、ansible的-t选项，提高ansible执行效率",charIndex:756}],headersStr:"三、开启SSH长连接 三、设置facts缓存 3.1 使用json文件缓存 3.2 使用redis存储facts文件需安装redis，还需要安装python库 四、Ansible取消交互 五、Ansible的-t选项，提高ansible执行效率",content:'原文链接：https://www.cnblogs.com/lvzhenjiang/p/14386197.html\n\n# [Ansible性能优化——提升ansible执行效率](https://www.cnblogs.com/lvzhenjiang/p/14386197.html)\n\n\n\n目录\n\n- [一、关闭gathering facts功能](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#一、关闭gathering-facts功能)\n- [二、开启SSH pipelining](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#二、开启ssh-pipelining)\n- [三、开启SSH长连接](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#三、开启ssh长连接)\n- 三、设置facts缓存\n  - [3.1 使用json文件缓存](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#31-使用json文件缓存)\n  - [3.2 使用redis存储facts文件需安装redis，还需要安装python库](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#32-使用redis存储facts文件需安装redis，还需要安装python库)\n- [四、Ansible取消交互](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#四、ansible取消交互)\n- [五、Ansible的-t选项，提高ansible执行效率](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#五、ansible的-t选项，提高ansible执行效率)\n\n\n\n最初，ansible的执行效率和saltstack(基于zeromq消息队列的方式)相比要慢的多的多，特别是被控节点量很大的时候。但是ansible发展到现在，它的效率得到了极大的改善。在被控节点不太多的时候，默认的设置已经够快。即使被控节点数量巨大的时候，也可以通过一些优化去极大的提高ansible的执行效率。所以在使用 Ansible 的过程中，当管理的服务器数量增加时，不得不面对一个无法避免的问题执行效率慢，这里列出一些解决办法。\n\n### 一、关闭gathering facts功能\n\n如果观察过ansible-playbook的执行过程，就会发现ansible-playbook的第1个步骤总是执行gather facts，不论你有没有在playbook设定这个tasks。\n如果你不需要获取被控机器的fact数据的话，就可以关闭获取fact数据功能。关闭之后，可以加快ansible-playbook的执行效率，尤其是你管理很大量的机器时，这非常明显。\n关闭获取facts很简单，只需要在playbook文件中加上`"gather_facts: False"` 或者 `"gather_facts: No"`即可（False和No都为小写也可以）。\n\n```shell\n$ cat test.yml\n- hosts: test_server\n  remote_user: root\n\n  tasks:\n    - name: this is a test\n      shell: echo "haha"\n\n\n# 执行这个paly，会发现第一个执行的是gather facts，因为默认是打开gather facts功能的！！！！\n$ ansible-playbook test.yml\n\nPLAY [test_server] ******************************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************************\nok: [10.4.7.102]\nok: [10.4.7.101]\n\nTASK [this is a test] ***************************************************************************************************\nchanged: [10.4.7.102]\nchanged: [10.4.7.101]\n\nPLAY RECAP **************************************************************************************************************\n10.4.7.101                 : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.4.7.102                 : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n```\n\n**现在关闭gathering facts功能**\n\n```shell\n$ cat test.yml\n- hosts: test_server\n  remote_user: root\n  gather_facts: False\n\n  tasks:\n    - name: this is a test\n      shell: echo "haha"\n\n# 再执行这个play，就会发现没有了gathering facts执行过程，整个执行速度也快了！\n$ ansible-playbook test.yml\n\nPLAY [test_server] ******************************************************************************************************\n\nTASK [this is a test] ***************************************************************************************************\nchanged: [10.4.7.102]\nchanged: [10.4.7.101]\n\nPLAY RECAP **************************************************************************************************************\n10.4.7.101                 : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.4.7.102                 : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n```\n\n### 二、开启SSH pipelining\n\npipeline是openssh的一个特性，`ssh pipelining` 是一个加速Ansible执行速度的简单方法。\n\n在ansible执行每个任务的整个流程中，有一个过程是将临时任务文件put到远程的ansible客户机上，然后通过`ssh`连接过去远程执行这个任务。\n如果开启了pipelining，一个任务的所有动作都在一个`ssh`会话中完成，也会省去`sftp`到远端的过程，它会直接将要执行的任务在`ssh`会话中进行。\n\nssh``pipelining 默认是关闭!!!!之所以默认关闭是为了兼容不同的`sudo`配置，主要是 requiretty 选项。如果不使用`sudo`，建议开启！！！\n打开此选项可以减少ansible执行没有传输时`ssh`在被控机器上执行任务的连接数。\n不过，如果使用`sudo`，必须关闭requiretty选项。修改`/etc/ansible/ansible.cfg` 文件可以开启pipelining\n\n```shell\n$ vim /etc/ansible/ansible.cfg\n........\npipelining = True\n```\n\n这样开启了pipelining之后, ansible执行的整个流程就少了一个PUT脚本去远程服务端的流程，然后就可以批量对机器执行命令试下，可以明显感受到速度的提升。\n\n---\n\n\n但是要注意的是： 如果在ansible中使用sudo命令的话(ssh user@host sudo cmd)，需要在被控节点的/etc/sudoers中禁用"requiretty"!!!!\n\n之所以要设置/etc/sudoers中的requiretty，是因为ssh远程执行命令时，它的环境是非登录式非交互式shell，默认不会分配tty，没有tty，ssh的sudo就无法关闭密码回显(使用 "-tt"选项强制SSH分配tty)。所以出于安全考虑，/etc/sudoers中默认是开启requiretty的，它要求只有拥有tty的用户才能使用sudo，也就是说ssh连接过去不允许执行sudo。 可以通过visudo编辑配置文件，注释该选项来禁用它。\n\n$ grep requiretty /etc/sudoers　　\n# Defaults  requiretty\n\n\n1\n2\n\n\n\n# 三、开启SSH长连接\n\nansible天然支持openssh，默认连接方式下，它对ssh的依赖性非常强。所以优化ssh连接，在一定程度上也在优化ansible。其中一点是开启ssh的长连接，即长时间保持连接状态。\n\nAnsible模式是使用SSH和远程主机进行通信, 所以Ansible对SSH的依赖性非常强, 在OpenSSH 5.6版本以后SSH就支持了Multiplexing（多路复用）。 所以如果Ansible中控机的SSH -V版本高于5.6时, 就可以使用ControlPersist来提高ssh连接速度，从而提高ansible执行效率。\n\n$ cat /etc/redhat-release \nCentOS Linux release 7.6.1810 (Core) \n\n$ ssh -V\nOpenSSH_7.4p1, OpenSSL 1.0.2k-fips  26 Jan 2017\n\n$ vim /etc/ansible/ansible.cfg\n..........\nssh_args = -C -o ControlMaster=auto -o ControlPersist=5d\n# 注意：ConrolPersist=5d, 这个参数是设置整个长连接保持时间为5天。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n开启此参数的ssh长连接功能后，在会话过期前会一直建立连接，在netstat的结果中会看到ssh连接是一直established状态，且通过SSH连接过的设备都会在当前用户家目录的 ".ansible/cp"目录下生成一个socket文件，每个会话对应生成一个socket文件。也可以通过netstat命令查看, 会发现有一个ESTABLISHED状态的连接一直与远程设备进行着TCP连接。\n\n$ ps -ef|grep ssh|grep ansible\nroot      26064      1  0 17:32 ?        00:00:00 ssh: /root/.ansible/cp/cb9972d2a5 [mux]\nroot      26067      1  0 17:32 ?        00:00:00 ssh: /root/.ansible/cp/baefa88ac8 [mux]\n\n$ ps -ef|grep ssh|grep /root\nroot      26064      1  0 17:32 ?        00:00:00 ssh: /root/.ansible/cp/cb9972d2a5 [mux]\nroot      26067      1  0 17:32 ?        00:00:00 ssh: /root/.ansible/cp/baefa88ac8 [mux]\n\n$ ls /root/.ansible/cp/\nbaefa88ac8  cb9972d2a5\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n需要注意： ControlPersist 特性需要高版本的SSH才支持，CentOS 6默认是不支持的，如果需要使用，需要自行升级openssh（确保SSH -V版本高于5.6）。 ControlPersist即持久化socket，一次验证，多次通信。并且只需要修改 ssh 客户端就行，也就是 Ansible 机器即可。\n\n\n# 三、设置facts缓存\n\n如果细心的话, 就会发现执行playbook的时候, 默认第一个task都是GATHERING FACTS, 这个过程就是Ansible在收集每台主机的facts信息。 方便我们在playbook中直接饮用facts里的信息，当然如果你的playbook中不需要facts信息, 可以在playbook中设置"gather_facts: False"来提高playbook效率.\n\n但是如果我们既想在每次执行playbook的时候都能收集facts, 又想加速这个收集过程, 那么就需要配置facts缓存了。\n\n# 3.1 使用json文件缓存\n\n$ vim /etc/ansible/ansible.cfg\n.........\ngathering = smart\nfact_caching_timeout = 86400\nfact_caching = jsonfile\nfact_caching_connection = /dev/shm/ansible_fact_cache\n\n# 正常配置palybook，不需要关闭gathering facts功能\n$ cat test.yml \n---\n- hosts: 10.4.7.101\n  remote_user: root\n  vars:\n    - list: [1,2,3]\n  tasks:\n    - name: this is loop\n      debug: msg="{{ item }}"\n      with_items: \'{{list}}\'\n\n查看这个playbook过程，用时6.699s（第一次可能稍微慢点，缓存之后，后面执行就很快了）\n$ time ansible-playbook test.yml \n\nPLAY [10.4.7.101] *******************************************************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\nTASK [this is loop] *****************************************************************************************************************************\nok: [10.4.7.101] => (item=1) => {\n    "msg": 1\n}\nok: [10.4.7.101] => (item=2) => {\n    "msg": 2\n}\nok: [10.4.7.101] => (item=3) => {\n    "msg": 3\n}\n\nPLAY RECAP **************************************************************************************************************************************\n10.4.7.101                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\n\nreal    0m6.699s\nuser    0m1.301s\nsys     0m0.250s\n\n# 如果去掉上面的facts缓存的四行配置，再次执行上面的playbok，发现用时10s左右！！！\n\n# 查看缓存文件\n$ ls /dev/shm/ansible_fact_cache/\n10.4.7.101\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n# 3.2 使用redis存储facts文件需安装redis，还需要安装python库\n\n$ yum install redis\n$ yum -y install epel-release\n$ yum install python-pip\n$ pip install redis\n$ vim /etc/ansible/ansible.cfg\n........\ngathering = smart\nfacts_caching_timeout = 86400      #设置缓存过期时间86400秒\nfacts_caching = redis              # 使用redis或者 (或者使用memcached，即"facts_caching = memcached")\nfact_caching_connection = 127.0.0.1:6379\n#若redis设置了密码,比如密码为"admin"，则配置修改如下：\n# fact_caching_connection = localhost:6379:0:admin\n\n$ systemctl start redis\n$ lsof -i:6379\nCOMMAND     PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\nredis-ser 26593 redis    4u  IPv4 125427      0t0  TCP localhost:6379 (LISTEN)\n\n$ time ansible-playbook test.yml \n\nPLAY [10.4.7.101] *******************************************************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\nTASK [this is loop] *****************************************************************************************************************************\nok: [10.4.7.101] => (item=1) => {\n    "msg": 1\n}\nok: [10.4.7.101] => (item=2) => {\n    "msg": 2\n}\nok: [10.4.7.101] => (item=3) => {\n    "msg": 3\n}\n\nPLAY RECAP **************************************************************************************************************************************\n10.4.7.101                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\n\nreal    0m6.720s\nuser    0m1.219s\nsys        0m0.338s\n\n需要注意：\n在使用redis缓存后，如果出现异常（若未出现，请忽略）：TypeError: the JSON object must be str, not \'bytes\'。\n解决办法：\n$ find / -name ansible\n$ vim /usr/lib/python2.7/site-packages/ansible/plugins/cache/redis.py\n..........\nself._cache[key] = json.loads(value.decode(\'utf-8\'))     # 修改为这个\n\n查看redis存储情况\n$ redis-cli\n127.0.0.1:6379> keys *\n1) "ansible_facts10.4.7.101"\n2) "ansible_cache_keys"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n总之：不同网络环境下的耗时肯定是不同的，但是设置缓存是肯定可以加快 Ansible 运行速度的，特别是 playbook 的运行。\n\n\n# 四、Ansible取消交互\n\n$ vim /etc/ansible/ansible.cfg\n........\nhost_key_checking = False          # 打开注释即可\n\n# 取消ssh的yes和no的交互：\n$ vim /root/.ssh/config\nUserKnownHostsFile /dev/null\nConnectTimeout 15\nStrictHostKeyChecking no\n\n或者直接ssh时增加一个参数\n$ ssh -o StrictHostKeyChecking=no -p22 root@10.4.7.101\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 五、Ansible的-t选项，提高ansible执行效率\n\nansible的"-t"或"--tree"选项是将ansible的执行结果按主机名保存在指定目录下的文件中。\n\n有些时候，ansible执行起来的速度会非常慢，这种慢体现在即使执行的是一个立即返回的简单命令(如ping模块)，也会耗时很久，且不是因为ssh连接慢导致的。 如果使用-t选项，将第一次执行得到的结果按inventory中定义的主机名保存在文件中，下次执行到同一台主机时速度将会变快很多，即使之后不再加上-t选项， 也可以在一定时间内保持迅速执行。即使执行速度正常（如执行一个Ping命令0.7秒左右），使用-t选项也可以在此基础上变得更快。\n\n除了使用-t选项，使用重定向将结果重定向到某个文件中也是一样的效果。 这也算是一种ansible提速方式，但在centos6上使用低版本ansible时，有时会出现执行很慢的现象，但不是每次都这样，且centos7执行速度正常 所以这也是一种"bug"式问题，故这种方式没有通用性。\n\n$ time ansible test_server -m command -a "hostname"\n$ time ansible test_server -m command -a "hostname" -t /tmp/test\n\n$ ll /tmp/test/\n总用量 8\n-rw-r--r--. 1 root root 236 2月   7 17:59 10.4.7.101\n-rw-r--r--. 1 root root 307 2月   7 17:59 10.4.7.102\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n上面做了对比，发现使用-t或重定向方式，将ansible的执行结果按主机名保存在指定目录下的文件中，ansible执行效率会有所提升。\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************',normalizedContent:'原文链接：https://www.cnblogs.com/lvzhenjiang/p/14386197.html\n\n# [ansible性能优化——提升ansible执行效率](https://www.cnblogs.com/lvzhenjiang/p/14386197.html)\n\n\n\n目录\n\n- [一、关闭gathering facts功能](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#一、关闭gathering-facts功能)\n- [二、开启ssh pipelining](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#二、开启ssh-pipelining)\n- [三、开启ssh长连接](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#三、开启ssh长连接)\n- 三、设置facts缓存\n  - [3.1 使用json文件缓存](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#31-使用json文件缓存)\n  - [3.2 使用redis存储facts文件需安装redis，还需要安装python库](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#32-使用redis存储facts文件需安装redis，还需要安装python库)\n- [四、ansible取消交互](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#四、ansible取消交互)\n- [五、ansible的-t选项，提高ansible执行效率](https://www.cnblogs.com/lvzhenjiang/p/14386197.html#五、ansible的-t选项，提高ansible执行效率)\n\n\n\n最初，ansible的执行效率和saltstack(基于zeromq消息队列的方式)相比要慢的多的多，特别是被控节点量很大的时候。但是ansible发展到现在，它的效率得到了极大的改善。在被控节点不太多的时候，默认的设置已经够快。即使被控节点数量巨大的时候，也可以通过一些优化去极大的提高ansible的执行效率。所以在使用 ansible 的过程中，当管理的服务器数量增加时，不得不面对一个无法避免的问题执行效率慢，这里列出一些解决办法。\n\n### 一、关闭gathering facts功能\n\n如果观察过ansible-playbook的执行过程，就会发现ansible-playbook的第1个步骤总是执行gather facts，不论你有没有在playbook设定这个tasks。\n如果你不需要获取被控机器的fact数据的话，就可以关闭获取fact数据功能。关闭之后，可以加快ansible-playbook的执行效率，尤其是你管理很大量的机器时，这非常明显。\n关闭获取facts很简单，只需要在playbook文件中加上`"gather_facts: false"` 或者 `"gather_facts: no"`即可（false和no都为小写也可以）。\n\n```shell\n$ cat test.yml\n- hosts: test_server\n  remote_user: root\n\n  tasks:\n    - name: this is a test\n      shell: echo "haha"\n\n\n# 执行这个paly，会发现第一个执行的是gather facts，因为默认是打开gather facts功能的！！！！\n$ ansible-playbook test.yml\n\nplay [test_server] ******************************************************************************************************\n\ntask [gathering facts] **************************************************************************************************\nok: [10.4.7.102]\nok: [10.4.7.101]\n\ntask [this is a test] ***************************************************************************************************\nchanged: [10.4.7.102]\nchanged: [10.4.7.101]\n\nplay recap **************************************************************************************************************\n10.4.7.101                 : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.4.7.102                 : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n```\n\n**现在关闭gathering facts功能**\n\n```shell\n$ cat test.yml\n- hosts: test_server\n  remote_user: root\n  gather_facts: false\n\n  tasks:\n    - name: this is a test\n      shell: echo "haha"\n\n# 再执行这个play，就会发现没有了gathering facts执行过程，整个执行速度也快了！\n$ ansible-playbook test.yml\n\nplay [test_server] ******************************************************************************************************\n\ntask [this is a test] ***************************************************************************************************\nchanged: [10.4.7.102]\nchanged: [10.4.7.101]\n\nplay recap **************************************************************************************************************\n10.4.7.101                 : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.4.7.102                 : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n```\n\n### 二、开启ssh pipelining\n\npipeline是openssh的一个特性，`ssh pipelining` 是一个加速ansible执行速度的简单方法。\n\n在ansible执行每个任务的整个流程中，有一个过程是将临时任务文件put到远程的ansible客户机上，然后通过`ssh`连接过去远程执行这个任务。\n如果开启了pipelining，一个任务的所有动作都在一个`ssh`会话中完成，也会省去`sftp`到远端的过程，它会直接将要执行的任务在`ssh`会话中进行。\n\nssh``pipelining 默认是关闭!!!!之所以默认关闭是为了兼容不同的`sudo`配置，主要是 requiretty 选项。如果不使用`sudo`，建议开启！！！\n打开此选项可以减少ansible执行没有传输时`ssh`在被控机器上执行任务的连接数。\n不过，如果使用`sudo`，必须关闭requiretty选项。修改`/etc/ansible/ansible.cfg` 文件可以开启pipelining\n\n```shell\n$ vim /etc/ansible/ansible.cfg\n........\npipelining = true\n```\n\n这样开启了pipelining之后, ansible执行的整个流程就少了一个put脚本去远程服务端的流程，然后就可以批量对机器执行命令试下，可以明显感受到速度的提升。\n\n---\n\n\n但是要注意的是： 如果在ansible中使用sudo命令的话(ssh user@host sudo cmd)，需要在被控节点的/etc/sudoers中禁用"requiretty"!!!!\n\n之所以要设置/etc/sudoers中的requiretty，是因为ssh远程执行命令时，它的环境是非登录式非交互式shell，默认不会分配tty，没有tty，ssh的sudo就无法关闭密码回显(使用 "-tt"选项强制ssh分配tty)。所以出于安全考虑，/etc/sudoers中默认是开启requiretty的，它要求只有拥有tty的用户才能使用sudo，也就是说ssh连接过去不允许执行sudo。 可以通过visudo编辑配置文件，注释该选项来禁用它。\n\n$ grep requiretty /etc/sudoers　　\n# defaults  requiretty\n\n\n1\n2\n\n\n\n# 三、开启ssh长连接\n\nansible天然支持openssh，默认连接方式下，它对ssh的依赖性非常强。所以优化ssh连接，在一定程度上也在优化ansible。其中一点是开启ssh的长连接，即长时间保持连接状态。\n\nansible模式是使用ssh和远程主机进行通信, 所以ansible对ssh的依赖性非常强, 在openssh 5.6版本以后ssh就支持了multiplexing（多路复用）。 所以如果ansible中控机的ssh -v版本高于5.6时, 就可以使用controlpersist来提高ssh连接速度，从而提高ansible执行效率。\n\n$ cat /etc/redhat-release \ncentos linux release 7.6.1810 (core) \n\n$ ssh -v\nopenssh_7.4p1, openssl 1.0.2k-fips  26 jan 2017\n\n$ vim /etc/ansible/ansible.cfg\n..........\nssh_args = -c -o controlmaster=auto -o controlpersist=5d\n# 注意：conrolpersist=5d, 这个参数是设置整个长连接保持时间为5天。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n开启此参数的ssh长连接功能后，在会话过期前会一直建立连接，在netstat的结果中会看到ssh连接是一直established状态，且通过ssh连接过的设备都会在当前用户家目录的 ".ansible/cp"目录下生成一个socket文件，每个会话对应生成一个socket文件。也可以通过netstat命令查看, 会发现有一个established状态的连接一直与远程设备进行着tcp连接。\n\n$ ps -ef|grep ssh|grep ansible\nroot      26064      1  0 17:32 ?        00:00:00 ssh: /root/.ansible/cp/cb9972d2a5 [mux]\nroot      26067      1  0 17:32 ?        00:00:00 ssh: /root/.ansible/cp/baefa88ac8 [mux]\n\n$ ps -ef|grep ssh|grep /root\nroot      26064      1  0 17:32 ?        00:00:00 ssh: /root/.ansible/cp/cb9972d2a5 [mux]\nroot      26067      1  0 17:32 ?        00:00:00 ssh: /root/.ansible/cp/baefa88ac8 [mux]\n\n$ ls /root/.ansible/cp/\nbaefa88ac8  cb9972d2a5\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n需要注意： controlpersist 特性需要高版本的ssh才支持，centos 6默认是不支持的，如果需要使用，需要自行升级openssh（确保ssh -v版本高于5.6）。 controlpersist即持久化socket，一次验证，多次通信。并且只需要修改 ssh 客户端就行，也就是 ansible 机器即可。\n\n\n# 三、设置facts缓存\n\n如果细心的话, 就会发现执行playbook的时候, 默认第一个task都是gathering facts, 这个过程就是ansible在收集每台主机的facts信息。 方便我们在playbook中直接饮用facts里的信息，当然如果你的playbook中不需要facts信息, 可以在playbook中设置"gather_facts: false"来提高playbook效率.\n\n但是如果我们既想在每次执行playbook的时候都能收集facts, 又想加速这个收集过程, 那么就需要配置facts缓存了。\n\n# 3.1 使用json文件缓存\n\n$ vim /etc/ansible/ansible.cfg\n.........\ngathering = smart\nfact_caching_timeout = 86400\nfact_caching = jsonfile\nfact_caching_connection = /dev/shm/ansible_fact_cache\n\n# 正常配置palybook，不需要关闭gathering facts功能\n$ cat test.yml \n---\n- hosts: 10.4.7.101\n  remote_user: root\n  vars:\n    - list: [1,2,3]\n  tasks:\n    - name: this is loop\n      debug: msg="{{ item }}"\n      with_items: \'{{list}}\'\n\n查看这个playbook过程，用时6.699s（第一次可能稍微慢点，缓存之后，后面执行就很快了）\n$ time ansible-playbook test.yml \n\nplay [10.4.7.101] *******************************************************************************************************************************\n\ntask [gathering facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\ntask [this is loop] *****************************************************************************************************************************\nok: [10.4.7.101] => (item=1) => {\n    "msg": 1\n}\nok: [10.4.7.101] => (item=2) => {\n    "msg": 2\n}\nok: [10.4.7.101] => (item=3) => {\n    "msg": 3\n}\n\nplay recap **************************************************************************************************************************************\n10.4.7.101                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\n\nreal    0m6.699s\nuser    0m1.301s\nsys     0m0.250s\n\n# 如果去掉上面的facts缓存的四行配置，再次执行上面的playbok，发现用时10s左右！！！\n\n# 查看缓存文件\n$ ls /dev/shm/ansible_fact_cache/\n10.4.7.101\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n# 3.2 使用redis存储facts文件需安装redis，还需要安装python库\n\n$ yum install redis\n$ yum -y install epel-release\n$ yum install python-pip\n$ pip install redis\n$ vim /etc/ansible/ansible.cfg\n........\ngathering = smart\nfacts_caching_timeout = 86400      #设置缓存过期时间86400秒\nfacts_caching = redis              # 使用redis或者 (或者使用memcached，即"facts_caching = memcached")\nfact_caching_connection = 127.0.0.1:6379\n#若redis设置了密码,比如密码为"admin"，则配置修改如下：\n# fact_caching_connection = localhost:6379:0:admin\n\n$ systemctl start redis\n$ lsof -i:6379\ncommand     pid  user   fd   type device size/off node name\nredis-ser 26593 redis    4u  ipv4 125427      0t0  tcp localhost:6379 (listen)\n\n$ time ansible-playbook test.yml \n\nplay [10.4.7.101] *******************************************************************************************************************************\n\ntask [gathering facts] **************************************************************************************************************************\nok: [10.4.7.101]\n\ntask [this is loop] *****************************************************************************************************************************\nok: [10.4.7.101] => (item=1) => {\n    "msg": 1\n}\nok: [10.4.7.101] => (item=2) => {\n    "msg": 2\n}\nok: [10.4.7.101] => (item=3) => {\n    "msg": 3\n}\n\nplay recap **************************************************************************************************************************************\n10.4.7.101                 : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\n\nreal    0m6.720s\nuser    0m1.219s\nsys        0m0.338s\n\n需要注意：\n在使用redis缓存后，如果出现异常（若未出现，请忽略）：typeerror: the json object must be str, not \'bytes\'。\n解决办法：\n$ find / -name ansible\n$ vim /usr/lib/python2.7/site-packages/ansible/plugins/cache/redis.py\n..........\nself._cache[key] = json.loads(value.decode(\'utf-8\'))     # 修改为这个\n\n查看redis存储情况\n$ redis-cli\n127.0.0.1:6379> keys *\n1) "ansible_facts10.4.7.101"\n2) "ansible_cache_keys"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n总之：不同网络环境下的耗时肯定是不同的，但是设置缓存是肯定可以加快 ansible 运行速度的，特别是 playbook 的运行。\n\n\n# 四、ansible取消交互\n\n$ vim /etc/ansible/ansible.cfg\n........\nhost_key_checking = false          # 打开注释即可\n\n# 取消ssh的yes和no的交互：\n$ vim /root/.ssh/config\nuserknownhostsfile /dev/null\nconnecttimeout 15\nstricthostkeychecking no\n\n或者直接ssh时增加一个参数\n$ ssh -o stricthostkeychecking=no -p22 root@10.4.7.101\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 五、ansible的-t选项，提高ansible执行效率\n\nansible的"-t"或"--tree"选项是将ansible的执行结果按主机名保存在指定目录下的文件中。\n\n有些时候，ansible执行起来的速度会非常慢，这种慢体现在即使执行的是一个立即返回的简单命令(如ping模块)，也会耗时很久，且不是因为ssh连接慢导致的。 如果使用-t选项，将第一次执行得到的结果按inventory中定义的主机名保存在文件中，下次执行到同一台主机时速度将会变快很多，即使之后不再加上-t选项， 也可以在一定时间内保持迅速执行。即使执行速度正常（如执行一个ping命令0.7秒左右），使用-t选项也可以在此基础上变得更快。\n\n除了使用-t选项，使用重定向将结果重定向到某个文件中也是一样的效果。 这也算是一种ansible提速方式，但在centos6上使用低版本ansible时，有时会出现执行很慢的现象，但不是每次都这样，且centos7执行速度正常 所以这也是一种"bug"式问题，故这种方式没有通用性。\n\n$ time ansible test_server -m command -a "hostname"\n$ time ansible test_server -m command -a "hostname" -t /tmp/test\n\n$ ll /tmp/test/\n总用量 8\n-rw-r--r--. 1 root root 236 2月   7 17:59 10.4.7.101\n-rw-r--r--. 1 root root 307 2月   7 17:59 10.4.7.102\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n上面做了对比，发现使用-t或重定向方式，将ansible的执行结果按主机名保存在指定目录下的文件中，ansible执行效率会有所提升。\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************',charsets:{cjk:!0}},{title:"ansible之roles简单使用",frontmatter:{title:"ansible之roles简单使用",categories:["系列专题","ansible"],tags:["ansible"],date:"2022-12-09T20:53:07.000Z",permalink:"/pages/847542/",readingShow:"top",description:"目录",meta:[{name:"image",content:"https://gitee.com/lvzhenjiang/document/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/20200322142808.png"},{name:"twitter:title",content:"ansible之roles简单使用"},{name:"twitter:description",content:"目录"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://gitee.com/lvzhenjiang/document/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/20200322142808.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/05.ansible%E4%B9%8Broles%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8.html"},{property:"og:type",content:"article"},{property:"og:title",content:"ansible之roles简单使用"},{property:"og:description",content:"目录"},{property:"og:image",content:"https://gitee.com/lvzhenjiang/document/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/20200322142808.png"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/05.ansible%E4%B9%8Broles%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:53:07.000Z"},{property:"article:tag",content:"ansible"},{itemprop:"name",content:"ansible之roles简单使用"},{itemprop:"description",content:"目录"},{itemprop:"image",content:"https://gitee.com/lvzhenjiang/document/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/20200322142808.png"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/05.ansible%E4%B9%8Broles%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8.html",relativePath:"01.专题/01.ansible系列文章/05.ansible之roles简单使用.md",key:"v-23f7a879",path:"/pages/847542/",headers:[{level:3,title:"一、roles简介",slug:"一、roles简介",normalizedTitle:"一、roles简介",charIndex:28},{level:3,title:"二、简单的roles示例",slug:"二、简单的roles示例",normalizedTitle:"二、简单的roles示例",charIndex:41},{level:3,title:"三、roles示例二",slug:"三、roles示例二",normalizedTitle:"三、roles示例二",charIndex:57}],headersStr:"一、roles简介 二、简单的roles示例 三、roles示例二",content:"# ansible之roles简单使用\n\n目录\n\n * 一、roles简介\n * 二、简单的roles示例\n * 三、roles示例二\n\n\n# 一、roles简介\n\n将多种不同的tasks的文件集中存储在某个目录下，则该目录就是角色，角色一般存放在/etc/ansible/roles/目录下，可通过ansible的配置文件来调整默认的角色目录，/etc/ansible/roles/目录下有很多子目录，其中每一个子目录对应一个角色，每个角色也有自己的目录结构，如图：\n\n\n\n每个角色的定义，以特定的层级目录结构进行组织。比如：\n\n * files：存放有copy或script等模块调用的文件；\n * templates：存放template模块查找所需要的模板文件的目录；\n * tasks：任务存放的目录；\n * handlers：存放相关触发执行器的目录；\n * vars：变量存放的目录；\n * meta：用于存放此角色元数据；\n * default：默认变量存放的目录，文件中定义了此角色使用的默认变量；\n\n上述目录中，tasks、handlers、vars、meta、default至少应该包含一个main.yaml文件，该目录下也可由其他.yaml文件，但是需要在main.yml文件中用include指令将其他.yml文件包含起来。\n\n\n# 二、简单的roles示例\n\n$ mkdir roles   \n#创建一个目录，名称为roles。官方推荐在/etc/ansible/roles/这个目录，不过在哪里都是可以的\n$ mkdir roles/nginx\n$ cd roles/nginx/\n$ mkdir tasks templates\n$ cd tasks/\n$ cat user.yaml \n- name: create user\n  user: name=nginx uid=80 group=nginx system=yes shell=/sbin/nologin\n\n$ cat group.yaml \n- name: create group\n  group: name=nginx gid=80\n$ cat yum.yaml \n- name: install package\n  yum: name=nginx\n\n$ cat templ.yaml \n- name: copy conf\n  template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf \n\n$ cat stservice.yaml \n- name: start service\n  service: name=nginx state=started enabled=yes\n\n$ cat main.yaml \n- include: group.yaml\n- include: user.yaml\n- include: yum.yaml\n- include: templ.yaml\n- include: stservice.yaml\n#如果需要调用别的角色的yaml文件，也可以这样写，比如：- include：roles/httpd/tasks/copyfile.yaml\n注意：copyfiile.yaml文件中的源路径必须是绝对路径\n\n$ ls\nnginx.conf.j2\n#该文件就是nginx的配置文件改名了而已\n$ cd ../../../\n$ cat nginx.yaml \n- hosts: webservers\n  remote_user: root\n  roles:\n    - role: nginx    \n#定义了多个角色，也可在接着写，每行调用一个角色\n#也可以定义tags标签，比如：- { roles： httpd ，tags：['web','httpd']}或- { roles： httpd ，tags：'web'  如果需要加when语句，在此处添加 }都可以\n$ ls\nnginx.yaml  roles\n#确保调用nginx的yaml文件是和roles是在同一目录下的\n$ tree\n.\n├── nginx.yaml\n└── roles\n    └── nginx\n        ├── tasks\n        │   ├── group.yaml\n        │   ├── main.yaml\n        │   ├── reservice.yaml\n        │   ├── stservice.yaml\n        │   ├── templ.yaml\n        │   ├── user.yaml\n        │   └── yum.yaml\n        └── templates\n            └── nginx.conf.j2\n#确保目录的层次效果是这样的\n$ ansible-playbook nginx.yaml \n#如果定义了标签，就可对标签进行操作（比如：web或httpd）\n$ ansible webservers -m shell -a 'ss -lntp | grep nginx'\n#确认被控端的主机nginx已经启动\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n\n\n一个简单的nginx的roles已经编写完成了！\n\n\n# 三、roles示例二\n\n$ mkdir app\n$ cd app/\n$ mkdir tasks templates vars handlers files\n$ cd tasks/\n$ cat group.yaml \n- name: create group\n  group: name=apache system=yes\n$ cat user.yaml \n- name: create user\n  user: name=apache group=apache system=yes shell=/sbin/nologin\n$ cat yum.yaml \n- name: install package\n  yum: name=httpd\n$ cat templ.yaml \n- name: copy file\n  template: src=httpd.conf.j2 dest=/etc/httpd/httpd.conf\n  notify: restart service\n\n$ cat copyfile.yaml \n- name: copy config\n  copy: src=vhosts.conf dest=/etc/httpd/conf.d/ owner=apache\n$ cat start.yaml \n- name: start service\n  service: name=httpd state=started\n$ cat main.yaml \n- include: group.yaml\n- include: user.yaml\n- include: yum.yaml\n- include: templ.yaml\n- include: copyfile.yaml\n- include: start.yaml\n$ ls files/vhosts.conf \nfiles/vhosts.conf\n#空文件用于测试\n$ cat handlers/main.yaml \n- name: restart service\n  service: name=httpd state=restarted\n$ cat templates/httpd.conf.j2 \n#httpd的配置文件\n$ cat vars/main.yaml \nusername: apache\ngroupname: apache\n$ tree\n.\n├── files\n│   └── vhosts.conf\n├── handlers\n│   └── main.yaml\n├── tasks\n│   ├── copyfile.yaml\n│   ├── group.yaml\n│   ├── main.yaml\n│   ├── start.yaml\n│   ├── templ.yaml\n│   ├── user.yaml\n│   └── yum.yaml\n├── templates\n│   └── httpd.conf.j2\n└── vars\n    └── main.yaml\n#目录结构\n$ cat httpd.yaml \n---\n- hosts: webservers\n  remote_user: root\n  roles:\n    - httpd\n$ ls\nhttpd.yaml  nginx.yaml  roles\n#确保与roles目录在同一目录下\n$ ansible-playbook httpd.yaml \n$ ansible all -m shell -a 'ps -ef | grep apache'\n//根据配置文件中修改的内容自行进行修改\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************\n\n原文链接：https://www.cnblogs.com/lvzhenjiang/p/14199463.html",normalizedContent:"# ansible之roles简单使用\n\n目录\n\n * 一、roles简介\n * 二、简单的roles示例\n * 三、roles示例二\n\n\n# 一、roles简介\n\n将多种不同的tasks的文件集中存储在某个目录下，则该目录就是角色，角色一般存放在/etc/ansible/roles/目录下，可通过ansible的配置文件来调整默认的角色目录，/etc/ansible/roles/目录下有很多子目录，其中每一个子目录对应一个角色，每个角色也有自己的目录结构，如图：\n\n\n\n每个角色的定义，以特定的层级目录结构进行组织。比如：\n\n * files：存放有copy或script等模块调用的文件；\n * templates：存放template模块查找所需要的模板文件的目录；\n * tasks：任务存放的目录；\n * handlers：存放相关触发执行器的目录；\n * vars：变量存放的目录；\n * meta：用于存放此角色元数据；\n * default：默认变量存放的目录，文件中定义了此角色使用的默认变量；\n\n上述目录中，tasks、handlers、vars、meta、default至少应该包含一个main.yaml文件，该目录下也可由其他.yaml文件，但是需要在main.yml文件中用include指令将其他.yml文件包含起来。\n\n\n# 二、简单的roles示例\n\n$ mkdir roles   \n#创建一个目录，名称为roles。官方推荐在/etc/ansible/roles/这个目录，不过在哪里都是可以的\n$ mkdir roles/nginx\n$ cd roles/nginx/\n$ mkdir tasks templates\n$ cd tasks/\n$ cat user.yaml \n- name: create user\n  user: name=nginx uid=80 group=nginx system=yes shell=/sbin/nologin\n\n$ cat group.yaml \n- name: create group\n  group: name=nginx gid=80\n$ cat yum.yaml \n- name: install package\n  yum: name=nginx\n\n$ cat templ.yaml \n- name: copy conf\n  template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf \n\n$ cat stservice.yaml \n- name: start service\n  service: name=nginx state=started enabled=yes\n\n$ cat main.yaml \n- include: group.yaml\n- include: user.yaml\n- include: yum.yaml\n- include: templ.yaml\n- include: stservice.yaml\n#如果需要调用别的角色的yaml文件，也可以这样写，比如：- include：roles/httpd/tasks/copyfile.yaml\n注意：copyfiile.yaml文件中的源路径必须是绝对路径\n\n$ ls\nnginx.conf.j2\n#该文件就是nginx的配置文件改名了而已\n$ cd ../../../\n$ cat nginx.yaml \n- hosts: webservers\n  remote_user: root\n  roles:\n    - role: nginx    \n#定义了多个角色，也可在接着写，每行调用一个角色\n#也可以定义tags标签，比如：- { roles： httpd ，tags：['web','httpd']}或- { roles： httpd ，tags：'web'  如果需要加when语句，在此处添加 }都可以\n$ ls\nnginx.yaml  roles\n#确保调用nginx的yaml文件是和roles是在同一目录下的\n$ tree\n.\n├── nginx.yaml\n└── roles\n    └── nginx\n        ├── tasks\n        │   ├── group.yaml\n        │   ├── main.yaml\n        │   ├── reservice.yaml\n        │   ├── stservice.yaml\n        │   ├── templ.yaml\n        │   ├── user.yaml\n        │   └── yum.yaml\n        └── templates\n            └── nginx.conf.j2\n#确保目录的层次效果是这样的\n$ ansible-playbook nginx.yaml \n#如果定义了标签，就可对标签进行操作（比如：web或httpd）\n$ ansible webservers -m shell -a 'ss -lntp | grep nginx'\n#确认被控端的主机nginx已经启动\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n\n\n一个简单的nginx的roles已经编写完成了！\n\n\n# 三、roles示例二\n\n$ mkdir app\n$ cd app/\n$ mkdir tasks templates vars handlers files\n$ cd tasks/\n$ cat group.yaml \n- name: create group\n  group: name=apache system=yes\n$ cat user.yaml \n- name: create user\n  user: name=apache group=apache system=yes shell=/sbin/nologin\n$ cat yum.yaml \n- name: install package\n  yum: name=httpd\n$ cat templ.yaml \n- name: copy file\n  template: src=httpd.conf.j2 dest=/etc/httpd/httpd.conf\n  notify: restart service\n\n$ cat copyfile.yaml \n- name: copy config\n  copy: src=vhosts.conf dest=/etc/httpd/conf.d/ owner=apache\n$ cat start.yaml \n- name: start service\n  service: name=httpd state=started\n$ cat main.yaml \n- include: group.yaml\n- include: user.yaml\n- include: yum.yaml\n- include: templ.yaml\n- include: copyfile.yaml\n- include: start.yaml\n$ ls files/vhosts.conf \nfiles/vhosts.conf\n#空文件用于测试\n$ cat handlers/main.yaml \n- name: restart service\n  service: name=httpd state=restarted\n$ cat templates/httpd.conf.j2 \n#httpd的配置文件\n$ cat vars/main.yaml \nusername: apache\ngroupname: apache\n$ tree\n.\n├── files\n│   └── vhosts.conf\n├── handlers\n│   └── main.yaml\n├── tasks\n│   ├── copyfile.yaml\n│   ├── group.yaml\n│   ├── main.yaml\n│   ├── start.yaml\n│   ├── templ.yaml\n│   ├── user.yaml\n│   └── yum.yaml\n├── templates\n│   └── httpd.conf.j2\n└── vars\n    └── main.yaml\n#目录结构\n$ cat httpd.yaml \n---\n- hosts: webservers\n  remote_user: root\n  roles:\n    - httpd\n$ ls\nhttpd.yaml  nginx.yaml  roles\n#确保与roles目录在同一目录下\n$ ansible-playbook httpd.yaml \n$ ansible all -m shell -a 'ps -ef | grep apache'\n//根据配置文件中修改的内容自行进行修改\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************\n\n原文链接：https://www.cnblogs.com/lvzhenjiang/p/14199463.html",charsets:{cjk:!0}},{title:"ansible中template简单使用",frontmatter:{title:"ansible中template简单使用",categories:["系列专题","ansible"],tags:["ansible"],date:"2022-12-09T20:53:07.000Z",permalink:"/pages/4189b8/",readingShow:"top",description:"目录",meta:[{name:"twitter:title",content:"ansible中template简单使用"},{name:"twitter:description",content:"目录"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/06.ansible%E4%B8%ADtemplate%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8.html"},{property:"og:type",content:"article"},{property:"og:title",content:"ansible中template简单使用"},{property:"og:description",content:"目录"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/06.ansible%E4%B8%ADtemplate%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:53:07.000Z"},{property:"article:tag",content:"ansible"},{itemprop:"name",content:"ansible中template简单使用"},{itemprop:"description",content:"目录"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/06.ansible%E4%B8%ADtemplate%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8.html",relativePath:"01.专题/01.ansible系列文章/06.ansible中template简单使用.md",key:"v-6f8ad9d9",path:"/pages/4189b8/",headers:[{level:3,title:"一、模板（template）简介",slug:"一、模板-template-简介",normalizedTitle:"一、模板（template）简介",charIndex:31},{level:3,title:"二、使用template部署nginx",slug:"二、使用template部署nginx",normalizedTitle:"二、使用template部署nginx",charIndex:51},{level:3,title:"三、playbook中when简单使用",slug:"三、playbook中when简单使用",normalizedTitle:"三、playbook中when简单使用",charIndex:74},{level:3,title:"四、playbook中with_items简单使用",slug:"四、playbook中with-items简单使用",normalizedTitle:"四、playbook中with_items简单使用",charIndex:97},{level:4,title:"4.1 迭代：with_items",slug:"_4-1-迭代-with-items",normalizedTitle:"4.1 迭代：with_items",charIndex:128},{level:4,title:"4.2 迭代嵌套子变量",slug:"_4-2-迭代嵌套子变量",normalizedTitle:"4.2 迭代嵌套子变量",charIndex:151},{level:3,title:"五、template循环示例",slug:"五、template循环示例",normalizedTitle:"五、template循环示例",charIndex:166},{level:4,title:"5.1 第一种写法",slug:"_5-1-第一种写法",normalizedTitle:"5.1 第一种写法",charIndex:186},{level:4,title:"5.2 第二种写法",slug:"_5-2-第二种写法",normalizedTitle:"5.2 第二种写法",charIndex:201},{level:4,title:"5.3 第三种写法",slug:"_5-3-第三种写法",normalizedTitle:"5.3 第三种写法",charIndex:216},{level:3,title:"六、playbook中if简单使用",slug:"六、playbook中if简单使用",normalizedTitle:"六、playbook中if简单使用",charIndex:229}],headersStr:"一、模板（template）简介 二、使用template部署nginx 三、playbook中when简单使用 四、playbook中with_items简单使用 4.1 迭代：with_items 4.2 迭代嵌套子变量 五、template循环示例 5.1 第一种写法 5.2 第二种写法 5.3 第三种写法 六、playbook中if简单使用",content:"# ansible中template简单使用\n\n目录\n\n * 一、模板（template）简介\n * 二、使用template部署nginx\n * 三、playbook中when简单使用\n * 四、playbook中with_items简单使用\n   * 4.1 迭代：with_items\n   * 4.2 迭代嵌套子变量\n * 五、template循环示例\n   * 5.1 第一种写法\n   * 5.2 第二种写法\n   * 5.3 第三种写法\n * 六、playbook中if简单使用\n\n\n# 一、模板（template）简介\n\n * 文件文件，嵌套有脚本（使用模板编程语言编写）；\n * jinja2语言，使用字面量，有以下形式：\n   * 字符串：使用单引号或双引号；\n   * 数字：整数，浮点数；\n   * 列表：[ item1,item2,……]\n   * 元组：(item1,item2,……)\n   * 字典：{key1:value1,key2,value2,……}\n   * 布尔型：true/false\n * 算术运算：+，-，*，/，//，%，**\n * 比较操作：==，!=，>，>=，<，<=\n * 逻辑运算：and，or，not\n * 流表达式：for，if，when\n\n\n# 二、使用template部署nginx\n\n$ ls     //建议，安装nginx的yaml和templates目录在同一目录下\ninstall_nginx.yaml  templates\n$ cat install_nginx.yaml \n---\n- hosts: webservers\n  remote_user: root\n  vars:         #创建变量信息\n    - http_port: 8888\n\n  tasks:\n    - name: install package\n      yum: name=nginx\n    - name: template copy\n      template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf   #由于模板文件在tmplate下，所以src后的路径就可以只写配置文件名称\n      notify: restart service    #定义notify便于修改文件重启服务\n    - name: start service\n      service: name=nginx state=started enabled=yes\n\n  handlers:    #定义重启服务策略\n    - name: restart service\n      service: name=nginx state=restarted\n$ ls templates/     #注意模板的文件名称有一定的要求\nnginx.conf.j2\n$ cat templates/nginx.conf.j2    #该文件就是nginx的配置文件复制而成的\n…………      #省略部分内容\nworker_processes {{ ansible_processor_vcpus**2 }};    #使用变量是cpu核心数的2次方\nlisten       {{ http_port }} default_server;\nlisten       [::]:{{ http_port }} default_server;\n#使用playbook中定义的变量信息\n$ ansible-playbook install_nginx.yaml\n#运行playbook文件\n$ ansible webservers -m shell -a 'rpm -q nginx'\n#nginx确实已经安装成功\n$ ansible webservers -m shell -a 'ss -lnt | grep 8888' \n#端口已经正常开启\n$ ansible webservers -m shell -a 'ps aux | grep nginx'\n#确认工作进程数是预期设定的值\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n至此template批量部署nginx已经实现！\n\n\n# 三、playbook中when简单使用\n\nwhen：可以简单理解为一个条件判断，类似于shell脚本中的if语句！\n\n因为ansible管理的主机可能不是一个系统版本的，那么就需要区别部署了！\n\n$ ansible all -m setup -a 'filter=*distribution*'\n#查看ansible默认支持的变量\n$ cat install_nginx.yaml \n---\n- hosts: all     #针对所有主机\n  remote_user: root\n  vars:\n    - http_port: 8888\n\n  tasks:\n    - name: install package\n      yum: name=nginx\n    - name: template copy for centos7\n      template: src=nginx.conf7.j2 dest=/etc/nginx/nginx.conf\n      when: ansible_distribution_major_version == \"7\"    #当检测到系统版本为7才执行本模块的操作\n      notify: restart service\n    - name: template copy for centos6\n      template: src=nginx.conf6.j2 dest=/etc/nginx/nginx.conf\n      when: ansible_distribution_major_version == \"6\"\n      notify: restart service\n    - name: start service\n      service: name=nginx state=started enabled=yes\n\n  handlers:\n    - name: restart service\n      service: name=nginx state=restarted\n$ ls templates/   #注意一个是nginx6的配置文件，一个nginx7的配置文件\nnginx.conf6.j2  nginx.conf7.j2\n$ ansible-playbook install_nginx.yaml\n#执行playbook文件\n$ ansible all -m shell -a 'ss -lntp | grep nginx'\n#确认centos 6系统的nginx已经启动\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 四、playbook中with_items简单使用\n\n# 4.1 迭代：with_items\n\n迭代：with_items：当有需要重复性执行任务是，可以使用迭代机制！\n\n * 带迭代项的引用，固定变量为“item”；\n * 在task中使用with_items定义需要迭代的元素列表；\n * 列表格式：\n   * 字符串；\n   * 字典；\n\n$ cat test.yaml \n---\n- hosts: all\n  remote_user: root\n\n  tasks:\n    - name: touch some file\n      file: name=/data/{{ item }} state=touch   #文件名定义为列表元素\n      when: ansible_distribution_major_version == \"7\"\n      with_items:      #定义列表元素\n        - file1\n        - file2\n        - file3\n$ ansible-playbook test.yaml\n$ ansible all -m shell -a 'ls -l /data'\n#当满足条件的主机都创建了文件\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 4.2 迭代嵌套子变量\n\n$ cat test1.yaml \n---\n- hosts: all\n  remote_user: root\n\n  tasks:\n    - name: create some group\n      group: name={{ item }}\n      with_items:\n        - g1\n        - g2\n        - g3\n    - name: create some users\n      user: name={{ item.name }} group={{ item.group }}\n      with_items:\n        - { name: 'user1', group: 'g1'}\n        - { name: 'user2', group: 'g2'}\n        - { name: 'user3', group: 'g3'}\n$ ansible-playbook test1.yaml\n$ ansible all -m shell -a 'getent group'\n$ ansible all -m shell -a 'getent passwd'\n$ ansible all -m shell -a 'id user1'\n#进行验证\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 五、template循环示例\n\n# 5.1 第一种写法\n\n$ cat test2.yaml \n---\n- hosts: webservers\n  remote_user: root\n  vars:\n    ports:\n      - 81\n      - 82\n      - 83\n\n  tasks:\n    - name: copy conf\n      template: src=for2.conf.j2 dest=/data/for2.conf\n$ cat templates/for2.conf.j2 \n{% for port in ports %}\nserver {\n    listen {{ port }}\n}\n{% endfor %}\n$ ansible-playbook test2.yaml\n$ ansible webservers -m shell -a 'cat /data/for2.conf'\n#进行验证\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 5.2 第二种写法\n\n$ cat test3.yaml \n---\n- hosts: webservers\n  remote_user: root\n  vars:\n    ports:\n      - listen_port: 81\n      - listen_port: 82\n      - listen_port: 83\n\n  tasks:\n    - name: copy conf\n      template: src=for3.conf.j2 dest=/data/for3.conf\n$ cat templates/for3.conf.j2 \n{% for port in ports %}\nserver {\n    listen {{ port.listen_port }}\n}\n{% endfor %}\n$ ansible-playbook test3.yaml\n$ ansible webservers -m shell -a 'cat /data/for3.conf'\n#进行验证  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 5.3 第三种写法\n\n$ cat test4.yaml \n---\n- hosts: webservers\n  remote_user: root\n  vars:\n    ports:\n      - web1:\n        port: 81\n        name: web1.lzj.com\n        rootdir: /data/web1\n      - web2:\n        port: 82\n        name: web2.lzj.com\n        rootdir: /data/web2\n      - web1:\n        port: 83\n        name: web3.lzj.com\n        rootdir: /data/web3\n\n  tasks:\n    - name: copy conf\n      template: src=for4.conf.j2 dest=/data/for4.conf\n$ cat templates/for4.conf.j2 \n{% for p in ports %}\nserver {\n    listen {{ p.port }}\n    servername {{ p.name }}\n    documentroot {{ p.rootdir }}\n}\n{% endfor %}\n$ ansible-playbook test4.yaml\n$ ansible webservers -m shell -a 'cat /data/for4.conf'\n#进行验证\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# 六、playbook中if简单使用\n\n$ cat test5.yaml \n---\n- hosts: webservers\n  remote_user: root\n  vars:\n    ports:\n      - web1:\n        port: 81\n        rootdir: /data/web1\n      - web2:\n        port: 82\n        name: web2.lzj.com\n        rootdir: /data/web2\n      - web1:\n        port: 83\n        rootdir: /data/web3\n\n  tasks:\n    - name: copy conf\n      template: src=for5.conf.j2 dest=/data/for5.conf\n$ cat templates/for5.conf.j2 \n{% for p in ports %}\nserver {\n    listen {{ p.port }}\n{% if p.name is defined%}    #如果名称被定义了才给名字赋值\n    servername {{ p.name }}\n{% endif %}\n    documentroot {{ p.rootdir }}\n}\n{% endfor %}\n$ ansible-playbook test5.yaml      \n$ ansible webservers -m shell -a 'cat /data/for5.conf'\n#进行验证\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************\n\n原文链接：https://www.cnblogs.com/lvzhenjiang/p/14199384.html",normalizedContent:"# ansible中template简单使用\n\n目录\n\n * 一、模板（template）简介\n * 二、使用template部署nginx\n * 三、playbook中when简单使用\n * 四、playbook中with_items简单使用\n   * 4.1 迭代：with_items\n   * 4.2 迭代嵌套子变量\n * 五、template循环示例\n   * 5.1 第一种写法\n   * 5.2 第二种写法\n   * 5.3 第三种写法\n * 六、playbook中if简单使用\n\n\n# 一、模板（template）简介\n\n * 文件文件，嵌套有脚本（使用模板编程语言编写）；\n * jinja2语言，使用字面量，有以下形式：\n   * 字符串：使用单引号或双引号；\n   * 数字：整数，浮点数；\n   * 列表：[ item1,item2,……]\n   * 元组：(item1,item2,……)\n   * 字典：{key1:value1,key2,value2,……}\n   * 布尔型：true/false\n * 算术运算：+，-，*，/，//，%，**\n * 比较操作：==，!=，>，>=，<，<=\n * 逻辑运算：and，or，not\n * 流表达式：for，if，when\n\n\n# 二、使用template部署nginx\n\n$ ls     //建议，安装nginx的yaml和templates目录在同一目录下\ninstall_nginx.yaml  templates\n$ cat install_nginx.yaml \n---\n- hosts: webservers\n  remote_user: root\n  vars:         #创建变量信息\n    - http_port: 8888\n\n  tasks:\n    - name: install package\n      yum: name=nginx\n    - name: template copy\n      template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf   #由于模板文件在tmplate下，所以src后的路径就可以只写配置文件名称\n      notify: restart service    #定义notify便于修改文件重启服务\n    - name: start service\n      service: name=nginx state=started enabled=yes\n\n  handlers:    #定义重启服务策略\n    - name: restart service\n      service: name=nginx state=restarted\n$ ls templates/     #注意模板的文件名称有一定的要求\nnginx.conf.j2\n$ cat templates/nginx.conf.j2    #该文件就是nginx的配置文件复制而成的\n…………      #省略部分内容\nworker_processes {{ ansible_processor_vcpus**2 }};    #使用变量是cpu核心数的2次方\nlisten       {{ http_port }} default_server;\nlisten       [::]:{{ http_port }} default_server;\n#使用playbook中定义的变量信息\n$ ansible-playbook install_nginx.yaml\n#运行playbook文件\n$ ansible webservers -m shell -a 'rpm -q nginx'\n#nginx确实已经安装成功\n$ ansible webservers -m shell -a 'ss -lnt | grep 8888' \n#端口已经正常开启\n$ ansible webservers -m shell -a 'ps aux | grep nginx'\n#确认工作进程数是预期设定的值\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n至此template批量部署nginx已经实现！\n\n\n# 三、playbook中when简单使用\n\nwhen：可以简单理解为一个条件判断，类似于shell脚本中的if语句！\n\n因为ansible管理的主机可能不是一个系统版本的，那么就需要区别部署了！\n\n$ ansible all -m setup -a 'filter=*distribution*'\n#查看ansible默认支持的变量\n$ cat install_nginx.yaml \n---\n- hosts: all     #针对所有主机\n  remote_user: root\n  vars:\n    - http_port: 8888\n\n  tasks:\n    - name: install package\n      yum: name=nginx\n    - name: template copy for centos7\n      template: src=nginx.conf7.j2 dest=/etc/nginx/nginx.conf\n      when: ansible_distribution_major_version == \"7\"    #当检测到系统版本为7才执行本模块的操作\n      notify: restart service\n    - name: template copy for centos6\n      template: src=nginx.conf6.j2 dest=/etc/nginx/nginx.conf\n      when: ansible_distribution_major_version == \"6\"\n      notify: restart service\n    - name: start service\n      service: name=nginx state=started enabled=yes\n\n  handlers:\n    - name: restart service\n      service: name=nginx state=restarted\n$ ls templates/   #注意一个是nginx6的配置文件，一个nginx7的配置文件\nnginx.conf6.j2  nginx.conf7.j2\n$ ansible-playbook install_nginx.yaml\n#执行playbook文件\n$ ansible all -m shell -a 'ss -lntp | grep nginx'\n#确认centos 6系统的nginx已经启动\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# 四、playbook中with_items简单使用\n\n# 4.1 迭代：with_items\n\n迭代：with_items：当有需要重复性执行任务是，可以使用迭代机制！\n\n * 带迭代项的引用，固定变量为“item”；\n * 在task中使用with_items定义需要迭代的元素列表；\n * 列表格式：\n   * 字符串；\n   * 字典；\n\n$ cat test.yaml \n---\n- hosts: all\n  remote_user: root\n\n  tasks:\n    - name: touch some file\n      file: name=/data/{{ item }} state=touch   #文件名定义为列表元素\n      when: ansible_distribution_major_version == \"7\"\n      with_items:      #定义列表元素\n        - file1\n        - file2\n        - file3\n$ ansible-playbook test.yaml\n$ ansible all -m shell -a 'ls -l /data'\n#当满足条件的主机都创建了文件\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n# 4.2 迭代嵌套子变量\n\n$ cat test1.yaml \n---\n- hosts: all\n  remote_user: root\n\n  tasks:\n    - name: create some group\n      group: name={{ item }}\n      with_items:\n        - g1\n        - g2\n        - g3\n    - name: create some users\n      user: name={{ item.name }} group={{ item.group }}\n      with_items:\n        - { name: 'user1', group: 'g1'}\n        - { name: 'user2', group: 'g2'}\n        - { name: 'user3', group: 'g3'}\n$ ansible-playbook test1.yaml\n$ ansible all -m shell -a 'getent group'\n$ ansible all -m shell -a 'getent passwd'\n$ ansible all -m shell -a 'id user1'\n#进行验证\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 五、template循环示例\n\n# 5.1 第一种写法\n\n$ cat test2.yaml \n---\n- hosts: webservers\n  remote_user: root\n  vars:\n    ports:\n      - 81\n      - 82\n      - 83\n\n  tasks:\n    - name: copy conf\n      template: src=for2.conf.j2 dest=/data/for2.conf\n$ cat templates/for2.conf.j2 \n{% for port in ports %}\nserver {\n    listen {{ port }}\n}\n{% endfor %}\n$ ansible-playbook test2.yaml\n$ ansible webservers -m shell -a 'cat /data/for2.conf'\n#进行验证\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 5.2 第二种写法\n\n$ cat test3.yaml \n---\n- hosts: webservers\n  remote_user: root\n  vars:\n    ports:\n      - listen_port: 81\n      - listen_port: 82\n      - listen_port: 83\n\n  tasks:\n    - name: copy conf\n      template: src=for3.conf.j2 dest=/data/for3.conf\n$ cat templates/for3.conf.j2 \n{% for port in ports %}\nserver {\n    listen {{ port.listen_port }}\n}\n{% endfor %}\n$ ansible-playbook test3.yaml\n$ ansible webservers -m shell -a 'cat /data/for3.conf'\n#进行验证  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 5.3 第三种写法\n\n$ cat test4.yaml \n---\n- hosts: webservers\n  remote_user: root\n  vars:\n    ports:\n      - web1:\n        port: 81\n        name: web1.lzj.com\n        rootdir: /data/web1\n      - web2:\n        port: 82\n        name: web2.lzj.com\n        rootdir: /data/web2\n      - web1:\n        port: 83\n        name: web3.lzj.com\n        rootdir: /data/web3\n\n  tasks:\n    - name: copy conf\n      template: src=for4.conf.j2 dest=/data/for4.conf\n$ cat templates/for4.conf.j2 \n{% for p in ports %}\nserver {\n    listen {{ p.port }}\n    servername {{ p.name }}\n    documentroot {{ p.rootdir }}\n}\n{% endfor %}\n$ ansible-playbook test4.yaml\n$ ansible webservers -m shell -a 'cat /data/for4.conf'\n#进行验证\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# 六、playbook中if简单使用\n\n$ cat test5.yaml \n---\n- hosts: webservers\n  remote_user: root\n  vars:\n    ports:\n      - web1:\n        port: 81\n        rootdir: /data/web1\n      - web2:\n        port: 82\n        name: web2.lzj.com\n        rootdir: /data/web2\n      - web1:\n        port: 83\n        rootdir: /data/web3\n\n  tasks:\n    - name: copy conf\n      template: src=for5.conf.j2 dest=/data/for5.conf\n$ cat templates/for5.conf.j2 \n{% for p in ports %}\nserver {\n    listen {{ p.port }}\n{% if p.name is defined%}    #如果名称被定义了才给名字赋值\n    servername {{ p.name }}\n{% endif %}\n    documentroot {{ p.rootdir }}\n}\n{% endfor %}\n$ ansible-playbook test5.yaml      \n$ ansible webservers -m shell -a 'cat /data/for5.conf'\n#进行验证\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************\n\n原文链接：https://www.cnblogs.com/lvzhenjiang/p/14199384.html",charsets:{cjk:!0}},{title:"playbook详解",frontmatter:{title:"playbook详解",categories:["系列专题","ansible"],tags:["ansible"],date:"2022-12-09T20:53:07.000Z",permalink:"/pages/14eda9/",readingShow:"top",description:"目录",meta:[{name:"twitter:title",content:"playbook详解"},{name:"twitter:description",content:"目录"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/07.Ansible%E4%B8%AD%E7%9A%84playbook%20%E8%AF%A6%E8%A7%A3.html"},{property:"og:type",content:"article"},{property:"og:title",content:"playbook详解"},{property:"og:description",content:"目录"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/07.Ansible%E4%B8%AD%E7%9A%84playbook%20%E8%AF%A6%E8%A7%A3.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:53:07.000Z"},{property:"article:tag",content:"ansible"},{itemprop:"name",content:"playbook详解"},{itemprop:"description",content:"目录"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/07.Ansible%E4%B8%AD%E7%9A%84playbook%20%E8%AF%A6%E8%A7%A3.html",relativePath:"01.专题/01.ansible系列文章/07.Ansible中的playbook 详解.md",key:"v-0d3f68c1",path:"/pages/14eda9/",headers:[{level:3,title:"一、playbook",slug:"一、playbook",normalizedTitle:"一、playbook",charIndex:33},{level:4,title:"1.1 playbook是什么",slug:"_1-1-playbook是什么",normalizedTitle:"1.1 playbook是什么",charIndex:49},{level:4,title:"1.2 playbook的语法结构",slug:"_1-2-playbook的语法结构",normalizedTitle:"1.2 playbook的语法结构",charIndex:70},{level:4,title:"1.3 限定主机范围执行",slug:"_1-3-限定主机范围执行",normalizedTitle:"1.3 限定主机范围执行",charIndex:93},{level:4,title:"1.4 ansible-palybook的小技巧",slug:"_1-4-ansible-palybook的小技巧",normalizedTitle:"1.4 ansible-palybook的小技巧",charIndex:111},{level:4,title:"1.5 ansible-playbook中的handlers",slug:"_1-5-ansible-playbook中的handlers",normalizedTitle:"1.5 ansible-playbook中的handlers",charIndex:141},{level:4,title:"1.6 使用handlers的注意事项",slug:"_1-6-使用handlers的注意事项",normalizedTitle:"1.6 使用handlers的注意事项",charIndex:177},{level:3,title:"二、变量",slug:"二、变量",normalizedTitle:"二、变量",charIndex:200},{level:4,title:"2.1 playbook中的变量",slug:"_2-1-playbook中的变量",normalizedTitle:"2.1 playbook中的变量",charIndex:210},{level:4,title:"2.2 playbook中使用vars代码块定义变量",slug:"_2-2-playbook中使用vars代码块定义变量",normalizedTitle:"2.2 playbook中使用vars代码块定义变量",charIndex:232},{level:4,title:"2.3 使用独立的文件来定义playbook变量",slug:"_2-3-使用独立的文件来定义playbook变量",normalizedTitle:"2.3 使用独立的文件来定义playbook变量",charIndex:264},{level:4,title:"2.4 inventory文件中的变量",slug:"_2-4-inventory文件中的变量",normalizedTitle:"2.4 inventory文件中的变量",charIndex:294},{level:4,title:"2.5 主机变量和组变量",slug:"_2-5-主机变量和组变量",normalizedTitle:"2.5 主机变量和组变量",charIndex:319},{level:4,title:"2.6 巧用主机变量和组变量",slug:"_2-6-巧用主机变量和组变量",normalizedTitle:"2.6 巧用主机变量和组变量",charIndex:337},{level:4,title:"2.7 注册变量",slug:"_2-7-注册变量",normalizedTitle:"2.7 注册变量",charIndex:357},{level:3,title:"三、高阶变量",slug:"三、高阶变量",normalizedTitle:"三、高阶变量",charIndex:369},{level:4,title:"3.1 fasts变量信息",slug:"_3-1-fasts变量信息",normalizedTitle:"3.1 fasts变量信息",charIndex:381},{level:4,title:"3.2 本地facts变量",slug:"_3-2-本地facts变量",normalizedTitle:"3.2 本地facts变量",charIndex:400},{level:3,title:"四、if/when/while流程控制语句",slug:"四、if-when-while流程控制语句",normalizedTitle:"四、if/when/while流程控制语句",charIndex:417},{level:4,title:"4.1 when条件判断",slug:"_4-1-when条件判断",normalizedTitle:"4.1 when条件判断",charIndex:444},{level:3,title:"五、任务间的流程控制",slug:"五、任务间的流程控制",normalizedTitle:"五、任务间的流程控制",charIndex:460},{level:4,title:"5.1 任务委托",slug:"_5-1-任务委托",normalizedTitle:"5.1 任务委托",charIndex:476},{level:4,title:"5.2 任务暂停",slug:"_5-2-任务暂停",normalizedTitle:"5.2 任务暂停",charIndex:490},{level:4,title:"5.3 交互式提示",slug:"_5-3-交互式提示",normalizedTitle:"5.3 交互式提示",charIndex:504},{level:3,title:"六、tags标签",slug:"六、tags标签",normalizedTitle:"六、tags标签",charIndex:517},{level:3,title:"七、Block块",slug:"七、block块",normalizedTitle:"七、block块",charIndex:529}],headersStr:"一、playbook 1.1 playbook是什么 1.2 playbook的语法结构 1.3 限定主机范围执行 1.4 ansible-palybook的小技巧 1.5 ansible-playbook中的handlers 1.6 使用handlers的注意事项 二、变量 2.1 playbook中的变量 2.2 playbook中使用vars代码块定义变量 2.3 使用独立的文件来定义playbook变量 2.4 inventory文件中的变量 2.5 主机变量和组变量 2.6 巧用主机变量和组变量 2.7 注册变量 三、高阶变量 3.1 fasts变量信息 3.2 本地facts变量 四、if/when/while流程控制语句 4.1 when条件判断 五、任务间的流程控制 5.1 任务委托 5.2 任务暂停 5.3 交互式提示 六、tags标签 七、Block块",content:'# Ansible 中的 playbook 详解\n\n目录\n\n * 一、playbook\n   * 1.1 playbook是什么\n   * 1.2 playbook的语法结构\n   * 1.3 限定主机范围执行\n   * 1.4 ansible-palybook的小技巧\n   * 1.5 ansible-playbook中的handlers\n   * 1.6 使用handlers的注意事项\n * 二、变量\n   * 2.1 playbook中的变量\n   * 2.2 playbook中使用vars代码块定义变量\n   * 2.3 使用独立的文件来定义playbook变量\n   * 2.4 inventory文件中的变量\n   * 2.5 主机变量和组变量\n   * 2.6 巧用主机变量和组变量\n   * 2.7 注册变量\n * 三、高阶变量\n   * 3.1 fasts变量信息\n   * 3.2 本地facts变量\n * 四、if/when/while流程控制语句\n   * 4.1 when条件判断\n * 五、任务间的流程控制\n   * 5.1 任务委托\n   * 5.2 任务暂停\n   * 5.3 交互式提示\n * 六、tags标签\n * 七、Block块\n\n\n# 一、playbook\n\n# 1.1 playbook是什么\n\n根本上说playbook和shell脚本没有任何的区别，playbook就像shell一样，也是把一堆的命令组合起来，然后加入对应条件判断等等，在shell脚本中是一条一条的命令，而在playbook中是一个一个的task任务构成，每个task任务可以看做shell中的一条命令；shell脚本一般只是在当前服务器上执行，而playbook则是在不止一个服务器上执行，因此playbook需要在其中指定运行该playbook的服务器名。\n\n# 1.2 playbook的语法结构\n\nplaybook使用yml标记语言，这是一种标记语言，这种标记语言在文件的最开始需要使用三个“-”来说明文件开始，然后使用缩进来说明代码块的范围。下面通过一个简易的实例，来说明playbook的语法。\n\n---      # 标记文件的开始\n- hosts: webservers   # 指定该playbook在哪个服务器上执行\n  vars:         # 表示下面是定义的变量，\n    http_port: 80    # 变量的形式，key: value，这里http_port是变量名，80是值\n    max_clients: 200\n  remote_user: root  # 指定远程的用户名，这里缩进和vars保持了一致，说明变量的代码块已经结束。\n  tasks:  # 下面构成playbook的tasks，每个task都有 - name: 开始，name指定该任务的名称。\n  - name: ensure apache is at the latest version  # 指定该任务的名称。\n    yum: pkg=httpd state=latest  # yum说明要是用的模板名称，后面指定对应的参数，这两行结合起来就相当于一个shell命令。\n\n  - name: write the apache config file      # 每个task之间可以使用空行来做区分。\n    template: src=/srv/httpd.j2 dest=/etc/httpd.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n需要说明的是缩进的意义和python中缩进的意义是一样，是来区分代码块的。\n\n一个简单的实例：检查MySQL的运行状态\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no   # 不收集对应主机的信息，这样运行会快点。\n   tasks:\n     - name: check the mysql stauts\n       service: name=mysqld state=running\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n运行playbook：\n\n$ ansible-playbook test.yml\n\nPLAY [all] ********************************************************************\n\nTASK: [check the mysql stauts] ************************************************\nok: [10.0.102.200]\nok: [10.0.102.162]\nok: [10.0.102.212]\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=1    changed=0    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=0    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=0    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 1.3 限定主机范围执行\n\n虽然playbook中定义了执行的主机，但是有时候我们可能仅想在定义的主机中的部分机器上执行，这时候怎么办？修改playbook中的hosts的范围，但是每次改变主机就修改一次，比较麻烦，我们可以使用--limit参数，指定该playbook在指定的主机上执行。有以下inventory文件，我们想在dbservers上执行上面测试用的playbook内容。\n\n[all]\n10.0.102.212\n10.0.102.200\n10.0.102.162\n\n[dbservers]\n10.0.102.162\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n上面测试的playbook中hosts定义all，我们想仅在dbservers上执行。\n\n$ ansible-playbook test.yml --limit dbservers\n\nPLAY [all] ********************************************************************\n\nTASK: [check the mysql stauts] ************************************************\nok: [10.0.102.162]\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=1    changed=0    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n查看当前playbook在哪些主机上执行\n\n$ ansible-playbook test.yml --list-hosts\n\nplaybook: test.yml\n\n  play              # 1 (all): host count=3\n    10.0.102.162\n    10.0.102.212\n    10.0.102.200\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 1.4 ansible-palybook的小技巧\n\n * --inventory=path，指定inventory文件，默认是在/etc/ansible/hosts下面。\n * --verbose，显示详细的输出，使用-vvvv显示精确到每分钟的输出。\n * --extra-vars=vars：定义在playbook使用的变量。\n * --forks：指定并发的线程数，默认是5.\n * --connection=type:指定远程连接主机的方式，默认是ssh，设置为local时，则只在本地执行playbook、\n * --check:检测模式，playbook中定义的所有任务将在每台主机上检测，但是并不执行。\n\n# 1.5 ansible-playbook中的handlers\n\n在系统中，我们修改了服务器的配置文件，这时候就需要重启操作服务，就可以使用到handlers。\n\nhandlers:             # 定义两个handlers\n    - name: restart memcached\n      service:  name=memcached state=restarted\n    - name: restart apache\n      service: name=apache state=restarted\n\n- name: template configuration file\n  template: src=template.j2 dest=/etc/foo.conf  # 修改了配置文件然后依次启动memcached和apache服务。\n  notify:        # 使用notify来声明引用handlers。\n     - restart memcached\n     - restart apache\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# 1.6 使用handlers的注意事项\n\n * Handlers只有在其所在的任务被执行时，才会被运行；如果一个任务中定义了notify调用Handlers，但是由于条件判断等原因，该任务未被执行，那么Handlers同样不会被执行。\n * Handlers只会在每一个play的末尾运行一次；如果想在一个playbook中间运行Handlers，则需要使用meta模块来实现。例如：-meta: flush_handlers.\n * 如果一个play在运行到调用Handlers的语句之前失败了，那么这个Handlers将不会被执行。我们可以使用meta模块的--force-handlers选项来强制执行Handlers，即使Handlers所在的play中途运行失败也能执行。\n\n\n# 二、变量\n\n这个变量我们来说明ansible中变量（不包含role中的变量）用法。\n\n# 2.1 playbook中的变量\n\n在运行playbook的时候使用--extra-vars来指定变量\n\n有如下playbook脚本：\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   tasks:\n     - name: test playbook variables\n       command: echo {{ test_var }}   # 打印出变量test_var的值。\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n运行playbook：\n\n$ ansible-playbook test.yml --extra-vars "test_var=test" -v    \n# 加上-v选项，会显示详细的信息\n\nPLAY [all] ********************************************************************\n\nTASK: [test playbook variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "test"], "delta": "0:00:00.006045", "end": "2019-02-15 23:04:49.789452", "rc": 0, "start": "2019-02-15 23:04:49.783407", "stderr": "", "stdout": "test"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "test"], "delta": "0:00:00.005318", "end": "2019-02-15 23:04:52.976471", "rc": 0, "start": "2019-02-15 23:04:52.971153", "stderr": "", "stdout": "test"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "test"], "delta": "0:00:00.005082", "end": "2019-02-15 23:04:52.424959", "rc": 0, "start": "2019-02-15 23:04:52.419877", "stderr": "", "stdout": "test"}\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n上面详细信息的标准输出为test，说明变量的值已经传递了。\n\n# 2.2 playbook中使用vars代码块定义变量\n\n$ cat test.yml\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   vars:                  # 在这里使用了vars代码块来定义变量\n       test_var: Hello World\n   tasks:\n     - name: test playbook variables\n       command: echo {{ test_var }}\n\n$ ansible-playbook test.yml  -v\n\nPLAY [all] ********************************************************************\n\nTASK: [test playbook variables] ***********************************************\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "Hello", "World"], "delta": "0:00:00.004940", "end": "2019-02-15 23:20:06.541672", "rc": 0, "start": "2019-02-15 23:20:06.536732", "stderr": "", "stdout": "Hello World"}\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "Hello", "World"], "delta": "0:00:00.004843", "end": "2019-02-15 23:20:03.957950", "rc": 0, "start": "2019-02-15 23:20:03.953107", "stderr": "", "stdout": "Hello World"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "Hello", "World"], "delta": "0:00:00.004219", "end": "2019-02-15 23:20:07.166900", "rc": 0, "start": "2019-02-15 23:20:07.162681", "stderr": "", "stdout": "Hello World"}\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# 2.3 使用独立的文件来定义playbook变量\n\nplaybook的内容：\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   vars_files:       # 这里使用了vars_files来引入变量文件\n     - vars.yml\n   tasks:\n     - name: test playbook variables\n       command: echo {{ test_var }}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n变量文件的定义：\n\n$ cat vars.yml\n---\n  test_var: Hello World\n\n\n1\n2\n3\n\n\n执行的结果：\n\n$ ansible-playbook test.yml  -v\n\nPLAY [all] ********************************************************************\n\nTASK: [test playbook variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "Hello", "World"], "delta": "0:00:00.005198", "end": "2019-02-15 23:23:16.397557", "rc": 0, "start": "2019-02-15 23:23:16.392359", "stderr": "", "stdout": "Hello World"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "Hello", "World"], "delta": "0:00:00.004359", "end": "2019-02-15 23:23:19.629804", "rc": 0, "start": "2019-02-15 23:23:19.625445", "stderr": "", "stdout": "Hello World"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "Hello", "World"], "delta": "0:00:01.006185", "end": "2019-02-15 23:23:20.039320", "rc": 0, "start": "2019-02-15 23:23:19.033135", "stderr": "", "stdout": "Hello World"}\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 2.4 inventory文件中的变量\n\n在ansible中，inventory文件通常是指ansible的主机和组定义文件hosts。在hosts文件中，变量会被定义在主机名后面或组名的下方。\n\n为特定的主机定义变量，变量名跟在对应主机的后边。\n\ninventory文件如下：\n\n$ cat /etc/ansible/hosts\n[all]\n10.0.102.212 test_var=212\n10.0.102.200 test_var=200\n10.0.102.162 test_var=162\n# 为三个主机定义了同名的变量，但是变量值却不一样。\n\n\n1\n2\n3\n4\n5\n6\n\n\nplaybook的内容如下：\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   tasks:\n     - name: test playbook variables\n       command: echo {{ test_var }}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n执行这个playbook，结果如下（对应的主机显示了各自对应的变量值）：\n\n$ ansible-playbook test.yml -v\n\nPLAY [all] ********************************************************************\n\nTASK: [test playbook variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "212"], "delta": "0:00:00.004399", "end": "2019-02-15 23:31:20.648111", "rc": 0, "start": "2019-02-15 23:31:20.643712", "stderr": "", "stdout": "212"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "200"], "delta": "0:00:00.005932", "end": "2019-02-15 23:31:23.873082", "rc": 0, "start": "2019-02-15 23:31:23.867150", "stderr": "", "stdout": "200"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "162"], "delta": "0:00:00.006723", "end": "2019-02-15 23:31:23.287861", "rc": 0, "start": "2019-02-15 23:31:23.281138", "stderr": "", "stdout": "162"}\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n给主机组定义变量，作用范围为整个主机组。\n\n$ cat /etc/ansible/hosts\n[all]\n10.0.102.212\n10.0.102.200\n10.0.102.162\n\n\n[all:vars]                            #给主机组定义变量\ntest_var=Hello World\n\n$ ansible-playbook test.yml -v\n\nPLAY [all] ********************************************************************\n\nTASK: [test playbook variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "Hello", "World"], "delta": "0:00:00.003923", "end": "2019-02-15 23:37:29.322158", "rc": 0, "start": "2019-02-15 23:37:29.318235", "stderr": "", "stdout": "Hello World"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "Hello", "World"], "delta": "0:00:00.004161", "end": "2019-02-15 23:37:32.548947", "rc": 0, "start": "2019-02-15 23:37:32.544786", "stderr": "", "stdout": "Hello World"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "Hello", "World"], "delta": "0:00:00.006090", "end": "2019-02-15 23:37:32.005067", "rc": 0, "start": "2019-02-15 23:37:31.998977", "stderr": "", "stdout": "Hello World"}\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n回想一下，这种方法定义变量虽然简单直观，但是若是变量特别多的情况下，会怎么样？特别是给对应的主机定义变量，若是变量太多，则管理起来会很不方便的，因此引入了主机变量和组变量。\n\n# 2.5 主机变量和组变量\n\ninventory文件仍然使用上面的文件！\n\n在执行ansbile命令时，ansible默认会从/etc/ansible/host_vars/和/etc/amsible/group_vars/两个目录下读取变量定义，如果/etc/ansible下面没有这两个目录，可以直接手动创建，并且可以在这两个目录中创建与hosts（这里是指inventory文件）文件中主机名或组名同名的文件来定义变量。\n\n先来看主机变量\n\n$ tree /etc/ansible/\n.\n├── group_vars\n├── hosts\n└── host_vars                 # 定义与主机名同名的文件\n    ├── 10.0.102.162\n    ├── 10.0.102.200\n    └── 10.0.102.212\n\n2 directories, 4 files\n\n# 文件中的内容如下：\n$ cat host_vars/10.0.102.162\n---\ntest_var: 162\n$ cat host_vars/10.0.102.200\n---\n  test_var: 200\n$ cat host_vars/10.0.102.212\n---\n  test_var: 212\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nplaybook的内容如下，执行结果如下：\n\n$ cat test.yml\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   tasks:\n     - name: test playbook variables\n       command: echo {{ test_var }}\n\n$ ansible-playbook test.yml -v\n\nPLAY [all] ********************************************************************\n\nTASK: [test playbook variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "212"], "delta": "0:00:00.003767", "end": "2019-02-15 23:55:58.595282", "rc": 0, "start": "2019-02-15 23:55:58.591515", "stderr": "", "stdout": "212"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "162"], "delta": "0:00:00.006254", "end": "2019-02-15 23:56:01.235307", "rc": 0, "start": "2019-02-15 23:56:01.229053", "stderr": "", "stdout": "162"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "200"], "delta": "0:00:01.004509", "end": "2019-02-15 23:56:02.775410", "rc": 0, "start": "2019-02-15 23:56:01.770901", "stderr": "", "stdout": "200"}\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n再来说明一下主机组变量！\n\n创建与组名同名的文件：\n\n$ tree\n.\n├── group_vars\n│   └── all               #创建与组名同名的文件\n├── hosts\n└── host_vars\n    ├── 10.0.102.162\n    ├── 10.0.102.200\n    └── 10.0.102.212\n\n2 directories, 5 files\n\n$ cat group_vars/all\n---\n  test_group_var: from group\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n执行结果如下：\n\n$ cat test.yml\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   tasks:\n     - name: test the host variables\n       command: echo {{ test_var }}\n\n     - name: test host group variables           #写入测试组变量的task\n       command: echo {{ test_group_var }}\n$ ansible-playbook test.yml -v\n\nPLAY [all] ********************************************************************\n\nTASK: [test the host variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "212"], "delta": "0:00:00.004613", "end": "2019-02-15 23:59:23.227722", "rc": 0, "start": "2019-02-15 23:59:23.223109", "stderr": "", "stdout": "212"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "200"], "delta": "0:00:00.006490", "end": "2019-02-15 23:59:26.422682", "rc": 0, "start": "2019-02-15 23:59:26.416192", "stderr": "", "stdout": "200"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "162"], "delta": "0:00:00.004709", "end": "2019-02-15 23:59:25.812786", "rc": 0, "start": "2019-02-15 23:59:25.808077", "stderr": "", "stdout": "162"}\n\nTASK: [test host group variables] *********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "from", "group"], "delta": "0:00:00.003759", "end": "2019-02-15 23:59:23.519180", "rc": 0, "start": "2019-02-15 23:59:23.515421", "stderr": "", "stdout": "from group"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "from", "group"], "delta": "0:00:00.003748", "end": "2019-02-15 23:59:26.109337", "rc": 0, "start": "2019-02-15 23:59:26.105589", "stderr": "", "stdout": "from group"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "from", "group"], "delta": "0:00:00.004339", "end": "2019-02-15 23:59:26.724525", "rc": 0, "start": "2019-02-15 23:59:26.720186", "stderr": "", "stdout": "from group"}\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=2    changed=2    unreachable=0    failed=0\n10.0.102.200               : ok=2    changed=2    unreachable=0    failed=0\n10.0.102.212               : ok=2    changed=2    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 2.6 巧用主机变量和组变量\n\n有时候在执行ansbile任务时，可能需要从一台远程主机上获取另一台远程主机的变量信息，这时候可以使用hostvars变量，这个变量包含了指定主机上所定义的所有变量。\n\n譬如，若是想获取host1上变量admin_user的内容，在任意主机上直接上使用下面代码即可。\n\n{{  hostvars[\'host1\'][\'admin_user\']}}\n\n\n1\n\n\nansible提供了一些非常有用的内置变量，几个常用的如下：\n\n * grorps：包含了所有hosts文件里的主机组的一个列表。\n * group_names: 包含了当前主机所在的所有主机组名的一个列表。\n * inventory_hostname: 通过hosts文件定义的主机名。（与ansible_home意义不同）\n * inventory_hostname_short：变量inventory_hostname的第一部分。譬如inventory_hostname的值为books.ansible.com，那么inventory_hostname_short的值就是books。\n * play_hosts: 将执行当前任务的所有主机\n\n# 2.7 注册变量\n\n注册变量，其实就是将操作结果，包括标准输出和标准错误输出，保存到变量中，然后再根据这个变量的内容来决定下一步的操作，在这个过程中用来保存操作结果的变量就叫注册变量。\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   tasks:\n     - name: test the register variables\n       shell: uptime\n       register: results     # 使用关键字register声明注册变量，上面uptime命令产生的结果，存入到results中。结果是字典形式。\n\n     - name: print the register result\n       debug: msg="{{ results.stdout }}"   # 使用debug模块，打印出上面命令的输出结果。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n上面的playbook执行结果如下：\n\n$ ansible-playbook test.yml\n\nPLAY [all] ********************************************************************\n\nTASK: [test the register variables] *******************************************\nchanged: [10.0.102.212]\nchanged: [10.0.102.200]\nchanged: [10.0.102.162]\n\nTASK: [print the register result] *********************************************\nok: [10.0.102.212] => {\n    "msg": " 00:18:01 up 3 days,  2:56,  3 users,  load average: 0.02, 0.03, 0.05"          #msg的结果就是注册变量的标准输出\n}\nok: [10.0.102.200] => {\n    "msg": " 00:18:04 up 4 days,  7:45,  3 users,  load average: 0.03, 0.06, 0.05"\n}\nok: [10.0.102.162] => {\n    "msg": " 00:18:04 up 4 days,  7:45,  3 users,  load average: 0.01, 0.02, 0.05"\n}\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=2    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=2    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=2    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n一个注册变量通常会有以下4个属性：\n\n * changed：任务是否对远程主机造成的变更。\n * delta：任务运行所用的时间。\n * stdout：正常的输出信息。\n * stderr：错误信息。\n\n\n# 三、高阶变量\n\n对于普通变量，在ansible命令行设定的，在hosts文件中定义的，或者在playbook中定义的等，这些都是普通变量，在引用时，可以使用使用{{ variable }}的形式。ansible是用python语言写的，因此也支持一种叫做列表的变量，形式如下：\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   vars:\n      var_list:     # 注意形式，定义了var_list列表，取值方法和列表取值一样，不推荐使用jinja2的方法取值。\n          - one\n          - two\n          - three\n   tasks:\n     - name: test the list variables\n       shell: echo {{ var_list[0] }}   # 取列表中的第一个字，也就是one\n       register: results\n\n     - name: print the register result\n       debug: msg="{{ results.stdout }}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n执行结果如下：\n\n$ ansible-playbook test.yml\n\nPLAY [all] ********************************************************************\n\nTASK: [test the list variables] ***********************************************\nchanged: [10.0.102.212]\nchanged: [10.0.102.200]\nchanged: [10.0.102.162]\n\nTASK: [print the register result] *********************************************\nok: [10.0.102.212] => {\n    "msg": "one"\n}\nok: [10.0.102.200] => {\n    "msg": "one"\n}\nok: [10.0.102.162] => {\n    "msg": "one"\n}\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=2    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=2    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=2    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# 3.1 fasts变量信息\n\n在上面的测试中，我们的playbook都执行了一条命令叫gater_facts:no，加入了这条命令后，playbook脚本的执行速度会快很多，这是因为默认情况下，ansible是会手机远程服务器的主机信息，这些信息包含了服务器的一些基本设置。\n\nGATHERING FACTS ***************************************************************\nok: [10.0.102.200]\nok: [10.0.102.212]\nok: [10.0.102.162]\n\n\n1\n2\n3\n4\n\n\n收集的主机信息可以使用setup模块查看，一个主机的收集信息如下：\n\nansible 10.0.102.162 -m setup\n10.0.102.162 | success >> {\n    "ansible_facts": {\n        "ansible_all_ipv4_addresses": [\n            "10.0.102.162"\n        ],\n        "ansible_all_ipv6_addresses": [\n            "fe80::1392:ecd3:5adf:c3ae"\n        ],\n        "ansible_architecture": "x86_64",\n        "ansible_bios_date": "04/01/2014",\n        "ansible_bios_version": "1.9.1-5.el7.centos",\n        "ansible_cmdline": {\n            "BOOT_IMAGE": "/vmlinuz-3.10.0-514.el7.x86_64",\n            "LANG": "en_US.UTF-8",\n            "crashkernel": "auto",\n            "quiet": true,\n            "rd.lvm.lv": "cl/swap",\n            "rhgb": true,\n            "ro": true,\n            "root": "/dev/mapper/cl-root"\n        },\n        "ansible_date_time": {\n            "date": "2019-02-16",\n            "day": "16",\n            "epoch": "1550248590",\n            "hour": "00",\n            "iso8601": "2019-02-15T16:36:30Z",\n            "iso8601_micro": "2019-02-15T16:36:30.311222Z",\n            "minute": "36",\n            "month": "02",\n            "second": "30",\n            "time": "00:36:30",\n            "tz": "CST",\n            "tz_offset": "+0800",\n            "weekday": "Saturday",\n            "year": "2019"\n        },\n        "ansible_default_ipv4": {\n            "address": "10.0.102.162",\n            "alias": "eth0",\n            "gateway": "10.0.100.1",\n            "interface": "eth0",\n            "macaddress": "fa:0a:e3:54:a6:00",\n            "mtu": 1500,\n            "netmask": "255.255.252.0",\n            "network": "10.0.100.0",\n            "type": "ether"\n        },\n        "ansible_default_ipv6": {},\n        "ansible_devices": {\n            "sr0": {\n                "holders": [],\n                "host": "IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]",\n                "model": "QEMU DVD-ROM",\n                "partitions": {},\n                "removable": "1",\n                "rotational": "1",\n                "scheduler_mode": "cfq",\n                "sectors": "2097151",\n                "sectorsize": "512",\n                "size": "1024.00 MB",\n                "support_discard": "0",\n                "vendor": "QEMU"\n            },\n            "vda": {\n                "holders": [],\n                "host": "SCSI storage controller: Red Hat, Inc Virtio block device",\n                "model": null,\n                "partitions": {\n                    "vda1": {\n                        "sectors": "2097152",\n                        "sectorsize": 512,\n                        "size": "1.00 GB",\n                        "start": "2048"\n                    },\n                    "vda2": {\n                        "sectors": "81786880",\n                        "sectorsize": 512,\n                        "size": "39.00 GB",\n                        "start": "2099200"\n                    }\n                },\n                "removable": "0",\n                "rotational": "1",\n                "scheduler_mode": "",\n                "sectors": "83886080",\n                "sectorsize": "512",\n                "size": "40.00 GB",\n                "support_discard": "0",\n                "vendor": "0x1af4"\n            }\n        },\n        "ansible_distribution": "CentOS",\n        "ansible_distribution_major_version": "7",\n        "ansible_distribution_release": "Core",\n        "ansible_distribution_version": "7.3.1611",\n        "ansible_domain": "",\n        "ansible_env": {\n            "HOME": "/root",\n            "LANG": "en_US.UTF-8",\n            "LC_CTYPE": "en_US.UTF-8",\n            "LESSOPEN": "||/usr/bin/lesspipe.sh %s",\n            "LOGNAME": "root",\n            "LS_COLORS": "rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:",\n            "MAIL": "/var/mail/root",\n            "PATH": "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin",\n            "PWD": "/root",\n            "SELINUX_LEVEL_REQUESTED": "",\n            "SELINUX_ROLE_REQUESTED": "",\n            "SELINUX_USE_CURRENT_RANGE": "",\n            "SHELL": "/bin/bash",\n            "SHLVL": "2",\n            "SSH_CLIENT": "10.0.102.204 4242 22",\n            "SSH_CONNECTION": "10.0.102.204 4242 10.0.102.162 22",\n            "SSH_TTY": "/dev/pts/1",\n            "TERM": "xterm",\n            "USER": "root",\n            "XDG_RUNTIME_DIR": "/run/user/0",\n            "XDG_SESSION_ID": "168",\n            "_": "/usr/bin/python"\n        },\n        "ansible_eth0": {\n            "active": true,\n            "device": "eth0",\n            "ipv4": {\n                "address": "10.0.102.162",\n                "netmask": "255.255.252.0",\n                "network": "10.0.100.0"\n            },\n            "ipv6": [\n                {\n                    "address": "fe80::1392:ecd3:5adf:c3ae",\n                    "prefix": "64",\n                    "scope": "link"\n                }\n            ],\n            "macaddress": "fa:0a:e3:54:a6:00",\n            "module": "virtio_net",\n            "mtu": 1500,\n            "promisc": false,\n            "type": "ether"\n        },\n        "ansible_form_factor": "Other",\n        "ansible_fqdn": "docker4",\n        "ansible_hostname": "docker4",\n        "ansible_interfaces": [\n            "lo",\n            "eth0"\n        ],\n        "ansible_kernel": "3.10.0-514.el7.x86_64",\n        "ansible_lo": {\n            "active": true,\n            "device": "lo",\n            "ipv4": {\n                "address": "127.0.0.1",\n                "netmask": "255.0.0.0",\n                "network": "127.0.0.0"\n            },\n            "ipv6": [\n                {\n                    "address": "::1",\n                    "prefix": "128",\n                    "scope": "host"\n                }\n            ],\n            "mtu": 65536,\n            "promisc": false,\n            "type": "loopback"\n        },\n        "ansible_machine": "x86_64",\n        "ansible_memfree_mb": 881,\n        "ansible_memtotal_mb": 1839,\n        "ansible_mounts": [\n            {\n                "device": "/dev/mapper/cl-root",\n                "fstype": "xfs",\n                "mount": "/",\n                "options": "rw,seclabel,relatime,attr2,inode64,noquota",\n                "size_available": 34615087104,\n                "size_total": 39700664320\n            },\n            {\n                "device": "/dev/vda1",\n                "fstype": "xfs",\n                "mount": "/boot",\n                "options": "rw,seclabel,relatime,attr2,inode64,noquota",\n                "size_available": 918556672,\n                "size_total": 1063256064\n            }\n        ],\n        "ansible_nodename": "docker4",\n        "ansible_os_family": "RedHat",\n        "ansible_pkg_mgr": "yum",\n        "ansible_processor": [\n            "QEMU Virtual CPU version 2.5+",\n            "QEMU Virtual CPU version 2.5+"\n        ],\n        "ansible_processor_cores": 2,\n        "ansible_processor_count": 1,\n        "ansible_processor_threads_per_core": 1,\n        "ansible_processor_vcpus": 2,\n        "ansible_product_name": "KVM",\n        "ansible_product_serial": "NA",\n        "ansible_product_uuid": "E5E1D5E6-1A4D-4E0D-98C3-B8AD422B10CC",\n        "ansible_product_version": "RHEL 7.3.0 PC (i440FX + PIIX, 1996)",\n        "ansible_python_version": "2.7.5",\n        "ansible_selinux": {\n            "config_mode": "enforcing",\n            "mode": "enforcing",\n            "policyvers": 28,\n            "status": "enabled",\n            "type": "targeted"\n        },\n        "ansible_ssh_host_key_ecdsa_public": "AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEp5iF/lAqB9Q9FNKfnsi3mLJSVvvooVhRRcuGTBHEJs+TaM36oBaIr764IX1zdn2sWFLdYgmcuaAeiPu3fK+UU=",\n        "ansible_ssh_host_key_rsa_public": "AAAAB3NzaC1yc2EAAAADAQABAAABAQC6yHI2+V64EMW3jDISBrzKmWurP7uF4IqemJgowpqC3mVlFsPOSqerDoJN9hE34fViXcbLUj9wIi0kc3QzxxNwTefwJCdPSL17ns9eIEDKJqrHswts7OXYC1948bdyhyGnaW57BEfVUJ+Vt8OI1JSKkKsi3aCumaZDz9tNGCVYiqW4PMUQFaT/yEnPqKhSp8mDX/SL/unpVsctB0w37o38ZVApKPaNkHW25uiwroStLGqY4VgoZHTqHUdvqk4EZQOD0+JmBcYKVj2ABBl1sMiH8mmrc2W2Gi0gJx31Ky/t5SWQtXTdMRB3D7N9yRd1pPcnh0zebS/OPnX4G5UWX/aP",\n        "ansible_swapfree_mb": 0,\n        "ansible_swaptotal_mb": 0,\n        "ansible_system": "Linux",\n        "ansible_system_vendor": "Red Hat",\n        "ansible_user_id": "root",\n        "ansible_userspace_architecture": "x86_64",\n        "ansible_userspace_bits": "64",\n        "ansible_virtualization_role": "guest",\n        "ansible_virtualization_type": "kvm",\n        "module_setup": true\n    },\n    "changed": false\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n\n\n在实际应用中，运用的比较多的facts变量有ansible_os_family，ansible_hostname等，这些变量通常会被拿来作为when条件语句的判断条件，来决定下一步的操作。一个简单的实例：\n\n---\n - hosts: all\n   remote_user: root\n   tasks:\n     - name: test the list variables\n       shell: echo {{ ansible_os_family }}\n       register: results\n\n     - name: print the register result\n       debug: msg="{{ results.stdout }}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n执行结果如下：\n\nansible-playbook test.yml\n\nPLAY [all] ********************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [10.0.102.162]\nok: [10.0.102.212]\nok: [10.0.102.200]\n\nTASK: [test the list variables] ***********************************************\nchanged: [10.0.102.162]\nchanged: [10.0.102.212]\nchanged: [10.0.102.200]\n\nTASK: [print the register result] *********************************************\nok: [10.0.102.212] => {\n    "msg": "RedHat"                   #对应变量的结果\n}\nok: [10.0.102.200] => {\n    "msg": "RedHat"\n}\nok: [10.0.102.162] => {\n    "msg": "RedHat"\n}\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=3    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=3    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=3    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 3.2 本地facts变量\n\n我们可以自己定义facts变量，把这个变量写入一个以.fact结尾的文件中，这个文件可以是json文件或ini文件，或者是一个可以返回json代码的可执行文件。然后将其放在远程主机的/etc/ansible/facts.d文件夹中，ansible在执行的任务时会自动到这个文件夹中读取变量的信息。\n\n在远程主机上做如下操作：\n\n# 自定义fact信息\n$ mkdir -p /etc/ansible/facts.d && cd /etc/ansible/facts.d\n$ cat test.fact\n[test_fact]\nadmin=hongkong\n\n\n1\n2\n3\n4\n5\n\n\n然后再ansible主机上获取自定义的信息。\n\n$ ansible 10.0.102.162 -m setup -a "filter=ansible_local"\n10.0.102.162 | success >> {\n    "ansible_facts": {\n        "ansible_local": {\n            "test": {\n                "test_fact": {\n                    "admin": "hongkong"\n                }\n            }\n        }\n    },\n    "changed": false\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 四、if/when/while流程控制语句\n\n条件判断在ansible任务中的使用频率非常高。我们可以根据一些条件的不一样执行不同的task。\n\n# 4.1 when条件判断\n\n很多任务只有在特定条件下才能执行，这就是when语句发挥作用的地方。\n\n一个简单的实例，关闭掉ip地址为10.0.102.162服务器上的mysql服务，如下：\n\n[root@test2 playbook]# cat test.yml\n---\n - hosts: all\n   remote_user: root\n   tasks:\n     - name: shut down the db server\n       service: name=mysqld state=stopped\n       when: ansible_eth0.ipv4.address  == "10.0.102.162"  # 这里使用了when条件语句\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n执行的结果如下：\n\n$ ansible-playbook test.yml\n\nPLAY [all] ********************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [10.0.102.212]\nok: [10.0.102.200]\nok: [10.0.102.162]\n\nTASK: [shut down the db server] ***********************************************\nskipping: [10.0.102.200]\nskipping: [10.0.102.212]\nchanged: [10.0.102.162]                      #162的服务状态已经改变\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=2    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=0    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=0    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n这个就是when条件语句的用法很简单。需要注意when语句的作用于paly的作用时间，当when的条件满足时，然后才会执行play中的任务。ansible还提供了另外两个与when相关的语句changed_when和failed_when条件判断。\n\n\n# 五、任务间的流程控制\n\n# 5.1 任务委托\n\n默认情况下，ansible所有任务都是在我们指定的机器上面运行的，当在一个独立的集群环境配置时，这并没有什么问题。而在有些情况下，比如给某台服务器发送通知或者向监控服务器中添加被监控的主机，这个时候任务就需要在特定的主机上运行，而非一开始指定的所有主机，此时就需要ansible的委托任务。\n\n使用delegate_to关键字可以配置任务在指定的服务器上执行，而其他任务还是在hosts关键字配置的所有机器上执行，当到了这个关键字所在的任务时，就使用委托的机器运行。\n\n查看MySQL是否在运行状态，因此在检查之前首先关掉162上的mysql服务。（为了方便查看状态）\n\n---\n - hosts: all\n   remote_user: root\n   tasks:\n     - name: stop the db server\n       service: name=mysqld state=stopped\n       delegate_to: 10.0.102.162       # 这里使用了委托，仅关闭162这台服务器上，这个play仅在162这台服务器上执行。\n\n     - name: check mysql status\n       service: name=mysqld state=running\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n这里委托是在指定的机器上执行，若是想在本地服务器上执行，可以把ip地址换为127.0.0.1即可。也可以使用local_action方法。\n\n---\n - hosts: all\n   remote_user: root\n   tasks:\n     - name: create the test file\n       local_action: shell touch test1111 # 在本地创建一个测试文件\n\n\n     - name: check mysql status\n       service: name=mysqld state=running\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n结果如下：\n\n$ ansible-playbook test.yml\n\nPLAY [all] ********************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [10.0.102.212]\nok: [10.0.102.200]\nok: [10.0.102.162]\n\nTASK: [create the test file] **************************************************\nchanged: [10.0.102.212 -> 127.0.0.1]\nchanged: [10.0.102.200 -> 127.0.0.1]\nchanged: [10.0.102.162 -> 127.0.0.1]\n\nTASK: [check mysql status] ****************************************************\nok: [10.0.102.200]\nok: [10.0.102.212]\nok: [10.0.102.162]\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=3    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=3    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=3    changed=1    unreachable=0    failed=0\n\n$ ls                     # 默认会在当前目录创建对应的文件\ntest1111  test.yml  vars.yml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n# 5.2 任务暂停\n\n有些情况下，一些任务的运行需要等待一些状态的恢复，比如某一台主机或者应用刚刚重启，我们需要等待它上面的某个端口开启，此时我们就不得不将正在运行的任务暂停，直到其状态满足我们的需求。\n\n下一个实例：\n\n- name: wait for webserver to start\n   local_action:\n        module: wait_for\n        host: webserver1\n        port: 80\n        delay: 10\n        timeout: 300\n        state: startted\n# 这个实例中，这个任务将会每10s检查一次主机webserver1上面的80端口是否开启，如果超过了300s则失败\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 5.3 交互式提示\n\n在少数情况下，ansible任务运行的过程中需要用户输入一些数据，这些数据要么比较秘密不方便，或者数据是动态的，不同的用户有不同的需求，比如输入用户自己的账户和密码或者输入不同的版本号会触发不同的后续操作等。ansible的vars_prompt关键字就是用来处理上述这种与用户交互的情况的。下面是一个简单的实例。\n\n---\n - hosts: all\n   remote_user: root\n   vars_prompt:\n      - name: share_user\n        prompt: "what is your network username?"\n        private: no\n\n      - name: share_pass\n        prompt: "what is your network password"\n        private: no\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n然后执行上面的playbook，因为我们只是测试，只需要在一台机器上执行，因此加入了--limit参数。\n\n$ ansible-playbook test.yml --limit 10.0.102.162\nwhat is your network username?: test        # 需要手动交互输入\nwhat is your network password: 123456       # 手动输入\n\nPLAY [all] ********************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [10.0.102.162]\n\nPLAY RECAP ********************************************************************\n10.0.102.162               : ok=1    changed=0    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n手动输入的变量值，在后面的play中仍然可以用{{ var_name }}的形式调用。\n\n关键字vars_prompt几个常用的选项总结如下：\n\n * private：默认值为yes，表示用户输入的值在命令行不可见；将值设为no时，用户输入可见。\n * default：为变量设置默认值，以节省用户输入时间。\n * confirm：特别适合输入密码的情况，如果将其设置为yes，则会要求用户输入两次，以增加输入的安全性。\n\n\n# 六、tags标签\n\n默认情况下，ansible在执行一个playbook时，会执行playbook中定义的所有任务。ansible的标签功能可以给角色，文件，单独的任务甚至整个playbook打上标签，然后利用这些标签来指定要运行playbook中的个别任务，或不执行指定的任务，并且它的语法非常简单。\n\n通过一段代码来说明tags的用法：\n\n---\n# 可以给整个playbook的所有任务打一个标签。\n  - hosts: all\n    tags: deploy\n    roles:\n     # 给角色打的标签将会应用与角色下所有的任务。\n       - {role: tomcat, tags : ["tomcat", "app"]}        # 一个对象添加多个tag的写法之一\n    tasks:\n       - name: Notify on completion\n         local_action:\n            module: osx_say\n            msg: "{{inventory_hostname}} is finished"\n            voice: Zarvox\n         tags:      # 一个对象添加多个tag写法之二\n            - notifications\n            - say\n       - include: foo.yml\n         tags:　foo\n\n# 缩进可能不太对\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n将上述代码保存，可以通过以下命令来只执行Notify on completion任务。\n\n$ ansible-playbook test.yml --tags "say"\n\n\n1\n\n\n如果想忽略掉某个任务，可以使用--skip-tags关键字指定。\n\n\n# 七、Block块\n\nansible从2.0.0版本开始引入了块功能。块功能可以将任务进行分组，并且可以在块级别上应用任务变量。同时，块功能还可以使用类似于其他编程语言处理异常那样的方法，来处理块内部的任务异常。\n\n---\n - hosts: all\n   remote_user: root\n\n   tasks:\n     - block:\n          - yum: name=httpd state=present\n          - service: name=httpd state=started enabled=no\n       when:  ansible_eth0.ipv4.address  == "10.0.102.162"\n\n     - block:\n          - yum: name=nginx state=present\n          - service: name=nginx state=started enabled=no\n       when:  ansible_eth0.ipv4.address  == "10.0.102.200"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n运行结果如下：\n\n$ ansible-playbook -i hosts test.yml\n\nPLAY [all] **************************************************************************************************************************************************************************************\n\nTASK [Gathering Facts] **************************************************************************************************************************************************************************\nok: [10.0.102.162]\nok: [10.0.102.200]\n\nTASK [yum] **************************************************************************************************************************************************************************************\nskipping: [10.0.102.200]          # 因为在inventory文件中注释了这一台服务器，因此这里忽略了。\nok: [10.0.102.162]\n\nTASK [service] **********************************************************************************************************************************************************************************\nskipping: [10.0.102.200]\nchanged: [10.0.102.162]\n\nTASK [yum] **************************************************************************************************************************************************************************************\nskipping: [10.0.102.162]\nok: [10.0.102.200]\n\nTASK [service] **********************************************************************************************************************************************************************************\nskipping: [10.0.102.162]\nchanged: [10.0.102.200]\n\nPLAY RECAP **************************************************************************************************************************************************************************************\n10.0.102.162               : ok=3    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=3    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n上面的playbook和之前的并没有什么不同，只是假如了block之后，代码更容易查看。\n\n块功能可以用来处理任务的异常。比如一个ansible任务时监控一个并不太重要的应用，这个应用的正常运行与否对后续的任务并不产生影响，这时候我们就可以通过块功能来处理这个应用的报错。如下代码：\n\ntasks:\n   - block:\n        - name: shell script to connect the app ti a mointoring service.\n          script: mointoring-connect.sh\n          rescue:\n             - name:只有脚本报错时才执行\n　　　　　　　　 debug：msg="There was an error in the block"\n          always:\n             - name: 无论结果如何都执行\n               debug: msg="This always executes"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n当块中任意任务出错时，rescue关键字对应的代码块就会被执行，而always关键字对应的代码块无论如何都会被执行。\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************\n\n原文链接：https://www.cnblogs.com/lvzhenjiang/p/14198755.html',normalizedContent:'# ansible 中的 playbook 详解\n\n目录\n\n * 一、playbook\n   * 1.1 playbook是什么\n   * 1.2 playbook的语法结构\n   * 1.3 限定主机范围执行\n   * 1.4 ansible-palybook的小技巧\n   * 1.5 ansible-playbook中的handlers\n   * 1.6 使用handlers的注意事项\n * 二、变量\n   * 2.1 playbook中的变量\n   * 2.2 playbook中使用vars代码块定义变量\n   * 2.3 使用独立的文件来定义playbook变量\n   * 2.4 inventory文件中的变量\n   * 2.5 主机变量和组变量\n   * 2.6 巧用主机变量和组变量\n   * 2.7 注册变量\n * 三、高阶变量\n   * 3.1 fasts变量信息\n   * 3.2 本地facts变量\n * 四、if/when/while流程控制语句\n   * 4.1 when条件判断\n * 五、任务间的流程控制\n   * 5.1 任务委托\n   * 5.2 任务暂停\n   * 5.3 交互式提示\n * 六、tags标签\n * 七、block块\n\n\n# 一、playbook\n\n# 1.1 playbook是什么\n\n根本上说playbook和shell脚本没有任何的区别，playbook就像shell一样，也是把一堆的命令组合起来，然后加入对应条件判断等等，在shell脚本中是一条一条的命令，而在playbook中是一个一个的task任务构成，每个task任务可以看做shell中的一条命令；shell脚本一般只是在当前服务器上执行，而playbook则是在不止一个服务器上执行，因此playbook需要在其中指定运行该playbook的服务器名。\n\n# 1.2 playbook的语法结构\n\nplaybook使用yml标记语言，这是一种标记语言，这种标记语言在文件的最开始需要使用三个“-”来说明文件开始，然后使用缩进来说明代码块的范围。下面通过一个简易的实例，来说明playbook的语法。\n\n---      # 标记文件的开始\n- hosts: webservers   # 指定该playbook在哪个服务器上执行\n  vars:         # 表示下面是定义的变量，\n    http_port: 80    # 变量的形式，key: value，这里http_port是变量名，80是值\n    max_clients: 200\n  remote_user: root  # 指定远程的用户名，这里缩进和vars保持了一致，说明变量的代码块已经结束。\n  tasks:  # 下面构成playbook的tasks，每个task都有 - name: 开始，name指定该任务的名称。\n  - name: ensure apache is at the latest version  # 指定该任务的名称。\n    yum: pkg=httpd state=latest  # yum说明要是用的模板名称，后面指定对应的参数，这两行结合起来就相当于一个shell命令。\n\n  - name: write the apache config file      # 每个task之间可以使用空行来做区分。\n    template: src=/srv/httpd.j2 dest=/etc/httpd.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n需要说明的是缩进的意义和python中缩进的意义是一样，是来区分代码块的。\n\n一个简单的实例：检查mysql的运行状态\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no   # 不收集对应主机的信息，这样运行会快点。\n   tasks:\n     - name: check the mysql stauts\n       service: name=mysqld state=running\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n运行playbook：\n\n$ ansible-playbook test.yml\n\nplay [all] ********************************************************************\n\ntask: [check the mysql stauts] ************************************************\nok: [10.0.102.200]\nok: [10.0.102.162]\nok: [10.0.102.212]\n\nplay recap ********************************************************************\n10.0.102.162               : ok=1    changed=0    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=0    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=0    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 1.3 限定主机范围执行\n\n虽然playbook中定义了执行的主机，但是有时候我们可能仅想在定义的主机中的部分机器上执行，这时候怎么办？修改playbook中的hosts的范围，但是每次改变主机就修改一次，比较麻烦，我们可以使用--limit参数，指定该playbook在指定的主机上执行。有以下inventory文件，我们想在dbservers上执行上面测试用的playbook内容。\n\n[all]\n10.0.102.212\n10.0.102.200\n10.0.102.162\n\n[dbservers]\n10.0.102.162\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n上面测试的playbook中hosts定义all，我们想仅在dbservers上执行。\n\n$ ansible-playbook test.yml --limit dbservers\n\nplay [all] ********************************************************************\n\ntask: [check the mysql stauts] ************************************************\nok: [10.0.102.162]\n\nplay recap ********************************************************************\n10.0.102.162               : ok=1    changed=0    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n查看当前playbook在哪些主机上执行\n\n$ ansible-playbook test.yml --list-hosts\n\nplaybook: test.yml\n\n  play              # 1 (all): host count=3\n    10.0.102.162\n    10.0.102.212\n    10.0.102.200\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 1.4 ansible-palybook的小技巧\n\n * --inventory=path，指定inventory文件，默认是在/etc/ansible/hosts下面。\n * --verbose，显示详细的输出，使用-vvvv显示精确到每分钟的输出。\n * --extra-vars=vars：定义在playbook使用的变量。\n * --forks：指定并发的线程数，默认是5.\n * --connection=type:指定远程连接主机的方式，默认是ssh，设置为local时，则只在本地执行playbook、\n * --check:检测模式，playbook中定义的所有任务将在每台主机上检测，但是并不执行。\n\n# 1.5 ansible-playbook中的handlers\n\n在系统中，我们修改了服务器的配置文件，这时候就需要重启操作服务，就可以使用到handlers。\n\nhandlers:             # 定义两个handlers\n    - name: restart memcached\n      service:  name=memcached state=restarted\n    - name: restart apache\n      service: name=apache state=restarted\n\n- name: template configuration file\n  template: src=template.j2 dest=/etc/foo.conf  # 修改了配置文件然后依次启动memcached和apache服务。\n  notify:        # 使用notify来声明引用handlers。\n     - restart memcached\n     - restart apache\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# 1.6 使用handlers的注意事项\n\n * handlers只有在其所在的任务被执行时，才会被运行；如果一个任务中定义了notify调用handlers，但是由于条件判断等原因，该任务未被执行，那么handlers同样不会被执行。\n * handlers只会在每一个play的末尾运行一次；如果想在一个playbook中间运行handlers，则需要使用meta模块来实现。例如：-meta: flush_handlers.\n * 如果一个play在运行到调用handlers的语句之前失败了，那么这个handlers将不会被执行。我们可以使用meta模块的--force-handlers选项来强制执行handlers，即使handlers所在的play中途运行失败也能执行。\n\n\n# 二、变量\n\n这个变量我们来说明ansible中变量（不包含role中的变量）用法。\n\n# 2.1 playbook中的变量\n\n在运行playbook的时候使用--extra-vars来指定变量\n\n有如下playbook脚本：\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   tasks:\n     - name: test playbook variables\n       command: echo {{ test_var }}   # 打印出变量test_var的值。\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n运行playbook：\n\n$ ansible-playbook test.yml --extra-vars "test_var=test" -v    \n# 加上-v选项，会显示详细的信息\n\nplay [all] ********************************************************************\n\ntask: [test playbook variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "test"], "delta": "0:00:00.006045", "end": "2019-02-15 23:04:49.789452", "rc": 0, "start": "2019-02-15 23:04:49.783407", "stderr": "", "stdout": "test"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "test"], "delta": "0:00:00.005318", "end": "2019-02-15 23:04:52.976471", "rc": 0, "start": "2019-02-15 23:04:52.971153", "stderr": "", "stdout": "test"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "test"], "delta": "0:00:00.005082", "end": "2019-02-15 23:04:52.424959", "rc": 0, "start": "2019-02-15 23:04:52.419877", "stderr": "", "stdout": "test"}\n\nplay recap ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n上面详细信息的标准输出为test，说明变量的值已经传递了。\n\n# 2.2 playbook中使用vars代码块定义变量\n\n$ cat test.yml\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   vars:                  # 在这里使用了vars代码块来定义变量\n       test_var: hello world\n   tasks:\n     - name: test playbook variables\n       command: echo {{ test_var }}\n\n$ ansible-playbook test.yml  -v\n\nplay [all] ********************************************************************\n\ntask: [test playbook variables] ***********************************************\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "hello", "world"], "delta": "0:00:00.004940", "end": "2019-02-15 23:20:06.541672", "rc": 0, "start": "2019-02-15 23:20:06.536732", "stderr": "", "stdout": "hello world"}\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "hello", "world"], "delta": "0:00:00.004843", "end": "2019-02-15 23:20:03.957950", "rc": 0, "start": "2019-02-15 23:20:03.953107", "stderr": "", "stdout": "hello world"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "hello", "world"], "delta": "0:00:00.004219", "end": "2019-02-15 23:20:07.166900", "rc": 0, "start": "2019-02-15 23:20:07.162681", "stderr": "", "stdout": "hello world"}\n\nplay recap ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# 2.3 使用独立的文件来定义playbook变量\n\nplaybook的内容：\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   vars_files:       # 这里使用了vars_files来引入变量文件\n     - vars.yml\n   tasks:\n     - name: test playbook variables\n       command: echo {{ test_var }}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n变量文件的定义：\n\n$ cat vars.yml\n---\n  test_var: hello world\n\n\n1\n2\n3\n\n\n执行的结果：\n\n$ ansible-playbook test.yml  -v\n\nplay [all] ********************************************************************\n\ntask: [test playbook variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "hello", "world"], "delta": "0:00:00.005198", "end": "2019-02-15 23:23:16.397557", "rc": 0, "start": "2019-02-15 23:23:16.392359", "stderr": "", "stdout": "hello world"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "hello", "world"], "delta": "0:00:00.004359", "end": "2019-02-15 23:23:19.629804", "rc": 0, "start": "2019-02-15 23:23:19.625445", "stderr": "", "stdout": "hello world"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "hello", "world"], "delta": "0:00:01.006185", "end": "2019-02-15 23:23:20.039320", "rc": 0, "start": "2019-02-15 23:23:19.033135", "stderr": "", "stdout": "hello world"}\n\nplay recap ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 2.4 inventory文件中的变量\n\n在ansible中，inventory文件通常是指ansible的主机和组定义文件hosts。在hosts文件中，变量会被定义在主机名后面或组名的下方。\n\n为特定的主机定义变量，变量名跟在对应主机的后边。\n\ninventory文件如下：\n\n$ cat /etc/ansible/hosts\n[all]\n10.0.102.212 test_var=212\n10.0.102.200 test_var=200\n10.0.102.162 test_var=162\n# 为三个主机定义了同名的变量，但是变量值却不一样。\n\n\n1\n2\n3\n4\n5\n6\n\n\nplaybook的内容如下：\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   tasks:\n     - name: test playbook variables\n       command: echo {{ test_var }}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n执行这个playbook，结果如下（对应的主机显示了各自对应的变量值）：\n\n$ ansible-playbook test.yml -v\n\nplay [all] ********************************************************************\n\ntask: [test playbook variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "212"], "delta": "0:00:00.004399", "end": "2019-02-15 23:31:20.648111", "rc": 0, "start": "2019-02-15 23:31:20.643712", "stderr": "", "stdout": "212"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "200"], "delta": "0:00:00.005932", "end": "2019-02-15 23:31:23.873082", "rc": 0, "start": "2019-02-15 23:31:23.867150", "stderr": "", "stdout": "200"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "162"], "delta": "0:00:00.006723", "end": "2019-02-15 23:31:23.287861", "rc": 0, "start": "2019-02-15 23:31:23.281138", "stderr": "", "stdout": "162"}\n\nplay recap ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n给主机组定义变量，作用范围为整个主机组。\n\n$ cat /etc/ansible/hosts\n[all]\n10.0.102.212\n10.0.102.200\n10.0.102.162\n\n\n[all:vars]                            #给主机组定义变量\ntest_var=hello world\n\n$ ansible-playbook test.yml -v\n\nplay [all] ********************************************************************\n\ntask: [test playbook variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "hello", "world"], "delta": "0:00:00.003923", "end": "2019-02-15 23:37:29.322158", "rc": 0, "start": "2019-02-15 23:37:29.318235", "stderr": "", "stdout": "hello world"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "hello", "world"], "delta": "0:00:00.004161", "end": "2019-02-15 23:37:32.548947", "rc": 0, "start": "2019-02-15 23:37:32.544786", "stderr": "", "stdout": "hello world"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "hello", "world"], "delta": "0:00:00.006090", "end": "2019-02-15 23:37:32.005067", "rc": 0, "start": "2019-02-15 23:37:31.998977", "stderr": "", "stdout": "hello world"}\n\nplay recap ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n回想一下，这种方法定义变量虽然简单直观，但是若是变量特别多的情况下，会怎么样？特别是给对应的主机定义变量，若是变量太多，则管理起来会很不方便的，因此引入了主机变量和组变量。\n\n# 2.5 主机变量和组变量\n\ninventory文件仍然使用上面的文件！\n\n在执行ansbile命令时，ansible默认会从/etc/ansible/host_vars/和/etc/amsible/group_vars/两个目录下读取变量定义，如果/etc/ansible下面没有这两个目录，可以直接手动创建，并且可以在这两个目录中创建与hosts（这里是指inventory文件）文件中主机名或组名同名的文件来定义变量。\n\n先来看主机变量\n\n$ tree /etc/ansible/\n.\n├── group_vars\n├── hosts\n└── host_vars                 # 定义与主机名同名的文件\n    ├── 10.0.102.162\n    ├── 10.0.102.200\n    └── 10.0.102.212\n\n2 directories, 4 files\n\n# 文件中的内容如下：\n$ cat host_vars/10.0.102.162\n---\ntest_var: 162\n$ cat host_vars/10.0.102.200\n---\n  test_var: 200\n$ cat host_vars/10.0.102.212\n---\n  test_var: 212\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nplaybook的内容如下，执行结果如下：\n\n$ cat test.yml\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   tasks:\n     - name: test playbook variables\n       command: echo {{ test_var }}\n\n$ ansible-playbook test.yml -v\n\nplay [all] ********************************************************************\n\ntask: [test playbook variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "212"], "delta": "0:00:00.003767", "end": "2019-02-15 23:55:58.595282", "rc": 0, "start": "2019-02-15 23:55:58.591515", "stderr": "", "stdout": "212"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "162"], "delta": "0:00:00.006254", "end": "2019-02-15 23:56:01.235307", "rc": 0, "start": "2019-02-15 23:56:01.229053", "stderr": "", "stdout": "162"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "200"], "delta": "0:00:01.004509", "end": "2019-02-15 23:56:02.775410", "rc": 0, "start": "2019-02-15 23:56:01.770901", "stderr": "", "stdout": "200"}\n\nplay recap ********************************************************************\n10.0.102.162               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n再来说明一下主机组变量！\n\n创建与组名同名的文件：\n\n$ tree\n.\n├── group_vars\n│   └── all               #创建与组名同名的文件\n├── hosts\n└── host_vars\n    ├── 10.0.102.162\n    ├── 10.0.102.200\n    └── 10.0.102.212\n\n2 directories, 5 files\n\n$ cat group_vars/all\n---\n  test_group_var: from group\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n执行结果如下：\n\n$ cat test.yml\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   tasks:\n     - name: test the host variables\n       command: echo {{ test_var }}\n\n     - name: test host group variables           #写入测试组变量的task\n       command: echo {{ test_group_var }}\n$ ansible-playbook test.yml -v\n\nplay [all] ********************************************************************\n\ntask: [test the host variables] ***********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "212"], "delta": "0:00:00.004613", "end": "2019-02-15 23:59:23.227722", "rc": 0, "start": "2019-02-15 23:59:23.223109", "stderr": "", "stdout": "212"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "200"], "delta": "0:00:00.006490", "end": "2019-02-15 23:59:26.422682", "rc": 0, "start": "2019-02-15 23:59:26.416192", "stderr": "", "stdout": "200"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "162"], "delta": "0:00:00.004709", "end": "2019-02-15 23:59:25.812786", "rc": 0, "start": "2019-02-15 23:59:25.808077", "stderr": "", "stdout": "162"}\n\ntask: [test host group variables] *********************************************\nchanged: [10.0.102.212] => {"changed": true, "cmd": ["echo", "from", "group"], "delta": "0:00:00.003759", "end": "2019-02-15 23:59:23.519180", "rc": 0, "start": "2019-02-15 23:59:23.515421", "stderr": "", "stdout": "from group"}\nchanged: [10.0.102.162] => {"changed": true, "cmd": ["echo", "from", "group"], "delta": "0:00:00.003748", "end": "2019-02-15 23:59:26.109337", "rc": 0, "start": "2019-02-15 23:59:26.105589", "stderr": "", "stdout": "from group"}\nchanged: [10.0.102.200] => {"changed": true, "cmd": ["echo", "from", "group"], "delta": "0:00:00.004339", "end": "2019-02-15 23:59:26.724525", "rc": 0, "start": "2019-02-15 23:59:26.720186", "stderr": "", "stdout": "from group"}\n\nplay recap ********************************************************************\n10.0.102.162               : ok=2    changed=2    unreachable=0    failed=0\n10.0.102.200               : ok=2    changed=2    unreachable=0    failed=0\n10.0.102.212               : ok=2    changed=2    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 2.6 巧用主机变量和组变量\n\n有时候在执行ansbile任务时，可能需要从一台远程主机上获取另一台远程主机的变量信息，这时候可以使用hostvars变量，这个变量包含了指定主机上所定义的所有变量。\n\n譬如，若是想获取host1上变量admin_user的内容，在任意主机上直接上使用下面代码即可。\n\n{{  hostvars[\'host1\'][\'admin_user\']}}\n\n\n1\n\n\nansible提供了一些非常有用的内置变量，几个常用的如下：\n\n * grorps：包含了所有hosts文件里的主机组的一个列表。\n * group_names: 包含了当前主机所在的所有主机组名的一个列表。\n * inventory_hostname: 通过hosts文件定义的主机名。（与ansible_home意义不同）\n * inventory_hostname_short：变量inventory_hostname的第一部分。譬如inventory_hostname的值为books.ansible.com，那么inventory_hostname_short的值就是books。\n * play_hosts: 将执行当前任务的所有主机\n\n# 2.7 注册变量\n\n注册变量，其实就是将操作结果，包括标准输出和标准错误输出，保存到变量中，然后再根据这个变量的内容来决定下一步的操作，在这个过程中用来保存操作结果的变量就叫注册变量。\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   tasks:\n     - name: test the register variables\n       shell: uptime\n       register: results     # 使用关键字register声明注册变量，上面uptime命令产生的结果，存入到results中。结果是字典形式。\n\n     - name: print the register result\n       debug: msg="{{ results.stdout }}"   # 使用debug模块，打印出上面命令的输出结果。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n上面的playbook执行结果如下：\n\n$ ansible-playbook test.yml\n\nplay [all] ********************************************************************\n\ntask: [test the register variables] *******************************************\nchanged: [10.0.102.212]\nchanged: [10.0.102.200]\nchanged: [10.0.102.162]\n\ntask: [print the register result] *********************************************\nok: [10.0.102.212] => {\n    "msg": " 00:18:01 up 3 days,  2:56,  3 users,  load average: 0.02, 0.03, 0.05"          #msg的结果就是注册变量的标准输出\n}\nok: [10.0.102.200] => {\n    "msg": " 00:18:04 up 4 days,  7:45,  3 users,  load average: 0.03, 0.06, 0.05"\n}\nok: [10.0.102.162] => {\n    "msg": " 00:18:04 up 4 days,  7:45,  3 users,  load average: 0.01, 0.02, 0.05"\n}\n\nplay recap ********************************************************************\n10.0.102.162               : ok=2    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=2    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=2    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n一个注册变量通常会有以下4个属性：\n\n * changed：任务是否对远程主机造成的变更。\n * delta：任务运行所用的时间。\n * stdout：正常的输出信息。\n * stderr：错误信息。\n\n\n# 三、高阶变量\n\n对于普通变量，在ansible命令行设定的，在hosts文件中定义的，或者在playbook中定义的等，这些都是普通变量，在引用时，可以使用使用{{ variable }}的形式。ansible是用python语言写的，因此也支持一种叫做列表的变量，形式如下：\n\n---\n - hosts: all\n   remote_user: root\n   gather_facts: no\n   vars:\n      var_list:     # 注意形式，定义了var_list列表，取值方法和列表取值一样，不推荐使用jinja2的方法取值。\n          - one\n          - two\n          - three\n   tasks:\n     - name: test the list variables\n       shell: echo {{ var_list[0] }}   # 取列表中的第一个字，也就是one\n       register: results\n\n     - name: print the register result\n       debug: msg="{{ results.stdout }}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n执行结果如下：\n\n$ ansible-playbook test.yml\n\nplay [all] ********************************************************************\n\ntask: [test the list variables] ***********************************************\nchanged: [10.0.102.212]\nchanged: [10.0.102.200]\nchanged: [10.0.102.162]\n\ntask: [print the register result] *********************************************\nok: [10.0.102.212] => {\n    "msg": "one"\n}\nok: [10.0.102.200] => {\n    "msg": "one"\n}\nok: [10.0.102.162] => {\n    "msg": "one"\n}\n\nplay recap ********************************************************************\n10.0.102.162               : ok=2    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=2    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=2    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# 3.1 fasts变量信息\n\n在上面的测试中，我们的playbook都执行了一条命令叫gater_facts:no，加入了这条命令后，playbook脚本的执行速度会快很多，这是因为默认情况下，ansible是会手机远程服务器的主机信息，这些信息包含了服务器的一些基本设置。\n\ngathering facts ***************************************************************\nok: [10.0.102.200]\nok: [10.0.102.212]\nok: [10.0.102.162]\n\n\n1\n2\n3\n4\n\n\n收集的主机信息可以使用setup模块查看，一个主机的收集信息如下：\n\nansible 10.0.102.162 -m setup\n10.0.102.162 | success >> {\n    "ansible_facts": {\n        "ansible_all_ipv4_addresses": [\n            "10.0.102.162"\n        ],\n        "ansible_all_ipv6_addresses": [\n            "fe80::1392:ecd3:5adf:c3ae"\n        ],\n        "ansible_architecture": "x86_64",\n        "ansible_bios_date": "04/01/2014",\n        "ansible_bios_version": "1.9.1-5.el7.centos",\n        "ansible_cmdline": {\n            "boot_image": "/vmlinuz-3.10.0-514.el7.x86_64",\n            "lang": "en_us.utf-8",\n            "crashkernel": "auto",\n            "quiet": true,\n            "rd.lvm.lv": "cl/swap",\n            "rhgb": true,\n            "ro": true,\n            "root": "/dev/mapper/cl-root"\n        },\n        "ansible_date_time": {\n            "date": "2019-02-16",\n            "day": "16",\n            "epoch": "1550248590",\n            "hour": "00",\n            "iso8601": "2019-02-15t16:36:30z",\n            "iso8601_micro": "2019-02-15t16:36:30.311222z",\n            "minute": "36",\n            "month": "02",\n            "second": "30",\n            "time": "00:36:30",\n            "tz": "cst",\n            "tz_offset": "+0800",\n            "weekday": "saturday",\n            "year": "2019"\n        },\n        "ansible_default_ipv4": {\n            "address": "10.0.102.162",\n            "alias": "eth0",\n            "gateway": "10.0.100.1",\n            "interface": "eth0",\n            "macaddress": "fa:0a:e3:54:a6:00",\n            "mtu": 1500,\n            "netmask": "255.255.252.0",\n            "network": "10.0.100.0",\n            "type": "ether"\n        },\n        "ansible_default_ipv6": {},\n        "ansible_devices": {\n            "sr0": {\n                "holders": [],\n                "host": "ide interface: intel corporation 82371sb piix3 ide [natoma/triton ii]",\n                "model": "qemu dvd-rom",\n                "partitions": {},\n                "removable": "1",\n                "rotational": "1",\n                "scheduler_mode": "cfq",\n                "sectors": "2097151",\n                "sectorsize": "512",\n                "size": "1024.00 mb",\n                "support_discard": "0",\n                "vendor": "qemu"\n            },\n            "vda": {\n                "holders": [],\n                "host": "scsi storage controller: red hat, inc virtio block device",\n                "model": null,\n                "partitions": {\n                    "vda1": {\n                        "sectors": "2097152",\n                        "sectorsize": 512,\n                        "size": "1.00 gb",\n                        "start": "2048"\n                    },\n                    "vda2": {\n                        "sectors": "81786880",\n                        "sectorsize": 512,\n                        "size": "39.00 gb",\n                        "start": "2099200"\n                    }\n                },\n                "removable": "0",\n                "rotational": "1",\n                "scheduler_mode": "",\n                "sectors": "83886080",\n                "sectorsize": "512",\n                "size": "40.00 gb",\n                "support_discard": "0",\n                "vendor": "0x1af4"\n            }\n        },\n        "ansible_distribution": "centos",\n        "ansible_distribution_major_version": "7",\n        "ansible_distribution_release": "core",\n        "ansible_distribution_version": "7.3.1611",\n        "ansible_domain": "",\n        "ansible_env": {\n            "home": "/root",\n            "lang": "en_us.utf-8",\n            "lc_ctype": "en_us.utf-8",\n            "lessopen": "||/usr/bin/lesspipe.sh %s",\n            "logname": "root",\n            "ls_colors": "rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:",\n            "mail": "/var/mail/root",\n            "path": "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin",\n            "pwd": "/root",\n            "selinux_level_requested": "",\n            "selinux_role_requested": "",\n            "selinux_use_current_range": "",\n            "shell": "/bin/bash",\n            "shlvl": "2",\n            "ssh_client": "10.0.102.204 4242 22",\n            "ssh_connection": "10.0.102.204 4242 10.0.102.162 22",\n            "ssh_tty": "/dev/pts/1",\n            "term": "xterm",\n            "user": "root",\n            "xdg_runtime_dir": "/run/user/0",\n            "xdg_session_id": "168",\n            "_": "/usr/bin/python"\n        },\n        "ansible_eth0": {\n            "active": true,\n            "device": "eth0",\n            "ipv4": {\n                "address": "10.0.102.162",\n                "netmask": "255.255.252.0",\n                "network": "10.0.100.0"\n            },\n            "ipv6": [\n                {\n                    "address": "fe80::1392:ecd3:5adf:c3ae",\n                    "prefix": "64",\n                    "scope": "link"\n                }\n            ],\n            "macaddress": "fa:0a:e3:54:a6:00",\n            "module": "virtio_net",\n            "mtu": 1500,\n            "promisc": false,\n            "type": "ether"\n        },\n        "ansible_form_factor": "other",\n        "ansible_fqdn": "docker4",\n        "ansible_hostname": "docker4",\n        "ansible_interfaces": [\n            "lo",\n            "eth0"\n        ],\n        "ansible_kernel": "3.10.0-514.el7.x86_64",\n        "ansible_lo": {\n            "active": true,\n            "device": "lo",\n            "ipv4": {\n                "address": "127.0.0.1",\n                "netmask": "255.0.0.0",\n                "network": "127.0.0.0"\n            },\n            "ipv6": [\n                {\n                    "address": "::1",\n                    "prefix": "128",\n                    "scope": "host"\n                }\n            ],\n            "mtu": 65536,\n            "promisc": false,\n            "type": "loopback"\n        },\n        "ansible_machine": "x86_64",\n        "ansible_memfree_mb": 881,\n        "ansible_memtotal_mb": 1839,\n        "ansible_mounts": [\n            {\n                "device": "/dev/mapper/cl-root",\n                "fstype": "xfs",\n                "mount": "/",\n                "options": "rw,seclabel,relatime,attr2,inode64,noquota",\n                "size_available": 34615087104,\n                "size_total": 39700664320\n            },\n            {\n                "device": "/dev/vda1",\n                "fstype": "xfs",\n                "mount": "/boot",\n                "options": "rw,seclabel,relatime,attr2,inode64,noquota",\n                "size_available": 918556672,\n                "size_total": 1063256064\n            }\n        ],\n        "ansible_nodename": "docker4",\n        "ansible_os_family": "redhat",\n        "ansible_pkg_mgr": "yum",\n        "ansible_processor": [\n            "qemu virtual cpu version 2.5+",\n            "qemu virtual cpu version 2.5+"\n        ],\n        "ansible_processor_cores": 2,\n        "ansible_processor_count": 1,\n        "ansible_processor_threads_per_core": 1,\n        "ansible_processor_vcpus": 2,\n        "ansible_product_name": "kvm",\n        "ansible_product_serial": "na",\n        "ansible_product_uuid": "e5e1d5e6-1a4d-4e0d-98c3-b8ad422b10cc",\n        "ansible_product_version": "rhel 7.3.0 pc (i440fx + piix, 1996)",\n        "ansible_python_version": "2.7.5",\n        "ansible_selinux": {\n            "config_mode": "enforcing",\n            "mode": "enforcing",\n            "policyvers": 28,\n            "status": "enabled",\n            "type": "targeted"\n        },\n        "ansible_ssh_host_key_ecdsa_public": "aaaae2vjzhnhlxnoytitbmlzdhayntyaaaaibmlzdhayntyaaabbbep5if/laqb9q9fnkfnsi3mljsvvvoovhrrcugtbhejs+tam36obair764ix1zdn2swfldygmcuaaeipu3fk+uu=",\n        "ansible_ssh_host_key_rsa_public": "aaaab3nzac1yc2eaaaadaqabaaabaqc6yhi2+v64emw3jdisbrzkmwurp7uf4iqemjgowpqc3mvlfsposqerdojn9he34fvixcbluj9wii0kc3qzxxnwtefwjcdpsl17ns9eiedkjqrhswts7oxyc1948bdyhygnaw57befvuj+vt8oi1jskkksi3acumazdz9tngcvyiqw4pmuqfat/yenpqkhsp8mdx/sl/unpvsctb0w37o38zvapkpankhw25uiwrostlgqy4vgozhtqhudvqk4ezqod0+jmbcykvj2abbl1smih8mmrc2w2gi0gjx31ky/t5swqtxtdmrb3d7n9yrd1ppcnh0zebs/opnx4g5uwx/ap",\n        "ansible_swapfree_mb": 0,\n        "ansible_swaptotal_mb": 0,\n        "ansible_system": "linux",\n        "ansible_system_vendor": "red hat",\n        "ansible_user_id": "root",\n        "ansible_userspace_architecture": "x86_64",\n        "ansible_userspace_bits": "64",\n        "ansible_virtualization_role": "guest",\n        "ansible_virtualization_type": "kvm",\n        "module_setup": true\n    },\n    "changed": false\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n\n\n在实际应用中，运用的比较多的facts变量有ansible_os_family，ansible_hostname等，这些变量通常会被拿来作为when条件语句的判断条件，来决定下一步的操作。一个简单的实例：\n\n---\n - hosts: all\n   remote_user: root\n   tasks:\n     - name: test the list variables\n       shell: echo {{ ansible_os_family }}\n       register: results\n\n     - name: print the register result\n       debug: msg="{{ results.stdout }}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n执行结果如下：\n\nansible-playbook test.yml\n\nplay [all] ********************************************************************\n\ngathering facts ***************************************************************\nok: [10.0.102.162]\nok: [10.0.102.212]\nok: [10.0.102.200]\n\ntask: [test the list variables] ***********************************************\nchanged: [10.0.102.162]\nchanged: [10.0.102.212]\nchanged: [10.0.102.200]\n\ntask: [print the register result] *********************************************\nok: [10.0.102.212] => {\n    "msg": "redhat"                   #对应变量的结果\n}\nok: [10.0.102.200] => {\n    "msg": "redhat"\n}\nok: [10.0.102.162] => {\n    "msg": "redhat"\n}\n\nplay recap ********************************************************************\n10.0.102.162               : ok=3    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=3    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=3    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 3.2 本地facts变量\n\n我们可以自己定义facts变量，把这个变量写入一个以.fact结尾的文件中，这个文件可以是json文件或ini文件，或者是一个可以返回json代码的可执行文件。然后将其放在远程主机的/etc/ansible/facts.d文件夹中，ansible在执行的任务时会自动到这个文件夹中读取变量的信息。\n\n在远程主机上做如下操作：\n\n# 自定义fact信息\n$ mkdir -p /etc/ansible/facts.d && cd /etc/ansible/facts.d\n$ cat test.fact\n[test_fact]\nadmin=hongkong\n\n\n1\n2\n3\n4\n5\n\n\n然后再ansible主机上获取自定义的信息。\n\n$ ansible 10.0.102.162 -m setup -a "filter=ansible_local"\n10.0.102.162 | success >> {\n    "ansible_facts": {\n        "ansible_local": {\n            "test": {\n                "test_fact": {\n                    "admin": "hongkong"\n                }\n            }\n        }\n    },\n    "changed": false\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 四、if/when/while流程控制语句\n\n条件判断在ansible任务中的使用频率非常高。我们可以根据一些条件的不一样执行不同的task。\n\n# 4.1 when条件判断\n\n很多任务只有在特定条件下才能执行，这就是when语句发挥作用的地方。\n\n一个简单的实例，关闭掉ip地址为10.0.102.162服务器上的mysql服务，如下：\n\n[root@test2 playbook]# cat test.yml\n---\n - hosts: all\n   remote_user: root\n   tasks:\n     - name: shut down the db server\n       service: name=mysqld state=stopped\n       when: ansible_eth0.ipv4.address  == "10.0.102.162"  # 这里使用了when条件语句\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n执行的结果如下：\n\n$ ansible-playbook test.yml\n\nplay [all] ********************************************************************\n\ngathering facts ***************************************************************\nok: [10.0.102.212]\nok: [10.0.102.200]\nok: [10.0.102.162]\n\ntask: [shut down the db server] ***********************************************\nskipping: [10.0.102.200]\nskipping: [10.0.102.212]\nchanged: [10.0.102.162]                      #162的服务状态已经改变\n\nplay recap ********************************************************************\n10.0.102.162               : ok=2    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=1    changed=0    unreachable=0    failed=0\n10.0.102.212               : ok=1    changed=0    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n这个就是when条件语句的用法很简单。需要注意when语句的作用于paly的作用时间，当when的条件满足时，然后才会执行play中的任务。ansible还提供了另外两个与when相关的语句changed_when和failed_when条件判断。\n\n\n# 五、任务间的流程控制\n\n# 5.1 任务委托\n\n默认情况下，ansible所有任务都是在我们指定的机器上面运行的，当在一个独立的集群环境配置时，这并没有什么问题。而在有些情况下，比如给某台服务器发送通知或者向监控服务器中添加被监控的主机，这个时候任务就需要在特定的主机上运行，而非一开始指定的所有主机，此时就需要ansible的委托任务。\n\n使用delegate_to关键字可以配置任务在指定的服务器上执行，而其他任务还是在hosts关键字配置的所有机器上执行，当到了这个关键字所在的任务时，就使用委托的机器运行。\n\n查看mysql是否在运行状态，因此在检查之前首先关掉162上的mysql服务。（为了方便查看状态）\n\n---\n - hosts: all\n   remote_user: root\n   tasks:\n     - name: stop the db server\n       service: name=mysqld state=stopped\n       delegate_to: 10.0.102.162       # 这里使用了委托，仅关闭162这台服务器上，这个play仅在162这台服务器上执行。\n\n     - name: check mysql status\n       service: name=mysqld state=running\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n这里委托是在指定的机器上执行，若是想在本地服务器上执行，可以把ip地址换为127.0.0.1即可。也可以使用local_action方法。\n\n---\n - hosts: all\n   remote_user: root\n   tasks:\n     - name: create the test file\n       local_action: shell touch test1111 # 在本地创建一个测试文件\n\n\n     - name: check mysql status\n       service: name=mysqld state=running\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n结果如下：\n\n$ ansible-playbook test.yml\n\nplay [all] ********************************************************************\n\ngathering facts ***************************************************************\nok: [10.0.102.212]\nok: [10.0.102.200]\nok: [10.0.102.162]\n\ntask: [create the test file] **************************************************\nchanged: [10.0.102.212 -> 127.0.0.1]\nchanged: [10.0.102.200 -> 127.0.0.1]\nchanged: [10.0.102.162 -> 127.0.0.1]\n\ntask: [check mysql status] ****************************************************\nok: [10.0.102.200]\nok: [10.0.102.212]\nok: [10.0.102.162]\n\nplay recap ********************************************************************\n10.0.102.162               : ok=3    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=3    changed=1    unreachable=0    failed=0\n10.0.102.212               : ok=3    changed=1    unreachable=0    failed=0\n\n$ ls                     # 默认会在当前目录创建对应的文件\ntest1111  test.yml  vars.yml\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n# 5.2 任务暂停\n\n有些情况下，一些任务的运行需要等待一些状态的恢复，比如某一台主机或者应用刚刚重启，我们需要等待它上面的某个端口开启，此时我们就不得不将正在运行的任务暂停，直到其状态满足我们的需求。\n\n下一个实例：\n\n- name: wait for webserver to start\n   local_action:\n        module: wait_for\n        host: webserver1\n        port: 80\n        delay: 10\n        timeout: 300\n        state: startted\n# 这个实例中，这个任务将会每10s检查一次主机webserver1上面的80端口是否开启，如果超过了300s则失败\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 5.3 交互式提示\n\n在少数情况下，ansible任务运行的过程中需要用户输入一些数据，这些数据要么比较秘密不方便，或者数据是动态的，不同的用户有不同的需求，比如输入用户自己的账户和密码或者输入不同的版本号会触发不同的后续操作等。ansible的vars_prompt关键字就是用来处理上述这种与用户交互的情况的。下面是一个简单的实例。\n\n---\n - hosts: all\n   remote_user: root\n   vars_prompt:\n      - name: share_user\n        prompt: "what is your network username?"\n        private: no\n\n      - name: share_pass\n        prompt: "what is your network password"\n        private: no\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n然后执行上面的playbook，因为我们只是测试，只需要在一台机器上执行，因此加入了--limit参数。\n\n$ ansible-playbook test.yml --limit 10.0.102.162\nwhat is your network username?: test        # 需要手动交互输入\nwhat is your network password: 123456       # 手动输入\n\nplay [all] ********************************************************************\n\ngathering facts ***************************************************************\nok: [10.0.102.162]\n\nplay recap ********************************************************************\n10.0.102.162               : ok=1    changed=0    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n手动输入的变量值，在后面的play中仍然可以用{{ var_name }}的形式调用。\n\n关键字vars_prompt几个常用的选项总结如下：\n\n * private：默认值为yes，表示用户输入的值在命令行不可见；将值设为no时，用户输入可见。\n * default：为变量设置默认值，以节省用户输入时间。\n * confirm：特别适合输入密码的情况，如果将其设置为yes，则会要求用户输入两次，以增加输入的安全性。\n\n\n# 六、tags标签\n\n默认情况下，ansible在执行一个playbook时，会执行playbook中定义的所有任务。ansible的标签功能可以给角色，文件，单独的任务甚至整个playbook打上标签，然后利用这些标签来指定要运行playbook中的个别任务，或不执行指定的任务，并且它的语法非常简单。\n\n通过一段代码来说明tags的用法：\n\n---\n# 可以给整个playbook的所有任务打一个标签。\n  - hosts: all\n    tags: deploy\n    roles:\n     # 给角色打的标签将会应用与角色下所有的任务。\n       - {role: tomcat, tags : ["tomcat", "app"]}        # 一个对象添加多个tag的写法之一\n    tasks:\n       - name: notify on completion\n         local_action:\n            module: osx_say\n            msg: "{{inventory_hostname}} is finished"\n            voice: zarvox\n         tags:      # 一个对象添加多个tag写法之二\n            - notifications\n            - say\n       - include: foo.yml\n         tags:　foo\n\n# 缩进可能不太对\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n将上述代码保存，可以通过以下命令来只执行notify on completion任务。\n\n$ ansible-playbook test.yml --tags "say"\n\n\n1\n\n\n如果想忽略掉某个任务，可以使用--skip-tags关键字指定。\n\n\n# 七、block块\n\nansible从2.0.0版本开始引入了块功能。块功能可以将任务进行分组，并且可以在块级别上应用任务变量。同时，块功能还可以使用类似于其他编程语言处理异常那样的方法，来处理块内部的任务异常。\n\n---\n - hosts: all\n   remote_user: root\n\n   tasks:\n     - block:\n          - yum: name=httpd state=present\n          - service: name=httpd state=started enabled=no\n       when:  ansible_eth0.ipv4.address  == "10.0.102.162"\n\n     - block:\n          - yum: name=nginx state=present\n          - service: name=nginx state=started enabled=no\n       when:  ansible_eth0.ipv4.address  == "10.0.102.200"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n运行结果如下：\n\n$ ansible-playbook -i hosts test.yml\n\nplay [all] **************************************************************************************************************************************************************************************\n\ntask [gathering facts] **************************************************************************************************************************************************************************\nok: [10.0.102.162]\nok: [10.0.102.200]\n\ntask [yum] **************************************************************************************************************************************************************************************\nskipping: [10.0.102.200]          # 因为在inventory文件中注释了这一台服务器，因此这里忽略了。\nok: [10.0.102.162]\n\ntask [service] **********************************************************************************************************************************************************************************\nskipping: [10.0.102.200]\nchanged: [10.0.102.162]\n\ntask [yum] **************************************************************************************************************************************************************************************\nskipping: [10.0.102.162]\nok: [10.0.102.200]\n\ntask [service] **********************************************************************************************************************************************************************************\nskipping: [10.0.102.162]\nchanged: [10.0.102.200]\n\nplay recap **************************************************************************************************************************************************************************************\n10.0.102.162               : ok=3    changed=1    unreachable=0    failed=0\n10.0.102.200               : ok=3    changed=1    unreachable=0    failed=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n上面的playbook和之前的并没有什么不同，只是假如了block之后，代码更容易查看。\n\n块功能可以用来处理任务的异常。比如一个ansible任务时监控一个并不太重要的应用，这个应用的正常运行与否对后续的任务并不产生影响，这时候我们就可以通过块功能来处理这个应用的报错。如下代码：\n\ntasks:\n   - block:\n        - name: shell script to connect the app ti a mointoring service.\n          script: mointoring-connect.sh\n          rescue:\n             - name:只有脚本报错时才执行\n　　　　　　　　 debug：msg="there was an error in the block"\n          always:\n             - name: 无论结果如何都执行\n               debug: msg="this always executes"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n当块中任意任务出错时，rescue关键字对应的代码块就会被执行，而always关键字对应的代码块无论如何都会被执行。\n\n*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************\n\n原文链接：https://www.cnblogs.com/lvzhenjiang/p/14198755.html',charsets:{cjk:!0}},{title:"ansible-playbook编排使用tips",frontmatter:{title:"ansible-playbook编排使用tips",date:"2023-01-11T15:06:48.000Z",permalink:"/pages/7eca6b/",categories:["专题","ansible系列文章"],tags:[null],readingShow:"top",description:"在日常配置编排过程中，我们经常需要用到一些内置的变量来进行一些判断或者配置的工作，这里整理一些常用的变量，以便于使用查阅。",meta:[{name:"twitter:title",content:"ansible-playbook编排使用tips"},{name:"twitter:description",content:"在日常配置编排过程中，我们经常需要用到一些内置的变量来进行一些判断或者配置的工作，这里整理一些常用的变量，以便于使用查阅。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/08.ansible-playbook%E7%BC%96%E6%8E%92%E4%BD%BF%E7%94%A8tips.html"},{property:"og:type",content:"article"},{property:"og:title",content:"ansible-playbook编排使用tips"},{property:"og:description",content:"在日常配置编排过程中，我们经常需要用到一些内置的变量来进行一些判断或者配置的工作，这里整理一些常用的变量，以便于使用查阅。"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/08.ansible-playbook%E7%BC%96%E6%8E%92%E4%BD%BF%E7%94%A8tips.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-11T15:06:48.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"ansible-playbook编排使用tips"},{itemprop:"description",content:"在日常配置编排过程中，我们经常需要用到一些内置的变量来进行一些判断或者配置的工作，这里整理一些常用的变量，以便于使用查阅。"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/08.ansible-playbook%E7%BC%96%E6%8E%92%E4%BD%BF%E7%94%A8tips.html",relativePath:"01.专题/01.ansible系列文章/08.ansible-playbook编排使用tips.md",key:"v-d692e0c2",path:"/pages/7eca6b/",headers:[{level:2,title:"常用变量",slug:"常用变量",normalizedTitle:"常用变量",charIndex:2},{level:2,title:"1，判断错误",slug:"_1-判断错误",normalizedTitle:"1，判断错误",charIndex:198},{level:2,title:"2，只跑一次",slug:"_2-只跑一次",normalizedTitle:"2，只跑一次",charIndex:3504},{level:2,title:"3，滚动更新",slug:"_3-滚动更新",normalizedTitle:"3，滚动更新",charIndex:5393},{level:2,title:"4，打印多行",slug:"_4-打印多行",normalizedTitle:"4，打印多行",charIndex:10210},{level:2,title:"只同步远程目录没有的文件",slug:"只同步远程目录没有的文件",normalizedTitle:"只同步远程目录没有的文件",charIndex:13211}],headersStr:"常用变量 1，判断错误 2，只跑一次 3，滚动更新 4，打印多行 只同步远程目录没有的文件",content:'# 常用变量\n\n在日常配置编排过程中，我们经常需要用到一些内置的变量来进行一些判断或者配置的工作，这里整理一些常用的变量，以便于使用查阅。\n\n * 部署客户端主机名：ansible_hostname\n * 部署客户端主机 IP：ansible_default_ipv4.address\n * 部署客户端主机详细信息：hostvars，返回主机详细信息，可以通过点操作定位具体需要的内容。\n\n\n# 1，判断错误\n\n有时候我们在部署服务的时候，会针对一些服务状态进行检测，从而依据检测结果来判断是否将主机放回到负载列表当中，这里举一个 web 检测的例子，首先通过 NGINX 定义了一个返回值：\n\n$ curl localhost/get_info\n{"status":"success","result":"hello world!"}\n\n\n1\n2\n\n\n然后定义剧本如下：\n\n$ cat site.yaml\n---\n  - hosts: localhost\n    tasks:\n      - uri:\n          url: "http://localhost/get_info"\n          method: GET\n        register: webpage\n        failed_when: webpage.status != 200\n      - debug:\n          var: webpage\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n这里使用本机进行验证此类功能，直接通过如下命令即可运行：\n\n$ ansible-playbook site.yaml\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match \'all\'\nPLAY [localhost] ********************************************************************************************************************************************************************\nTASK [Gathering Facts] **************************************************************************************************************************************************************\nok: [localhost]\nTASK [uri] **************************************************************************************************************************************************************************\nok: [localhost]\nTASK [debug] ************************************************************************************************************************************************************************\nok: [localhost] => {\n    "webpage": {\n        "changed": false,\n        "connection": "close",\n        "content_length": "44",\n        "content_type": "application/json",\n        "cookies": {},\n        "cookies_string": "",\n        "date": "Tue, 02 Jun 2020 06:30:53 GMT",\n        "elapsed": 0,\n        "failed": false,\n        "failed_when_result": false,\n        "json": {\n            "result": "hello world!",\n            "status": "success"\n        },\n        "msg": "OK (44 bytes)",\n        "redirected": false,\n        "server": "openresty",\n        "status": 200,\n        "url": "http://localhost/get_info"\n    }\n}\nPLAY RECAP **************************************************************************************************************************************************************************\nlocalhost                  : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n上边通过输出的状态码进行判断，除此之外，还可以通过输出当中的具体内容匹配来进行判断，比如上边内容当中，我们看到对应的 json 字段里输出的返回内容，也可以根据具体内容来进行判断，此时调整剧本内容如下：\n\n---\n  - hosts: localhost\n    tasks:\n      - uri:\n          url: "http://localhost/get_info"\n          method: GET\n        register: webpage\n        failed_when: webpage.status != 200\n      - fail:\n          msg: "check status field"\n        when: "\'hello\' not in webpage.json.result"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nwebpage.json.result可以将返回的内容定位到相应位置，然后再进行判断，当然不少时候也可以用如下方式判断：\n\n---\n  - hosts: localhost\n    tasks:\n      - uri:\n          url: "http://localhost/get_info"\n          method: GET\n          return_content: yes\n        register: webpage\n        failed_when: webpage.status != 200\n      - fail:\n          msg: "check status field"\n        when: "\'hello\' not in webpage.content"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n多了一个参数 return_content: yes， 但需要注意的一个地方是：webpage.content中的 json 字段是无法展开的。\n\n\n# 2，只跑一次\n\nrun_once: true\n\n有一些在列表中的任务，只需要跑一次就好，比如上线部署剧本中，发布列表可能有 10 台，那么如下步骤中，一些地方就不需要执行 10 次了，比如生成版本号以及同步到本地的两个步骤。\n\n---\n- name: "创建远程主机上的版本目录"\n  file: path=/release/{{project}}/{{_version}} state=directory\n  tags: deploy\n- name: "将代码同步到远程主机版本目录"\n  synchronize:\n    src: /{{WORKSPACE}}/\n    dest: /release/{{project}}/{{_version}}/\n    delete: yes\n  tags: deploy\n- name: "将项目部署到生产目录"\n  file: path=/data/www/{{project}} state=link src=/release/{{project}}/{{_version}}\n  tags: deploy\n- name: "使版本目录保持五个版本历史"\n  script: chdir=/release/{{project}} keepfive.sh\n  tags: deploy\n- name: "生成远程版本号"\n  shell: "ls /release/{{project}} > /release/{{project}}.log"\n  tags: deploy\n- name: "同步版本号到本地"\n  synchronize: "src=/release/{{project}}.log dest=/root/.jenkins/version/{{JOB_NAME}}.log mode=pull"\n  tags: deploy\n- name: "执行自由脚本"\n  script: chdir=/release/{{project}} free.sh\n  tags: deploy\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n通过添加 run_once: true可以定义对应的任务只执行一次：\n\n---\n- name: "创建远程主机上的版本目录"\n  file: path=/release/{{project}}/{{_version}} state=directory\n  tags: deploy\n- name: "将代码同步到远程主机版本目录"\n  synchronize:\n    src: /{{WORKSPACE}}/\n    dest: /release/{{project}}/{{_version}}/\n    delete: yes\n  tags: deploy\n- name: "将项目部署到生产目录"\n  file: path=/data/www/{{project}} state=link src=/release/{{project}}/{{_version}}\n  tags: deploy\n- name: "使版本目录保持五个版本历史"\n  script: chdir=/release/{{project}} keepfive.sh\n  tags: deploy\n- name: "生成远程版本号"\n  shell: "ls /release/{{project}} > /release/{{project}}.log"\n  run_once: true\n  tags: deploy\n- name: "同步版本号到本地"\n  synchronize: "src=/release/{{project}}.log dest=/root/.jenkins/version/{{JOB_NAME}}.log mode=pull"\n  run_once: true\n  tags: deploy\n- name: "执行自由脚本"\n  script: chdir=/release/{{project}} free.sh\n  tags: deploy\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n这样即便是发布了 10 台机器，那么版本号这两步也就只执行一次。\n\n\n# 3，滚动更新\n\nansible 默认情况下，执行剧本时是从剧本角度出发，每个任务在主机列表中逐一执行，有时候我们发布一些服务，需要一台一台从负载中摘出部署，然后放回去，这个时候，走默认的方向就不太合适了。用如下一个简单例子来说明：\n\n$ cat site.yml\n---\n  - hosts: remote\n    tasks:\n      - debug:\n          msg: "this is task a"\n      - debug:\n          msg: "this is task b"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n执行如上剧本看输出：\n\nansible-playbook -i test_hosts s.yaml\nPLAY [remote] ***********************************************************************************************************************************************************************\nTASK [Gathering Facts] **************************************************************************************************************************************************************\nok: [10.3.22.90]\nok: [10.3.22.87]\nTASK [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.90] => {\n    "msg": "this is task a"\n}\nok: [10.3.22.87] => {\n    "msg": "this is task a"\n}\nTASK [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.90] => {\n    "msg": "this is task b"\n}\nok: [10.3.22.87] => {\n    "msg": "this is task b"\n}\nPLAY RECAP **************************************************************************************************************************************************************************\n10.3.22.87                 : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.3.22.90                 : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n可以看到两个任务被分开，在两台主机上分别来执行。\n\nansible 中的 serial就是用来控制任务执行数量的参数，上面的场景，我们可以把对应值设为 1，剧本就会每台主机逐一执行了。\n\n---\n  - hosts: remote\n    serial: 1\n    tasks:\n      - debug:\n          msg: "this is task a"\n      - debug:\n          msg: "this is task b"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n执行结果如下：\n\nansible-playbook -i test_hosts s.yaml\nPLAY [remote] ***********************************************************************************************************************************************************************\nTASK [Gathering Facts] **************************************************************************************************************************************************************\nok: [10.3.22.90]\nTASK [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.90] => {\n    "msg": "this is task a"\n}\nTASK [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.90] => {\n    "msg": "this is task b"\n}\nPLAY [remote] ***********************************************************************************************************************************************************************\nTASK [Gathering Facts] **************************************************************************************************************************************************************\nok: [10.3.22.87]\nTASK [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.87] => {\n    "msg": "this is task a"\n}\nTASK [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.87] => {\n    "msg": "this is task b"\n}\nPLAY RECAP **************************************************************************************************************************************************************************\n10.3.22.87                 : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.3.22.90                 : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n结果与上边的一对比就很容易理解这个参数的含义了。\n\n除了定义 serial: 1，当然还可以定义更多，比如主机列表有 30 台，我们可以定义 serial: 5，就变成 5 台 5 台执行了。\n\n在实际应用中，某些主机执行失败，只会中断当次构建，但不影响其他主机往后执行，这对我们来说并不安全，因为这可能会让没有走完发布流程的主机放回到负载列表中，因此，常常还有一个搭配使用的参数是：max_fail_percentage，表示当最大失败主机的比例达到多少时，ansible 就让整个 play 失败。\n\n---\n  - hosts: remote\n    serial: 1\n    max_fail_percentage: 25\n    tasks:\n      - debug:\n          msg: "this is task a"\n      - debug:\n          msg: "this is task b"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n25 是一个比例数据，表示构建列表中，如果有四分之一的失败，则会退出构建，如果我们希望一有失败就退出，则可以将之设为 0。\n\n注意：当serial: 1时，就不必再用 run_once: true，因为这种定义是无意义的。\n\n\n# 4，打印多行\n\n有时候我们在运行完一些任务之后，希望能够批量打印一些内容返回出来，此时可以使用如下方式进行打印：\n\n$ cat test.yaml\n---\n- hosts: localhost\n  connection: local\n  vars:\n    msg: |\n       lookupd-tcp\n        {{ node1 }}:4160\n        {{ node2 }}:4160\n        {{ node3 }}:4160\n        lookupd-http\n        {{ node1 }}:4161\n        {{ node2 }}:4161\n        {{ node3 }}:4161\n        data-tcp\n        {{ node1 }}:41501\n        {{ node1 }}:41502\n        {{ node2 }}:41501\n        {{ node2 }}:41502\n        {{ node3 }}:41501\n        {{ node3 }}:41502\n        data-http\n        {{ node1 }}:41511\n        {{ node1 }}:41512\n        {{ node2 }}:41511\n        {{ node2 }}:41512\n        {{ node3 }}:41511\n        {{ node3 }}:41512\n  tasks:\n    - name: test\n      debug:\n        msg: "{{ msg.split(\'\\n\') }}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n然后运行看下：\n\n$ansible-playbook test.yaml -e "node1=10.3.0.0 node2=10.0.0.1 node3=10.0.0.2"\n[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match \'all\'\nPLAY [localhost] ***************************************************************************************************************************************************************************************************************************************************************\nTASK [Gathering Facts] *********************************************************************************************************************************************************************************************************************************************************\nok: [localhost]\nTASK [test] ********************************************************************************************************************************************************************************************************************************************************************\nok: [localhost] => {\n    "msg": [\n        "lookupd-tcp",\n        " 10.3.0.0:4160",\n        " 10.0.0.1:4160",\n        " 10.0.0.2:4160",\n        " lookupd-http",\n        " 10.3.0.0:4161",\n        " 10.0.0.1:4161",\n        " 10.0.0.2:4161",\n        " data-tcp",\n        " 10.3.0.0:41501",\n        " 10.3.0.0:41502",\n        " 10.0.0.1:41501",\n        " 10.0.0.1:41502",\n        " 10.0.0.2:41501",\n        " 10.0.0.2:41502",\n        " data-http",\n        " 10.3.0.0:41511",\n        " 10.3.0.0:41512",\n        " 10.0.0.1:41511",\n        " 10.0.0.1:41512",\n        " 10.0.0.2:41511",\n        " 10.0.0.2:41512",\n        ""\n    ]\n}\nPLAY RECAP *********************************************************************************************************************************************************************************************************************************************************************\nlocalhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n\n# 只同步远程目录没有的文件\n\n在Nginx对接consul的场景中，upsync的dump文件一般只需要第一次同步上去，而后就不需要再同步，这个时候可以用rsync的参数来实现：\n\n- name: sync the upsync config\n  synchronize:\n    src: /data/.jenkins/jobs/ops-prod/jobs/ops-nginx-config/workspace/prod-nginx/upsync/\n    dest: /etc/nginx/upsync/\n    mode: push\n    rsync_opts: "--ignore-existing"\n    delete: yes\n    rsync_timeout: 30\n  tags:\n    - prod-nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n--ignore-existing值得是跳过更新已存在于DST的文件。\n\n原文链接：https://wiki.eryajf.net/pages/5173.html',normalizedContent:'# 常用变量\n\n在日常配置编排过程中，我们经常需要用到一些内置的变量来进行一些判断或者配置的工作，这里整理一些常用的变量，以便于使用查阅。\n\n * 部署客户端主机名：ansible_hostname\n * 部署客户端主机 ip：ansible_default_ipv4.address\n * 部署客户端主机详细信息：hostvars，返回主机详细信息，可以通过点操作定位具体需要的内容。\n\n\n# 1，判断错误\n\n有时候我们在部署服务的时候，会针对一些服务状态进行检测，从而依据检测结果来判断是否将主机放回到负载列表当中，这里举一个 web 检测的例子，首先通过 nginx 定义了一个返回值：\n\n$ curl localhost/get_info\n{"status":"success","result":"hello world!"}\n\n\n1\n2\n\n\n然后定义剧本如下：\n\n$ cat site.yaml\n---\n  - hosts: localhost\n    tasks:\n      - uri:\n          url: "http://localhost/get_info"\n          method: get\n        register: webpage\n        failed_when: webpage.status != 200\n      - debug:\n          var: webpage\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n这里使用本机进行验证此类功能，直接通过如下命令即可运行：\n\n$ ansible-playbook site.yaml\n[warning]: provided hosts list is empty, only localhost is available. note that the implicit localhost does not match \'all\'\nplay [localhost] ********************************************************************************************************************************************************************\ntask [gathering facts] **************************************************************************************************************************************************************\nok: [localhost]\ntask [uri] **************************************************************************************************************************************************************************\nok: [localhost]\ntask [debug] ************************************************************************************************************************************************************************\nok: [localhost] => {\n    "webpage": {\n        "changed": false,\n        "connection": "close",\n        "content_length": "44",\n        "content_type": "application/json",\n        "cookies": {},\n        "cookies_string": "",\n        "date": "tue, 02 jun 2020 06:30:53 gmt",\n        "elapsed": 0,\n        "failed": false,\n        "failed_when_result": false,\n        "json": {\n            "result": "hello world!",\n            "status": "success"\n        },\n        "msg": "ok (44 bytes)",\n        "redirected": false,\n        "server": "openresty",\n        "status": 200,\n        "url": "http://localhost/get_info"\n    }\n}\nplay recap **************************************************************************************************************************************************************************\nlocalhost                  : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n上边通过输出的状态码进行判断，除此之外，还可以通过输出当中的具体内容匹配来进行判断，比如上边内容当中，我们看到对应的 json 字段里输出的返回内容，也可以根据具体内容来进行判断，此时调整剧本内容如下：\n\n---\n  - hosts: localhost\n    tasks:\n      - uri:\n          url: "http://localhost/get_info"\n          method: get\n        register: webpage\n        failed_when: webpage.status != 200\n      - fail:\n          msg: "check status field"\n        when: "\'hello\' not in webpage.json.result"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nwebpage.json.result可以将返回的内容定位到相应位置，然后再进行判断，当然不少时候也可以用如下方式判断：\n\n---\n  - hosts: localhost\n    tasks:\n      - uri:\n          url: "http://localhost/get_info"\n          method: get\n          return_content: yes\n        register: webpage\n        failed_when: webpage.status != 200\n      - fail:\n          msg: "check status field"\n        when: "\'hello\' not in webpage.content"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n多了一个参数 return_content: yes， 但需要注意的一个地方是：webpage.content中的 json 字段是无法展开的。\n\n\n# 2，只跑一次\n\nrun_once: true\n\n有一些在列表中的任务，只需要跑一次就好，比如上线部署剧本中，发布列表可能有 10 台，那么如下步骤中，一些地方就不需要执行 10 次了，比如生成版本号以及同步到本地的两个步骤。\n\n---\n- name: "创建远程主机上的版本目录"\n  file: path=/release/{{project}}/{{_version}} state=directory\n  tags: deploy\n- name: "将代码同步到远程主机版本目录"\n  synchronize:\n    src: /{{workspace}}/\n    dest: /release/{{project}}/{{_version}}/\n    delete: yes\n  tags: deploy\n- name: "将项目部署到生产目录"\n  file: path=/data/www/{{project}} state=link src=/release/{{project}}/{{_version}}\n  tags: deploy\n- name: "使版本目录保持五个版本历史"\n  script: chdir=/release/{{project}} keepfive.sh\n  tags: deploy\n- name: "生成远程版本号"\n  shell: "ls /release/{{project}} > /release/{{project}}.log"\n  tags: deploy\n- name: "同步版本号到本地"\n  synchronize: "src=/release/{{project}}.log dest=/root/.jenkins/version/{{job_name}}.log mode=pull"\n  tags: deploy\n- name: "执行自由脚本"\n  script: chdir=/release/{{project}} free.sh\n  tags: deploy\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n通过添加 run_once: true可以定义对应的任务只执行一次：\n\n---\n- name: "创建远程主机上的版本目录"\n  file: path=/release/{{project}}/{{_version}} state=directory\n  tags: deploy\n- name: "将代码同步到远程主机版本目录"\n  synchronize:\n    src: /{{workspace}}/\n    dest: /release/{{project}}/{{_version}}/\n    delete: yes\n  tags: deploy\n- name: "将项目部署到生产目录"\n  file: path=/data/www/{{project}} state=link src=/release/{{project}}/{{_version}}\n  tags: deploy\n- name: "使版本目录保持五个版本历史"\n  script: chdir=/release/{{project}} keepfive.sh\n  tags: deploy\n- name: "生成远程版本号"\n  shell: "ls /release/{{project}} > /release/{{project}}.log"\n  run_once: true\n  tags: deploy\n- name: "同步版本号到本地"\n  synchronize: "src=/release/{{project}}.log dest=/root/.jenkins/version/{{job_name}}.log mode=pull"\n  run_once: true\n  tags: deploy\n- name: "执行自由脚本"\n  script: chdir=/release/{{project}} free.sh\n  tags: deploy\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n这样即便是发布了 10 台机器，那么版本号这两步也就只执行一次。\n\n\n# 3，滚动更新\n\nansible 默认情况下，执行剧本时是从剧本角度出发，每个任务在主机列表中逐一执行，有时候我们发布一些服务，需要一台一台从负载中摘出部署，然后放回去，这个时候，走默认的方向就不太合适了。用如下一个简单例子来说明：\n\n$ cat site.yml\n---\n  - hosts: remote\n    tasks:\n      - debug:\n          msg: "this is task a"\n      - debug:\n          msg: "this is task b"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n执行如上剧本看输出：\n\nansible-playbook -i test_hosts s.yaml\nplay [remote] ***********************************************************************************************************************************************************************\ntask [gathering facts] **************************************************************************************************************************************************************\nok: [10.3.22.90]\nok: [10.3.22.87]\ntask [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.90] => {\n    "msg": "this is task a"\n}\nok: [10.3.22.87] => {\n    "msg": "this is task a"\n}\ntask [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.90] => {\n    "msg": "this is task b"\n}\nok: [10.3.22.87] => {\n    "msg": "this is task b"\n}\nplay recap **************************************************************************************************************************************************************************\n10.3.22.87                 : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.3.22.90                 : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n可以看到两个任务被分开，在两台主机上分别来执行。\n\nansible 中的 serial就是用来控制任务执行数量的参数，上面的场景，我们可以把对应值设为 1，剧本就会每台主机逐一执行了。\n\n---\n  - hosts: remote\n    serial: 1\n    tasks:\n      - debug:\n          msg: "this is task a"\n      - debug:\n          msg: "this is task b"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n执行结果如下：\n\nansible-playbook -i test_hosts s.yaml\nplay [remote] ***********************************************************************************************************************************************************************\ntask [gathering facts] **************************************************************************************************************************************************************\nok: [10.3.22.90]\ntask [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.90] => {\n    "msg": "this is task a"\n}\ntask [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.90] => {\n    "msg": "this is task b"\n}\nplay [remote] ***********************************************************************************************************************************************************************\ntask [gathering facts] **************************************************************************************************************************************************************\nok: [10.3.22.87]\ntask [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.87] => {\n    "msg": "this is task a"\n}\ntask [debug] ************************************************************************************************************************************************************************\nok: [10.3.22.87] => {\n    "msg": "this is task b"\n}\nplay recap **************************************************************************************************************************************************************************\n10.3.22.87                 : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n10.3.22.90                 : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n结果与上边的一对比就很容易理解这个参数的含义了。\n\n除了定义 serial: 1，当然还可以定义更多，比如主机列表有 30 台，我们可以定义 serial: 5，就变成 5 台 5 台执行了。\n\n在实际应用中，某些主机执行失败，只会中断当次构建，但不影响其他主机往后执行，这对我们来说并不安全，因为这可能会让没有走完发布流程的主机放回到负载列表中，因此，常常还有一个搭配使用的参数是：max_fail_percentage，表示当最大失败主机的比例达到多少时，ansible 就让整个 play 失败。\n\n---\n  - hosts: remote\n    serial: 1\n    max_fail_percentage: 25\n    tasks:\n      - debug:\n          msg: "this is task a"\n      - debug:\n          msg: "this is task b"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n25 是一个比例数据，表示构建列表中，如果有四分之一的失败，则会退出构建，如果我们希望一有失败就退出，则可以将之设为 0。\n\n注意：当serial: 1时，就不必再用 run_once: true，因为这种定义是无意义的。\n\n\n# 4，打印多行\n\n有时候我们在运行完一些任务之后，希望能够批量打印一些内容返回出来，此时可以使用如下方式进行打印：\n\n$ cat test.yaml\n---\n- hosts: localhost\n  connection: local\n  vars:\n    msg: |\n       lookupd-tcp\n        {{ node1 }}:4160\n        {{ node2 }}:4160\n        {{ node3 }}:4160\n        lookupd-http\n        {{ node1 }}:4161\n        {{ node2 }}:4161\n        {{ node3 }}:4161\n        data-tcp\n        {{ node1 }}:41501\n        {{ node1 }}:41502\n        {{ node2 }}:41501\n        {{ node2 }}:41502\n        {{ node3 }}:41501\n        {{ node3 }}:41502\n        data-http\n        {{ node1 }}:41511\n        {{ node1 }}:41512\n        {{ node2 }}:41511\n        {{ node2 }}:41512\n        {{ node3 }}:41511\n        {{ node3 }}:41512\n  tasks:\n    - name: test\n      debug:\n        msg: "{{ msg.split(\'\\n\') }}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n然后运行看下：\n\n$ansible-playbook test.yaml -e "node1=10.3.0.0 node2=10.0.0.1 node3=10.0.0.2"\n[warning]: provided hosts list is empty, only localhost is available. note that the implicit localhost does not match \'all\'\nplay [localhost] ***************************************************************************************************************************************************************************************************************************************************************\ntask [gathering facts] *********************************************************************************************************************************************************************************************************************************************************\nok: [localhost]\ntask [test] ********************************************************************************************************************************************************************************************************************************************************************\nok: [localhost] => {\n    "msg": [\n        "lookupd-tcp",\n        " 10.3.0.0:4160",\n        " 10.0.0.1:4160",\n        " 10.0.0.2:4160",\n        " lookupd-http",\n        " 10.3.0.0:4161",\n        " 10.0.0.1:4161",\n        " 10.0.0.2:4161",\n        " data-tcp",\n        " 10.3.0.0:41501",\n        " 10.3.0.0:41502",\n        " 10.0.0.1:41501",\n        " 10.0.0.1:41502",\n        " 10.0.0.2:41501",\n        " 10.0.0.2:41502",\n        " data-http",\n        " 10.3.0.0:41511",\n        " 10.3.0.0:41512",\n        " 10.0.0.1:41511",\n        " 10.0.0.1:41512",\n        " 10.0.0.2:41511",\n        " 10.0.0.2:41512",\n        ""\n    ]\n}\nplay recap *********************************************************************************************************************************************************************************************************************************************************************\nlocalhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n\n# 只同步远程目录没有的文件\n\n在nginx对接consul的场景中，upsync的dump文件一般只需要第一次同步上去，而后就不需要再同步，这个时候可以用rsync的参数来实现：\n\n- name: sync the upsync config\n  synchronize:\n    src: /data/.jenkins/jobs/ops-prod/jobs/ops-nginx-config/workspace/prod-nginx/upsync/\n    dest: /etc/nginx/upsync/\n    mode: push\n    rsync_opts: "--ignore-existing"\n    delete: yes\n    rsync_timeout: 30\n  tags:\n    - prod-nginx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n--ignore-existing值得是跳过更新已存在于dst的文件。\n\n原文链接：https://wiki.eryajf.net/pages/5173.html',charsets:{cjk:!0}},{title:"ansible优秀案例",frontmatter:{title:"ansible优秀案例",date:"2023-01-29T15:16:58.000Z",permalink:"/pages/4fbba1/",categories:["专题","ansible系列文章"],tags:[null],readingShow:"top",description:"command: /sbin/swapoff -a\n  when:\n    swapon.stdout\n  ignore_errors: yes\n`",meta:[{name:"twitter:title",content:"ansible优秀案例"},{name:"twitter:description",content:"command: /sbin/swapoff -a\n  when:\n    swapon.stdout\n  ignore_errors: yes\n`"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/09.ansible%E4%BC%98%E7%A7%80%E6%A1%88%E4%BE%8B.html"},{property:"og:type",content:"article"},{property:"og:title",content:"ansible优秀案例"},{property:"og:description",content:"command: /sbin/swapoff -a\n  when:\n    swapon.stdout\n  ignore_errors: yes\n`"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/09.ansible%E4%BC%98%E7%A7%80%E6%A1%88%E4%BE%8B.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-29T15:16:58.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"ansible优秀案例"},{itemprop:"description",content:"command: /sbin/swapoff -a\n  when:\n    swapon.stdout\n  ignore_errors: yes\n`"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/01.ansible%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0/09.ansible%E4%BC%98%E7%A7%80%E6%A1%88%E4%BE%8B.html",relativePath:"01.专题/01.ansible系列文章/09.ansible优秀案例.md",key:"v-4e6cb445",path:"/pages/4fbba1/",headers:[{level:4,title:"优化的取消交换分区",slug:"优化的取消交换分区",normalizedTitle:"优化的取消交换分区",charIndex:2}],headersStr:"优化的取消交换分区",content:'# 优化的取消交换分区\n\n- name: Remove swapfile from /etc/fstab\n  mount:\n    name: "{{ item }"\n    fstype: swap\n    state: absent\n  with_items:\n    - swap\n    - none\n- name: check swap\n  command: /sbin/swapon -s\n  register: swapon\n  changed_when: no\n- name: Disable swap\n  command: /sbin/swapoff -a\n  when:\n    - swapon.stdout\n  ignore_errors: yes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n- name: shutdown  get the status of swap\n  shell: grep swap /etc/fstab l grep -c "#\n  register: swap\n  ignore errors: yes\n  tags: swap\n- name: shutdown  swapoff -a\n  shell: swapoff -a\n  when: swap.stdout == "0"\n  ignore errors: yes\n  tags: swap\n- name: shutdown | 注释/etc/fstab/swap 那一行\n  replace: dest=/etc/fstab regexp=\'(.*) swap(\\s+) swap\' replace=\'f  swap\\2swap\'\n  when: swap.stdout == "0"\n  ignore errors: yes\n  tags: swap\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n',normalizedContent:'# 优化的取消交换分区\n\n- name: remove swapfile from /etc/fstab\n  mount:\n    name: "{{ item }"\n    fstype: swap\n    state: absent\n  with_items:\n    - swap\n    - none\n- name: check swap\n  command: /sbin/swapon -s\n  register: swapon\n  changed_when: no\n- name: disable swap\n  command: /sbin/swapoff -a\n  when:\n    - swapon.stdout\n  ignore_errors: yes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n- name: shutdown  get the status of swap\n  shell: grep swap /etc/fstab l grep -c "#\n  register: swap\n  ignore errors: yes\n  tags: swap\n- name: shutdown  swapoff -a\n  shell: swapoff -a\n  when: swap.stdout == "0"\n  ignore errors: yes\n  tags: swap\n- name: shutdown | 注释/etc/fstab/swap 那一行\n  replace: dest=/etc/fstab regexp=\'(.*) swap(\\s+) swap\' replace=\'f  swap\\2swap\'\n  when: swap.stdout == "0"\n  ignore errors: yes\n  tags: swap\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n',charsets:{cjk:!0}},{title:"快速部署k8s集群",frontmatter:{title:"快速部署k8s集群",categories:["k8s"],tags:["k8s"],date:"2022-12-09T20:51:06.000Z",permalink:"/pages/fc6bfb/",readingShow:"top",description:"kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。",meta:[{name:"twitter:title",content:"快速部署k8s集群"},{name:"twitter:description",content:"kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/02.k8s/01.%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2k8s%E9%9B%86%E7%BE%A4.html"},{property:"og:type",content:"article"},{property:"og:title",content:"快速部署k8s集群"},{property:"og:description",content:"kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/02.k8s/01.%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2k8s%E9%9B%86%E7%BE%A4.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:51:06.000Z"},{property:"article:tag",content:"k8s"},{itemprop:"name",content:"快速部署k8s集群"},{itemprop:"description",content:"kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/02.k8s/01.%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2k8s%E9%9B%86%E7%BE%A4.html",relativePath:"01.专题/02.k8s/01.快速部署k8s集群.md",key:"v-12346ac4",path:"/pages/fc6bfb/",headers:[{level:2,title:"1. 安装要求",slug:"_1-安装要求",normalizedTitle:"1. 安装要求",charIndex:181},{level:2,title:"2. 准备环境",slug:"_2-准备环境",normalizedTitle:"2. 准备环境",charIndex:355},{level:2,title:"3. 所有节点安装Docker/kubeadm/kubelet",slug:"_3-所有节点安装docker-kubeadm-kubelet",normalizedTitle:"3. 所有节点安装docker/kubeadm/kubelet",charIndex:1060},{level:3,title:"3.1 安装Docker",slug:"_3-1-安装docker",normalizedTitle:"3.1 安装docker",charIndex:1140},{level:3,title:"3.2 添加阿里云YUM软件源",slug:"_3-2-添加阿里云yum软件源",normalizedTitle:"3.2 添加阿里云yum软件源",charIndex:1538},{level:3,title:"3.3 安装kubeadm，kubelet和kubectl",slug:"_3-3-安装kubeadm-kubelet和kubectl",normalizedTitle:"3.3 安装kubeadm，kubelet和kubectl",charIndex:1894},{level:2,title:"4. 部署Kubernetes Master",slug:"_4-部署kubernetes-master",normalizedTitle:"4. 部署kubernetes master",charIndex:2040},{level:2,title:"5. 加入Kubernetes Node",slug:"_5-加入kubernetes-node",normalizedTitle:"5. 加入kubernetes node",charIndex:2524},{level:2,title:"6. 部署CNI网络插件",slug:"_6-部署cni网络插件",normalizedTitle:"6. 部署cni网络插件",charIndex:2891},{level:2,title:"7. 测试kubernetes集群",slug:"_7-测试kubernetes集群",normalizedTitle:"7. 测试kubernetes集群",charIndex:3254}],headersStr:"1. 安装要求 2. 准备环境 3. 所有节点安装Docker/kubeadm/kubelet 3.1 安装Docker 3.2 添加阿里云YUM软件源 3.3 安装kubeadm，kubelet和kubectl 4. 部署Kubernetes Master 5. 加入Kubernetes Node 6. 部署CNI网络插件 7. 测试kubernetes集群",content:"# 快速部署k8s集群\n\nkubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。\n\n这个工具能通过两条指令完成一个kubernetes集群的部署：\n\n# 创建一个 Master 节点\n\n$ kubeadm init\n\n# 将一个 Node 节点加入到当前集群中\n\n$ kubeadm join <Master节点的IP和端口 >\n\n\n# 1. 安装要求\n\n在开始之前，部署Kubernetes集群机器需要满足以下几个条件：\n\n * 一台或多台机器，操作系统 CentOS7.x-86_x64\n * 硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多\n * 可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点\n * 禁止swap分区\n\n\n# 2. 准备环境\n\n角色       IP\nmaster   192.168.1.11\nnode1    192.168.1.12\nnode2    192.168.1.13\n\n# 关闭防火墙\n\nsystemctl stop firewalld\n\nsystemctl disable firewalld\n\n# 关闭selinux\n\nsed -i 's/enforcing/disabled/' /etc/selinux/config # 永久\n\nsetenforce 0 # 临时\n\n# 关闭swap\n\nswapoff -a # 临时\n\nsed -ri 's/.swap./#&/' /etc/fstab # 永久\n\n# 根据规划设置主机名\n\nhostnamectl set-hostname\n\n# 在master添加hosts\n\ncat >> /etc/hosts << EOF\n\n192.168.44.146 k8smaster\n\n192.168.44.145 k8snode1\n\n192.168.44.144 k8snode2\n\nEOF\n\n# 将桥接的IPv4流量传递到iptables的链\n\ncat > /etc/sysctl.d/k8s.conf << EOF\n\nnet.bridge.bridge-nf-call-ip6tables = 1\n\nnet.bridge.bridge-nf-call-iptables = 1\n\nEOF\n\nsysctl --system # 生效\n\n# 时间同步\n\nyum install ntpdate -y\n\nntpdate time.windows.com\n\n\n# 3. 所有节点安装Docker/kubeadm/kubelet\n\nKubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。\n\n\n# 3.1 安装Docker\n\n$ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n\n$ yum -y install docker-ce-18.06.1.ce-3.el7\n\n$ systemctl enable docker && systemctl start docker\n\n$ docker --version\n\nDocker version 18.06.1-ce, build e68fc7a\n\n$ cat > /etc/docker/daemon.json << EOF\n\n{\n\n\"registry-mirrors\": [\"https://b9pmyelo.mirror.aliyuncs.com\"]\n\n}\n\nEOF\n\n\n# 3.2 添加阿里云YUM软件源\n\n$ cat > /etc/yum.repos.d/kubernetes.repo << EOF\n\n[kubernetes]\n\nname=Kubernetes\n\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\n\nenabled=1\n\ngpgcheck=0\n\nrepo_gpgcheck=0\n\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\n\nEOF\n\n\n# 3.3 安装kubeadm，kubelet和kubectl\n\n由于版本更新频繁，这里指定版本号部署：\n\n$ yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0\n\n$ systemctl enable kubelet\n\n\n# 4. 部署Kubernetes Master\n\n在192.168.1.11（Master）执行。\n\n$ kubeadm init \\\n\n--apiserver-advertise-address=192.168.31.61 \\\n\n--image-repository registry.aliyuncs.com/google_containers \\\n\n--kubernetes-version v1.18.0 \\\n\n--service-cidr=10.96.0.0/12 \\\n\n--pod-network-cidr=10.244.0.0/16\n\n由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。\n\n使用kubectl工具：\n\nmkdir -p $HOME/.kube\n\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n\nsudo chown $(id -u)😒(id -g) $HOME/.kube/config\n\n$ kubectl get nodes\n\n\n# 5. 加入Kubernetes Node\n\n在192.168.1.12/13（Node）执行。\n\n向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：\n\n$ kubeadm join 192.168.1.11:6443 --token esce21.q6hetwm8si29qxwn \\\n\n--discovery-token-ca-cert-hash sha256:00603a05805807501d7181c3d60b478788408cfe6cedefedb1f97569708be9c5\n\n默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下：\n\nkubeadm token create --print-join-command\n\n\n# 6. 部署CNI网络插件\n\nwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\n默认镜像地址无法访问，sed命令修改为docker hub镜像仓库。\n\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\nkubectl get pods -n kube-system\n\nNAME READY STATUS RESTARTS AGE\n\nkube-flannel-ds-amd64-2pc95 1/1 Running 0 72s\n\n\n# 7. 测试kubernetes集群\n\n在Kubernetes集群中创建一个pod，验证是否正常运行：\n\n$ kubectl create deployment nginx --image=nginx\n\n$ kubectl expose deployment nginx --port=80 --type=NodePort\n\n$ kubectl get pod,svc\n\n访问地址：http://NodeIP:Port",normalizedContent:"# 快速部署k8s集群\n\nkubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。\n\n这个工具能通过两条指令完成一个kubernetes集群的部署：\n\n# 创建一个 master 节点\n\n$ kubeadm init\n\n# 将一个 node 节点加入到当前集群中\n\n$ kubeadm join <master节点的ip和端口 >\n\n\n# 1. 安装要求\n\n在开始之前，部署kubernetes集群机器需要满足以下几个条件：\n\n * 一台或多台机器，操作系统 centos7.x-86_x64\n * 硬件配置：2gb或更多ram，2个cpu或更多cpu，硬盘30gb或更多\n * 可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点\n * 禁止swap分区\n\n\n# 2. 准备环境\n\n角色       ip\nmaster   192.168.1.11\nnode1    192.168.1.12\nnode2    192.168.1.13\n\n# 关闭防火墙\n\nsystemctl stop firewalld\n\nsystemctl disable firewalld\n\n# 关闭selinux\n\nsed -i 's/enforcing/disabled/' /etc/selinux/config # 永久\n\nsetenforce 0 # 临时\n\n# 关闭swap\n\nswapoff -a # 临时\n\nsed -ri 's/.swap./#&/' /etc/fstab # 永久\n\n# 根据规划设置主机名\n\nhostnamectl set-hostname\n\n# 在master添加hosts\n\ncat >> /etc/hosts << eof\n\n192.168.44.146 k8smaster\n\n192.168.44.145 k8snode1\n\n192.168.44.144 k8snode2\n\neof\n\n# 将桥接的ipv4流量传递到iptables的链\n\ncat > /etc/sysctl.d/k8s.conf << eof\n\nnet.bridge.bridge-nf-call-ip6tables = 1\n\nnet.bridge.bridge-nf-call-iptables = 1\n\neof\n\nsysctl --system # 生效\n\n# 时间同步\n\nyum install ntpdate -y\n\nntpdate time.windows.com\n\n\n# 3. 所有节点安装docker/kubeadm/kubelet\n\nkubernetes默认cri（容器运行时）为docker，因此先安装docker。\n\n\n# 3.1 安装docker\n\n$ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo\n\n$ yum -y install docker-ce-18.06.1.ce-3.el7\n\n$ systemctl enable docker && systemctl start docker\n\n$ docker --version\n\ndocker version 18.06.1-ce, build e68fc7a\n\n$ cat > /etc/docker/daemon.json << eof\n\n{\n\n\"registry-mirrors\": [\"https://b9pmyelo.mirror.aliyuncs.com\"]\n\n}\n\neof\n\n\n# 3.2 添加阿里云yum软件源\n\n$ cat > /etc/yum.repos.d/kubernetes.repo << eof\n\n[kubernetes]\n\nname=kubernetes\n\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\n\nenabled=1\n\ngpgcheck=0\n\nrepo_gpgcheck=0\n\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\n\neof\n\n\n# 3.3 安装kubeadm，kubelet和kubectl\n\n由于版本更新频繁，这里指定版本号部署：\n\n$ yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0\n\n$ systemctl enable kubelet\n\n\n# 4. 部署kubernetes master\n\n在192.168.1.11（master）执行。\n\n$ kubeadm init \\\n\n--apiserver-advertise-address=192.168.31.61 \\\n\n--image-repository registry.aliyuncs.com/google_containers \\\n\n--kubernetes-version v1.18.0 \\\n\n--service-cidr=10.96.0.0/12 \\\n\n--pod-network-cidr=10.244.0.0/16\n\n由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。\n\n使用kubectl工具：\n\nmkdir -p $home/.kube\n\nsudo cp -i /etc/kubernetes/admin.conf $home/.kube/config\n\nsudo chown $(id -u)😒(id -g) $home/.kube/config\n\n$ kubectl get nodes\n\n\n# 5. 加入kubernetes node\n\n在192.168.1.12/13（node）执行。\n\n向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：\n\n$ kubeadm join 192.168.1.11:6443 --token esce21.q6hetwm8si29qxwn \\\n\n--discovery-token-ca-cert-hash sha256:00603a05805807501d7181c3d60b478788408cfe6cedefedb1f97569708be9c5\n\n默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下：\n\nkubeadm token create --print-join-command\n\n\n# 6. 部署cni网络插件\n\nwget https://raw.githubusercontent.com/coreos/flannel/master/documentation/kube-flannel.yml\n\n默认镜像地址无法访问，sed命令修改为docker hub镜像仓库。\n\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/documentation/kube-flannel.yml\n\nkubectl get pods -n kube-system\n\nname ready status restarts age\n\nkube-flannel-ds-amd64-2pc95 1/1 running 0 72s\n\n\n# 7. 测试kubernetes集群\n\n在kubernetes集群中创建一个pod，验证是否正常运行：\n\n$ kubectl create deployment nginx --image=nginx\n\n$ kubectl expose deployment nginx --port=80 --type=nodeport\n\n$ kubectl get pod,svc\n\n访问地址：http://nodeip:port",charsets:{cjk:!0}},{title:"elk安装",frontmatter:{title:"elk安装",categories:"elk",tags:["elk"],date:"2022-12-09T20:50:18.000Z",permalink:"/pages/98c9f5/",readingShow:"top",description:"elasticsearch 官网下载地址：\nhttps://www.elastic.co/cn/downloads/elasticsearch\nelasticsearch 历史版本下载地址：\n https://www.elastic.co/cn/downloads/past-releases#elasticsearch",meta:[{name:"twitter:title",content:"elk安装"},{name:"twitter:description",content:"elasticsearch 官网下载地址：\nhttps://www.elastic.co/cn/downloads/elasticsearch\nelasticsearch 历史版本下载地址：\n https://www.elastic.co/cn/downloads/past-releases#elasticsearch"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/03.elk/01.elk%E5%AE%89%E8%A3%85.html"},{property:"og:type",content:"article"},{property:"og:title",content:"elk安装"},{property:"og:description",content:"elasticsearch 官网下载地址：\nhttps://www.elastic.co/cn/downloads/elasticsearch\nelasticsearch 历史版本下载地址：\n https://www.elastic.co/cn/downloads/past-releases#elasticsearch"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/03.elk/01.elk%E5%AE%89%E8%A3%85.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:50:18.000Z"},{property:"article:tag",content:"elk"},{itemprop:"name",content:"elk安装"},{itemprop:"description",content:"elasticsearch 官网下载地址：\nhttps://www.elastic.co/cn/downloads/elasticsearch\nelasticsearch 历史版本下载地址：\n https://www.elastic.co/cn/downloads/past-releases#elasticsearch"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/03.elk/01.elk%E5%AE%89%E8%A3%85.html",relativePath:"01.专题/03.elk/01.elk安装.md",key:"v-5a9f7a7c",path:"/pages/98c9f5/",headers:[{level:2,title:"ELK服务部署",slug:"elk服务部署",normalizedTitle:"elk服务部署",charIndex:2},{level:3,title:"官网下载对应版本的二进制压缩包",slug:"官网下载对应版本的二进制压缩包",normalizedTitle:"官网下载对应版本的二进制压缩包",charIndex:14},{level:3,title:"解压、准备工作、启动ELK",slug:"解压、准备工作、启动elk",normalizedTitle:"解压、准备工作、启动elk",charIndex:504}],headersStr:"ELK服务部署 官网下载对应版本的二进制压缩包 解压、准备工作、启动ELK",content:'# ELK服务部署\n\n\n# 官网下载对应版本的二进制压缩包\n\nelasticsearch 官网下载地址： https://www.elastic.co/cn/downloads/elasticsearch elasticsearch 历史版本下载地址： https://www.elastic.co/cn/downloads/past-releases#elasticsearch\n\nkinbana 官网下载地址： https://www.elastic.co/cn/downloads/kibana kinbana 历史版本下载地址： https://www.elastic.co/cn/downloads/past-releases#kibana\n\nlogstash 官网下载地址： https://www.elastic.co/cn/downloads/logstash logstash 历史版本下载地址： https://www.elastic.co/cn/downloads/past-releases\n\n----------------------------------------\n\n\n# 解压、准备工作、启动ELK\n\nelasticsearch.tar.gz文件解压\n\ntar -zxvf 就可以啦 太基础的废话命令就不罗列啦，主要是提几个关键的细节的点 比如elasticsearch部署的时候需要注意：\n\n 1. 使用普通用户\n 2. 开启max_map_count\n 3. 设置对应普通用户limit.conf\n 4. useradd -d /data/es es &&echo es | passwd --stdin es\n 5. echo "vm.max_map_count=655360" >> /etc/sysctl.conf\n 6. echo "es soft nofile 655350" >> /etc/security/limits.conf && echo "es hard nofile 655350" >> /etc/security/limits.conf sysctl -p 刷新一下另其生效\n\n然后准备个对应的jdk配置个全局的环境变量：\n\n> JAVA_HOME=/opt/jdk1.8.0_65 PATH=$JAVA_HOME/bin:$PATH CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 追加至系统环境变量文件尾部即可或者普通用户的 /etc/profile .bash_profile .bashrc 然后给刷新一下环境变量或者exit退出用户重新登录即可刷新 登录普通用户记得带上杠表示带着普通用户的环境变量呢 su - es source /etc/profile 当我使用elasticsearch-no-jdk-7.9.6.tar.gz版本包时,启动提示要找本地普通用户家目录下面的java 例如上面的es用户 elastic为解压包 /data/es/elastic/jdk/bin/java 配置了全局的环境变量 指定目录的opt jdk java都不行还要去找上面目录的java 此时我们可以懒得理他直接根据启动log错误信息解决即可啦 使用“小大招”直接软连接射过去就好啦 当然前提对应的elastic解压包目录要创建对应的log提示的路劲目录 不然软连接找不到上一层目录无法创建哒 mkdir -p /data/es/elastic/jdk/bin/ ln -sf /opt/jdk/java/bin/java /data/es/elastic/jdk/bin/\n\n然后配置改吧改吧修改修改elasticsearch.yml文件\n\n> cat elasticsearch/config/elasticsearch.yml cluster.name: pengge #bootstrap.mlockall: true thread_pool.search.size: 10 thread_pool.search.queue_size: 3000 thread_pool.bulk.size: 5 thread_pool.bulk.queue_size: 2000 thread_pool.index.size: 5 thread_pool.index.queue_size: 1500 node.name: node-pengge cluster.initial_master_nodes: ["node-pengge"] path.data: /data/es/elastic/data path.logs: /data/elastic/logs network.host: 192.168.108.8 http.port: 9200 transport.tcp.port: 9300 transport.tcp.compress: false discovery.zen.ping.unicast.hosts: ["192.168.108.8", "192.168.108.9", "192.168.108.10"]\n\n然后就可以启动es节点啦，上面实例yml配置文件是配置的三节点集群，多节点集群也是一样的道理逗号添加集群节点ip即可啦，简简单单部署那是十分的轻松啦 ./elasticsearch &> esstart.log & && tail -f esstart.log 直接sh或者./启动二进制脚本&>将屏幕标准正确错误输出输出至本地目录的esstart.log命名的启动日志文件内，同时使用tail -f 观察es服务启动情况即可\n\n下面附上一些基础的es查询语句： 命令行get方式查询，pengge为示例索引名称\n\n> curl 192.168.108.8:9200/_cat/indices curl 192.168.108.8:9200/_cat/health curl 192.168.108.8:9200/_cat/nodes curl 192.168.108.8:9200/_cat/shards curl 192.168.108.8:9200/_cat/allocation curl 192.168.108.8:9200/pengge/_count curl 192.168.108.8:9200/pengge/_search kibana页面方式查询： GET _cat/indices GET _cat/health GET _cat/nodes GET _cat/shards GET _cat/allocation GET /pengge/_count GET /pengge/_search\n\n删除索引\nDELETE pengge*\n删除索引内的指定字段，一定注意删除字段是这个动作参数哦：_update_by_query\n删除字段：@timestamp\nPOST /pengge/_update_by_query\n{\n "script": {\n "inline": "ctx._source.remove(\'@timestamp\')",\n "lang": "painless"\n },\n "query": {\n "bool": {\n "must": [\n {\n "exists": {\n "field": "@timestamp"\n }\n }\n ]\n }\n }\n}\n\n\n***至此elasticsearch服务就简简单单的启动成功啦 ***\n\n至于kibana、logstash一样的道理简单解压./启动即可啦 不在废话叙述啦 简单配置个es 9200读取的节点就可以了如下：\n\n[root@zabbix config]# egrep -v "#|^$" kibana.yml\nserver.port: 5601\nserver.host: "192.168.108.143"\nelasticsearch.hosts: ["http://192.168.108.9:9200"]\n\n./kibana &> start.log & && tail -f start.log\n\n\nlogstash的话主要就是为了针对log日志文件或者txt csv等源数据文件进行整理汇总切割推送给es从而在kibana展示，细节东西下篇文章进行讲解，本期内容就简简单单写写部署启动完事啦 logstash配置的job conf文件可以使用-t参数简单配置的字段语法是否正确如下：\n\n[root@zabbix bin]# ./logstash -f job/csv-es-debug.conf -t\nSending Logstash logs to /data/kinbana/logstash7.8.1/logstash-7.8.1/logs which is now configured via log4j2.properties\n[2021-03-09T15:24:03,282][WARN ][logstash.config.source.multilocal] Ignoring the \'pipelines.yml\' file because modules or command line options are specified\n[2021-03-09T15:24:05,485][INFO ][org.reflections.Reflections] Reflections took 46 ms to scan 1 urls, producing 21 keys and 41 values\nConfiguration OK\n[2021-03-09T15:24:06,291][INFO ][logstash.runner ] Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash\n\n\n完事就可以-f指定 & 放入后台进行运行使用可爱的logstash啦~\n\n----------------------------------------',normalizedContent:'# elk服务部署\n\n\n# 官网下载对应版本的二进制压缩包\n\nelasticsearch 官网下载地址： https://www.elastic.co/cn/downloads/elasticsearch elasticsearch 历史版本下载地址： https://www.elastic.co/cn/downloads/past-releases#elasticsearch\n\nkinbana 官网下载地址： https://www.elastic.co/cn/downloads/kibana kinbana 历史版本下载地址： https://www.elastic.co/cn/downloads/past-releases#kibana\n\nlogstash 官网下载地址： https://www.elastic.co/cn/downloads/logstash logstash 历史版本下载地址： https://www.elastic.co/cn/downloads/past-releases\n\n----------------------------------------\n\n\n# 解压、准备工作、启动elk\n\nelasticsearch.tar.gz文件解压\n\ntar -zxvf 就可以啦 太基础的废话命令就不罗列啦，主要是提几个关键的细节的点 比如elasticsearch部署的时候需要注意：\n\n 1. 使用普通用户\n 2. 开启max_map_count\n 3. 设置对应普通用户limit.conf\n 4. useradd -d /data/es es &&echo es | passwd --stdin es\n 5. echo "vm.max_map_count=655360" >> /etc/sysctl.conf\n 6. echo "es soft nofile 655350" >> /etc/security/limits.conf && echo "es hard nofile 655350" >> /etc/security/limits.conf sysctl -p 刷新一下另其生效\n\n然后准备个对应的jdk配置个全局的环境变量：\n\n> java_home=/opt/jdk1.8.0_65 path=$java_home/bin:$path classpath=.:$java_home/lib/dt.jar:$java_home/lib/tools.jar 追加至系统环境变量文件尾部即可或者普通用户的 /etc/profile .bash_profile .bashrc 然后给刷新一下环境变量或者exit退出用户重新登录即可刷新 登录普通用户记得带上杠表示带着普通用户的环境变量呢 su - es source /etc/profile 当我使用elasticsearch-no-jdk-7.9.6.tar.gz版本包时,启动提示要找本地普通用户家目录下面的java 例如上面的es用户 elastic为解压包 /data/es/elastic/jdk/bin/java 配置了全局的环境变量 指定目录的opt jdk java都不行还要去找上面目录的java 此时我们可以懒得理他直接根据启动log错误信息解决即可啦 使用“小大招”直接软连接射过去就好啦 当然前提对应的elastic解压包目录要创建对应的log提示的路劲目录 不然软连接找不到上一层目录无法创建哒 mkdir -p /data/es/elastic/jdk/bin/ ln -sf /opt/jdk/java/bin/java /data/es/elastic/jdk/bin/\n\n然后配置改吧改吧修改修改elasticsearch.yml文件\n\n> cat elasticsearch/config/elasticsearch.yml cluster.name: pengge #bootstrap.mlockall: true thread_pool.search.size: 10 thread_pool.search.queue_size: 3000 thread_pool.bulk.size: 5 thread_pool.bulk.queue_size: 2000 thread_pool.index.size: 5 thread_pool.index.queue_size: 1500 node.name: node-pengge cluster.initial_master_nodes: ["node-pengge"] path.data: /data/es/elastic/data path.logs: /data/elastic/logs network.host: 192.168.108.8 http.port: 9200 transport.tcp.port: 9300 transport.tcp.compress: false discovery.zen.ping.unicast.hosts: ["192.168.108.8", "192.168.108.9", "192.168.108.10"]\n\n然后就可以启动es节点啦，上面实例yml配置文件是配置的三节点集群，多节点集群也是一样的道理逗号添加集群节点ip即可啦，简简单单部署那是十分的轻松啦 ./elasticsearch &> esstart.log & && tail -f esstart.log 直接sh或者./启动二进制脚本&>将屏幕标准正确错误输出输出至本地目录的esstart.log命名的启动日志文件内，同时使用tail -f 观察es服务启动情况即可\n\n下面附上一些基础的es查询语句： 命令行get方式查询，pengge为示例索引名称\n\n> curl 192.168.108.8:9200/_cat/indices curl 192.168.108.8:9200/_cat/health curl 192.168.108.8:9200/_cat/nodes curl 192.168.108.8:9200/_cat/shards curl 192.168.108.8:9200/_cat/allocation curl 192.168.108.8:9200/pengge/_count curl 192.168.108.8:9200/pengge/_search kibana页面方式查询： get _cat/indices get _cat/health get _cat/nodes get _cat/shards get _cat/allocation get /pengge/_count get /pengge/_search\n\n删除索引\ndelete pengge*\n删除索引内的指定字段，一定注意删除字段是这个动作参数哦：_update_by_query\n删除字段：@timestamp\npost /pengge/_update_by_query\n{\n "script": {\n "inline": "ctx._source.remove(\'@timestamp\')",\n "lang": "painless"\n },\n "query": {\n "bool": {\n "must": [\n {\n "exists": {\n "field": "@timestamp"\n }\n }\n ]\n }\n }\n}\n\n\n***至此elasticsearch服务就简简单单的启动成功啦 ***\n\n至于kibana、logstash一样的道理简单解压./启动即可啦 不在废话叙述啦 简单配置个es 9200读取的节点就可以了如下：\n\n[root@zabbix config]# egrep -v "#|^$" kibana.yml\nserver.port: 5601\nserver.host: "192.168.108.143"\nelasticsearch.hosts: ["http://192.168.108.9:9200"]\n\n./kibana &> start.log & && tail -f start.log\n\n\nlogstash的话主要就是为了针对log日志文件或者txt csv等源数据文件进行整理汇总切割推送给es从而在kibana展示，细节东西下篇文章进行讲解，本期内容就简简单单写写部署启动完事啦 logstash配置的job conf文件可以使用-t参数简单配置的字段语法是否正确如下：\n\n[root@zabbix bin]# ./logstash -f job/csv-es-debug.conf -t\nsending logstash logs to /data/kinbana/logstash7.8.1/logstash-7.8.1/logs which is now configured via log4j2.properties\n[2021-03-09t15:24:03,282][warn ][logstash.config.source.multilocal] ignoring the \'pipelines.yml\' file because modules or command line options are specified\n[2021-03-09t15:24:05,485][info ][org.reflections.reflections] reflections took 46 ms to scan 1 urls, producing 21 keys and 41 values\nconfiguration ok\n[2021-03-09t15:24:06,291][info ][logstash.runner ] using config.test_and_exit mode. config validation result: ok. exiting logstash\n\n\n完事就可以-f指定 & 放入后台进行运行使用可爱的logstash啦~\n\n----------------------------------------',charsets:{cjk:!0}},{title:"docker-compose安装elk",frontmatter:{title:"docker-compose安装elk",date:"2023-01-13T16:45:28.000Z",permalink:"/pages/bbe108/",categories:["专题","elk"],tags:[null],readingShow:"top",description:'version: "3"\nservices:\n  es-master:\n    container_name: es-master\n    hostname: es-master\n    image: elasticsearch:7.12.1\n    restart: always\n    ports:\n      9200:9200\n      9300:9300\n    volumes:\n      ./elasticsearch/master/conf/es-master.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n      /data/elk/es-master/data:/usr/share/elasticsearch/data\n      /data/elk/es-master/logs:/usr/share/elasticsearch/logs\n      /etc/localtime:/etc/localtime\n      ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12\n    environment:\n      "ESJAVAOPTS=-Xms1024m -Xmx1024m"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1',meta:[{name:"twitter:title",content:"docker-compose安装elk"},{name:"twitter:description",content:'version: "3"\nservices:\n  es-master:\n    container_name: es-master\n    hostname: es-master\n    image: elasticsearch:7.12.1\n    restart: always\n    ports:\n      9200:9200\n      9300:9300\n    volumes:\n      ./elasticsearch/master/conf/es-master.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n      /data/elk/es-master/data:/usr/share/elasticsearch/data\n      /data/elk/es-master/logs:/usr/share/elasticsearch/logs\n      /etc/localtime:/etc/localtime\n      ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12\n    environment:\n      "ESJAVAOPTS=-Xms1024m -Xmx1024m"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1'},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/03.elk/02.docker-compose%E5%AE%89%E8%A3%85elk.html"},{property:"og:type",content:"article"},{property:"og:title",content:"docker-compose安装elk"},{property:"og:description",content:'version: "3"\nservices:\n  es-master:\n    container_name: es-master\n    hostname: es-master\n    image: elasticsearch:7.12.1\n    restart: always\n    ports:\n      9200:9200\n      9300:9300\n    volumes:\n      ./elasticsearch/master/conf/es-master.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n      /data/elk/es-master/data:/usr/share/elasticsearch/data\n      /data/elk/es-master/logs:/usr/share/elasticsearch/logs\n      /etc/localtime:/etc/localtime\n      ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12\n    environment:\n      "ESJAVAOPTS=-Xms1024m -Xmx1024m"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1'},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/03.elk/02.docker-compose%E5%AE%89%E8%A3%85elk.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-13T16:45:28.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"docker-compose安装elk"},{itemprop:"description",content:'version: "3"\nservices:\n  es-master:\n    container_name: es-master\n    hostname: es-master\n    image: elasticsearch:7.12.1\n    restart: always\n    ports:\n      9200:9200\n      9300:9300\n    volumes:\n      ./elasticsearch/master/conf/es-master.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n      /data/elk/es-master/data:/usr/share/elasticsearch/data\n      /data/elk/es-master/logs:/usr/share/elasticsearch/logs\n      /etc/localtime:/etc/localtime\n      ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12\n    environment:\n      "ESJAVAOPTS=-Xms1024m -Xmx1024m"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1'}]},regularPath:"/01.%E4%B8%93%E9%A2%98/03.elk/02.docker-compose%E5%AE%89%E8%A3%85elk.html",relativePath:"01.专题/03.elk/02.docker-compose安装elk.md",key:"v-4b66e9c6",path:"/pages/bbe108/",headers:[{level:3,title:"安装文件",slug:"安装文件",normalizedTitle:"安装文件",charIndex:2}],headersStr:"安装文件",content:'# 安装文件\n\nversion: "3"\nservices:\n  es-master:\n    container_name: es-master\n    hostname: es-master\n    image: elasticsearch:7.12.1\n    restart: always\n    ports:\n      - 9200:9200\n      - 9300:9300\n    volumes:\n      - ./elasticsearch/master/conf/es-master.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n      - /data/elk/es-master/data:/usr/share/elasticsearch/data\n      - /data/elk/es-master/logs:/usr/share/elasticsearch/logs\n      - /etc/localtime:/etc/localtime\n      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12\n    environment:\n      - "ES_JAVA_OPTS=-Xms1024m -Xmx1024m"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n\n  es-slave1:\n    container_name: es-slave1\n    image: elasticsearch:7.12.1\n    restart: always\n    ports:\n      - 9201:9200\n      #- 9301:9300\n    volumes:\n      - ./elasticsearch/slave1/conf/es-slave1.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n      - /data/elk/es-slave1/data:/usr/share/elasticsearch/data\n      - /data/elk/es-slave1/logs:/usr/share/elasticsearch/logs\n      - /etc/localtime:/etc/localtime\n      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12\n    environment:\n      - "ES_JAVA_OPTS=-Xms1024m -Xmx1024m"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n\n  es-slave2:\n    container_name: es-slave2\n    image: elasticsearch:7.12.1\n    restart: always\n    ports:\n      - 9202:9200\n      #- 9302:9300\n    volumes:\n      - ./elasticsearch/slave2/conf/es-slave2.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n      - /data/elk/es-slave2/data:/usr/share/elasticsearch/data\n      - /data/elk/es-slave2/logs:/usr/share/elasticsearch/logs\n      - /etc/localtime:/etc/localtime\n      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12\n    environment:\n      - "ES_JAVA_OPTS=-Xms1024m -Xmx1024m"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n\n  kibana:\n    container_name: kibana\n    hostname: kibana\n    image: kibana:7.12.1\n    restart: always\n    ports:\n      - 5601:5601\n    volumes:\n      - ./kibana/conf/kibana.yml:/usr/share/kibana/config/kibana.yml\n      - /etc/localtime:/etc/localtime\n    environment:\n      - elasticsearch.hosts=http://es-master:9200\n    depends_on:\n      - es-master\n      - es-slave1\n      - es-slave2\n  logstash:\n    container_name: logstash\n    hostname: logstash\n    image: logstash:7.12.1\n    command: logstash -f ./conf/logstash-kafka.conf\n    restart: always\n    volumes:\n      # 映射到容器中\n      - ./logstash/conf/logstash-kafka.conf:/usr/share/logstash/conf/logstash-kafka.conf\n      - ./logstash/ssl:/usr/share/logstash/ssl\n      - ./logstash/template:/template\n    environment:\n      - elasticsearch.hosts=http://es-master:9200\n      - /etc/localtime:/etc/localtime\n      # 解决logstash监控连接报错\n      - xpack.monitoring.elasticsearch.hosts=http://es-master:9200\n      - xpack.monitoring.elasticsearch.username="logstash_system"\n      - xpack.monitoring.elasticsearch.password="xxx"\n    ports:\n      - 5044:5044\n    depends_on:\n      - es-master\n      - es-slave1\n      - es-slave2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n',normalizedContent:'# 安装文件\n\nversion: "3"\nservices:\n  es-master:\n    container_name: es-master\n    hostname: es-master\n    image: elasticsearch:7.12.1\n    restart: always\n    ports:\n      - 9200:9200\n      - 9300:9300\n    volumes:\n      - ./elasticsearch/master/conf/es-master.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n      - /data/elk/es-master/data:/usr/share/elasticsearch/data\n      - /data/elk/es-master/logs:/usr/share/elasticsearch/logs\n      - /etc/localtime:/etc/localtime\n      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12\n    environment:\n      - "es_java_opts=-xms1024m -xmx1024m"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n\n  es-slave1:\n    container_name: es-slave1\n    image: elasticsearch:7.12.1\n    restart: always\n    ports:\n      - 9201:9200\n      #- 9301:9300\n    volumes:\n      - ./elasticsearch/slave1/conf/es-slave1.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n      - /data/elk/es-slave1/data:/usr/share/elasticsearch/data\n      - /data/elk/es-slave1/logs:/usr/share/elasticsearch/logs\n      - /etc/localtime:/etc/localtime\n      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12\n    environment:\n      - "es_java_opts=-xms1024m -xmx1024m"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n\n  es-slave2:\n    container_name: es-slave2\n    image: elasticsearch:7.12.1\n    restart: always\n    ports:\n      - 9202:9200\n      #- 9302:9300\n    volumes:\n      - ./elasticsearch/slave2/conf/es-slave2.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n      - /data/elk/es-slave2/data:/usr/share/elasticsearch/data\n      - /data/elk/es-slave2/logs:/usr/share/elasticsearch/logs\n      - /etc/localtime:/etc/localtime\n      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12\n    environment:\n      - "es_java_opts=-xms1024m -xmx1024m"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n\n  kibana:\n    container_name: kibana\n    hostname: kibana\n    image: kibana:7.12.1\n    restart: always\n    ports:\n      - 5601:5601\n    volumes:\n      - ./kibana/conf/kibana.yml:/usr/share/kibana/config/kibana.yml\n      - /etc/localtime:/etc/localtime\n    environment:\n      - elasticsearch.hosts=http://es-master:9200\n    depends_on:\n      - es-master\n      - es-slave1\n      - es-slave2\n  logstash:\n    container_name: logstash\n    hostname: logstash\n    image: logstash:7.12.1\n    command: logstash -f ./conf/logstash-kafka.conf\n    restart: always\n    volumes:\n      # 映射到容器中\n      - ./logstash/conf/logstash-kafka.conf:/usr/share/logstash/conf/logstash-kafka.conf\n      - ./logstash/ssl:/usr/share/logstash/ssl\n      - ./logstash/template:/template\n    environment:\n      - elasticsearch.hosts=http://es-master:9200\n      - /etc/localtime:/etc/localtime\n      # 解决logstash监控连接报错\n      - xpack.monitoring.elasticsearch.hosts=http://es-master:9200\n      - xpack.monitoring.elasticsearch.username="logstash_system"\n      - xpack.monitoring.elasticsearch.password="xxx"\n    ports:\n      - 5044:5044\n    depends_on:\n      - es-master\n      - es-slave1\n      - es-slave2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n',charsets:{cjk:!0}},{title:"filebeat-log相关配置指南",frontmatter:{title:"filebeat-log相关配置指南",date:"2023-02-23T09:01:59.000Z",permalink:"/pages/ed9e8d/",categories:["专题","elk"],tags:[null],readingShow:"top",description:"本文主要介绍 Filebeat 7.5 版本中 Log 相关的各个配置项的含义以及其应用场景。",meta:[{name:"twitter:title",content:"filebeat-log相关配置指南"},{name:"twitter:description",content:"本文主要介绍 Filebeat 7.5 版本中 Log 相关的各个配置项的含义以及其应用场景。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/03.elk/03.filebeat-log%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97.html"},{property:"og:type",content:"article"},{property:"og:title",content:"filebeat-log相关配置指南"},{property:"og:description",content:"本文主要介绍 Filebeat 7.5 版本中 Log 相关的各个配置项的含义以及其应用场景。"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/03.elk/03.filebeat-log%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-23T09:01:59.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"filebeat-log相关配置指南"},{itemprop:"description",content:"本文主要介绍 Filebeat 7.5 版本中 Log 相关的各个配置项的含义以及其应用场景。"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/03.elk/03.filebeat-log%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97.html",relativePath:"01.专题/03.elk/03.filebeat-log相关配置指南.md",key:"v-dd6a2a08",path:"/pages/ed9e8d/",headers:[{level:2,title:"paths",slug:"paths",normalizedTitle:"paths",charIndex:85},{level:3,title:"Glob 模式",slug:"glob-模式",normalizedTitle:"glob 模式",charIndex:468},{level:3,title:"递归的 Glob 模式",slug:"递归的-glob-模式",normalizedTitle:"递归的 glob 模式",charIndex:833},{level:2,title:"recursive_glob.enabled:",slug:"recursive-glob-enabled",normalizedTitle:"recursive_glob.enabled:",charIndex:1481},{level:2,title:"exclude_lines",slug:"exclude-lines",normalizedTitle:"exclude_lines",charIndex:1577},{level:2,title:"include_lines",slug:"include-lines",normalizedTitle:"include_lines",charIndex:1717},{level:2,title:"exclude_files",slug:"exclude-files",normalizedTitle:"exclude_files",charIndex:2411},{level:2,title:"harvesterbuffersize",slug:"harvester-buffer-size",normalizedTitle:"harvesterbuffersize",charIndex:null},{level:2,title:"max_bytes",slug:"max-bytes",normalizedTitle:"max_bytes",charIndex:2744},{level:2,title:"multiline",slug:"multiline",normalizedTitle:"multiline",charIndex:2822},{level:2,title:"ignore_older",slug:"ignore-older",normalizedTitle:"ignore_older",charIndex:3160},{level:2,title:"close_* 系列",slug:"close-系列",normalizedTitle:"close_* 系列",charIndex:3629},{level:2,title:"clean_* 系列",slug:"clean-系列",normalizedTitle:"clean_* 系列",charIndex:4314},{level:2,title:"scan_frequency",slug:"scan-frequency",normalizedTitle:"scan_frequency",charIndex:4564},{level:2,title:"scan.sort 和 scan.order",slug:"scan-sort-和-scan-order",normalizedTitle:"scan.sort 和 scan.order",charIndex:4649},{level:2,title:"tail_files",slug:"tail-files",normalizedTitle:"tail_files",charIndex:4840},{level:2,title:"harvester_limit",slug:"harvester-limit",normalizedTitle:"harvester_limit",charIndex:5109},{level:2,title:"symlinks",slug:"symlinks",normalizedTitle:"symlinks",charIndex:5384},{level:2,title:"backoff 相关配置",slug:"backoff-相关配置",normalizedTitle:"backoff 相关配置",charIndex:5431},{level:2,title:"queue 相关配置",slug:"queue-相关配置",normalizedTitle:"queue 相关配置",charIndex:6431},{level:2,title:"registry 相关配置",slug:"registry-相关配置",normalizedTitle:"registry 相关配置",charIndex:6922},{level:2,title:"日志相关配置",slug:"日志相关配置",normalizedTitle:"日志相关配置",charIndex:7316},{level:2,title:"使用环境变量",slug:"使用环境变量",normalizedTitle:"使用环境变量",charIndex:8949}],headersStr:"paths Glob 模式 递归的 Glob 模式 recursive_glob.enabled: exclude_lines include_lines exclude_files harvesterbuffersize max_bytes multiline ignore_older close_* 系列 clean_* 系列 scan_frequency scan.sort 和 scan.order tail_files harvester_limit symlinks backoff 相关配置 queue 相关配置 registry 相关配置 日志相关配置 使用环境变量",content:"本文主要介绍 Filebeat 7.5 版本中 Log 相关的各个配置项的含义以及其应用场景。\n\n一般情况下，我们使用 log input 的方式如下，只需要指定一系列 paths 即可。\n\nfilebeat.inputs:\n- type: log\n  paths:\n    - /var/log/messages\n    - /var/log/*.log\n\n\n1\n2\n3\n4\n5\n\n\n但其实除了基本的 paths 配置外，log input 还有大概十几个配置项需要我们关注。\n\n这些配置项或多或少都会影响到 Filebeat 的使用方式以及性能。虽然其默认值基本足够日常使用，但是还是需要深刻理解每个配置项背后的含义，这样才能够对其完全把控。\n\n同时，在 filebeat 的日常线上运维中，也会涉及到这些配置参数的调节。\n\n\n# log input 配置\n\n\n# paths\n\n我们可以指定一系列的 paths 作为信息输入源，在指定 path 的时候，注意以下规则：\n\n 1. 指定的路径必须是文件，不能是目录。\n 2. 支持 Glob 模式。\n 3. 默认支持递归路径，如 /**/ 形式，Filebeat 将会展开 8 层嵌套目录。\n\n\n# Glob 模式\n\nGlob 模式支持通配符匹配，目前支持的语法有：\n\n通配符     解释            示例            匹配\n*       匹配任意数目的任意字符   La*           Law, Lawyer\n?       匹配任意的单字符      ?at           Cat, cat, Bat or bat\n[abc]   匹配一个在中括号的字符   [CB]at        Cat or Bat\n[a-z]   匹配一个指定范围的字符   Letter[0-9]   Letter0, Letter1, Letter2 up to Letter9\n\n\n# 递归的 Glob 模式\n\n此外，filebeat 对传统的 Glob 模式进行了扩展，支持用户指定 /**/ 模式的路径，filebeat 可以将其展开为 8 层的 Glob 路径。\n\n例如，假如指定了 /home/data/**/my*.log, filebeat 将会把 /**/ 翻译成 8 层的子目录，如下：\n\n/home/data/my*.log\n/home/data/*/my*.log\n/home/data/*/*/my*.log\n/home/data/*/*/*/my*.log\n/home/data/*/*/*/*/my*.log\n/home/data/*/*/*/*/*/my*.log\n/home/data/*/*/*/*/*/*/my*.log\n/home/data/*/*/*/*/*/*/*/my*.log\n/home/data/*/*/*/*/*/*/*/*/my*.log\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n加上不带子目录的 Glob 路径，一共会有 8 条 Glob 路径。这些路径都会作为 input 的输入源路径进行搜索。\n\n但是在使用的时候需要注意：\n\n 1. filebeat 展开为 8 层子目录的规则，是直接 hardcode 在代码中的，无法通过配置修改匹配层数\n 2. 只支持单纯的 /**/ 模式，对于 /data**/ 模式不支持\n 3. 递归模式默认开启，可通过 recursive_glob.enabled 配置项关闭\n\n\n# recursive_glob.enabled:\n\n是否开启递归的 Glob 模式，默认为 true。\n\n##encoding\n\n指定日志编码，默认是 plain。即 ASCII 模式\n\n\n# exclude_lines\n\n可指定多个正则表达式，来去除某些不需要上报的行。例如：\n\nfilebeat.inputs:\n- type: log\n  ...\n  exclude_lines: ['^DBG']\n\n\n1\n2\n3\n4\n\n\n该配置将会去除以 DBG 开头的行。\n\n\n# include_lines\n\n可指定多项正则表达式，来仅上报匹配的行。例如：\n\nfilebeat.inputs:\n- type: log\n  ...\n  include_lines: ['^ERR', '^WARN']\n\n\n1\n2\n3\n4\n\n\n该配置将会仅上报以 ERR 和 WARN 开头的行。\n\n问题来了，如果同时指定了 exclude_lines 和 include_lines 会怎么处理？\n\n对于这种情况，Filebeat 将会先校验 include_lines，再校验 exclude_lines，其代码实现如下：\n\nfunc (h *Harvester) shouldExportLine(line string) bool {\n    if len(h.config.IncludeLines) > 0 {\n        if !harvester.MatchAny(h.config.IncludeLines, line) {\n            // drop line\n            return false\n        }\n    }\n    if len(h.config.ExcludeLines) > 0 {\n        if harvester.MatchAny(h.config.ExcludeLines, line) {\n            return false\n        }\n    }\n\n    return true\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# exclude_files\n\n可指定多个正则表达式，匹配到的文件名将不会被处理。\n\n例如:\n\nexclude_files: ['.gz$']\n\n\n1\n\n\n这里需要注意的是，不管是 exclude_files，还是 exclude_lines、include_lines, 声明正则的时候，最好使用单引号引用正则表达式，不要用双引号。否则 yaml 会报转义问题\n\n\n# harvester_buffer_size\n\n读文件时的 buffer 大小，最终会应用在 golang 的 File.Read 函数上面。\n\nfunc (f *File) Read(b []byte) (n int, err error)\n\n\n1\n\n\n默认是 16384。即 16k。\n\n\n# max_bytes\n\n表示一条 log 消息的最大 bytes 数目。超过这个大小，剩余就会被截断。\n默认值为 10485760(即 10MB)。\n\n\n# multiline\n\nmultiline 是为了解决需要多行聚合在一起发送的情况，例如 Java Stack Traces 信息等。\n虽然 filebeat 默认不开启 multiline，但是官方的配置文件给了一个例子，可以支持 Java Stack Traces 或者是 C 语言式的换行连续符 \\, 可在 Examples of multiline configuration 中查看。\n\n由于大部分场景不涉及 multiline，本文不再进行深入讨论。关于 multiline 配置的详细资料可查看官方文档：\nhttps://www.elastic.co/guide/en/beats/filebeat/7.5/multiline-examples.html\n\n\n# ignore_older\n\nignore_older 表示对于最近修改时间距离当前时间已经超过某个时长的文件，就暂时不进行处理。默认值为 0，表示禁用该功能。\n\n注意：ignore_older 只是暂时不处理该文件，并不会在 Registrar 中改变该文件的状态。\n\n其代码实现如下：\n\nfunc (p *Input) isIgnoreOlder(state file.State) bool {\n    // ignore_older is disable\n    if p.config.IgnoreOlder == 0 {\n        return false\n    }\n\n    modTime := state.Fileinfo.ModTime()\n    if time.Since(modTime) > p.config.IgnoreOlder {\n        return true\n    }\n\n    return false\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# close_* 系列\n\nlog input 中有一系列以 close_开头配置，这些配置决定了 Harvester 何时结束对文件的读取。\n\n 1. close_eof\n    如果读取到了 EOF(即文件末尾)，是否要结束读取。如果为 true，则读取到文件末尾就结束读取，否则 Harvester 将会继续工作。默认只为 false。\n 2. close_inactive\n    如果配置了 close_eof 为 false，则 Harvester 即使读取到了文件末尾也不会终止。close_inactive 决定了最长没有读到新消息的时长，默认为 5m(即五分钟)。如果超过了 close_inactive 规定的时间依然没有新消息，则 Harvester 退出。\n 3. close_timeout\n    决定了一个 Harvester 的最长工作时间，如果 Harvester 工作了一段时间后依然没有停止，则强行停止 Harvester。默认为 0，表示不强行停止 Harvester。\n 4. close_renamed\n    文件更名时是否退出，默认为 false。文件更名一般发生在日志轮替的场景下。\n 5. close_removed\n    表示当文件被删除时 Harvester 是否要继续。默认为 true，表示当文件被删除时，Harvester 即刻停止工作。\n\n不过即使 Harvester 关闭了也关系不大。因为根据 filebeat 会定时扫描文件，如果关闭后又有了新增内容，filebeat 依然是可以检查出来的。\n\n\n# clean_* 系列\n\nclean_开头的一系列配置用来清理 Registrar 中的文件状态，同时也可以起到减小 Registrar 文件大小、防止 inode 复用等作用。\n\n 1. clean_inactive\n    表示一个时间段。用于移除已经一长段时间没有新产生内容的日志文件，默认为 0，表示禁用该功能。\n\n 2. clean_removed\n    在 Registrar 中移除那些已经不存在的文件。默认为 true，表示当文件不存在时，则从 Registrar 中移除。\n\n\n# scan_frequency\n\n代表 input 的扫描频率，默认为 10s。\ninput 会按照此频率，启动定时器定时扫描路径，以发现新文件和文件的改动情况。\n\n\n# scan.sort 和 scan.order\n\n这两个配置项需要放在一起讲。\nscan.sort 可取的值为: modtime 和 filename。默认值为空，不进行排序。\nscan.order 可取的值为：asc 和 desc。默认值为 asc。scan.order 仅在 scan.sort 非空时生效。\n\n需要注意的是：该功能目前为实验功能，可能会在以后版本移除。\n\n\n# tail_files\n\n默认情况下，Harvester 处理文件时，会文件头开始读取文件。开启此功能后，filebeat 将直接会把文件的 offset 置到末尾，从文件末尾监听消息。默认值是 false。\n\n注意： 开启了 tail_files, 则所有文件中的当前内容将不会被上报，只有新产生消息时才会上报。\n\n在真实的实现中，tail_files 被当做 ignore_older=1ns 处理。因此，在启动的时候，只要是新文件，里面的内容都会被忽略，直接把 offset 置为文件末尾。\n\n所以使用该配置项时千万要谨慎！\n\n\n# harvester_limit\n\nharvester_limit 决定了一个 input 最多同时有多少个 harvester 启动。默认为 0，代表不对 harvester 个数进行限制。\n在使用时要注意两点：\n\n 1. 如果一个文件对应的 harvester 在本轮扫描时没能启动，那会在下次扫描时，有其他文件的 harvester 完全退出时，该文件的 harvester 才能启动。\n 2. harvester_limit 仅对针对配置的 input 进行了限制，多个 input 之间的 harvester_limit 互不影响。\n\n\n# symlinks\n\n代表是否要对符号链接进行处理，默认值为 false，代表不处理。\n\n\n# backoff 相关配置\n\n我们上文讲到 close_eof 选项，当读取到 eof 时，且 close_eof 为 false，则 Harvester 还会一直尝试读取文件。\n\n在这种情况下，Harvester 继续读取之前，其实 filebeat 还会等待一段时间。等待的时长就是由 backoff、backoff_factor 和 max_backoff 三个配置项共同决定。\n\n对应的代码实现为：\n\nfunc (f *Log) wait() {\n    // Wait before trying to read file again. File reached EOF.\n    select {\n    case <-f.done:\n        return\n    case <-time.After(f.backoff):\n    }\n\n    // Increment backoff up to maxBackoff\n    if f.backoff < f.config.MaxBackoff {\n        f.backoff = f.backoff * time.Duration(f.config.BackoffFactor)\n        if f.backoff > f.config.MaxBackoff {\n            f.backoff = f.config.MaxBackoff\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n其中，backoff 默认值为 1s, backoff_factor 默认值为 2，max_backoff 默认值为 10s。\n\n该配置项意味着，如果读到 EOF，则 filebeat 将会等待一段时间再去读文件。\n等待时间开始为 1s，如果一直是 EOF，则会逐渐增大等待时间，每次的等待时间是前一次的两倍，且一次最长等待 10s。\n\n再结合 close_inactive 选项，如果等待时间超过了默认值 5 分钟，则 Harvester 结束。\n\n此外，如果等待的时候文件又追加了新的数据，则 backoff 将会重新置为初始值。\n\n\n# 全局配置\n\n除了 log input 相关的属性外，有一些全局属性也需要我们注意。\n\n\n# queue 相关配置\n\nfilebeat 会将 event 暂时存放在 queue 里面。filebeat 的 queue 目前有 mem 和 spool 两种实现，默认是 mem。\n本文只介绍下 mem 的相关配置项。\n\nqueue:\n  mem:\n    events: 4096\n    flush.min_events: 2048\n    flush.timeout: 1s\n\n\n1\n2\n3\n4\n5\n\n\nevents 代表 queue 最多能够承载的 event 的个数。如果个数达到最大值，则 input 将不能再向 queue 中插入数据，直至 output 将数据消费。\n\nflush.min_events 代表只有 queue 里面的数据到达了指定个数，才将数据发送给 output。设为 0 代表直接发送给 output，不进行等待。默认值是2048。\n\nflush.timeout 代表定时刷新 event 到 output 中，即使其个数没有达到 flush.min_events。该配置项只会在 flush.min_events 大于 0 时生效。\n\n\n# registry 相关配置\n\n 1. filebeat.registry.path\n    定制 registry 文件的目录，默认值是 registry。\n\n注意，这里指定的只是 registry 的目录，最终的 registry 文件的路径会是这样:\n\n${filebeat.registry.path}/filebeat/data.json\n\n\n1\n\n\n 2. filebeat.registry.flush\n    将 registry 文件内容定时刷新到磁盘中。默认为 0s，代表每次更新时直接写文件。\n    配置了该选项可以提高些 filebeat 的性能，避免频繁写磁盘，但是也增加了一定数据丢失的风险。\n\n 3. filebeat.registry.file_permissions\n    默认为 0600，即只有拥有者可以读写该用户, 其他用户不可以修改。\n\n\n# 日志相关配置\n\nfilebeat 可以对输出日志的进行相关配置，filebeat 提供了如下日志相关的配置:\n\nlogging.level: info # 日志输出的最小级别\nlogging.selectors: [] # 过滤器，用户可在 logp.NewLogger 时指定\nlogging.to_stderr: false # 将日志输出到 stderr\nlogging.to_syslog: false # 将日志输出到 syslog (主要用于 unix)\nlogging.to_eventlog: false # 将日志输出到 windows 的 event log\nlogging.to_files: true # 将日志输出到文件中\nlogging.files:\n    path: ${filebeat_bin_path}/logs/ # 日志目录\n    name: filebeat  # 文件名 filebeat filebeat.1 filebeat.2\n    rotateonstartup: true # 在 filebeat 启动时进行日志轮替\n    rotateeverybytes: 10485760 # = 10MB 日志轮替的默认值\n    keepfiles: 7 # 日志保留个数\n    permissions: 0600 # 日志权限\n    interval: 0 # 日志轮替\nlogging.metrics.enabled: true\nlogging.metrics.period: 30s\nlogging.json: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\nfilebeat 可以选择将日志输出到许多地方，在线上运营时我们常常会将日志输出到文件, 所以接下来讲下文件相关的配置。\n\n我们可以配置日志文件的所在目录以及文件名，分别对应 logging.files.path 和 logging.files.name。\n默认情况下，日志的输出目录是在 filebeat 的 bin 文件所在目录下的 logs 文件。\n\nfilebeat 会进行日志轮替，一般情况下，常见的日志轮替规则有按大小和按时间，filebeat 两种规则均支持。\n其中:\n\n 1. rotateeverybytes 决定了日志文件的最大值，如果日志文件超过了该值，将发生日志轮替，默认值为 10MB。\n 2. rotateonstartup 是说明是否在每次启动时都进行一次日志轮替，这样的话，每次启动的日志都会从一个新文件开始。默认为 true\n\n按文件大小进行轮替后，日志文件名将会变成 filebeat、filebeat.1、filebeat.2 这种格式，后缀越大文件越旧。\n\nfilebeat 也支持按时间进行轮替，可以配置 logging.files 下的 interval 属性，支持按照秒、分钟、小时、周、月、年进行轮替，对应值为 1s,1m, 1h, 24h, 7*24h, 30*24h, 和 365*24h。当然，最小值是 1s。\n\n按照时间进行轮替时，时间将会以连字符进行分割, 例如：按照 1 小时进行轮替的话，文件格式为：filebeat-2019-11-28-15。filebeat 目前还不支持日期格式的自定义。\n\n同时，我们也可以指定日志的保留策略，目前只能通过设置 keepfiles 来决定保留日志的个数。\n\n在日志里面还有 logging.metrics 相关配置，filebeat 会定时输出一些当前的运行指标，例如输出下当前 ack 成功的数目、当前的内存占用情况等：\n\n * logging.metrics.enabled 决定是否开启指标搜集\n * logging.metrics.period 决定指标输出的间隔\n\n\n# 使用环境变量\n\n我们可以在使用配置文件中直接使用环境变量，使用方式如下:\n\nfields:\n    env: ${ENV_NAME}\n\n\n1\n2\n\n\n我们可以直接用 ${ENV_NAME} 来引用系统的环境变量。\n除了直接引用外，filebeat 还提供了两个表达式配合使用:\n\n 1. ${VAR:default_value}。如果没有环境变量 VAR, 则使用默认值 default_value\n 2. ${VAR:?error_text}。如果没有环境变量 VAR，则显示错误提示 error_text\n\nfilebeat 也支持在启动时指定命令行参数来提供环境变量: -E name=${NAME}\n\n#相关阅读\n\n * Elastic-Filebeat 实现剖析\n\n\n# 参考\n\n * https://www.elastic.co/guide/en/beats/filebeat/7.5/filebeat-input-log.html\n * https://github.com/elastic/beats/blob/7.5/filebeat/filebeat.reference.yml\n * https://en.wikipedia.org/wiki/Glob_(programming)\n * Use environment variables in the configuration | Filebeat Reference [8.6] | Elastic\n\n原文链接： https://www.cyhone.com/articles/usage-of-filebeat-log-config/",normalizedContent:"本文主要介绍 filebeat 7.5 版本中 log 相关的各个配置项的含义以及其应用场景。\n\n一般情况下，我们使用 log input 的方式如下，只需要指定一系列 paths 即可。\n\nfilebeat.inputs:\n- type: log\n  paths:\n    - /var/log/messages\n    - /var/log/*.log\n\n\n1\n2\n3\n4\n5\n\n\n但其实除了基本的 paths 配置外，log input 还有大概十几个配置项需要我们关注。\n\n这些配置项或多或少都会影响到 filebeat 的使用方式以及性能。虽然其默认值基本足够日常使用，但是还是需要深刻理解每个配置项背后的含义，这样才能够对其完全把控。\n\n同时，在 filebeat 的日常线上运维中，也会涉及到这些配置参数的调节。\n\n\n# log input 配置\n\n\n# paths\n\n我们可以指定一系列的 paths 作为信息输入源，在指定 path 的时候，注意以下规则：\n\n 1. 指定的路径必须是文件，不能是目录。\n 2. 支持 glob 模式。\n 3. 默认支持递归路径，如 /**/ 形式，filebeat 将会展开 8 层嵌套目录。\n\n\n# glob 模式\n\nglob 模式支持通配符匹配，目前支持的语法有：\n\n通配符     解释            示例            匹配\n*       匹配任意数目的任意字符   la*           law, lawyer\n?       匹配任意的单字符      ?at           cat, cat, bat or bat\n[abc]   匹配一个在中括号的字符   [cb]at        cat or bat\n[a-z]   匹配一个指定范围的字符   letter[0-9]   letter0, letter1, letter2 up to letter9\n\n\n# 递归的 glob 模式\n\n此外，filebeat 对传统的 glob 模式进行了扩展，支持用户指定 /**/ 模式的路径，filebeat 可以将其展开为 8 层的 glob 路径。\n\n例如，假如指定了 /home/data/**/my*.log, filebeat 将会把 /**/ 翻译成 8 层的子目录，如下：\n\n/home/data/my*.log\n/home/data/*/my*.log\n/home/data/*/*/my*.log\n/home/data/*/*/*/my*.log\n/home/data/*/*/*/*/my*.log\n/home/data/*/*/*/*/*/my*.log\n/home/data/*/*/*/*/*/*/my*.log\n/home/data/*/*/*/*/*/*/*/my*.log\n/home/data/*/*/*/*/*/*/*/*/my*.log\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n加上不带子目录的 glob 路径，一共会有 8 条 glob 路径。这些路径都会作为 input 的输入源路径进行搜索。\n\n但是在使用的时候需要注意：\n\n 1. filebeat 展开为 8 层子目录的规则，是直接 hardcode 在代码中的，无法通过配置修改匹配层数\n 2. 只支持单纯的 /**/ 模式，对于 /data**/ 模式不支持\n 3. 递归模式默认开启，可通过 recursive_glob.enabled 配置项关闭\n\n\n# recursive_glob.enabled:\n\n是否开启递归的 glob 模式，默认为 true。\n\n##encoding\n\n指定日志编码，默认是 plain。即 ascii 模式\n\n\n# exclude_lines\n\n可指定多个正则表达式，来去除某些不需要上报的行。例如：\n\nfilebeat.inputs:\n- type: log\n  ...\n  exclude_lines: ['^dbg']\n\n\n1\n2\n3\n4\n\n\n该配置将会去除以 dbg 开头的行。\n\n\n# include_lines\n\n可指定多项正则表达式，来仅上报匹配的行。例如：\n\nfilebeat.inputs:\n- type: log\n  ...\n  include_lines: ['^err', '^warn']\n\n\n1\n2\n3\n4\n\n\n该配置将会仅上报以 err 和 warn 开头的行。\n\n问题来了，如果同时指定了 exclude_lines 和 include_lines 会怎么处理？\n\n对于这种情况，filebeat 将会先校验 include_lines，再校验 exclude_lines，其代码实现如下：\n\nfunc (h *harvester) shouldexportline(line string) bool {\n    if len(h.config.includelines) > 0 {\n        if !harvester.matchany(h.config.includelines, line) {\n            // drop line\n            return false\n        }\n    }\n    if len(h.config.excludelines) > 0 {\n        if harvester.matchany(h.config.excludelines, line) {\n            return false\n        }\n    }\n\n    return true\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# exclude_files\n\n可指定多个正则表达式，匹配到的文件名将不会被处理。\n\n例如:\n\nexclude_files: ['.gz$']\n\n\n1\n\n\n这里需要注意的是，不管是 exclude_files，还是 exclude_lines、include_lines, 声明正则的时候，最好使用单引号引用正则表达式，不要用双引号。否则 yaml 会报转义问题\n\n\n# harvester_buffer_size\n\n读文件时的 buffer 大小，最终会应用在 golang 的 file.read 函数上面。\n\nfunc (f *file) read(b []byte) (n int, err error)\n\n\n1\n\n\n默认是 16384。即 16k。\n\n\n# max_bytes\n\n表示一条 log 消息的最大 bytes 数目。超过这个大小，剩余就会被截断。\n默认值为 10485760(即 10mb)。\n\n\n# multiline\n\nmultiline 是为了解决需要多行聚合在一起发送的情况，例如 java stack traces 信息等。\n虽然 filebeat 默认不开启 multiline，但是官方的配置文件给了一个例子，可以支持 java stack traces 或者是 c 语言式的换行连续符 \\, 可在 examples of multiline configuration 中查看。\n\n由于大部分场景不涉及 multiline，本文不再进行深入讨论。关于 multiline 配置的详细资料可查看官方文档：\nhttps://www.elastic.co/guide/en/beats/filebeat/7.5/multiline-examples.html\n\n\n# ignore_older\n\nignore_older 表示对于最近修改时间距离当前时间已经超过某个时长的文件，就暂时不进行处理。默认值为 0，表示禁用该功能。\n\n注意：ignore_older 只是暂时不处理该文件，并不会在 registrar 中改变该文件的状态。\n\n其代码实现如下：\n\nfunc (p *input) isignoreolder(state file.state) bool {\n    // ignore_older is disable\n    if p.config.ignoreolder == 0 {\n        return false\n    }\n\n    modtime := state.fileinfo.modtime()\n    if time.since(modtime) > p.config.ignoreolder {\n        return true\n    }\n\n    return false\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# close_* 系列\n\nlog input 中有一系列以 close_开头配置，这些配置决定了 harvester 何时结束对文件的读取。\n\n 1. close_eof\n    如果读取到了 eof(即文件末尾)，是否要结束读取。如果为 true，则读取到文件末尾就结束读取，否则 harvester 将会继续工作。默认只为 false。\n 2. close_inactive\n    如果配置了 close_eof 为 false，则 harvester 即使读取到了文件末尾也不会终止。close_inactive 决定了最长没有读到新消息的时长，默认为 5m(即五分钟)。如果超过了 close_inactive 规定的时间依然没有新消息，则 harvester 退出。\n 3. close_timeout\n    决定了一个 harvester 的最长工作时间，如果 harvester 工作了一段时间后依然没有停止，则强行停止 harvester。默认为 0，表示不强行停止 harvester。\n 4. close_renamed\n    文件更名时是否退出，默认为 false。文件更名一般发生在日志轮替的场景下。\n 5. close_removed\n    表示当文件被删除时 harvester 是否要继续。默认为 true，表示当文件被删除时，harvester 即刻停止工作。\n\n不过即使 harvester 关闭了也关系不大。因为根据 filebeat 会定时扫描文件，如果关闭后又有了新增内容，filebeat 依然是可以检查出来的。\n\n\n# clean_* 系列\n\nclean_开头的一系列配置用来清理 registrar 中的文件状态，同时也可以起到减小 registrar 文件大小、防止 inode 复用等作用。\n\n 1. clean_inactive\n    表示一个时间段。用于移除已经一长段时间没有新产生内容的日志文件，默认为 0，表示禁用该功能。\n\n 2. clean_removed\n    在 registrar 中移除那些已经不存在的文件。默认为 true，表示当文件不存在时，则从 registrar 中移除。\n\n\n# scan_frequency\n\n代表 input 的扫描频率，默认为 10s。\ninput 会按照此频率，启动定时器定时扫描路径，以发现新文件和文件的改动情况。\n\n\n# scan.sort 和 scan.order\n\n这两个配置项需要放在一起讲。\nscan.sort 可取的值为: modtime 和 filename。默认值为空，不进行排序。\nscan.order 可取的值为：asc 和 desc。默认值为 asc。scan.order 仅在 scan.sort 非空时生效。\n\n需要注意的是：该功能目前为实验功能，可能会在以后版本移除。\n\n\n# tail_files\n\n默认情况下，harvester 处理文件时，会文件头开始读取文件。开启此功能后，filebeat 将直接会把文件的 offset 置到末尾，从文件末尾监听消息。默认值是 false。\n\n注意： 开启了 tail_files, 则所有文件中的当前内容将不会被上报，只有新产生消息时才会上报。\n\n在真实的实现中，tail_files 被当做 ignore_older=1ns 处理。因此，在启动的时候，只要是新文件，里面的内容都会被忽略，直接把 offset 置为文件末尾。\n\n所以使用该配置项时千万要谨慎！\n\n\n# harvester_limit\n\nharvester_limit 决定了一个 input 最多同时有多少个 harvester 启动。默认为 0，代表不对 harvester 个数进行限制。\n在使用时要注意两点：\n\n 1. 如果一个文件对应的 harvester 在本轮扫描时没能启动，那会在下次扫描时，有其他文件的 harvester 完全退出时，该文件的 harvester 才能启动。\n 2. harvester_limit 仅对针对配置的 input 进行了限制，多个 input 之间的 harvester_limit 互不影响。\n\n\n# symlinks\n\n代表是否要对符号链接进行处理，默认值为 false，代表不处理。\n\n\n# backoff 相关配置\n\n我们上文讲到 close_eof 选项，当读取到 eof 时，且 close_eof 为 false，则 harvester 还会一直尝试读取文件。\n\n在这种情况下，harvester 继续读取之前，其实 filebeat 还会等待一段时间。等待的时长就是由 backoff、backoff_factor 和 max_backoff 三个配置项共同决定。\n\n对应的代码实现为：\n\nfunc (f *log) wait() {\n    // wait before trying to read file again. file reached eof.\n    select {\n    case <-f.done:\n        return\n    case <-time.after(f.backoff):\n    }\n\n    // increment backoff up to maxbackoff\n    if f.backoff < f.config.maxbackoff {\n        f.backoff = f.backoff * time.duration(f.config.backofffactor)\n        if f.backoff > f.config.maxbackoff {\n            f.backoff = f.config.maxbackoff\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n其中，backoff 默认值为 1s, backoff_factor 默认值为 2，max_backoff 默认值为 10s。\n\n该配置项意味着，如果读到 eof，则 filebeat 将会等待一段时间再去读文件。\n等待时间开始为 1s，如果一直是 eof，则会逐渐增大等待时间，每次的等待时间是前一次的两倍，且一次最长等待 10s。\n\n再结合 close_inactive 选项，如果等待时间超过了默认值 5 分钟，则 harvester 结束。\n\n此外，如果等待的时候文件又追加了新的数据，则 backoff 将会重新置为初始值。\n\n\n# 全局配置\n\n除了 log input 相关的属性外，有一些全局属性也需要我们注意。\n\n\n# queue 相关配置\n\nfilebeat 会将 event 暂时存放在 queue 里面。filebeat 的 queue 目前有 mem 和 spool 两种实现，默认是 mem。\n本文只介绍下 mem 的相关配置项。\n\nqueue:\n  mem:\n    events: 4096\n    flush.min_events: 2048\n    flush.timeout: 1s\n\n\n1\n2\n3\n4\n5\n\n\nevents 代表 queue 最多能够承载的 event 的个数。如果个数达到最大值，则 input 将不能再向 queue 中插入数据，直至 output 将数据消费。\n\nflush.min_events 代表只有 queue 里面的数据到达了指定个数，才将数据发送给 output。设为 0 代表直接发送给 output，不进行等待。默认值是2048。\n\nflush.timeout 代表定时刷新 event 到 output 中，即使其个数没有达到 flush.min_events。该配置项只会在 flush.min_events 大于 0 时生效。\n\n\n# registry 相关配置\n\n 1. filebeat.registry.path\n    定制 registry 文件的目录，默认值是 registry。\n\n注意，这里指定的只是 registry 的目录，最终的 registry 文件的路径会是这样:\n\n${filebeat.registry.path}/filebeat/data.json\n\n\n1\n\n\n 2. filebeat.registry.flush\n    将 registry 文件内容定时刷新到磁盘中。默认为 0s，代表每次更新时直接写文件。\n    配置了该选项可以提高些 filebeat 的性能，避免频繁写磁盘，但是也增加了一定数据丢失的风险。\n\n 3. filebeat.registry.file_permissions\n    默认为 0600，即只有拥有者可以读写该用户, 其他用户不可以修改。\n\n\n# 日志相关配置\n\nfilebeat 可以对输出日志的进行相关配置，filebeat 提供了如下日志相关的配置:\n\nlogging.level: info # 日志输出的最小级别\nlogging.selectors: [] # 过滤器，用户可在 logp.newlogger 时指定\nlogging.to_stderr: false # 将日志输出到 stderr\nlogging.to_syslog: false # 将日志输出到 syslog (主要用于 unix)\nlogging.to_eventlog: false # 将日志输出到 windows 的 event log\nlogging.to_files: true # 将日志输出到文件中\nlogging.files:\n    path: ${filebeat_bin_path}/logs/ # 日志目录\n    name: filebeat  # 文件名 filebeat filebeat.1 filebeat.2\n    rotateonstartup: true # 在 filebeat 启动时进行日志轮替\n    rotateeverybytes: 10485760 # = 10mb 日志轮替的默认值\n    keepfiles: 7 # 日志保留个数\n    permissions: 0600 # 日志权限\n    interval: 0 # 日志轮替\nlogging.metrics.enabled: true\nlogging.metrics.period: 30s\nlogging.json: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\nfilebeat 可以选择将日志输出到许多地方，在线上运营时我们常常会将日志输出到文件, 所以接下来讲下文件相关的配置。\n\n我们可以配置日志文件的所在目录以及文件名，分别对应 logging.files.path 和 logging.files.name。\n默认情况下，日志的输出目录是在 filebeat 的 bin 文件所在目录下的 logs 文件。\n\nfilebeat 会进行日志轮替，一般情况下，常见的日志轮替规则有按大小和按时间，filebeat 两种规则均支持。\n其中:\n\n 1. rotateeverybytes 决定了日志文件的最大值，如果日志文件超过了该值，将发生日志轮替，默认值为 10mb。\n 2. rotateonstartup 是说明是否在每次启动时都进行一次日志轮替，这样的话，每次启动的日志都会从一个新文件开始。默认为 true\n\n按文件大小进行轮替后，日志文件名将会变成 filebeat、filebeat.1、filebeat.2 这种格式，后缀越大文件越旧。\n\nfilebeat 也支持按时间进行轮替，可以配置 logging.files 下的 interval 属性，支持按照秒、分钟、小时、周、月、年进行轮替，对应值为 1s,1m, 1h, 24h, 7*24h, 30*24h, 和 365*24h。当然，最小值是 1s。\n\n按照时间进行轮替时，时间将会以连字符进行分割, 例如：按照 1 小时进行轮替的话，文件格式为：filebeat-2019-11-28-15。filebeat 目前还不支持日期格式的自定义。\n\n同时，我们也可以指定日志的保留策略，目前只能通过设置 keepfiles 来决定保留日志的个数。\n\n在日志里面还有 logging.metrics 相关配置，filebeat 会定时输出一些当前的运行指标，例如输出下当前 ack 成功的数目、当前的内存占用情况等：\n\n * logging.metrics.enabled 决定是否开启指标搜集\n * logging.metrics.period 决定指标输出的间隔\n\n\n# 使用环境变量\n\n我们可以在使用配置文件中直接使用环境变量，使用方式如下:\n\nfields:\n    env: ${env_name}\n\n\n1\n2\n\n\n我们可以直接用 ${env_name} 来引用系统的环境变量。\n除了直接引用外，filebeat 还提供了两个表达式配合使用:\n\n 1. ${var:default_value}。如果没有环境变量 var, 则使用默认值 default_value\n 2. ${var:?error_text}。如果没有环境变量 var，则显示错误提示 error_text\n\nfilebeat 也支持在启动时指定命令行参数来提供环境变量: -e name=${name}\n\n#相关阅读\n\n * elastic-filebeat 实现剖析\n\n\n# 参考\n\n * https://www.elastic.co/guide/en/beats/filebeat/7.5/filebeat-input-log.html\n * https://github.com/elastic/beats/blob/7.5/filebeat/filebeat.reference.yml\n * https://en.wikipedia.org/wiki/glob_(programming)\n * use environment variables in the configuration | filebeat reference [8.6] | elastic\n\n原文链接： https://www.cyhone.com/articles/usage-of-filebeat-log-config/",charsets:{cjk:!0}},{title:"jenkins容器安装",frontmatter:{title:"jenkins容器安装",date:"2022-12-15T14:01:31.000Z",permalink:"/pages/ce2b89/",categories:["专题","jenkins"],tags:[null],readingShow:"top",description:"FROM jenkins/jenkins:2.332.3",meta:[{name:"twitter:title",content:"jenkins容器安装"},{name:"twitter:description",content:"FROM jenkins/jenkins:2.332.3"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/04.jenkins/01.jenkins%E5%AE%B9%E5%99%A8%E5%AE%89%E8%A3%85.html"},{property:"og:type",content:"article"},{property:"og:title",content:"jenkins容器安装"},{property:"og:description",content:"FROM jenkins/jenkins:2.332.3"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/04.jenkins/01.jenkins%E5%AE%B9%E5%99%A8%E5%AE%89%E8%A3%85.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:01:31.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"jenkins容器安装"},{itemprop:"description",content:"FROM jenkins/jenkins:2.332.3"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/04.jenkins/01.jenkins%E5%AE%B9%E5%99%A8%E5%AE%89%E8%A3%85.html",relativePath:"01.专题/04.jenkins/01.jenkins容器安装.md",key:"v-b8800ef2",path:"/pages/ce2b89/",headersStr:null,content:"# 安装的dockerfile\n\nFROM jenkins/jenkins:2.332.3\n\nUSER root\nRUN echo '' > /etc/apt/sources.list \\\n&& echo \"deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free\" > /etc/apt/sources.list \\\n&& echo \"deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free\" >> /etc/apt/sources.list \\\n&& echo \"deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free\" >> /etc/apt/sources.list \\\n&& echo \"deb http://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free\" >> /etc/apt/sources.list \\\n&& apt update \\\n&& apt install sudo -y \\\n&& apt install vim -y \\\n&& apt install sshpass -y \\\n&& apt install python -y \\\n&& sed -i '27a\\jenkins ALL=(ALL)       NOPASSWD:ALL' /etc/sudoers\nUSER jenkins\nENV LANG C.UTF-8\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# docker-compose文件\n\nversion: '3'\nservices:\n  jenkins:\n    image: di.zzppjj.top/library/jenkins:v6\n    container_name: jenkins\n    restart: always\n    ports:\n      - '8000:8080'\n      - '50000:50000'\n    environment:\n      JAVA_OPTS: -Duser.timezone=Asia/Shanghai\n    extra_hosts:\n      - \"di.zzppjj.top:172.16.30.215\"\n    volumes:\n      - /data/jenkins/jenkins_home:/var/jenkins_home\n      - /etc/localtime:/etc/localtime:ro\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /usr/bin/docker:/usr/bin/docker  \n      - /data:/data\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n",normalizedContent:"# 安装的dockerfile\n\nfrom jenkins/jenkins:2.332.3\n\nuser root\nrun echo '' > /etc/apt/sources.list \\\n&& echo \"deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free\" > /etc/apt/sources.list \\\n&& echo \"deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free\" >> /etc/apt/sources.list \\\n&& echo \"deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free\" >> /etc/apt/sources.list \\\n&& echo \"deb http://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free\" >> /etc/apt/sources.list \\\n&& apt update \\\n&& apt install sudo -y \\\n&& apt install vim -y \\\n&& apt install sshpass -y \\\n&& apt install python -y \\\n&& sed -i '27a\\jenkins all=(all)       nopasswd:all' /etc/sudoers\nuser jenkins\nenv lang c.utf-8\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# docker-compose文件\n\nversion: '3'\nservices:\n  jenkins:\n    image: di.zzppjj.top/library/jenkins:v6\n    container_name: jenkins\n    restart: always\n    ports:\n      - '8000:8080'\n      - '50000:50000'\n    environment:\n      java_opts: -duser.timezone=asia/shanghai\n    extra_hosts:\n      - \"di.zzppjj.top:172.16.30.215\"\n    volumes:\n      - /data/jenkins/jenkins_home:/var/jenkins_home\n      - /etc/localtime:/etc/localtime:ro\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /usr/bin/docker:/usr/bin/docker  \n      - /data:/data\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n",charsets:{cjk:!0}},{title:"jenkins流水线部署",frontmatter:{title:"jenkins流水线部署",date:"2023-01-06T14:41:01.000Z",permalink:"/pages/a9ff44/",categories:["专题","jenkins"],tags:[null],readingShow:"top",description:"tar -zcf scrmwebphp_business.tar.gz $(ls php.sh | grep -vE \"bizshow|cms\")\n                        tar -zcf scrmwebphp_bizshow.tar.gz $(ls php.sh | grep \"bizshow\")\n                        tar -zcf scrmwebphp_cms.tar.gz $(ls php.sh | grep \"cms\")\n                        '''\n                    }catch(exc) {\n                        env.Reason = \"打包文件出错\"\n                        throw(exc)\n                    }\n                }\n            }\n        }\n        stage('拷贝') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'cp_business'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'cp'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'cp_go'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'cp-saas-backend01'\n            }\n        }\n        stage('部署go服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrm_go'\n            }\n        }\n        stage('部署webphpbusiness服务') {\n            parallel{\n                stage('部署backend服务') {\n                    steps {\n                        ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrmwebphp_business'\n                    }\n                }\n                stage('部署backend01服务') {\n                    steps {\n                        ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'scrm_backend01'\n                    }\n                }\n            }\n        }\n        // stage('部署webphpbusiness服务') {\n        //     steps {\n        //         ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrmwebphp_business'\n        //         ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'scrm_backend01'\n        //     }\n        // }\n        stage('部署web_admin服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'scrmwebadmin'\n            }\n        }\n        stage('部署web_h5服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'scrmwebh5'\n            }\n        }",meta:[{name:"twitter:title",content:"jenkins流水线部署"},{name:"twitter:description",content:"tar -zcf scrmwebphp_business.tar.gz $(ls php.sh | grep -vE \"bizshow|cms\")\n                        tar -zcf scrmwebphp_bizshow.tar.gz $(ls php.sh | grep \"bizshow\")\n                        tar -zcf scrmwebphp_cms.tar.gz $(ls php.sh | grep \"cms\")\n                        '''\n                    }catch(exc) {\n                        env.Reason = \"打包文件出错\"\n                        throw(exc)\n                    }\n                }\n            }\n        }\n        stage('拷贝') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'cp_business'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'cp'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'cp_go'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'cp-saas-backend01'\n            }\n        }\n        stage('部署go服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrm_go'\n            }\n        }\n        stage('部署webphpbusiness服务') {\n            parallel{\n                stage('部署backend服务') {\n                    steps {\n                        ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrmwebphp_business'\n                    }\n                }\n                stage('部署backend01服务') {\n                    steps {\n                        ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'scrm_backend01'\n                    }\n                }\n            }\n        }\n        // stage('部署webphpbusiness服务') {\n        //     steps {\n        //         ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrmwebphp_business'\n        //         ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'scrm_backend01'\n        //     }\n        // }\n        stage('部署web_admin服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'scrmwebadmin'\n            }\n        }\n        stage('部署web_h5服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'scrmwebh5'\n            }\n        }"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/04.jenkins/02.jenkins%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2.html"},{property:"og:type",content:"article"},{property:"og:title",content:"jenkins流水线部署"},{property:"og:description",content:"tar -zcf scrmwebphp_business.tar.gz $(ls php.sh | grep -vE \"bizshow|cms\")\n                        tar -zcf scrmwebphp_bizshow.tar.gz $(ls php.sh | grep \"bizshow\")\n                        tar -zcf scrmwebphp_cms.tar.gz $(ls php.sh | grep \"cms\")\n                        '''\n                    }catch(exc) {\n                        env.Reason = \"打包文件出错\"\n                        throw(exc)\n                    }\n                }\n            }\n        }\n        stage('拷贝') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'cp_business'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'cp'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'cp_go'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'cp-saas-backend01'\n            }\n        }\n        stage('部署go服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrm_go'\n            }\n        }\n        stage('部署webphpbusiness服务') {\n            parallel{\n                stage('部署backend服务') {\n                    steps {\n                        ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrmwebphp_business'\n                    }\n                }\n                stage('部署backend01服务') {\n                    steps {\n                        ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'scrm_backend01'\n                    }\n                }\n            }\n        }\n        // stage('部署webphpbusiness服务') {\n        //     steps {\n        //         ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrmwebphp_business'\n        //         ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'scrm_backend01'\n        //     }\n        // }\n        stage('部署web_admin服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'scrmwebadmin'\n            }\n        }\n        stage('部署web_h5服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'scrmwebh5'\n            }\n        }"},{property:"og:url",content:"https://blog.zzppjj.top/01.%E4%B8%93%E9%A2%98/04.jenkins/02.jenkins%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-06T14:41:01.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"jenkins流水线部署"},{itemprop:"description",content:"tar -zcf scrmwebphp_business.tar.gz $(ls php.sh | grep -vE \"bizshow|cms\")\n                        tar -zcf scrmwebphp_bizshow.tar.gz $(ls php.sh | grep \"bizshow\")\n                        tar -zcf scrmwebphp_cms.tar.gz $(ls php.sh | grep \"cms\")\n                        '''\n                    }catch(exc) {\n                        env.Reason = \"打包文件出错\"\n                        throw(exc)\n                    }\n                }\n            }\n        }\n        stage('拷贝') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'cp_business'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'cp'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'cp_go'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'cp-saas-backend01'\n            }\n        }\n        stage('部署go服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrm_go'\n            }\n        }\n        stage('部署webphpbusiness服务') {\n            parallel{\n                stage('部署backend服务') {\n                    steps {\n                        ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrmwebphp_business'\n                    }\n                }\n                stage('部署backend01服务') {\n                    steps {\n                        ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'scrm_backend01'\n                    }\n                }\n            }\n        }\n        // stage('部署webphpbusiness服务') {\n        //     steps {\n        //         ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend.yml', sudoUser: null, tags: 'scrmwebphp_business'\n        //         ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasbackend01.yml', sudoUser: null, tags: 'scrm_backend01'\n        //     }\n        // }\n        stage('部署web_admin服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'scrmwebadmin'\n            }\n        }\n        stage('部署web_h5服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploypro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkinshome/ansible/hosts', playbook: '/var/jenkinshome/ansible/saas/saasfronted.yml', sudoUser: null, tags: 'scrmwebh5'\n            }\n        }"}]},regularPath:"/01.%E4%B8%93%E9%A2%98/04.jenkins/02.jenkins%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2.html",relativePath:"01.专题/04.jenkins/02.jenkins流水线部署.md",key:"v-9066c126",path:"/pages/a9ff44/",headers:[{level:4,title:"jenkinsfile文件",slug:"jenkinsfile文件",normalizedTitle:"jenkinsfile文件",charIndex:2},{level:4,title:"ansible-playbook剧本",slug:"ansible-playbook剧本",normalizedTitle:"ansible-playbook剧本",charIndex:10701}],headersStr:"jenkinsfile文件 ansible-playbook剧本",content:"# jenkinsfile文件\n\npipeline {\n    agent any\n    environment {       \n        DB_SYS_PASSWORD=credentials('saas_pro_dbpasswd')\n        DATABASE_PWD=credentials('saas_pro_dbpasswd')\n        REDIS_PWD=credentials('saas_pro_redisspwd')\n        VUE_CATEGORY_ID='8'\n        PRODUCT_CATEGORY_YUN='8'\n        PRODUCT_CATEGORY='5,6'\n    }\n    parameters {\n        string(name: 'version_number', defaultValue: '', description: '')\n        string(name: 'product_code', defaultValue: 'saas', description: '')\n        string(name: 'DATABASE_USER', defaultValue: 'saas_user', description: '')\n        string(name: 'DATABASE_HOST', defaultValue: 'tcp://xx:3306', description: '')\n        string(name: 'REDIS_HOST', defaultValue: 'xx:6379', description: '')\n        choice choices: ['否', '是'], description: 'bizshow和cms是否部署', name: 'bizshow_cms_deploy'\n    }\n    stages {\n        stage('拉取脚本') {\n            steps {\n                dir(\"/var/jenkins_home/script\"){\n                    script {\n                        try {\n                            checkout(\n                                [$class: 'GitSCM', doGenerateSubmoduleConfigurations: false, submoduleCfg: [], extensions: [[$class: 'CloneOption', depth: 1, noTags: false, reference: '', shallow: true]],\n                                branches: [[name: \"master\"]],userRemoteConfigs: [[url: \"ssh://git@xx:2224/rdito/yw_db.git\"]]]\n                            )\n                        }catch(exc) {\n                            env.REASON = \"拉取脚本出错\"\n                            throw(exc)\n                        }\n                    }\n                }\n            }\n        }\n        stage('拉取ansible剧本') {\n            steps {\n                dir(\"/var/jenkins_home/ansible\"){\n                    script {\n                        try {\n                            checkout(\n                                [$class: 'GitSCM', doGenerateSubmoduleConfigurations: false, submoduleCfg: [], extensions: [[$class: 'CloneOption', depth: 1, noTags: false, reference: '', shallow: true]],\n                                branches: [[name: \"master\"]],userRemoteConfigs: [[url: \"ssh://git@xx:2224/rdito/ansible.git\"]]]\n                            )\n                        }catch(exc) {\n                            env.REASON = \"拉取ansible剧本出错\"\n                            throw(exc)\n                        }\n                    }\n                }\n            }\n        }\n        stage('生成部署文件') {\n            steps {\n                sh label: '', script: '/var/jenkins_home/script/saas/php_deploy1.sh scrm.web_php_business'\n                sh label: '', script: '/var/jenkins_home/script/saas/go_deploy.sh'\n                sh label: '', script: '/var/jenkins_home/script/saas/h5_deploy.sh scrm.web_admin'\n                sh label: '', script: '/var/jenkins_home/script/saas/h5_deploy.sh scrm.web_h5'\n                sh label: '', script: '/var/jenkins_home/script/saas/h5_deploy.sh web_h5_bizshow'\n                sh label: '', script: '/var/jenkins_home/script/saas/php_deploy1.sh web_php_bizshow'\n                sh label: '', script: '/var/jenkins_home/script/saas/php_deploy1.sh web_php_cms'\n            }\n        }\n        stage('打包') {\n            steps {\n                script{\n                    try{\n                        sh '''        \n                        cd \"$WORKSPACE\"\n                        chmod +x *.sh\n                        tar -zcf scrm_web_php_business.tar.gz $(ls *php*.sh | grep -vE \"bizshow|cms\")\n                        tar -zcf scrm_web_php_bizshow.tar.gz $(ls *php*.sh | grep \"bizshow\")\n                        tar -zcf scrm_web_php_cms.tar.gz $(ls *php*.sh | grep \"cms\")\n                        '''\n                    }catch(exc) {\n                        env.Reason = \"打包文件出错\"\n                        throw(exc)\n                    }\n                }\n            }\n        }\n        stage('拷贝') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend.yml', sudoUser: null, tags: 'cp_business'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_fronted.yml', sudoUser: null, tags: 'cp'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend.yml', sudoUser: null, tags: 'cp_go'               \n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend01.yml', sudoUser: null, tags: 'cp-saas-backend01'\n            }\n        }\n        stage('部署go服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend.yml', sudoUser: null, tags: 'scrm_go'\n            }\n        }\n        stage('部署web_php_business服务') {\n            parallel{\n                stage('部署backend服务') {\n                    steps {\n                        ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend.yml', sudoUser: null, tags: 'scrm_web_php_business'\n                    }\n                }\n                stage('部署backend01服务') {\n                    steps {\n                        ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend01.yml', sudoUser: null, tags: 'scrm_backend01'\n                    }\n                }                \n            }\n        }\n        // stage('部署web_php_business服务') {\n        //     steps {\n        //         ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend.yml', sudoUser: null, tags: 'scrm_web_php_business'\n        //         ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend01.yml', sudoUser: null, tags: 'scrm_backend01'\n        //     }\n        // }\n        stage('部署web_admin服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_fronted.yml', sudoUser: null, tags: 'scrm_web_admin'\n            }\n        }\n        stage('部署web_h5服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_fronted.yml', sudoUser: null, tags: 'scrm_web_h5'\n            }\n        }\n\n        stage('部署web_h5_bizshow服务') {\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_fronted.yml', sudoUser: null, tags: 'scrm_web_h5_bizshow'\n            }\n        }\n\n        stage('部署web_php_bizshow服务') {\n            when {\n                expression { params.bizshow_cms_deploy == '是'}\n            }\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend_bizshow.yml', sudoUser: null, tags: 'cp_bizshow'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend_bizshow.yml', sudoUser: null, tags: 'scrm_web_php_bizshow'\n            }\n        }\n\n        stage('部署web_php_cms服务') {\n            when {\n                expression { params.bizshow_cms_deploy == '是'}\n            }\n            steps {\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend_cms.yml', sudoUser: null, tags: 'cp_cms'\n                ansiblePlaybook become: true, becomeUser: 'admin', credentialsId: 'deploy_pro', extras: '--extra-vars \"WORKSPACE=${WORKSPACE}\"', disableHostKeyChecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend_cms.yml', sudoUser: null, tags: 'scrm_web_php_cms'\n            }\n        }\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n\n\n# ansible-playbook剧本\n\n---\n- hosts: saas_dev\n  remote_user: admin\n  gather_facts: false\n  become: yes\n  vars:\n    filename: /var/jenkins_home/workspace/saas/scrm.web_php_bizshow/*php*.sh\n    filename1: /var/jenkins_home/workspace/saas/scrm.web_php_cms/*php*.sh\n  tasks:\n    - name: \"复制主机上的文件到test服务器上\"\n      copy:\n        src: \"{{ item }}\"\n        dest: \"/opt/apps/saas_docker/scrm_web_php_bizshow/\"\n        owner: admin\n        group: admin\n        mode: 0755\n      with_fileglob:\n        - \"{{ filename }}\"\n      tags:\n        - cp_bizshow\n    - name: \"复制主机上的文件到test服务器上\"\n      copy:\n        src: \"{{ item }}\"\n        dest: \"/opt/apps/saas_docker/scrm_web_php_cms/\"\n        owner: admin\n        group: admin\n        mode: 0755\n      with_fileglob:\n        - \"{{ filename1 }}\"\n      tags:\n        - cp_cms\n    - name: \"创建远程主机上的部署目录\"\n      file: path=/opt/apps/saas_docker/{{sub_dir}} state=directory\n      tags: create_dir\n    - name: \"复制主机上的文件到web_php_business服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"/var/jenkins_home/workspace/saas/scrm.web_php_business/web_php_business.sh\",dest: \"/opt/apps/saas_docker/scrm_web_php_business/\"}\n      tags:\n        - cp_php_business\n    - name: \"复制主机上的文件到web_admin服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"/var/jenkins_home/workspace/saas/scrm.web_admin/web_admin.sh\",dest: \"/opt/apps/saas_docker/scrm_web_admin/\"}\n      tags:\n        - cp_web_admin\n    - name: \"复制主机上的文件到dev服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"/var/jenkins_home/workspace/saas/scrm.web_h5/web_h5.sh\",dest: \"/opt/apps/saas_docker/scrm_web_h5/\"}\n      tags:\n        - cp_web_h5\n    - name: \"复制主机上的文件到dev服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"/var/jenkins_home/workspace/saas/scrm.web_h5_bizshow/h5_bizshow_deploy.sh\",dest: \"/opt/apps/saas_docker/scrm_web_h5_bizshow/\"}\n      tags:\n        - cp_web_h5_bizshow\n    - name: \"复制主机上的文件到dev服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"/var/jenkins_home/workspace/saas/scrm.web_php_bizshow/php_bizshow_deploy.sh\",dest: \"/opt/apps/saas_docker/scrm_web_php_bizshow/\"}\n      tags:\n        - cp_php_bizshow\n    - name: \"复制主机上的文件到dev服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - { src: \"/var/jenkins_home/workspace/saas/scrm.web_php_cms/php_cms_deploy.sh\",dest: \"/opt/apps/saas_docker/scrm_web_php_cms/\" }\n      tags:\n        - cp_php_cms\n    - name: \"Execute the scrm_web_php_business script\"\n      shell: cd /opt/apps/saas_docker/scrm_web_php_business && ./web_php_business.sh\n      tags:\n        - scrm_web_php_business\n    - name: \"Execute the scrm_web_php_bizshow script\"\n      shell: cd /opt/apps/saas_docker/scrm_web_php_bizshow && ./php_bizshow_deploy.sh\n      tags:\n        - scrm_web_php_bizshow\n    - name: \"复制主机上的文件到saas-backend01服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"{{ WORKSPACE }}/scrm_web_php_GAOYINIBM.sh\",dest: \"/opt/apps/saas_docker/scrm_web_php_GAOYINIBM/\"}\n      tags:\n        - cp-saas-backend01\n    - name: \"Execute the script\"\n      shell: |\n        /home/admin/script/init.sh stop \n        cd /opt/apps/saas_docker/scrm_web_php_GAOYINIBM && chmod +x scrm_web_php_GAOYINIBM.sh && ./scrm_web_php_GAOYINIBM.sh\n        /home/admin/script/init.sh start\n      tags:\n        - scrm_backend01\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n",normalizedContent:"# jenkinsfile文件\n\npipeline {\n    agent any\n    environment {       \n        db_sys_password=credentials('saas_pro_dbpasswd')\n        database_pwd=credentials('saas_pro_dbpasswd')\n        redis_pwd=credentials('saas_pro_redisspwd')\n        vue_category_id='8'\n        product_category_yun='8'\n        product_category='5,6'\n    }\n    parameters {\n        string(name: 'version_number', defaultvalue: '', description: '')\n        string(name: 'product_code', defaultvalue: 'saas', description: '')\n        string(name: 'database_user', defaultvalue: 'saas_user', description: '')\n        string(name: 'database_host', defaultvalue: 'tcp://xx:3306', description: '')\n        string(name: 'redis_host', defaultvalue: 'xx:6379', description: '')\n        choice choices: ['否', '是'], description: 'bizshow和cms是否部署', name: 'bizshow_cms_deploy'\n    }\n    stages {\n        stage('拉取脚本') {\n            steps {\n                dir(\"/var/jenkins_home/script\"){\n                    script {\n                        try {\n                            checkout(\n                                [$class: 'gitscm', dogeneratesubmoduleconfigurations: false, submodulecfg: [], extensions: [[$class: 'cloneoption', depth: 1, notags: false, reference: '', shallow: true]],\n                                branches: [[name: \"master\"]],userremoteconfigs: [[url: \"ssh://git@xx:2224/rdito/yw_db.git\"]]]\n                            )\n                        }catch(exc) {\n                            env.reason = \"拉取脚本出错\"\n                            throw(exc)\n                        }\n                    }\n                }\n            }\n        }\n        stage('拉取ansible剧本') {\n            steps {\n                dir(\"/var/jenkins_home/ansible\"){\n                    script {\n                        try {\n                            checkout(\n                                [$class: 'gitscm', dogeneratesubmoduleconfigurations: false, submodulecfg: [], extensions: [[$class: 'cloneoption', depth: 1, notags: false, reference: '', shallow: true]],\n                                branches: [[name: \"master\"]],userremoteconfigs: [[url: \"ssh://git@xx:2224/rdito/ansible.git\"]]]\n                            )\n                        }catch(exc) {\n                            env.reason = \"拉取ansible剧本出错\"\n                            throw(exc)\n                        }\n                    }\n                }\n            }\n        }\n        stage('生成部署文件') {\n            steps {\n                sh label: '', script: '/var/jenkins_home/script/saas/php_deploy1.sh scrm.web_php_business'\n                sh label: '', script: '/var/jenkins_home/script/saas/go_deploy.sh'\n                sh label: '', script: '/var/jenkins_home/script/saas/h5_deploy.sh scrm.web_admin'\n                sh label: '', script: '/var/jenkins_home/script/saas/h5_deploy.sh scrm.web_h5'\n                sh label: '', script: '/var/jenkins_home/script/saas/h5_deploy.sh web_h5_bizshow'\n                sh label: '', script: '/var/jenkins_home/script/saas/php_deploy1.sh web_php_bizshow'\n                sh label: '', script: '/var/jenkins_home/script/saas/php_deploy1.sh web_php_cms'\n            }\n        }\n        stage('打包') {\n            steps {\n                script{\n                    try{\n                        sh '''        \n                        cd \"$workspace\"\n                        chmod +x *.sh\n                        tar -zcf scrm_web_php_business.tar.gz $(ls *php*.sh | grep -ve \"bizshow|cms\")\n                        tar -zcf scrm_web_php_bizshow.tar.gz $(ls *php*.sh | grep \"bizshow\")\n                        tar -zcf scrm_web_php_cms.tar.gz $(ls *php*.sh | grep \"cms\")\n                        '''\n                    }catch(exc) {\n                        env.reason = \"打包文件出错\"\n                        throw(exc)\n                    }\n                }\n            }\n        }\n        stage('拷贝') {\n            steps {\n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend.yml', sudouser: null, tags: 'cp_business'\n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_fronted.yml', sudouser: null, tags: 'cp'\n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend.yml', sudouser: null, tags: 'cp_go'               \n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend01.yml', sudouser: null, tags: 'cp-saas-backend01'\n            }\n        }\n        stage('部署go服务') {\n            steps {\n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend.yml', sudouser: null, tags: 'scrm_go'\n            }\n        }\n        stage('部署web_php_business服务') {\n            parallel{\n                stage('部署backend服务') {\n                    steps {\n                        ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend.yml', sudouser: null, tags: 'scrm_web_php_business'\n                    }\n                }\n                stage('部署backend01服务') {\n                    steps {\n                        ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend01.yml', sudouser: null, tags: 'scrm_backend01'\n                    }\n                }                \n            }\n        }\n        // stage('部署web_php_business服务') {\n        //     steps {\n        //         ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend.yml', sudouser: null, tags: 'scrm_web_php_business'\n        //         ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend01.yml', sudouser: null, tags: 'scrm_backend01'\n        //     }\n        // }\n        stage('部署web_admin服务') {\n            steps {\n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_fronted.yml', sudouser: null, tags: 'scrm_web_admin'\n            }\n        }\n        stage('部署web_h5服务') {\n            steps {\n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_fronted.yml', sudouser: null, tags: 'scrm_web_h5'\n            }\n        }\n\n        stage('部署web_h5_bizshow服务') {\n            steps {\n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_fronted.yml', sudouser: null, tags: 'scrm_web_h5_bizshow'\n            }\n        }\n\n        stage('部署web_php_bizshow服务') {\n            when {\n                expression { params.bizshow_cms_deploy == '是'}\n            }\n            steps {\n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend_bizshow.yml', sudouser: null, tags: 'cp_bizshow'\n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend_bizshow.yml', sudouser: null, tags: 'scrm_web_php_bizshow'\n            }\n        }\n\n        stage('部署web_php_cms服务') {\n            when {\n                expression { params.bizshow_cms_deploy == '是'}\n            }\n            steps {\n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend_cms.yml', sudouser: null, tags: 'cp_cms'\n                ansibleplaybook become: true, becomeuser: 'admin', credentialsid: 'deploy_pro', extras: '--extra-vars \"workspace=${workspace}\"', disablehostkeychecking: true, inventory: '/var/jenkins_home/ansible/hosts', playbook: '/var/jenkins_home/ansible/saas/saas_backend_cms.yml', sudouser: null, tags: 'scrm_web_php_cms'\n            }\n        }\n\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n\n\n# ansible-playbook剧本\n\n---\n- hosts: saas_dev\n  remote_user: admin\n  gather_facts: false\n  become: yes\n  vars:\n    filename: /var/jenkins_home/workspace/saas/scrm.web_php_bizshow/*php*.sh\n    filename1: /var/jenkins_home/workspace/saas/scrm.web_php_cms/*php*.sh\n  tasks:\n    - name: \"复制主机上的文件到test服务器上\"\n      copy:\n        src: \"{{ item }}\"\n        dest: \"/opt/apps/saas_docker/scrm_web_php_bizshow/\"\n        owner: admin\n        group: admin\n        mode: 0755\n      with_fileglob:\n        - \"{{ filename }}\"\n      tags:\n        - cp_bizshow\n    - name: \"复制主机上的文件到test服务器上\"\n      copy:\n        src: \"{{ item }}\"\n        dest: \"/opt/apps/saas_docker/scrm_web_php_cms/\"\n        owner: admin\n        group: admin\n        mode: 0755\n      with_fileglob:\n        - \"{{ filename1 }}\"\n      tags:\n        - cp_cms\n    - name: \"创建远程主机上的部署目录\"\n      file: path=/opt/apps/saas_docker/{{sub_dir}} state=directory\n      tags: create_dir\n    - name: \"复制主机上的文件到web_php_business服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"/var/jenkins_home/workspace/saas/scrm.web_php_business/web_php_business.sh\",dest: \"/opt/apps/saas_docker/scrm_web_php_business/\"}\n      tags:\n        - cp_php_business\n    - name: \"复制主机上的文件到web_admin服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"/var/jenkins_home/workspace/saas/scrm.web_admin/web_admin.sh\",dest: \"/opt/apps/saas_docker/scrm_web_admin/\"}\n      tags:\n        - cp_web_admin\n    - name: \"复制主机上的文件到dev服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"/var/jenkins_home/workspace/saas/scrm.web_h5/web_h5.sh\",dest: \"/opt/apps/saas_docker/scrm_web_h5/\"}\n      tags:\n        - cp_web_h5\n    - name: \"复制主机上的文件到dev服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"/var/jenkins_home/workspace/saas/scrm.web_h5_bizshow/h5_bizshow_deploy.sh\",dest: \"/opt/apps/saas_docker/scrm_web_h5_bizshow/\"}\n      tags:\n        - cp_web_h5_bizshow\n    - name: \"复制主机上的文件到dev服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"/var/jenkins_home/workspace/saas/scrm.web_php_bizshow/php_bizshow_deploy.sh\",dest: \"/opt/apps/saas_docker/scrm_web_php_bizshow/\"}\n      tags:\n        - cp_php_bizshow\n    - name: \"复制主机上的文件到dev服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - { src: \"/var/jenkins_home/workspace/saas/scrm.web_php_cms/php_cms_deploy.sh\",dest: \"/opt/apps/saas_docker/scrm_web_php_cms/\" }\n      tags:\n        - cp_php_cms\n    - name: \"execute the scrm_web_php_business script\"\n      shell: cd /opt/apps/saas_docker/scrm_web_php_business && ./web_php_business.sh\n      tags:\n        - scrm_web_php_business\n    - name: \"execute the scrm_web_php_bizshow script\"\n      shell: cd /opt/apps/saas_docker/scrm_web_php_bizshow && ./php_bizshow_deploy.sh\n      tags:\n        - scrm_web_php_bizshow\n    - name: \"复制主机上的文件到saas-backend01服务器上\"\n      copy:\n        src: \"{{item.src}}\"\n        dest: \"{{item.dest}}\"\n        owner: admin\n        group: admin\n        mode: 0700\n      with_items:\n        - {src: \"{{ workspace }}/scrm_web_php_gaoyinibm.sh\",dest: \"/opt/apps/saas_docker/scrm_web_php_gaoyinibm/\"}\n      tags:\n        - cp-saas-backend01\n    - name: \"execute the script\"\n      shell: |\n        /home/admin/script/init.sh stop \n        cd /opt/apps/saas_docker/scrm_web_php_gaoyinibm && chmod +x scrm_web_php_gaoyinibm.sh && ./scrm_web_php_gaoyinibm.sh\n        /home/admin/script/init.sh start\n      tags:\n        - scrm_backend01\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n",charsets:{cjk:!0}},{title:"如果面试时大家都说真话",frontmatter:{title:"如果面试时大家都说真话",date:"2020-01-06T14:28:22.000Z",categories:"随笔",tags:["面试题集"],permalink:"/pages/0cd089/",readingShow:"top",description:"如果面试时大家都说真话… ",meta:[{name:"twitter:title",content:"如果面试时大家都说真话"},{name:"twitter:description",content:"如果面试时大家都说真话… "},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/02.%E5%A6%82%E6%9E%9C%E9%9D%A2%E8%AF%95%E6%97%B6%E5%A4%A7%E5%AE%B6%E9%83%BD%E8%AF%B4%E7%9C%9F%E8%AF%9D.html"},{property:"og:type",content:"article"},{property:"og:title",content:"如果面试时大家都说真话"},{property:"og:description",content:"如果面试时大家都说真话… "},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/02.%E5%A6%82%E6%9E%9C%E9%9D%A2%E8%AF%95%E6%97%B6%E5%A4%A7%E5%AE%B6%E9%83%BD%E8%AF%B4%E7%9C%9F%E8%AF%9D.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2020-01-06T14:28:22.000Z"},{property:"article:tag",content:"面试题集"},{itemprop:"name",content:"如果面试时大家都说真话"},{itemprop:"description",content:"如果面试时大家都说真话… "}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/02.%E5%A6%82%E6%9E%9C%E9%9D%A2%E8%AF%95%E6%97%B6%E5%A4%A7%E5%AE%B6%E9%83%BD%E8%AF%B4%E7%9C%9F%E8%AF%9D.html",relativePath:"02.生活/01.随笔/02.如果面试时大家都说真话.md",key:"v-5aabcc3a",path:"/pages/0cd089/",headersStr:null,content:"**如果面试时大家都说真话… **\n\n**面试官：**你好，这是你面试的第一家公司吗？\n\n**程序员小王：**当然不是啦，面了30多家，都不要我。\n\n**面试官：**哦哦哦，没事，我们面试了50多个，1个都不愿意来呢。你简历上写的5年Java开发经验…\n\n**程序员小王：**大学编程设计也算进去了，全靠同学我划水！实际上工作不到3年…\n\n**面试官：**曾参与主导十万级以上用户的中大型项目研发…\n\n**程序员小王：**之前公司负责一个政府外包项目，我提了一丁点儿意见…\n\n**面试官：**精通JAVA/JavaScript，熟练掌握IO，多线程、集合等基础类库；熟悉常见设计模式，熟悉dubbo以及dubbo的服务治理；精通Spring、MyBatis等流行开源框架；有高并发高流量互联网分布式开发经验；熟悉数据库原理和常用性能优化技术…\n\n**程序员小王：**都是吹的，知道一点儿，也就性能优化稍微了解点儿。\n\n**面试官：**那就好！吓我一跳，这些你要是都精通，我们肯定要不起！\n\n我们公司最近打算做个电商app项目，类似淘宝那种，那你就讲讲性能优化相关吧。\n\n**程序员小王：**性能优化涉及到的是方方面面，从基础代码性能优化，到JVM深度调优、设计模式优化，再到数据库调优、并发编程性能优化，这些我虽然没用过，但是都听过！工作中一边百度，一边Google，大都可以解决的！\n\n**面试官：**外瑞外瑞good啊！！！那谈谈薪资，你期望薪资\n\n是多少？\n\n**程序员小王：**我期望薪资写的25K，但7K也可以干，就是会偷懒。钱多点，干活就勤快点！\n\n**面试官：**Hmmm，我们写的是15~30K，实际上最多只给到10K，既然你水平有限，那我就大方点给到8K！但是要经常加班哦！\n\n**程序员小王：**可以的！反正加班我也是摸鱼！\n\n**面试官：**行吧，明天就来上班吧！\n\n**程序员小王：**好嘞！",normalizedContent:"**如果面试时大家都说真话… **\n\n**面试官：**你好，这是你面试的第一家公司吗？\n\n**程序员小王：**当然不是啦，面了30多家，都不要我。\n\n**面试官：**哦哦哦，没事，我们面试了50多个，1个都不愿意来呢。你简历上写的5年java开发经验…\n\n**程序员小王：**大学编程设计也算进去了，全靠同学我划水！实际上工作不到3年…\n\n**面试官：**曾参与主导十万级以上用户的中大型项目研发…\n\n**程序员小王：**之前公司负责一个政府外包项目，我提了一丁点儿意见…\n\n**面试官：**精通java/javascript，熟练掌握io，多线程、集合等基础类库；熟悉常见设计模式，熟悉dubbo以及dubbo的服务治理；精通spring、mybatis等流行开源框架；有高并发高流量互联网分布式开发经验；熟悉数据库原理和常用性能优化技术…\n\n**程序员小王：**都是吹的，知道一点儿，也就性能优化稍微了解点儿。\n\n**面试官：**那就好！吓我一跳，这些你要是都精通，我们肯定要不起！\n\n我们公司最近打算做个电商app项目，类似淘宝那种，那你就讲讲性能优化相关吧。\n\n**程序员小王：**性能优化涉及到的是方方面面，从基础代码性能优化，到jvm深度调优、设计模式优化，再到数据库调优、并发编程性能优化，这些我虽然没用过，但是都听过！工作中一边百度，一边google，大都可以解决的！\n\n**面试官：**外瑞外瑞good啊！！！那谈谈薪资，你期望薪资\n\n是多少？\n\n**程序员小王：**我期望薪资写的25k，但7k也可以干，就是会偷懒。钱多点，干活就勤快点！\n\n**面试官：**hmmm，我们写的是15~30k，实际上最多只给到10k，既然你水平有限，那我就大方点给到8k！但是要经常加班哦！\n\n**程序员小王：**可以的！反正加班我也是摸鱼！\n\n**面试官：**行吧，明天就来上班吧！\n\n**程序员小王：**好嘞！",charsets:{cjk:!0}},{title:"男人心智成熟的九大表现",frontmatter:{title:"男人心智成熟的九大表现",categories:"随笔",tags:["随笔"],date:"2022-12-09T20:46:02.000Z",permalink:"/pages/aa6ada/",readingShow:"top",description:"我国《婚姻法》规定，结婚年龄，男不得早于22周岁，女不得早于20周岁。",meta:[{name:"twitter:title",content:"男人心智成熟的九大表现"},{name:"twitter:description",content:"我国《婚姻法》规定，结婚年龄，男不得早于22周岁，女不得早于20周岁。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/01.%E7%94%B7%E4%BA%BA%E5%BF%83%E6%99%BA%E6%88%90%E7%86%9F%E7%9A%84%E4%B9%9D%E5%A4%A7%E8%A1%A8%E7%8E%B0.html"},{property:"og:type",content:"article"},{property:"og:title",content:"男人心智成熟的九大表现"},{property:"og:description",content:"我国《婚姻法》规定，结婚年龄，男不得早于22周岁，女不得早于20周岁。"},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/01.%E7%94%B7%E4%BA%BA%E5%BF%83%E6%99%BA%E6%88%90%E7%86%9F%E7%9A%84%E4%B9%9D%E5%A4%A7%E8%A1%A8%E7%8E%B0.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:46:02.000Z"},{property:"article:tag",content:"随笔"},{itemprop:"name",content:"男人心智成熟的九大表现"},{itemprop:"description",content:"我国《婚姻法》规定，结婚年龄，男不得早于22周岁，女不得早于20周岁。"}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/01.%E7%94%B7%E4%BA%BA%E5%BF%83%E6%99%BA%E6%88%90%E7%86%9F%E7%9A%84%E4%B9%9D%E5%A4%A7%E8%A1%A8%E7%8E%B0.html",relativePath:"02.生活/01.随笔/01.男人心智成熟的九大表现.md",key:"v-099c9ca4",path:"/pages/aa6ada/",headersStr:null,content:"我国《婚姻法》规定，结婚年龄，男不得早于22周岁，女不得早于20周岁。\n\n很多网友会调侃，说这是因为，男人普遍要比女人晚成熟，我在网上也搜索了一些相关的资料，并没有找到权威的信息来源渠道，以证明此观点的权威性。\n\n一个男人的成熟与否，先要看其心智是否成熟，那么，男人心智成熟的表现有哪些呢？\n\n1、建立原则\n\n不论是在工作中，还是在生活中，又或是在两个人的热恋中，都能建立自己的原则。原则能让我们在一段恋爱过程中，更清晰地承担起对另一方的爱，同时，又能保证自己的独立性。\n\n2、逻辑能力\n\n有基本的分辨是非对错、社会中的黑白灰的逻辑思维能力。其实，目前社会中的大多数人，都是比较缺乏逻辑思维能力的，很多人话说了很多，但是，还是搞不清楚TA想要表达什么？说话没有主次之分的人，随处可见。\n\n更严重点来说，遇到什么事件，或是听到什么事件，就会立即下结论，而不愿意动动大脑，理性地思考思考，再决定自己的下一步要怎么走。所以，逻辑能力，是一个男人心智成熟很重要的一点表现。\n\n3、看淡某些事的能力\n\n遇到一些让自己不开心的事，或是遇到一些跟自己想法相反的人，就想着一定要让TA人的想法转变，跟自己的想法一样。经常性地想着如何去改变别人的想法，这是男人心智不成熟的表现之一。\n\n要知道，世界上的人各种各样，人最难做的一件事就是：去改变TA人的想法。就算是上帝，至今也没能完成这项伟业，你又谈何能行！\n\n4、不卑不亢\n\n当别人赞美自己的时候，自己仍然知道自己的能力边界在哪里，不高傲，不作作，还能很低调地应对。当别人恶意打击时，也能做到不自卑，相信自己。\n\n当自己失败时，能够理性地坦然面对，并从失败中吸取教训，为下一次启航做准备，而不是失败了就从此一蹶不振。\n\n5、懂得规划\n\n不再盲目地生活、工作、学习，懂得给自己做相应的规划，并且，严格要求自己去执行自己制定的规划。\n\n6、时间是命\n\n懂得珍惜时间，明白时间的重要性，时间对于我们每人都一样的公平。不论是富人还是穷人，每天都是24小时，关键看我们每个人如何去使用时间。\n\n7、控制情绪\n\n这一点我知道很难，不论是几岁的孩子，还是几十岁的老人，始终都会有情绪，但是心智成熟的男人，会懂得更好地去控制自己的情绪，而不是只顾着发泄情绪。\n\n8、理解父母\n\n父母的唠叨，在父母一辈看来，就是对我们的爱，但是，对于我们看来，总会觉得很反感。但是，当你能换位思考，或是你有一天为人父母的时候，此情此景你会怎么做，就能对父母多点理解了。\n\n9、做自己\n\n做真正的自己，不用太刻意关注别人看待自己的眼光，因为，你的生活不是别人给的，那么，你又为何要那么在意别人如何看待你呢。\n\n中国人最喜欢干的一件事就是“管别人”，但是，这些人其实没搞明白一点，别人的决定，之后产生的后果，只有当事人自己承担，而那些“管别人”的人却不承担任何风险，这难道不是很有问题吗？",normalizedContent:"我国《婚姻法》规定，结婚年龄，男不得早于22周岁，女不得早于20周岁。\n\n很多网友会调侃，说这是因为，男人普遍要比女人晚成熟，我在网上也搜索了一些相关的资料，并没有找到权威的信息来源渠道，以证明此观点的权威性。\n\n一个男人的成熟与否，先要看其心智是否成熟，那么，男人心智成熟的表现有哪些呢？\n\n1、建立原则\n\n不论是在工作中，还是在生活中，又或是在两个人的热恋中，都能建立自己的原则。原则能让我们在一段恋爱过程中，更清晰地承担起对另一方的爱，同时，又能保证自己的独立性。\n\n2、逻辑能力\n\n有基本的分辨是非对错、社会中的黑白灰的逻辑思维能力。其实，目前社会中的大多数人，都是比较缺乏逻辑思维能力的，很多人话说了很多，但是，还是搞不清楚ta想要表达什么？说话没有主次之分的人，随处可见。\n\n更严重点来说，遇到什么事件，或是听到什么事件，就会立即下结论，而不愿意动动大脑，理性地思考思考，再决定自己的下一步要怎么走。所以，逻辑能力，是一个男人心智成熟很重要的一点表现。\n\n3、看淡某些事的能力\n\n遇到一些让自己不开心的事，或是遇到一些跟自己想法相反的人，就想着一定要让ta人的想法转变，跟自己的想法一样。经常性地想着如何去改变别人的想法，这是男人心智不成熟的表现之一。\n\n要知道，世界上的人各种各样，人最难做的一件事就是：去改变ta人的想法。就算是上帝，至今也没能完成这项伟业，你又谈何能行！\n\n4、不卑不亢\n\n当别人赞美自己的时候，自己仍然知道自己的能力边界在哪里，不高傲，不作作，还能很低调地应对。当别人恶意打击时，也能做到不自卑，相信自己。\n\n当自己失败时，能够理性地坦然面对，并从失败中吸取教训，为下一次启航做准备，而不是失败了就从此一蹶不振。\n\n5、懂得规划\n\n不再盲目地生活、工作、学习，懂得给自己做相应的规划，并且，严格要求自己去执行自己制定的规划。\n\n6、时间是命\n\n懂得珍惜时间，明白时间的重要性，时间对于我们每人都一样的公平。不论是富人还是穷人，每天都是24小时，关键看我们每个人如何去使用时间。\n\n7、控制情绪\n\n这一点我知道很难，不论是几岁的孩子，还是几十岁的老人，始终都会有情绪，但是心智成熟的男人，会懂得更好地去控制自己的情绪，而不是只顾着发泄情绪。\n\n8、理解父母\n\n父母的唠叨，在父母一辈看来，就是对我们的爱，但是，对于我们看来，总会觉得很反感。但是，当你能换位思考，或是你有一天为人父母的时候，此情此景你会怎么做，就能对父母多点理解了。\n\n9、做自己\n\n做真正的自己，不用太刻意关注别人看待自己的眼光，因为，你的生活不是别人给的，那么，你又为何要那么在意别人如何看待你呢。\n\n中国人最喜欢干的一件事就是“管别人”，但是，这些人其实没搞明白一点，别人的决定，之后产生的后果，只有当事人自己承担，而那些“管别人”的人却不承担任何风险，这难道不是很有问题吗？",charsets:{cjk:!0}},{title:"这四个故事小段，够你受用一生",frontmatter:{title:"这四个故事小段，够你受用一生",date:"2023-02-23T12:14:52.000Z",permalink:"/pages/7b0831/",categories:["生活","随笔"],tags:[null],readingShow:"top",description:"一、深刻",meta:[{name:"twitter:title",content:"这四个故事小段，够你受用一生"},{name:"twitter:description",content:"一、深刻"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/03.%E8%BF%99%E5%9B%9B%E4%B8%AA%E6%95%85%E4%BA%8B%E5%B0%8F%E6%AE%B5%EF%BC%8C%E5%A4%9F%E4%BD%A0%E5%8F%97%E7%94%A8%E4%B8%80%E7%94%9F.html"},{property:"og:type",content:"article"},{property:"og:title",content:"这四个故事小段，够你受用一生"},{property:"og:description",content:"一、深刻"},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/03.%E8%BF%99%E5%9B%9B%E4%B8%AA%E6%95%85%E4%BA%8B%E5%B0%8F%E6%AE%B5%EF%BC%8C%E5%A4%9F%E4%BD%A0%E5%8F%97%E7%94%A8%E4%B8%80%E7%94%9F.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-23T12:14:52.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"这四个故事小段，够你受用一生"},{itemprop:"description",content:"一、深刻"}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/03.%E8%BF%99%E5%9B%9B%E4%B8%AA%E6%95%85%E4%BA%8B%E5%B0%8F%E6%AE%B5%EF%BC%8C%E5%A4%9F%E4%BD%A0%E5%8F%97%E7%94%A8%E4%B8%80%E7%94%9F.html",relativePath:"02.生活/01.随笔/03.这四个故事小段，够你受用一生.md",key:"v-5436895e",path:"/pages/7b0831/",headersStr:null,content:"# 这四个故事小段，够你受用一生\n\n一、深刻\n\n刚刚在电梯间看见一小孩儿在吃雪糕，出于关心，顺口告诉他：“这么凉的天，会吃坏身         体的！”。\n\n小孩告诉我，他的奶奶活了103岁。\n\n我问：“吃雪糕吃的？”\n\n他说：“不是，我奶奶从来不管闲事！”\n\n多么深刻！现在终于知道自己为什么衰老得这么快了！\n\n瞎操心操的……\n\n二、心累\n\n现在骗子真多，刚刚又看到新闻里说储户存款消失，几十万不翼而飞。\n\n我急忙骑自行车到银行，赶紧插卡输密码查看。\n\n还好，8块钱还在，这才松了一口气。\n\n急死我了！\n\n以后再也不看新闻了，心好累！\n\n走出银行，心更累了：8块钱还在，自行车却不见了…\n\n三、别急\n\n大妈早上去广场散步，看到有个老头拿着海绵笔在地上写大字，忍不住凑上去看。\n\n老头看了大妈一眼，提笔写了个“滚”字。\n\n大妈心想：看一下至于吗？……\n\n老头又看大妈一眼，又写个“滚”。\n\n大妈再也忍不住了，上去一脚将老头踢倒在地……\n\n警察来了问咋回事，老头委屈地说：\n\n“我就想写句‘滚滚长江东逝水’，刚写头两个字，就被这个神经病踹倒了”。\n\n所以呀，朋友们，凡事别急！\n\n四、忍着点\n\n一个姑娘上了高铁，见自己的座位上坐着一男士。\n\n她核对自己的票，客气地说：“先生，您坐错位置了吧？”\n\n男士拿出票，大声嚷嚷：“看清楚点，这是我的座位，你瞎了眼吗？！”\n\n女孩仔细看了他的票，不再做声，默默地站在他的身旁。\n\n一会儿火车开动了，女孩低头轻轻地对男士说：“先生，您没坐错位，但您坐错了车！这         是开往上海的，你的车票是去哈尔滨的。”\n\n有一种忍让，叫做让你后悔都来不及，如果嚎叫能解决问题，驴早就统治了世界。",normalizedContent:"# 这四个故事小段，够你受用一生\n\n一、深刻\n\n刚刚在电梯间看见一小孩儿在吃雪糕，出于关心，顺口告诉他：“这么凉的天，会吃坏身         体的！”。\n\n小孩告诉我，他的奶奶活了103岁。\n\n我问：“吃雪糕吃的？”\n\n他说：“不是，我奶奶从来不管闲事！”\n\n多么深刻！现在终于知道自己为什么衰老得这么快了！\n\n瞎操心操的……\n\n二、心累\n\n现在骗子真多，刚刚又看到新闻里说储户存款消失，几十万不翼而飞。\n\n我急忙骑自行车到银行，赶紧插卡输密码查看。\n\n还好，8块钱还在，这才松了一口气。\n\n急死我了！\n\n以后再也不看新闻了，心好累！\n\n走出银行，心更累了：8块钱还在，自行车却不见了…\n\n三、别急\n\n大妈早上去广场散步，看到有个老头拿着海绵笔在地上写大字，忍不住凑上去看。\n\n老头看了大妈一眼，提笔写了个“滚”字。\n\n大妈心想：看一下至于吗？……\n\n老头又看大妈一眼，又写个“滚”。\n\n大妈再也忍不住了，上去一脚将老头踢倒在地……\n\n警察来了问咋回事，老头委屈地说：\n\n“我就想写句‘滚滚长江东逝水’，刚写头两个字，就被这个神经病踹倒了”。\n\n所以呀，朋友们，凡事别急！\n\n四、忍着点\n\n一个姑娘上了高铁，见自己的座位上坐着一男士。\n\n她核对自己的票，客气地说：“先生，您坐错位置了吧？”\n\n男士拿出票，大声嚷嚷：“看清楚点，这是我的座位，你瞎了眼吗？！”\n\n女孩仔细看了他的票，不再做声，默默地站在他的身旁。\n\n一会儿火车开动了，女孩低头轻轻地对男士说：“先生，您没坐错位，但您坐错了车！这         是开往上海的，你的车票是去哈尔滨的。”\n\n有一种忍让，叫做让你后悔都来不及，如果嚎叫能解决问题，驴早就统治了世界。",charsets:{cjk:!0}},{title:"小说活着摘录",frontmatter:{title:"小说活着摘录",date:"2023-02-24T16:40:24.000Z",permalink:"/pages/e7e74b/",categories:["生活","随笔"],tags:[null],readingShow:"top",description:"苦，是生活的常态 那一天，蜜罐子里长大的徐福贵第一次挑起扁担，第一次来回步行数十里，用变卖全部祖产得来的铜钱， 还清了欠下的赌债。 沉重的扁担将他的肩膀压得血肉模糊。这是福贵第一次尝到生活的苦。 却不知道，在以后的四十年里，每一天都比今天更苦。",meta:[{name:"twitter:title",content:"小说活着摘录"},{name:"twitter:description",content:"苦，是生活的常态 那一天，蜜罐子里长大的徐福贵第一次挑起扁担，第一次来回步行数十里，用变卖全部祖产得来的铜钱， 还清了欠下的赌债。 沉重的扁担将他的肩膀压得血肉模糊。这是福贵第一次尝到生活的苦。 却不知道，在以后的四十年里，每一天都比今天更苦。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/04.%E5%B0%8F%E8%AF%B4%E6%B4%BB%E7%9D%80%E6%91%98%E5%BD%95.html"},{property:"og:type",content:"article"},{property:"og:title",content:"小说活着摘录"},{property:"og:description",content:"苦，是生活的常态 那一天，蜜罐子里长大的徐福贵第一次挑起扁担，第一次来回步行数十里，用变卖全部祖产得来的铜钱， 还清了欠下的赌债。 沉重的扁担将他的肩膀压得血肉模糊。这是福贵第一次尝到生活的苦。 却不知道，在以后的四十年里，每一天都比今天更苦。"},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/04.%E5%B0%8F%E8%AF%B4%E6%B4%BB%E7%9D%80%E6%91%98%E5%BD%95.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-24T16:40:24.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"小说活着摘录"},{itemprop:"description",content:"苦，是生活的常态 那一天，蜜罐子里长大的徐福贵第一次挑起扁担，第一次来回步行数十里，用变卖全部祖产得来的铜钱， 还清了欠下的赌债。 沉重的扁担将他的肩膀压得血肉模糊。这是福贵第一次尝到生活的苦。 却不知道，在以后的四十年里，每一天都比今天更苦。"}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/04.%E5%B0%8F%E8%AF%B4%E6%B4%BB%E7%9D%80%E6%91%98%E5%BD%95.html",relativePath:"02.生活/01.随笔/04.小说活着摘录.md",key:"v-5cac2a3b",path:"/pages/e7e74b/",headers:[{level:3,title:"《活着》-摘录",slug:"《活着》-摘录-2",normalizedTitle:"《活着》-摘录",charIndex:2}],headersStr:"《活着》-摘录",content:"# 《活着》-摘录\n\n\n# 《活着》-摘录\n\n * 人是为活着本身而活着的，而不是为活着之外的任何事物所活着。\n * 功名利禄，是我们行走世俗的奖励，却不是桎梏生命的枷锁。\n * 艰难困苦，是每个人成长必经的修行，而不是埋没希望的恶浪。\n\n苦，是生活的常态 那一天，蜜罐子里长大的徐福贵第一次挑起扁担，第一次来回步行数十里，用变卖全部祖产得来的铜钱， 还清了欠下的赌债。 沉重的扁担将他的肩膀压得血肉模糊。这是福贵第一次尝到生活的苦。 却不知道，在以后的四十年里，每一天都比今天更苦。\n\n老父被他的荒唐行径活活气死。 怀孕的妻子家珍被失望的岳父强行接走。 只剩下年迈的母亲、幼小的女儿，和福贵一起缩在破旧的茅草屋里相依为命。 为了养活自己和家人，福贵只得厚起颜面，走进曾经属于自己的大宅院，央求买走自家全部祖产的地主龙二租给他五亩地。 他笨拙着跟着其他佃户学习耕地、播种，每天都累得筋疲力竭。 直到家珍产子后从娘家归来，和丈夫一起劳作，才让福贵稍稍缓了一口气。\n\n可这段时间的艰苦日子，已经严重损害了福贵母亲的健康。 眼看养尊处优了大半辈子的母亲，被病痛折磨地痛苦不堪，福贵赶紧进城找医生。 却不料，恰好赶上军队在抓壮丁。福贵被裹挟着一路远走。这一走，就是两年。 两年里，福贵无时无刻不牵挂着母亲的病情，家人的安危，每一天，心里都如油煎火烧。 可他却不敢私逃。因为同行的老兵说，逃就意味着死得更快。而且即使逃离了这支队伍，还会被下一支队伍拉走。 好在，经历过无数次战火纷飞的生死一线，福贵还是活了下来，活着等到这支队伍溃败投降。 他急急忙忙赶回家中，却发现母亲早就病死，妻子憔悴不堪。曾经伶俐的女儿，变得又聋又哑。\n\n苦难如潮水般呼啸而至，一波未平一波又起。 像极了每个人的人生，总有问题层出不穷，总有难处让人不堪重负。\n\n有人月底考核已经迫在眉睫，难缠的客户还在百般刁难。 有人起早贪黑四处兼职，依然凑不够房贷、车贷和孩子高昂的补习费。 有人每天在996里筋疲力竭，在35岁危机的阴影下如履薄冰。 有人眼前还岁月静好，可只要出现一丁点意外，就能让生活分崩离析。 张爱玲说，在这个光怪陆离的人间，没有谁可以将日子过得行云流水。 身为成年人，谁不是一边在深夜里崩溃，一边在太阳升起后，咬紧牙关继续死扛。\n\n暖，是生命的慰藉 福贵被拉壮丁的那两年，最受煎熬的就是家珍。 她无数次进城打听丈夫的下落，却一次又一次失望而归。 没人知道福贵的下落，甚至没人知道福贵现在是生是死。 可即使最难的日子，家珍也没有想过离开徐家。 这个出身富裕家庭，又嫁入富贵人家的女学生，脱下了华美的旗袍，换上了粗布衣衫。 她用一双没沾过阳春水的柔弱双手，每日操持繁重的农活，艰难地将儿女拉扯长大，为婆婆养老送终。 等到福贵回来的时候，这个家虽然破败，却还有亲人在守候。 对于游子而言，这就是最大的安慰。\n\n回来以后，福贵安下心来与家人一起踏踏实实过日子。 可无论福贵和家珍如何拼命劳作，无论女儿凤霞和儿子有庆如何贴心懂事，田里的那点活计也填不饱全家人的肚子。 眼见着有庆已经到了上学的年纪，家里还凑不出上学的学费，福贵和家珍忍痛把凤霞送给别人当女儿。 每天牵肠挂肚，担心又聋又哑的女儿在别人家受了欺负。可当想家的凤霞偷跑回来后，福贵又是感动，又要硬起心肠，把女儿送了回去。 懂事的凤霞没有哭闹。她虽然无比舍不得父母，舍不得弟弟，还是乖乖牵着父亲的手，走了十几里路赶回城里。 临别的时候，她用冰凉的指尖，抚摸着父亲的脸庞。这份浓浓的依恋，一下子打动了福贵。 他转身背起女儿，一步步回到破败却温暖的家中，坚定地对家珍说：“就是全家都饿死，也不把凤霞送走。” 是啊，就算生活再难，只要有家人在一起，又有什么坎儿是过不去的呢。 就算人生再苦，只要有家人陪伴，总能在最细微处感受到温暖。\n\n也许是深夜到家时，发现妻子在保温桶里留了一碗热汤。 也许是筋疲力竭时，孩子送上一个沾满口水的亲吻。 也许是在忙到昏天黑地时，微信里静静躺着母亲发来的问候。 也许是到了走投无路的时候，父亲轻轻拍拍肩膀：“放心，有我。” 即使这力量十分轻微，哪怕能提供的支持微不足道。 可只要有这份温暖，就足以慰藉整个生命。 只要有家人站在身后，我们的人生就还有退路。\n\n活着很难，但熬过去就有希望 有段时间，福贵觉得生活开始有了盼头。 凤霞已经长大，每日挣的工分可以养活自己。 有庆一边读书，一边放羊，练就出一双飞毛腿，在学校的运动会上拿了第一名。 家珍依然贤惠能干，将所有家人照顾地妥妥帖帖。福贵自己也在地里踏踏实实苦干。 日子依旧穷困潦倒，可每一天都生机勃勃。 可在他最向往未来的时候，命运狞笑着砸灭了所有希望。\n\n有庆已经上到小学五年级，眼瞅着就能回家挣工分。却在一场献血中，被抽干血当场惨死。 凤霞幸运地找了个知冷知热的丈夫，好容易过了几天和美的日子，就因为难产大出血而死。 丧子丧女的悲痛压垮了本就患病多年的家珍。女儿死后三个月，她也撒手人寰。 临死前，她嘱托福贵：“你还得好好活下去，还有苦根和二喜。” 可命运的残酷超乎想象。没过几年，女婿二喜和外孙苦根先后因意外身亡。 在本应该儿孙绕膝的年龄，福贵失去了所有的亲人，所有的依靠，所有的指望。 这时的老汉，已经苦得连叹息都发不出来了。\n\n他在枕下放了十元钱。只想等自己死后，收尸的人能看在钱的份上，把自己和家人葬在一起。 却没想到，虽然腰常常疼，眼睛越来越花，他还是一天天活了下来。 攒够一头牛钱的时候，福贵去买了一头和自己一样老的牛。一人一牛，每天说说话，耕耕地，彼此相依为命。 也许从世俗的角度看，福贵的这一生活得既悲惨又失败。\n\n因为少年荒唐，他早早败光了所有的家业，没能像父亲的期望那样光宗耀祖。 因为中年恋家，他没有像一同当兵的春生那样，选择随解放军南下，也错失了走上仕途的机会。 因为命运不济，他失去了所有的亲人，只能孤独地等待老去。 可毕竟福贵还活着。 比起谋夺了他田产的龙二、曾经春风得意过的春生活得都长久。 这就是人生最大的收获。因为只有生命，才是世界上独一无二的稀缺品。 看透了人生的福贵，把每一天都活得极其认真。 种地的时候，他想尽办法鼓舞起老牛的志气。回家的路上，他的歌声在田埂上如晚风般飘扬。 这个老人的命运，明明比世上的大多数人都悲苦。却活成了比大多数人，都明亮的样子。 这就是生命的真相，你怎么想，就会怎么活。\n\n选择哭泣哀伤，注定将一路灰暗。 选择昂扬向上，就算风雨如晦，也能徒手劈开满目星光。 生活很难，活着更难。 可只要肯用力感受，这人间终究值得希望。",normalizedContent:"# 《活着》-摘录\n\n\n# 《活着》-摘录\n\n * 人是为活着本身而活着的，而不是为活着之外的任何事物所活着。\n * 功名利禄，是我们行走世俗的奖励，却不是桎梏生命的枷锁。\n * 艰难困苦，是每个人成长必经的修行，而不是埋没希望的恶浪。\n\n苦，是生活的常态 那一天，蜜罐子里长大的徐福贵第一次挑起扁担，第一次来回步行数十里，用变卖全部祖产得来的铜钱， 还清了欠下的赌债。 沉重的扁担将他的肩膀压得血肉模糊。这是福贵第一次尝到生活的苦。 却不知道，在以后的四十年里，每一天都比今天更苦。\n\n老父被他的荒唐行径活活气死。 怀孕的妻子家珍被失望的岳父强行接走。 只剩下年迈的母亲、幼小的女儿，和福贵一起缩在破旧的茅草屋里相依为命。 为了养活自己和家人，福贵只得厚起颜面，走进曾经属于自己的大宅院，央求买走自家全部祖产的地主龙二租给他五亩地。 他笨拙着跟着其他佃户学习耕地、播种，每天都累得筋疲力竭。 直到家珍产子后从娘家归来，和丈夫一起劳作，才让福贵稍稍缓了一口气。\n\n可这段时间的艰苦日子，已经严重损害了福贵母亲的健康。 眼看养尊处优了大半辈子的母亲，被病痛折磨地痛苦不堪，福贵赶紧进城找医生。 却不料，恰好赶上军队在抓壮丁。福贵被裹挟着一路远走。这一走，就是两年。 两年里，福贵无时无刻不牵挂着母亲的病情，家人的安危，每一天，心里都如油煎火烧。 可他却不敢私逃。因为同行的老兵说，逃就意味着死得更快。而且即使逃离了这支队伍，还会被下一支队伍拉走。 好在，经历过无数次战火纷飞的生死一线，福贵还是活了下来，活着等到这支队伍溃败投降。 他急急忙忙赶回家中，却发现母亲早就病死，妻子憔悴不堪。曾经伶俐的女儿，变得又聋又哑。\n\n苦难如潮水般呼啸而至，一波未平一波又起。 像极了每个人的人生，总有问题层出不穷，总有难处让人不堪重负。\n\n有人月底考核已经迫在眉睫，难缠的客户还在百般刁难。 有人起早贪黑四处兼职，依然凑不够房贷、车贷和孩子高昂的补习费。 有人每天在996里筋疲力竭，在35岁危机的阴影下如履薄冰。 有人眼前还岁月静好，可只要出现一丁点意外，就能让生活分崩离析。 张爱玲说，在这个光怪陆离的人间，没有谁可以将日子过得行云流水。 身为成年人，谁不是一边在深夜里崩溃，一边在太阳升起后，咬紧牙关继续死扛。\n\n暖，是生命的慰藉 福贵被拉壮丁的那两年，最受煎熬的就是家珍。 她无数次进城打听丈夫的下落，却一次又一次失望而归。 没人知道福贵的下落，甚至没人知道福贵现在是生是死。 可即使最难的日子，家珍也没有想过离开徐家。 这个出身富裕家庭，又嫁入富贵人家的女学生，脱下了华美的旗袍，换上了粗布衣衫。 她用一双没沾过阳春水的柔弱双手，每日操持繁重的农活，艰难地将儿女拉扯长大，为婆婆养老送终。 等到福贵回来的时候，这个家虽然破败，却还有亲人在守候。 对于游子而言，这就是最大的安慰。\n\n回来以后，福贵安下心来与家人一起踏踏实实过日子。 可无论福贵和家珍如何拼命劳作，无论女儿凤霞和儿子有庆如何贴心懂事，田里的那点活计也填不饱全家人的肚子。 眼见着有庆已经到了上学的年纪，家里还凑不出上学的学费，福贵和家珍忍痛把凤霞送给别人当女儿。 每天牵肠挂肚，担心又聋又哑的女儿在别人家受了欺负。可当想家的凤霞偷跑回来后，福贵又是感动，又要硬起心肠，把女儿送了回去。 懂事的凤霞没有哭闹。她虽然无比舍不得父母，舍不得弟弟，还是乖乖牵着父亲的手，走了十几里路赶回城里。 临别的时候，她用冰凉的指尖，抚摸着父亲的脸庞。这份浓浓的依恋，一下子打动了福贵。 他转身背起女儿，一步步回到破败却温暖的家中，坚定地对家珍说：“就是全家都饿死，也不把凤霞送走。” 是啊，就算生活再难，只要有家人在一起，又有什么坎儿是过不去的呢。 就算人生再苦，只要有家人陪伴，总能在最细微处感受到温暖。\n\n也许是深夜到家时，发现妻子在保温桶里留了一碗热汤。 也许是筋疲力竭时，孩子送上一个沾满口水的亲吻。 也许是在忙到昏天黑地时，微信里静静躺着母亲发来的问候。 也许是到了走投无路的时候，父亲轻轻拍拍肩膀：“放心，有我。” 即使这力量十分轻微，哪怕能提供的支持微不足道。 可只要有这份温暖，就足以慰藉整个生命。 只要有家人站在身后，我们的人生就还有退路。\n\n活着很难，但熬过去就有希望 有段时间，福贵觉得生活开始有了盼头。 凤霞已经长大，每日挣的工分可以养活自己。 有庆一边读书，一边放羊，练就出一双飞毛腿，在学校的运动会上拿了第一名。 家珍依然贤惠能干，将所有家人照顾地妥妥帖帖。福贵自己也在地里踏踏实实苦干。 日子依旧穷困潦倒，可每一天都生机勃勃。 可在他最向往未来的时候，命运狞笑着砸灭了所有希望。\n\n有庆已经上到小学五年级，眼瞅着就能回家挣工分。却在一场献血中，被抽干血当场惨死。 凤霞幸运地找了个知冷知热的丈夫，好容易过了几天和美的日子，就因为难产大出血而死。 丧子丧女的悲痛压垮了本就患病多年的家珍。女儿死后三个月，她也撒手人寰。 临死前，她嘱托福贵：“你还得好好活下去，还有苦根和二喜。” 可命运的残酷超乎想象。没过几年，女婿二喜和外孙苦根先后因意外身亡。 在本应该儿孙绕膝的年龄，福贵失去了所有的亲人，所有的依靠，所有的指望。 这时的老汉，已经苦得连叹息都发不出来了。\n\n他在枕下放了十元钱。只想等自己死后，收尸的人能看在钱的份上，把自己和家人葬在一起。 却没想到，虽然腰常常疼，眼睛越来越花，他还是一天天活了下来。 攒够一头牛钱的时候，福贵去买了一头和自己一样老的牛。一人一牛，每天说说话，耕耕地，彼此相依为命。 也许从世俗的角度看，福贵的这一生活得既悲惨又失败。\n\n因为少年荒唐，他早早败光了所有的家业，没能像父亲的期望那样光宗耀祖。 因为中年恋家，他没有像一同当兵的春生那样，选择随解放军南下，也错失了走上仕途的机会。 因为命运不济，他失去了所有的亲人，只能孤独地等待老去。 可毕竟福贵还活着。 比起谋夺了他田产的龙二、曾经春风得意过的春生活得都长久。 这就是人生最大的收获。因为只有生命，才是世界上独一无二的稀缺品。 看透了人生的福贵，把每一天都活得极其认真。 种地的时候，他想尽办法鼓舞起老牛的志气。回家的路上，他的歌声在田埂上如晚风般飘扬。 这个老人的命运，明明比世上的大多数人都悲苦。却活成了比大多数人，都明亮的样子。 这就是生命的真相，你怎么想，就会怎么活。\n\n选择哭泣哀伤，注定将一路灰暗。 选择昂扬向上，就算风雨如晦，也能徒手劈开满目星光。 生活很难，活着更难。 可只要肯用力感受，这人间终究值得希望。",charsets:{cjk:!0}},{title:"人生格言",frontmatter:{title:"人生格言",date:"2023-02-24T16:42:33.000Z",permalink:"/pages/58b3c0/",categories:["生活","随笔"],tags:[null],readingShow:"top",description:"",meta:[{name:"twitter:title",content:"人生格言"},{name:"twitter:description",content:""},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/05.%E4%BA%BA%E7%94%9F%E6%A0%BC%E8%A8%80.html"},{property:"og:type",content:"article"},{property:"og:title",content:"人生格言"},{property:"og:description",content:""},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/05.%E4%BA%BA%E7%94%9F%E6%A0%BC%E8%A8%80.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-24T16:42:33.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"人生格言"},{itemprop:"description",content:""}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/01.%E9%9A%8F%E7%AC%94/05.%E4%BA%BA%E7%94%9F%E6%A0%BC%E8%A8%80.html",relativePath:"02.生活/01.随笔/05.人生格言.md",key:"v-228a9bce",path:"/pages/58b3c0/",headersStr:null,content:"莫泊桑在《一生》里写道： “生活不可能像你想象得那么好， 但也不会像你想象得那么糟。 有时，我们可能脆弱得一句话就泪流满面； 有时，也发现自己咬着牙走了很长的路。” 人生的刺，就在这里， 留恋着不肯走的， 偏是你所不留恋的东西。",normalizedContent:"莫泊桑在《一生》里写道： “生活不可能像你想象得那么好， 但也不会像你想象得那么糟。 有时，我们可能脆弱得一句话就泪流满面； 有时，也发现自己咬着牙走了很长的路。” 人生的刺，就在这里， 留恋着不肯走的， 偏是你所不留恋的东西。",charsets:{cjk:!0}},{title:"运维10道基础面试题",frontmatter:{title:"运维10道基础面试题",date:"2020-01-13T14:28:22.000Z",categories:["面试"],tags:["面试题集"],permalink:"/pages/f6c9c9/",readingShow:"top",description:"反向代理、负载均衡",meta:[{name:"image",content:"https://i.loli.net/2020/06/14/BywTI6QVEYtHr9v.png"},{name:"twitter:title",content:"运维10道基础面试题"},{name:"twitter:description",content:"反向代理、负载均衡"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://i.loli.net/2020/06/14/BywTI6QVEYtHr9v.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/02.%E9%9D%A2%E8%AF%95/01.%E8%BF%90%E7%BB%B410%E9%81%93%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%98.html"},{property:"og:type",content:"article"},{property:"og:title",content:"运维10道基础面试题"},{property:"og:description",content:"反向代理、负载均衡"},{property:"og:image",content:"https://i.loli.net/2020/06/14/BywTI6QVEYtHr9v.png"},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/02.%E9%9D%A2%E8%AF%95/01.%E8%BF%90%E7%BB%B410%E9%81%93%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%98.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2020-01-13T14:28:22.000Z"},{property:"article:tag",content:"面试题集"},{itemprop:"name",content:"运维10道基础面试题"},{itemprop:"description",content:"反向代理、负载均衡"},{itemprop:"image",content:"https://i.loli.net/2020/06/14/BywTI6QVEYtHr9v.png"}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/02.%E9%9D%A2%E8%AF%95/01.%E8%BF%90%E7%BB%B410%E9%81%93%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%98.html",relativePath:"02.生活/02.面试/01.运维10道基础面试题.md",key:"v-4c3a43d2",path:"/pages/f6c9c9/",headers:[{level:2,title:"1.date命令获取昨天时间",slug:"_1-date命令获取昨天时间",normalizedTitle:"1.date命令获取昨天时间",charIndex:2},{level:2,title:"2.nginx用过哪些模块？反向代理、负载均衡如何配置",slug:"_2-nginx用过哪些模块-反向代理、负载均衡如何配置",normalizedTitle:"2.nginx用过哪些模块？反向代理、负载均衡如何配置",charIndex:56},{level:2,title:"3.安装centos系统后会做哪些优化，请举例说明",slug:"_3-安装centos系统后会做哪些优化-请举例说明",normalizedTitle:"3.安装centos系统后会做哪些优化，请举例说明",charIndex:463},{level:2,title:"4.某时刻流量超过阀值，该如何应对",slug:"_4-某时刻流量超过阀值-该如何应对",normalizedTitle:"4.某时刻流量超过阀值，该如何应对",charIndex:733},{level:2,title:"5.由于磁盘空间紧张现在要求只能保留最近7天的访问日志，请问如何解决？请给",slug:"_5-由于磁盘空间紧张现在要求只能保留最近7天的访问日志-请问如何解决-请给",normalizedTitle:"5.由于磁盘空间紧张现在要求只能保留最近7天的访问日志，请问如何解决？请给",charIndex:1145},{level:2,title:"6.每天晚上12点，打包站点目录/var/www/html 备份到/data目录下",slug:"_6-每天晚上12点-打包站点目录-var-www-html-备份到-data目录下",normalizedTitle:"6.每天晚上12点，打包站点目录/var/www/html 备份到/data目录下",charIndex:1322},{level:2,title:"7.写一个脚本，实现判断192.168.1.0/24网络里，当前在线的ip有哪些，能ping通则认为在线",slug:"_7-写一个脚本-实现判断192-168-1-0-24网络里-当前在线的ip有哪些-能ping通则认为在线",normalizedTitle:"7.写一个脚本，实现判断192.168.1.0/24网络里，当前在线的ip有哪些，能ping通则认为在线",charIndex:1432},{level:2,title:"#!/bin/bash",slug:"bin-bash",normalizedTitle:"#!/bin/bash",charIndex:1489},{level:2,title:"8.统计出nginx的access.log中每个接口的响应码情况，url为第5列，响应码为第6列，分隔符为“|”",slug:"_8-统计出nginx的access-log中每个接口的响应码情况-url为第5列-响应码为第6列-分隔符为",normalizedTitle:"8.统计出nginx的access.log中每个接口的响应码情况，url为第5列，响应码为第6列，分隔符为“|”",charIndex:1753},{level:2,title:"9.如何查看http的并发请求数与其tcp连接状态？",slug:"_9-如何查看http的并发请求数与其tcp连接状态",normalizedTitle:"9.如何查看http的并发请求数与其tcp连接状态？",charIndex:1856},{level:2,title:"10.说说tcp/ip的七层模型",slug:"_10-说说tcp-ip的七层模型",normalizedTitle:"10.说说tcp/ip的七层模型",charIndex:1974}],headersStr:"1.date命令获取昨天时间 2.nginx用过哪些模块？反向代理、负载均衡如何配置 3.安装centos系统后会做哪些优化，请举例说明 4.某时刻流量超过阀值，该如何应对 5.由于磁盘空间紧张现在要求只能保留最近7天的访问日志，请问如何解决？请给 6.每天晚上12点，打包站点目录/var/www/html 备份到/data目录下 7.写一个脚本，实现判断192.168.1.0/24网络里，当前在线的ip有哪些，能ping通则认为在线 #!/bin/bash 8.统计出nginx的access.log中每个接口的响应码情况，url为第5列，响应码为第6列，分隔符为“|” 9.如何查看http的并发请求数与其tcp连接状态？ 10.说说tcp/ip的七层模型",content:'# 1.date命令获取昨天时间\n\ndate -d"1 day ago" +"%F %H:%M:%S"\n\n\n# 2.nginx用过哪些模块？反向代理、负载均衡如何配置\n\n反向代理、负载均衡\n\n反向代理配置\n\nserver{\n\nlisten 80;\n\nserver_name www.baidu.com;\n\nroot /var/log/www;\n\nlocation / {\n\nproxy_pass htttp://localhost:8089;\n\n}\n\n}\n\n负载均衡\n\nupstream www.baidu.com {\n\nip_hash;\n\nserver 192.168.1.10:8080;\n\nserver 192.168.1.11:8081;\n\n}\n\nserver{\n\nlisten 80;\n\nserver_name www.baidu.com;\n\nlocation / {\n\nproxy_pass http://www.baidu.com;\n\nindex index.html index.htm;\n\n}\n\n}\n\n\n# 3.安装centos系统后会做哪些优化，请举例说明\n\n修改ip地址、网关、主机名、DNS等\n\n关闭selinux，清空iptables\n\n添加普通用户并进行sudo授权管理\n\n更新yum源及必要软件安装\n\n定时自动更新服务器时间\n\n精简开机自启动服务\n\n定时自动清理/var/spool/clientmqueue/目录垃圾文件，放置inode节点被占满\n\n变更默认的ssh服务端口，禁止root用户远程连接\n\n锁定关键文件系统\n\n调整文件描述符大小\n\n调整字符集，使其支持中文\n\n去除系统及内核版本登录前的屏幕显示\n\n内核参数优化\n\n\n# 4.某时刻流量超过阀值，该如何应对\n\n1、不可预测流量（网站被恶意刷量；CDN回源抓取数据；合作业务平台调取平台数据等）\n\n2、可预测流量（突然爆发的社会热点，营销活动的宣传；）\n\n不管是可预测流量还是不可预测流量都会表现在带宽和网站整体架构的应对方案\n\n如果由于带宽原因引起，由于网站的并发量太高，达到服务器的吞吐极限，导致服务器宕机，这时需要做临时申请加大带宽，然后负载均衡分流。\n\n如果由于外网请求数据库，导致数据库频繁读写，数据库处理能力低，导致大量请求积压；如果是这种情况，就需要优化SQL，存储过程等，如果是请求过大，就要考虑做集群等。\n\n可预测流量的暴增也会拖慢网页的打开速度，甚至导致网站服务器宕机。要应对正常流量暴增，在流量高峰期到来之前就可以适当的调整，一般针对应用服务器的调整可以防止单点，负载均衡，高可用，增加后端web应用服务器数量，数据库读写分离，拆库拆表等，防止流量暴增导致服务器挂掉\n\n\n# 5.由于磁盘空间紧张现在要求只能保留最近7天的访问日志，请问如何解决？请给\n\n出解决办法或者配置或处理命令\n\ndate=date +%F_%T_%A\n\ncp ./access.log ./bak/access_"$date".log\n\nfind ./bak/ -name *.log -type f -mtime +7 -exec rm {};\n\n\n# 6.每天晚上12点，打包站点目录/var/www/html 备份到/data目录下\n\n00 00 * * * /usr/bin/tar -zvcf /data/html.tar.gz /var/www/html\n\n\n# 7.写一个脚本，实现判断192.168.1.0/24网络里，当前在线的ip有哪些，能ping通则认为在线\n\n\n# #!/bin/bash\n\nfor ip in {1..255}\n\ndo\n\nping -c3 192.168.1.$ip\n\nif [ $? -ne 0 ]\n\nthen echo "the host is 192.168.1.$ip up" >> ./up.list\n\nelse echo "the host is 192.168.1.$ip down" >> ./down.list\n\nfi\n\ndone\n\n或者\n\nyum install nmap -y\n\nnmap -sP 192.168.1.0/24 > ./upip\n\n\n# 8.统计出nginx的access.log中每个接口的响应码情况，url为第5列，响应码为第6列，分隔符为“|”\n\ncat access.log | awk -F \'|\' \'{print $6}\'\n\n\n# 9.如何查看http的并发请求数与其tcp连接状态？\n\n并发netstat -na | grep ESTABLIS | wc -l\n\ntcp数netstat -an|grep ":80 "｜grep -v grep|wc -l\n\n\n# 10.说说tcp/ip的七层模型\n\n',normalizedContent:'# 1.date命令获取昨天时间\n\ndate -d"1 day ago" +"%f %h:%m:%s"\n\n\n# 2.nginx用过哪些模块？反向代理、负载均衡如何配置\n\n反向代理、负载均衡\n\n反向代理配置\n\nserver{\n\nlisten 80;\n\nserver_name www.baidu.com;\n\nroot /var/log/www;\n\nlocation / {\n\nproxy_pass htttp://localhost:8089;\n\n}\n\n}\n\n负载均衡\n\nupstream www.baidu.com {\n\nip_hash;\n\nserver 192.168.1.10:8080;\n\nserver 192.168.1.11:8081;\n\n}\n\nserver{\n\nlisten 80;\n\nserver_name www.baidu.com;\n\nlocation / {\n\nproxy_pass http://www.baidu.com;\n\nindex index.html index.htm;\n\n}\n\n}\n\n\n# 3.安装centos系统后会做哪些优化，请举例说明\n\n修改ip地址、网关、主机名、dns等\n\n关闭selinux，清空iptables\n\n添加普通用户并进行sudo授权管理\n\n更新yum源及必要软件安装\n\n定时自动更新服务器时间\n\n精简开机自启动服务\n\n定时自动清理/var/spool/clientmqueue/目录垃圾文件，放置inode节点被占满\n\n变更默认的ssh服务端口，禁止root用户远程连接\n\n锁定关键文件系统\n\n调整文件描述符大小\n\n调整字符集，使其支持中文\n\n去除系统及内核版本登录前的屏幕显示\n\n内核参数优化\n\n\n# 4.某时刻流量超过阀值，该如何应对\n\n1、不可预测流量（网站被恶意刷量；cdn回源抓取数据；合作业务平台调取平台数据等）\n\n2、可预测流量（突然爆发的社会热点，营销活动的宣传；）\n\n不管是可预测流量还是不可预测流量都会表现在带宽和网站整体架构的应对方案\n\n如果由于带宽原因引起，由于网站的并发量太高，达到服务器的吞吐极限，导致服务器宕机，这时需要做临时申请加大带宽，然后负载均衡分流。\n\n如果由于外网请求数据库，导致数据库频繁读写，数据库处理能力低，导致大量请求积压；如果是这种情况，就需要优化sql，存储过程等，如果是请求过大，就要考虑做集群等。\n\n可预测流量的暴增也会拖慢网页的打开速度，甚至导致网站服务器宕机。要应对正常流量暴增，在流量高峰期到来之前就可以适当的调整，一般针对应用服务器的调整可以防止单点，负载均衡，高可用，增加后端web应用服务器数量，数据库读写分离，拆库拆表等，防止流量暴增导致服务器挂掉\n\n\n# 5.由于磁盘空间紧张现在要求只能保留最近7天的访问日志，请问如何解决？请给\n\n出解决办法或者配置或处理命令\n\ndate=date +%f_%t_%a\n\ncp ./access.log ./bak/access_"$date".log\n\nfind ./bak/ -name *.log -type f -mtime +7 -exec rm {};\n\n\n# 6.每天晚上12点，打包站点目录/var/www/html 备份到/data目录下\n\n00 00 * * * /usr/bin/tar -zvcf /data/html.tar.gz /var/www/html\n\n\n# 7.写一个脚本，实现判断192.168.1.0/24网络里，当前在线的ip有哪些，能ping通则认为在线\n\n\n# #!/bin/bash\n\nfor ip in {1..255}\n\ndo\n\nping -c3 192.168.1.$ip\n\nif [ $? -ne 0 ]\n\nthen echo "the host is 192.168.1.$ip up" >> ./up.list\n\nelse echo "the host is 192.168.1.$ip down" >> ./down.list\n\nfi\n\ndone\n\n或者\n\nyum install nmap -y\n\nnmap -sp 192.168.1.0/24 > ./upip\n\n\n# 8.统计出nginx的access.log中每个接口的响应码情况，url为第5列，响应码为第6列，分隔符为“|”\n\ncat access.log | awk -f \'|\' \'{print $6}\'\n\n\n# 9.如何查看http的并发请求数与其tcp连接状态？\n\n并发netstat -na | grep establis | wc -l\n\ntcp数netstat -an|grep ":80 "｜grep -v grep|wc -l\n\n\n# 10.说说tcp/ip的七层模型\n\n',charsets:{cjk:!0}},{title:"http状态码",frontmatter:{title:"http状态码",date:"2020-01-06T14:28:22.000Z",categories:"技术",tags:["面试题集"],permalink:"/pages/a2e381/",readingShow:"top",description:"写出以下http状态码的含义",meta:[{name:"twitter:title",content:"http状态码"},{name:"twitter:description",content:"写出以下http状态码的含义"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/02.%E9%9D%A2%E8%AF%95/02.http%E7%8A%B6%E6%80%81%E7%A0%81.html"},{property:"og:type",content:"article"},{property:"og:title",content:"http状态码"},{property:"og:description",content:"写出以下http状态码的含义"},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/02.%E9%9D%A2%E8%AF%95/02.http%E7%8A%B6%E6%80%81%E7%A0%81.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2020-01-06T14:28:22.000Z"},{property:"article:tag",content:"面试题集"},{itemprop:"name",content:"http状态码"},{itemprop:"description",content:"写出以下http状态码的含义"}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/02.%E9%9D%A2%E8%AF%95/02.http%E7%8A%B6%E6%80%81%E7%A0%81.html",relativePath:"02.生活/02.面试/02.http状态码.md",key:"v-765df57e",path:"/pages/a2e381/",headersStr:null,content:"写出以下http状态码的含义\n\n301/302/304/400/403/404/405/500/503/504\n\n301 Moved Permanently 请求的网页已永久移动到新位置。\n\n302 Found 临时性重定向。\n\n304 Not Modified 自从上次请求后，请求的网页未修改过\n\n400 Bad Request 服务器无法理解请求的格式，客户端不应当尝试再次使用相同的内容发起请求。\n\n403 Forbidden 禁止访问\n\n404 Not Found 找不到如何与 URI 相匹配的资源\n\n500 Internal Server Error 最常见的服务器端错误。\n\n503 Service Unavailable 服务器端暂时无法处理请求（可能是过载或维护）。",normalizedContent:"写出以下http状态码的含义\n\n301/302/304/400/403/404/405/500/503/504\n\n301 moved permanently 请求的网页已永久移动到新位置。\n\n302 found 临时性重定向。\n\n304 not modified 自从上次请求后，请求的网页未修改过\n\n400 bad request 服务器无法理解请求的格式，客户端不应当尝试再次使用相同的内容发起请求。\n\n403 forbidden 禁止访问\n\n404 not found 找不到如何与 uri 相匹配的资源\n\n500 internal server error 最常见的服务器端错误。\n\n503 service unavailable 服务器端暂时无法处理请求（可能是过载或维护）。",charsets:{cjk:!0}},{title:"高级运维工程需要掌握的技能",frontmatter:{title:"高级运维工程需要掌握的技能",date:"2022-12-15T10:29:16.000Z",permalink:"/pages/9245d9/",categories:["专题","面试"],tags:[null],readingShow:"top",description:"想成为合格运维工程师，Linux运维高级工程师要掌握以下技能：",meta:[{name:"twitter:title",content:"高级运维工程需要掌握的技能"},{name:"twitter:description",content:"想成为合格运维工程师，Linux运维高级工程师要掌握以下技能："},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/02.%E9%9D%A2%E8%AF%95/03.%E9%AB%98%E7%BA%A7%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E9%9C%80%E8%A6%81%E6%8E%8C%E6%8F%A1%E7%9A%84%E6%8A%80%E8%83%BD.html"},{property:"og:type",content:"article"},{property:"og:title",content:"高级运维工程需要掌握的技能"},{property:"og:description",content:"想成为合格运维工程师，Linux运维高级工程师要掌握以下技能："},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/02.%E9%9D%A2%E8%AF%95/03.%E9%AB%98%E7%BA%A7%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E9%9C%80%E8%A6%81%E6%8E%8C%E6%8F%A1%E7%9A%84%E6%8A%80%E8%83%BD.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T10:29:16.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"高级运维工程需要掌握的技能"},{itemprop:"description",content:"想成为合格运维工程师，Linux运维高级工程师要掌握以下技能："}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/02.%E9%9D%A2%E8%AF%95/03.%E9%AB%98%E7%BA%A7%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E9%9C%80%E8%A6%81%E6%8E%8C%E6%8F%A1%E7%9A%84%E6%8A%80%E8%83%BD.html",relativePath:"02.生活/02.面试/03.高级运维工程需要掌握的技能.md",key:"v-e52dd084",path:"/pages/9245d9/",headersStr:null,content:"# linux运维高级工程要掌握的技能\n\n想成为合格运维工程师，Linux运维高级工程师要掌握以下技能：\n\n1、Linux 系统基础⼊门-Linux的基础知识内容，和命令使用，以及用户和权限等核⼼知识点\n\n2、Linux 系统管理和进阶-Linux从进程、资源、任务、⽂件、软件包、磁盘等管理⽅法\n\n3、Linux 企业常用服务-企业级常用服务如DNS、FTP、Http、mail\n\n4、Linux 企业级安全原理和防范技巧以及网络和安全-Linux安全架构、安全威胁模型、以及加密、解密等原理，常见攻击和防范⼿段\n\n5、Shell 编程⼊门及进阶-Shell脚本基本用法以及进阶，从基础到精通，需要学习一些企业级常见脚本用法\n\n6、MySQL 应用原理及管理⼊门-Mysql安装、管理、授权、增删改查\n\n7、http 服务代理缓存加速-http ⾼级协议应用、缓存、web服务nginx\n\n8、企业级负载集群-企业级4层负载均衡LVS、和7层负载均衡nginx以及haproxy的应用\n\n9、企业级⾼可用集群-⾼可用集群原理，实现以keepalived为核⼼的⾼可用集群，以及主从高可用、双主\n\n10、运维监控zabbix-企业级监控体系以及zabbix流⾏开源监控系统的功用及架构\n\n11、云计算运维自动化-ansible、puppet等运维自动化解决方案\n\n12、WEB 服务体系架构-WEB服务体系架构，JSP体系、tomcat、CDN、缓存原理、压测、评估\n\n13、⼤型互联⽹集群架构和实战⽅案-LB集群：nginx、Haproxy、LVS HA集群、动静分离\n\n14、MySQL DBA 实战技能和优化-数据库参数优化、分库分表、备份方案、数据恢复策略、主从复制、读写分离、连接池及sharding技术、MHA等\n\n15、企业级云计算Openstack-Keystone、Glance、Nova核⼼组件、网络模块、块存储服务等\n\n16、企业级⼤数据Hadoop 运维实战-列式数据库HBase基础原理、安装配置及其应⽤、Zookeeper集群构建、hadoop实现Namenode⾼可⽤\n\n17、企业级虚拟化KVM 实战-KVM环境：KVM的安装、配置及应⽤\n\n18、NoSQL 企业级应用-Nosql 应用，如redis、MongoDB 复制、集群等⾼级应用\n\n19、企业级日志收集系统ELK 实战-海量数据日志收集系统Elasticsearch+Logstash+kibana 应用\n\n20、可持续化集成-Jenkins+github企业级应用\n\n21、虚拟化容器Docker-Linux 轻量虚拟化Docker ，Docker 的原理和安装，配置以及应用\n\n22、企业级K8S实战-组件功能、安装、配置、企业级应用场景和常见故障分析\n\n23、Linux 系统调优实战-从内存、CPU、进程调度、磁盘IO、⽹络参数等全面讲解Linux系统调优\n\n24、Python 编程基础⼊门-python安装、逻辑判断、模块使用等\n\n25、个人综合能力提升-表达能力、团队协作能力、执行能力、胜任力等\n\n以上25条技能，每一条大概价值2K，你的工资多少，在这个行业，就取决于你会多少。要想挣钱，你要先值钱。 原文链接：https://blog.csdn.net/chj_1224365967/article/details/107537182",normalizedContent:"# linux运维高级工程要掌握的技能\n\n想成为合格运维工程师，linux运维高级工程师要掌握以下技能：\n\n1、linux 系统基础⼊门-linux的基础知识内容，和命令使用，以及用户和权限等核⼼知识点\n\n2、linux 系统管理和进阶-linux从进程、资源、任务、⽂件、软件包、磁盘等管理⽅法\n\n3、linux 企业常用服务-企业级常用服务如dns、ftp、http、mail\n\n4、linux 企业级安全原理和防范技巧以及网络和安全-linux安全架构、安全威胁模型、以及加密、解密等原理，常见攻击和防范⼿段\n\n5、shell 编程⼊门及进阶-shell脚本基本用法以及进阶，从基础到精通，需要学习一些企业级常见脚本用法\n\n6、mysql 应用原理及管理⼊门-mysql安装、管理、授权、增删改查\n\n7、http 服务代理缓存加速-http ⾼级协议应用、缓存、web服务nginx\n\n8、企业级负载集群-企业级4层负载均衡lvs、和7层负载均衡nginx以及haproxy的应用\n\n9、企业级⾼可用集群-⾼可用集群原理，实现以keepalived为核⼼的⾼可用集群，以及主从高可用、双主\n\n10、运维监控zabbix-企业级监控体系以及zabbix流⾏开源监控系统的功用及架构\n\n11、云计算运维自动化-ansible、puppet等运维自动化解决方案\n\n12、web 服务体系架构-web服务体系架构，jsp体系、tomcat、cdn、缓存原理、压测、评估\n\n13、⼤型互联⽹集群架构和实战⽅案-lb集群：nginx、haproxy、lvs ha集群、动静分离\n\n14、mysql dba 实战技能和优化-数据库参数优化、分库分表、备份方案、数据恢复策略、主从复制、读写分离、连接池及sharding技术、mha等\n\n15、企业级云计算openstack-keystone、glance、nova核⼼组件、网络模块、块存储服务等\n\n16、企业级⼤数据hadoop 运维实战-列式数据库hbase基础原理、安装配置及其应⽤、zookeeper集群构建、hadoop实现namenode⾼可⽤\n\n17、企业级虚拟化kvm 实战-kvm环境：kvm的安装、配置及应⽤\n\n18、nosql 企业级应用-nosql 应用，如redis、mongodb 复制、集群等⾼级应用\n\n19、企业级日志收集系统elk 实战-海量数据日志收集系统elasticsearch+logstash+kibana 应用\n\n20、可持续化集成-jenkins+github企业级应用\n\n21、虚拟化容器docker-linux 轻量虚拟化docker ，docker 的原理和安装，配置以及应用\n\n22、企业级k8s实战-组件功能、安装、配置、企业级应用场景和常见故障分析\n\n23、linux 系统调优实战-从内存、cpu、进程调度、磁盘io、⽹络参数等全面讲解linux系统调优\n\n24、python 编程基础⼊门-python安装、逻辑判断、模块使用等\n\n25、个人综合能力提升-表达能力、团队协作能力、执行能力、胜任力等\n\n以上25条技能，每一条大概价值2k，你的工资多少，在这个行业，就取决于你会多少。要想挣钱，你要先值钱。 原文链接：https://blog.csdn.net/chj_1224365967/article/details/107537182",charsets:{cjk:!0}},{title:"企业级openvpn搭建",frontmatter:{title:"企业级openvpn搭建",date:"2022-12-15T19:13:50.000Z",permalink:"/pages/45fa3c/",categories:["运维","vpn"],tags:[null],readingShow:"top",description:"OpenVPN是一个用于创建虚拟专用网络加密通道的软件包，最早由James Yonan编写。OpenVPN允许创建的VPN使用公开密钥、电子证书、或者用户名／密码来进行身份验证。\n> 它大量使用了OpenSSL加密库中的SSLv3/TLSv1协议函数库。\n> 目前OpenVPN能在Solaris、Linux、OpenBSD、FreeBSD、NetBSD、Mac OS X与Microsoft Windows以及Android、iOS、MacOS(2020年官方推出Mac客户端)上运行，并包含了许多安全性的功能。它并不是一个基于Web的VPN软件，也不与IPsec及其他VPN软\n> OpenVPN2.0后引入了用户名/口令组合的身份验证方式，它可以省略客户端证书，但是仍有一份服务器证书需要被用作加密。 OpenVPN所有的通信都基于一个单一的IP端口， 默认且推荐使用UDP协议通讯，同时TCP也被支持。OpenVPN连接能通过大多数的代理服务器，并且能够在NAT的环境中很好地工作。服务端具有向客 户端“推送”某些网络配置信息的功能，这些信息包括：IP地址、路由设置等。\n> OpenVPN提供了两种虚拟网络接口：通用Tun/Tap驱动，通过它们， 可以建立三层IP隧道，或者虚拟二层以太网，后者可以传送任何类型的二层以太网络数据。传送的数据可通过LZO算法压缩。在选择协议时候，需要注意2个加密隧道之间的网络状况，如有高延迟或者丢包较多的情况下，请选择TCP协议作为底层协议，UDP协议由于存在无连接和重传机制，导致要隧道上层的协议进行重传，效率非常低下。\n> VPN的主要作用是在局域网外部时也可以访问局域网的资源，比如公司内网有许多资源只能公司内网访问，而我们外派出差员工还想访问公司内网资源[如:GitLab/测试库/其他测试环境接口]就需要通过连接公司公网出口暴露VPN来实现。\n>\n> 由于众所周知的原因，几种简单的VPN协议比如L2TP/IPsec和PPTP协议在大陆地区已经被干扰的基本无法正常使用了。因此也就SSL VPN还可以使用。SSL VPN有非常多种，但很多都是商业软件，不开源也不适合个人搭建。而OpenVPN是一款开源的SSL VPN，可以很容易找到搭建的方法，非常符合我们的要求。",meta:[{name:"twitter:title",content:"企业级openvpn搭建"},{name:"twitter:description",content:"OpenVPN是一个用于创建虚拟专用网络加密通道的软件包，最早由James Yonan编写。OpenVPN允许创建的VPN使用公开密钥、电子证书、或者用户名／密码来进行身份验证。\n> 它大量使用了OpenSSL加密库中的SSLv3/TLSv1协议函数库。\n> 目前OpenVPN能在Solaris、Linux、OpenBSD、FreeBSD、NetBSD、Mac OS X与Microsoft Windows以及Android、iOS、MacOS(2020年官方推出Mac客户端)上运行，并包含了许多安全性的功能。它并不是一个基于Web的VPN软件，也不与IPsec及其他VPN软\n> OpenVPN2.0后引入了用户名/口令组合的身份验证方式，它可以省略客户端证书，但是仍有一份服务器证书需要被用作加密。 OpenVPN所有的通信都基于一个单一的IP端口， 默认且推荐使用UDP协议通讯，同时TCP也被支持。OpenVPN连接能通过大多数的代理服务器，并且能够在NAT的环境中很好地工作。服务端具有向客 户端“推送”某些网络配置信息的功能，这些信息包括：IP地址、路由设置等。\n> OpenVPN提供了两种虚拟网络接口：通用Tun/Tap驱动，通过它们， 可以建立三层IP隧道，或者虚拟二层以太网，后者可以传送任何类型的二层以太网络数据。传送的数据可通过LZO算法压缩。在选择协议时候，需要注意2个加密隧道之间的网络状况，如有高延迟或者丢包较多的情况下，请选择TCP协议作为底层协议，UDP协议由于存在无连接和重传机制，导致要隧道上层的协议进行重传，效率非常低下。\n> VPN的主要作用是在局域网外部时也可以访问局域网的资源，比如公司内网有许多资源只能公司内网访问，而我们外派出差员工还想访问公司内网资源[如:GitLab/测试库/其他测试环境接口]就需要通过连接公司公网出口暴露VPN来实现。\n>\n> 由于众所周知的原因，几种简单的VPN协议比如L2TP/IPsec和PPTP协议在大陆地区已经被干扰的基本无法正常使用了。因此也就SSL VPN还可以使用。SSL VPN有非常多种，但很多都是商业软件，不开源也不适合个人搭建。而OpenVPN是一款开源的SSL VPN，可以很容易找到搭建的方法，非常符合我们的要求。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/01.vpn/01.%E4%BC%81%E4%B8%9A%E7%BA%A7openvpn%E6%90%AD%E5%BB%BA.html"},{property:"og:type",content:"article"},{property:"og:title",content:"企业级openvpn搭建"},{property:"og:description",content:"OpenVPN是一个用于创建虚拟专用网络加密通道的软件包，最早由James Yonan编写。OpenVPN允许创建的VPN使用公开密钥、电子证书、或者用户名／密码来进行身份验证。\n> 它大量使用了OpenSSL加密库中的SSLv3/TLSv1协议函数库。\n> 目前OpenVPN能在Solaris、Linux、OpenBSD、FreeBSD、NetBSD、Mac OS X与Microsoft Windows以及Android、iOS、MacOS(2020年官方推出Mac客户端)上运行，并包含了许多安全性的功能。它并不是一个基于Web的VPN软件，也不与IPsec及其他VPN软\n> OpenVPN2.0后引入了用户名/口令组合的身份验证方式，它可以省略客户端证书，但是仍有一份服务器证书需要被用作加密。 OpenVPN所有的通信都基于一个单一的IP端口， 默认且推荐使用UDP协议通讯，同时TCP也被支持。OpenVPN连接能通过大多数的代理服务器，并且能够在NAT的环境中很好地工作。服务端具有向客 户端“推送”某些网络配置信息的功能，这些信息包括：IP地址、路由设置等。\n> OpenVPN提供了两种虚拟网络接口：通用Tun/Tap驱动，通过它们， 可以建立三层IP隧道，或者虚拟二层以太网，后者可以传送任何类型的二层以太网络数据。传送的数据可通过LZO算法压缩。在选择协议时候，需要注意2个加密隧道之间的网络状况，如有高延迟或者丢包较多的情况下，请选择TCP协议作为底层协议，UDP协议由于存在无连接和重传机制，导致要隧道上层的协议进行重传，效率非常低下。\n> VPN的主要作用是在局域网外部时也可以访问局域网的资源，比如公司内网有许多资源只能公司内网访问，而我们外派出差员工还想访问公司内网资源[如:GitLab/测试库/其他测试环境接口]就需要通过连接公司公网出口暴露VPN来实现。\n>\n> 由于众所周知的原因，几种简单的VPN协议比如L2TP/IPsec和PPTP协议在大陆地区已经被干扰的基本无法正常使用了。因此也就SSL VPN还可以使用。SSL VPN有非常多种，但很多都是商业软件，不开源也不适合个人搭建。而OpenVPN是一款开源的SSL VPN，可以很容易找到搭建的方法，非常符合我们的要求。"},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/01.vpn/01.%E4%BC%81%E4%B8%9A%E7%BA%A7openvpn%E6%90%AD%E5%BB%BA.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T19:13:50.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"企业级openvpn搭建"},{itemprop:"description",content:"OpenVPN是一个用于创建虚拟专用网络加密通道的软件包，最早由James Yonan编写。OpenVPN允许创建的VPN使用公开密钥、电子证书、或者用户名／密码来进行身份验证。\n> 它大量使用了OpenSSL加密库中的SSLv3/TLSv1协议函数库。\n> 目前OpenVPN能在Solaris、Linux、OpenBSD、FreeBSD、NetBSD、Mac OS X与Microsoft Windows以及Android、iOS、MacOS(2020年官方推出Mac客户端)上运行，并包含了许多安全性的功能。它并不是一个基于Web的VPN软件，也不与IPsec及其他VPN软\n> OpenVPN2.0后引入了用户名/口令组合的身份验证方式，它可以省略客户端证书，但是仍有一份服务器证书需要被用作加密。 OpenVPN所有的通信都基于一个单一的IP端口， 默认且推荐使用UDP协议通讯，同时TCP也被支持。OpenVPN连接能通过大多数的代理服务器，并且能够在NAT的环境中很好地工作。服务端具有向客 户端“推送”某些网络配置信息的功能，这些信息包括：IP地址、路由设置等。\n> OpenVPN提供了两种虚拟网络接口：通用Tun/Tap驱动，通过它们， 可以建立三层IP隧道，或者虚拟二层以太网，后者可以传送任何类型的二层以太网络数据。传送的数据可通过LZO算法压缩。在选择协议时候，需要注意2个加密隧道之间的网络状况，如有高延迟或者丢包较多的情况下，请选择TCP协议作为底层协议，UDP协议由于存在无连接和重传机制，导致要隧道上层的协议进行重传，效率非常低下。\n> VPN的主要作用是在局域网外部时也可以访问局域网的资源，比如公司内网有许多资源只能公司内网访问，而我们外派出差员工还想访问公司内网资源[如:GitLab/测试库/其他测试环境接口]就需要通过连接公司公网出口暴露VPN来实现。\n>\n> 由于众所周知的原因，几种简单的VPN协议比如L2TP/IPsec和PPTP协议在大陆地区已经被干扰的基本无法正常使用了。因此也就SSL VPN还可以使用。SSL VPN有非常多种，但很多都是商业软件，不开源也不适合个人搭建。而OpenVPN是一款开源的SSL VPN，可以很容易找到搭建的方法，非常符合我们的要求。"}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/01.vpn/01.%E4%BC%81%E4%B8%9A%E7%BA%A7openvpn%E6%90%AD%E5%BB%BA.html",relativePath:"02.生活/03.工具/01.vpn/01.企业级openvpn搭建.md",key:"v-e2ff91da",path:"/pages/45fa3c/",headers:[{level:2,title:"基于账号密码的搭建",slug:"基于账号密码的搭建",normalizedTitle:"基于账号密码的搭建",charIndex:974},{level:4,title:"安装依赖",slug:"安装依赖",normalizedTitle:"安装依赖",charIndex:987},{level:4,title:"安装目录",slug:"安装目录",normalizedTitle:"安装目录",charIndex:1097},{level:4,title:"安装easy-rsa并生成相关证书",slug:"安装easy-rsa并生成相关证书",normalizedTitle:"安装easy-rsa并生成相关证书",charIndex:1127},{level:4,title:"生成相关证书",slug:"生成相关证书",normalizedTitle:"生成相关证书",charIndex:1138},{level:4,title:"准备openssl相关文件",slug:"准备openssl相关文件",normalizedTitle:"准备openssl相关文件",charIndex:1477},{level:4,title:"编辑证书基本信息",slug:"编辑证书基本信息",normalizedTitle:"编辑证书基本信息",charIndex:1536},{level:4,title:"生成服务器端秘钥",slug:"生成服务器端秘钥",normalizedTitle:"生成服务器端秘钥",charIndex:1805},{level:4,title:"生成客户端证书",slug:"生成客户端证书",normalizedTitle:"生成客户端证书",charIndex:1849},{level:4,title:"创建迪菲·赫尔曼密钥",slug:"创建迪菲·赫尔曼密钥",normalizedTitle:"创建迪菲·赫尔曼密钥",charIndex:1885},{level:4,title:"查看生成的所有证书",slug:"查看生成的所有证书",normalizedTitle:"查看生成的所有证书",charIndex:1916},{level:4,title:"安装openvpn",slug:"安装openvpn",normalizedTitle:"安装openvpn",charIndex:2191},{level:4,title:"配置openvpn",slug:"配置openvpn",normalizedTitle:"配置openvpn",charIndex:2776},{level:4,title:"生成ta.key文件（防DDos攻击、udp淹没等恶意攻击）",slug:"生成ta-key文件-防ddos攻击、udp淹没等恶意攻击",normalizedTitle:"生成ta.key文件（防ddos攻击、udp淹没等恶意攻击）",charIndex:2923},{level:4,title:"将签名生成的CA证书秘钥和服务端证书秘钥拷贝到证书目录中",slug:"将签名生成的ca证书秘钥和服务端证书秘钥拷贝到证书目录中",normalizedTitle:"将签名生成的ca证书秘钥和服务端证书秘钥拷贝到证书目录中",charIndex:3072},{level:4,title:"拷贝配置文件模板",slug:"拷贝配置文件模板",normalizedTitle:"拷贝配置文件模板",charIndex:3545},{level:4,title:"拷贝配置文件模板",slug:"拷贝配置文件模板-2",normalizedTitle:"拷贝配置文件模板",charIndex:3545},{level:4,title:"编辑openvpn配置文件",slug:"编辑openvpn配置文件",normalizedTitle:"编辑openvpn配置文件",charIndex:3723},{level:4,title:"centos7添加防火墙方式访问内部网络",slug:"centos7添加防火墙方式访问内部网络",normalizedTitle:"centos7添加防火墙方式访问内部网络",charIndex:4635},{level:4,title:"iptables配置防火墙方式访问内部网络",slug:"iptables配置防火墙方式访问内部网络",normalizedTitle:"iptables配置防火墙方式访问内部网络",charIndex:4887},{level:4,title:"上传密码验证脚本",slug:"上传密码验证脚本",normalizedTitle:"上传密码验证脚本",charIndex:5263},{level:4,title:"配置systemd启动脚本",slug:"配置systemd启动脚本",normalizedTitle:"配置systemd启动脚本",charIndex:6575},{level:4,title:"开启内核路由转发功能",slug:"开启内核路由转发功能",normalizedTitle:"开启内核路由转发功能",charIndex:8472},{level:4,title:"安装openvpn客户端",slug:"安装openvpn客户端",normalizedTitle:"安装openvpn客户端",charIndex:8560},{level:4,title:"客户端配置",slug:"客户端配置",normalizedTitle:"客户端配置",charIndex:9155}],headersStr:"基于账号密码的搭建 安装依赖 安装目录 安装easy-rsa并生成相关证书 生成相关证书 准备openssl相关文件 编辑证书基本信息 生成服务器端秘钥 生成客户端证书 创建迪菲·赫尔曼密钥 查看生成的所有证书 安装openvpn 配置openvpn 生成ta.key文件（防DDos攻击、udp淹没等恶意攻击） 将签名生成的CA证书秘钥和服务端证书秘钥拷贝到证书目录中 拷贝配置文件模板 拷贝配置文件模板 编辑openvpn配置文件 centos7添加防火墙方式访问内部网络 iptables配置防火墙方式访问内部网络 上传密码验证脚本 配置systemd启动脚本 开启内核路由转发功能 安装openvpn客户端 客户端配置",content:'# 企业级openvpn搭建\n\n> OpenVPN是一个用于创建虚拟专用网络加密通道的软件包，最早由James Yonan编写。OpenVPN允许创建的VPN使用公开密钥、电子证书、或者用户名／密码来进行身份验证。 它大量使用了OpenSSL加密库中的SSLv3/TLSv1协议函数库。 目前OpenVPN能在Solaris、Linux、OpenBSD、FreeBSD、NetBSD、Mac OS X与Microsoft Windows以及Android、iOS、MacOS(2020年官方推出Mac客户端)上运行，并包含了许多安全性的功能。它并不是一个基于Web的VPN软件，也不与IPsec及其他VPN软 OpenVPN2.0后引入了用户名/口令组合的身份验证方式，它可以省略客户端证书，但是仍有一份服务器证书需要被用作加密。 OpenVPN所有的通信都基于一个单一的IP端口， 默认且推荐使用UDP协议通讯，同时TCP也被支持。OpenVPN连接能通过大多数的代理服务器，并且能够在NAT的环境中很好地工作。服务端具有向客 户端“推送”某些网络配置信息的功能，这些信息包括：IP地址、路由设置等。 OpenVPN提供了两种虚拟网络接口：通用Tun/Tap驱动，通过它们， 可以建立三层IP隧道，或者虚拟二层以太网，后者可以传送任何类型的二层以太网络数据。传送的数据可通过LZO算法压缩。在选择协议时候，需要注意2个加密隧道之间的网络状况，如有高延迟或者丢包较多的情况下，请选择TCP协议作为底层协议，UDP协议由于存在无连接和重传机制，导致要隧道上层的协议进行重传，效率非常低下。 VPN的主要作用是在局域网外部时也可以访问局域网的资源，比如公司内网有许多资源只能公司内网访问，而我们外派出差员工还想访问公司内网资源[如:GitLab/测试库/其他测试环境接口]就需要通过连接公司公网出口暴露VPN来实现。\n> \n> 由于众所周知的原因，几种简单的VPN协议比如L2TP/IPsec和PPTP协议在大陆地区已经被干扰的基本无法正常使用了。因此也就SSL VPN还可以使用。SSL VPN有非常多种，但很多都是商业软件，不开源也不适合个人搭建。而OpenVPN是一款开源的SSL VPN，可以很容易找到搭建的方法，非常符合我们的要求。\n\n\n# 基于账号密码的搭建\n\n# 安装依赖\n\nyum install -y openssl openssl-devel lzo lzo-devel pam pam-devel automake pkgconfig gcc gcc-c++\n\n\n1\n\n\n# 安装目录\n\nmkdir /opt/apps\n\n\n1\n\n\n# 安装easy-rsa并生成相关证书\n\neasy-rsa 下载地址：https://codeload.github.com/OpenVPN/easy-rsa-old/zip/master\n\nunzip easy-rsa-old-master.zip\n[root@host-172-16-30-22 easy-rsa-old-master]# ls\nconfigure.ac  COPYING  COPYRIGHT.GPL  distro  doc  easy-rsa  Makefile.am\n\n\n1\n2\n3\n\n\n# 生成相关证书\n\n[root@host-172-16-30-22 2.0]# cd /opt/apps/easy-rsa-old-master/easy-rsa/2.0\n\n\n1\n\n\n# 准备openssl相关文件\n\nln -s openssl-1.0.0.cnf openssl.cnf\n\n\n1\n\n\n# 编辑证书基本信息\n\n[root@host-172-16-30-22 2.0]# pwd\n/opt/apps/easy-rsa-old-master/easy-rsa/2.0\nvim vars\nexport KEY_COUNTRY="CN"\nexport KEY_PROVINCE="sz"\nexport KEY_CITY="shenzhen"\nexport KEY_ORG="1quant"\nexport KEY_EMAIL="xxx@1quant.com"\n. vars #生效环境变量\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 生成服务器端秘钥\n\n./build-key-server server\n\n\n1\n\n\n# 生成客户端证书\n\n./build-key client\n\n\n1\n\n\n# 创建迪菲·赫尔曼密钥\n\n./build-dh\n\n\n1\n\n\n# 查看生成的所有证书\n\n[root@host-172-16-30-22 keys]# ls\n01.pem  ca.crt  client.crt  client.key  index.txt       index.txt.attr.old  serial      server.crt  server.key\n02.pem  ca.key  client.csr  dh2048.pem  index.txt.attr  index.txt.old       serial.old  server.csr  ta.key\n\n\n1\n2\n3\n\n\n# 安装openvpn\n\n下载地址：https://openvpn.net/community-downloads/（可能需要翻墙）\n\n tar -xf openvpn-2.5.8.tar.gz\n cd /opt/apps/openvpn-2.5.8\n ./configure --prefix=/opt/apps/openvpn && make && make install\n [root@host-172-16-30-22 apps]# ll\ntotal 1904\ndrwxr-xr-x  5 root  root     4096 Jan 23  2018 easy-rsa-old-master\n-rw-------  1 root  root    59661 Dec 27 12:30 easy-rsa-old-master-1.zip\ndrwxr-xr-x  7 root  root     4096 Dec 27 14:37 openvpn\ndrwxrwxr-x 12 admin admin    4096 Dec 27 14:02 openvpn-2.5.8\n-rw-------  1 root  root  1875551 Dec 27 11:37 openvpn-2.5.8.tar.gz\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 配置openvpn\n\n配置 OpenVPN 服务端\n创建配置文件目录和证书目录：\nmkdir -p /etc/openvpn        # openvpn 配置文件路径\nmkdir -p /etc/openvpn/pki    # openvpn 证书存放位置\n\n\n1\n2\n3\n4\n\n\n# 生成ta.key文件（防DDos攻击、udp淹没等恶意攻击）\n\ncd /opt/apps/easy-rsa-old-master/easy-rsa/2.0/keys\n/opt/apps/openvpn/sbin/openvpn --genkey --secret ta.key\n\n\n1\n2\n\n\n# 将签名生成的CA证书秘钥和服务端证书秘钥拷贝到证书目录中\n\ncp {ca.crt,ca.key,dh2048.pem,server.crt,server.key,ta.key} /etc/openvpn/pki/\n[root@host-172-16-30-22 pki]# ll\ntotal 32\n-rw-r--r-- 1 root root 2419 Dec 27 14:15 ca.crt\n-rw------- 1 root root 3268 Dec 27 14:15 ca.key\n-rw-r--r-- 1 root root  424 Dec 27 14:15 dh2048.pem\n-rw-r--r-- 1 root root 8203 Dec 27 14:15 server.crt\n-rw------- 1 root root 3272 Dec 27 14:15 server.key\n-rw------- 1 root root  636 Dec 27 14:15 ta.key\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 拷贝配置文件模板\n\ncp openvpn-2.5.8/sample/sample-config-files/server.conf /etc/openvpn/\n\n\n1\n\n\n# 拷贝配置文件模板\n\ncp openvpn-2.5.8/sample/sample-config-files/server.conf /etc/openvpn\n\n\n\n1\n2\n\n\n# 编辑openvpn配置文件\n\nlocal 0.0.0.0\nport 8888\nproto tcp\ndev tun\nca /etc/openvpn/pki/ca.crt\ncert /etc/openvpn/pki/server.crt\nkey /etc/openvpn/pki/server.key  # This file should be kept secret\ndh /etc/openvpn/pki/dh2048.pem\nserver 10.8.0.0 255.255.255.0\npush "dhcp-option DNS 114.114.114.114"\npush "dhcp-option DNS 8.8.8.8"\npush "route 172.16.30.0 255.255.255.0"\npush "route 172.16.60.253 255.255.255.0"\nifconfig-pool-persist ipp.txt\nclient-to-client\nkeepalive 10 120\ntls-auth /etc/openvpn/pki/ta.key 0 # This file is secret\ncipher AES-256-CBC\ncomp-lzo\nuser nobody\ngroup nobody\nauth-user-pass-verify /etc/openvpn/checkpsw.sh via-env\nscript-security 3\nverify-client-cert none\nusername-as-common-name\npersist-key\npersist-tun\nstatus /var/log/openvpn-status.log\nlog         /var/log/openvpn.log\nlog-append  /var/log/openvpn.log\nverb 3\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n# centos7添加防火墙方式访问内部网络\n\n#在服务端开启防火墙，放行openvpn服务，并且开启masquerade。\n#优点：只需在OpenVPN服务端配置防火墙规则，内部网络主机无需配置\nsystemctl start firewalld\nfirewall-cmd --add-masquerade --permanent\nfirewall-cmd --add-service=openvpn --permanent\nfirewall-cmd --reload\n\n\n1\n2\n3\n4\n5\n6\n\n\n# iptables配置防火墙方式访问内部网络\n\niptables -t nat -A POSTROUTING -s 10.10.10.0/24 -o eth0 -j MASQUERADE\n也可以这样做：添加 iptables 转发规则，对所有源地址（openvpn为客户端分配的地址）为 10.10.10.0/24 的数据包转发后进行源地址转换，伪装成 openvpn 服务器内网地址 x.x.x.x， 这样 VPN 客户端就可以访问服务器内网的其他机器了。\n\niptables -A POSTROUTING -s 10.10.10.0/24 -j SNAT --to-source 192.168.1.160\niptables -I INPUT -p tcp -m tcp --dport 8888 -j ACCEPT\n\n\n1\n2\n3\n4\n5\n\n\n# 上传密码验证脚本\n\n[root@host-172-16-30-22 openvpn]# ll\ntotal 28\n-rwxr-xr-x 1 root root   901 Dec 27 14:36 checkpsw.sh\n-rw------- 1 root root     0 Dec 27 14:40 ipp.txt\ndrwxr-xr-x 2 root root  4096 Dec 27 14:15 pki\n-rw-r--r-- 1 root root    22 Dec 27 14:37 psw-file\n-rw-r--r-- 1 root root   807 Dec 28 11:42 server.conf\ncat checkpsw.sh\n#!/bin/sh\n###########################################################\nPASSFILE="/etc/openvpn/psw-file"\nLOG_FILE="/opt/apps/openvpn/logs/openvpn-password.log"\nTIME_STAMP=`date "+%Y-%m-%d %T"`\n###########################################################\nif [ ! -r "${PASSFILE}" ]; then\necho "${TIME_STAMP}: Could not open password file \\"${PASSFILE}\\" for reading." >> ${LOG_FILE}\nexit 1\nfi\nCORRECT_PASSWORD=`awk \'!/^;/&&!/^#/&&$1=="\'${username}\'"{print $2;exit}\' ${PASSFILE}`\nif [ "${CORRECT_PASSWORD}" = "" ]; then\necho "${TIME_STAMP}: User does not exist: username=\\"${username}\\", password=\\"${password}\\"." >> ${LOG_FILE}\nexit 1\nfi\nif [ "${password}" = "${CORRECT_PASSWORD}" ]; then\necho "${TIME_STAMP}: Successful authentication: username=\\"${username}\\"." >> ${LOG_FILE}\nexit 0\nfi\necho "${TIME_STAMP}: Incorrect password: username=\\"${username}\\", password=\\"${password}\\"." >> ${LOG_FILE}\nexit 1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 配置systemd启动脚本\n\n[root@host-172-16-30-22 openvpn]# cat /usr/lib/systemd/system/openvpn.service\n[Unit]\nDescription=openvpn\nAfter=network.target\n[Service]\nEnvironmentFile=-/etc/openvpn/openvpn\nExecStart=/opt/apps/openvpn/sbin/openvpn --config /etc/openvpn/server.conf\nRestart=on-failure\nType=simple\nLimitNOFILE=65536\n[Install]\nWantedBy=multi-user.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n[root@host-172-16-30-22 openvpn]# systemctl status openvpn\n● openvpn.service - openvpn\n   Loaded: loaded (/usr/lib/systemd/system/openvpn.service; disabled; vendor preset: disabled)\n   Active: active (running) since Wed 2022-12-28 11:43:13 CST; 6min ago\n Main PID: 10319 (openvpn)\n    Tasks: 1\n   Memory: 1004.0K\n   CGroup: /system.slice/openvpn.service\n           └─10319 /opt/apps/openvpn/sbin/openvpn --config /etc/openvpn/server.conf\n\nDec 28 11:43:13 host-172-16-30-22 systemd[1]: Stopped openvpn.\nDec 28 11:43:13 host-172-16-30-22 systemd[1]: Started openvpn.\nDec 28 11:43:13 host-172-16-30-22 openvpn[10319]: 2022-12-28 11:43:13 WARNING: Compression for receiving enabled. Compression has been used in the past to break encryption. Sent packets are not compressed unless "allow-comp...yes" is also set.\nHint: Some lines were ellipsized, use -l to show in full.\n[root@host-172-16-30-22 openvpn]# netstat -tunlp\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      911/sshd            \ntcp        0      0 0.0.0.0:8888            0.0.0.0:*               LISTEN      10319/openvpn       \ntcp6       0      0 :::22                   :::*                    LISTEN      911/sshd            \nudp        0      0 0.0.0.0:68              0.0.0.0:*                           593/dhclient\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 开启内核路由转发功能\n\nvim /etc/sysctl.conf\n添加如下内容\nnet.ipv4.ip_forward = 1\nsysctl -p\n\n\n1\n2\n3\n4\n\n\n# 安装openvpn客户端\n\nwindows和linux下载地址：https://openvpn.net/community-downloads/（可能需要翻墙）\n\n 1. 安装成功后，打开应用，选择导入配置、导入client.ovpn文件\n\n 2. 导入成功后，选择连接即可\n\nmacos安装包：Tunnelblick_3.7.8_build_5180.dmg\n\n 1. 下载安装MacOS客户端 https://tunnelblick.net/downloads.html 选择Stable稳定版 Tunnelblick 3.7.8\n\n 2. 安装成功后 双击client.ovpn（附件）\n\n 3. 选择连接client 弹窗显示connected 则连接成功\n\nAndroid系统 安装包：openvpn-connect-3-0-5.apk\n\n 1. 下载安装客户端 https://openvpn-connect.en.uptodown.com/android 选择最新版本\n\n 2. 安装成功后、点击client.ovpn, 选择使用openVPN connect 打开\n\n 3. 进入app, 点击连接即可\n\nios客户端\n\n1.国内app store屏蔽了openvpn client，暂时不支持\n\n2.国外app store下载后导入openvpn client即可\n\n# 客户端配置\n\nwindows客户端配置\n\nclient\ndev tun\nproto tcp\nremote 220.231.216.147 8888\nresolv-retry infinite\nnobind\npersist-key\npersist-tun\nca ca.crt\ntls-auth ta.key 1\ncipher AES-256-CBC\ncomp-lzo\nverb 3\nauth-user-pass\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\napp端配置\n\n注意：APP端配置在windwos主机配置要注意文件格式，不然会报错line is too long\n\nclient\ndev tun\nproto tcp\nremote 220.231.216.147 8888 tcp-client\nresolv-retry infinite\nnobind\npersist-key\npersist-tun\ncipher AES-256-CBC\ncomp-lzo\nverb 3\n<ca>\n-----BEGIN CERTIFICATE-----\nxxx\n-----END CERTIFICATE-----\n\n</ca>\n\nkey-direction 1\n\n<tls-auth>\n#\n# 2048 bit OpenVPN static key\n#\n-----BEGIN OpenVPN Static key V1-----\nxxxx\n-----END OpenVPN Static key V1-----\n\n</tls-auth>\nauth-user-pass\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n',normalizedContent:'# 企业级openvpn搭建\n\n> openvpn是一个用于创建虚拟专用网络加密通道的软件包，最早由james yonan编写。openvpn允许创建的vpn使用公开密钥、电子证书、或者用户名／密码来进行身份验证。 它大量使用了openssl加密库中的sslv3/tlsv1协议函数库。 目前openvpn能在solaris、linux、openbsd、freebsd、netbsd、mac os x与microsoft windows以及android、ios、macos(2020年官方推出mac客户端)上运行，并包含了许多安全性的功能。它并不是一个基于web的vpn软件，也不与ipsec及其他vpn软 openvpn2.0后引入了用户名/口令组合的身份验证方式，它可以省略客户端证书，但是仍有一份服务器证书需要被用作加密。 openvpn所有的通信都基于一个单一的ip端口， 默认且推荐使用udp协议通讯，同时tcp也被支持。openvpn连接能通过大多数的代理服务器，并且能够在nat的环境中很好地工作。服务端具有向客 户端“推送”某些网络配置信息的功能，这些信息包括：ip地址、路由设置等。 openvpn提供了两种虚拟网络接口：通用tun/tap驱动，通过它们， 可以建立三层ip隧道，或者虚拟二层以太网，后者可以传送任何类型的二层以太网络数据。传送的数据可通过lzo算法压缩。在选择协议时候，需要注意2个加密隧道之间的网络状况，如有高延迟或者丢包较多的情况下，请选择tcp协议作为底层协议，udp协议由于存在无连接和重传机制，导致要隧道上层的协议进行重传，效率非常低下。 vpn的主要作用是在局域网外部时也可以访问局域网的资源，比如公司内网有许多资源只能公司内网访问，而我们外派出差员工还想访问公司内网资源[如:gitlab/测试库/其他测试环境接口]就需要通过连接公司公网出口暴露vpn来实现。\n> \n> 由于众所周知的原因，几种简单的vpn协议比如l2tp/ipsec和pptp协议在大陆地区已经被干扰的基本无法正常使用了。因此也就ssl vpn还可以使用。ssl vpn有非常多种，但很多都是商业软件，不开源也不适合个人搭建。而openvpn是一款开源的ssl vpn，可以很容易找到搭建的方法，非常符合我们的要求。\n\n\n# 基于账号密码的搭建\n\n# 安装依赖\n\nyum install -y openssl openssl-devel lzo lzo-devel pam pam-devel automake pkgconfig gcc gcc-c++\n\n\n1\n\n\n# 安装目录\n\nmkdir /opt/apps\n\n\n1\n\n\n# 安装easy-rsa并生成相关证书\n\neasy-rsa 下载地址：https://codeload.github.com/openvpn/easy-rsa-old/zip/master\n\nunzip easy-rsa-old-master.zip\n[root@host-172-16-30-22 easy-rsa-old-master]# ls\nconfigure.ac  copying  copyright.gpl  distro  doc  easy-rsa  makefile.am\n\n\n1\n2\n3\n\n\n# 生成相关证书\n\n[root@host-172-16-30-22 2.0]# cd /opt/apps/easy-rsa-old-master/easy-rsa/2.0\n\n\n1\n\n\n# 准备openssl相关文件\n\nln -s openssl-1.0.0.cnf openssl.cnf\n\n\n1\n\n\n# 编辑证书基本信息\n\n[root@host-172-16-30-22 2.0]# pwd\n/opt/apps/easy-rsa-old-master/easy-rsa/2.0\nvim vars\nexport key_country="cn"\nexport key_province="sz"\nexport key_city="shenzhen"\nexport key_org="1quant"\nexport key_email="xxx@1quant.com"\n. vars #生效环境变量\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 生成服务器端秘钥\n\n./build-key-server server\n\n\n1\n\n\n# 生成客户端证书\n\n./build-key client\n\n\n1\n\n\n# 创建迪菲·赫尔曼密钥\n\n./build-dh\n\n\n1\n\n\n# 查看生成的所有证书\n\n[root@host-172-16-30-22 keys]# ls\n01.pem  ca.crt  client.crt  client.key  index.txt       index.txt.attr.old  serial      server.crt  server.key\n02.pem  ca.key  client.csr  dh2048.pem  index.txt.attr  index.txt.old       serial.old  server.csr  ta.key\n\n\n1\n2\n3\n\n\n# 安装openvpn\n\n下载地址：https://openvpn.net/community-downloads/（可能需要翻墙）\n\n tar -xf openvpn-2.5.8.tar.gz\n cd /opt/apps/openvpn-2.5.8\n ./configure --prefix=/opt/apps/openvpn && make && make install\n [root@host-172-16-30-22 apps]# ll\ntotal 1904\ndrwxr-xr-x  5 root  root     4096 jan 23  2018 easy-rsa-old-master\n-rw-------  1 root  root    59661 dec 27 12:30 easy-rsa-old-master-1.zip\ndrwxr-xr-x  7 root  root     4096 dec 27 14:37 openvpn\ndrwxrwxr-x 12 admin admin    4096 dec 27 14:02 openvpn-2.5.8\n-rw-------  1 root  root  1875551 dec 27 11:37 openvpn-2.5.8.tar.gz\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 配置openvpn\n\n配置 openvpn 服务端\n创建配置文件目录和证书目录：\nmkdir -p /etc/openvpn        # openvpn 配置文件路径\nmkdir -p /etc/openvpn/pki    # openvpn 证书存放位置\n\n\n1\n2\n3\n4\n\n\n# 生成ta.key文件（防ddos攻击、udp淹没等恶意攻击）\n\ncd /opt/apps/easy-rsa-old-master/easy-rsa/2.0/keys\n/opt/apps/openvpn/sbin/openvpn --genkey --secret ta.key\n\n\n1\n2\n\n\n# 将签名生成的ca证书秘钥和服务端证书秘钥拷贝到证书目录中\n\ncp {ca.crt,ca.key,dh2048.pem,server.crt,server.key,ta.key} /etc/openvpn/pki/\n[root@host-172-16-30-22 pki]# ll\ntotal 32\n-rw-r--r-- 1 root root 2419 dec 27 14:15 ca.crt\n-rw------- 1 root root 3268 dec 27 14:15 ca.key\n-rw-r--r-- 1 root root  424 dec 27 14:15 dh2048.pem\n-rw-r--r-- 1 root root 8203 dec 27 14:15 server.crt\n-rw------- 1 root root 3272 dec 27 14:15 server.key\n-rw------- 1 root root  636 dec 27 14:15 ta.key\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 拷贝配置文件模板\n\ncp openvpn-2.5.8/sample/sample-config-files/server.conf /etc/openvpn/\n\n\n1\n\n\n# 拷贝配置文件模板\n\ncp openvpn-2.5.8/sample/sample-config-files/server.conf /etc/openvpn\n\n\n\n1\n2\n\n\n# 编辑openvpn配置文件\n\nlocal 0.0.0.0\nport 8888\nproto tcp\ndev tun\nca /etc/openvpn/pki/ca.crt\ncert /etc/openvpn/pki/server.crt\nkey /etc/openvpn/pki/server.key  # this file should be kept secret\ndh /etc/openvpn/pki/dh2048.pem\nserver 10.8.0.0 255.255.255.0\npush "dhcp-option dns 114.114.114.114"\npush "dhcp-option dns 8.8.8.8"\npush "route 172.16.30.0 255.255.255.0"\npush "route 172.16.60.253 255.255.255.0"\nifconfig-pool-persist ipp.txt\nclient-to-client\nkeepalive 10 120\ntls-auth /etc/openvpn/pki/ta.key 0 # this file is secret\ncipher aes-256-cbc\ncomp-lzo\nuser nobody\ngroup nobody\nauth-user-pass-verify /etc/openvpn/checkpsw.sh via-env\nscript-security 3\nverify-client-cert none\nusername-as-common-name\npersist-key\npersist-tun\nstatus /var/log/openvpn-status.log\nlog         /var/log/openvpn.log\nlog-append  /var/log/openvpn.log\nverb 3\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n# centos7添加防火墙方式访问内部网络\n\n#在服务端开启防火墙，放行openvpn服务，并且开启masquerade。\n#优点：只需在openvpn服务端配置防火墙规则，内部网络主机无需配置\nsystemctl start firewalld\nfirewall-cmd --add-masquerade --permanent\nfirewall-cmd --add-service=openvpn --permanent\nfirewall-cmd --reload\n\n\n1\n2\n3\n4\n5\n6\n\n\n# iptables配置防火墙方式访问内部网络\n\niptables -t nat -a postrouting -s 10.10.10.0/24 -o eth0 -j masquerade\n也可以这样做：添加 iptables 转发规则，对所有源地址（openvpn为客户端分配的地址）为 10.10.10.0/24 的数据包转发后进行源地址转换，伪装成 openvpn 服务器内网地址 x.x.x.x， 这样 vpn 客户端就可以访问服务器内网的其他机器了。\n\niptables -a postrouting -s 10.10.10.0/24 -j snat --to-source 192.168.1.160\niptables -i input -p tcp -m tcp --dport 8888 -j accept\n\n\n1\n2\n3\n4\n5\n\n\n# 上传密码验证脚本\n\n[root@host-172-16-30-22 openvpn]# ll\ntotal 28\n-rwxr-xr-x 1 root root   901 dec 27 14:36 checkpsw.sh\n-rw------- 1 root root     0 dec 27 14:40 ipp.txt\ndrwxr-xr-x 2 root root  4096 dec 27 14:15 pki\n-rw-r--r-- 1 root root    22 dec 27 14:37 psw-file\n-rw-r--r-- 1 root root   807 dec 28 11:42 server.conf\ncat checkpsw.sh\n#!/bin/sh\n###########################################################\npassfile="/etc/openvpn/psw-file"\nlog_file="/opt/apps/openvpn/logs/openvpn-password.log"\ntime_stamp=`date "+%y-%m-%d %t"`\n###########################################################\nif [ ! -r "${passfile}" ]; then\necho "${time_stamp}: could not open password file \\"${passfile}\\" for reading." >> ${log_file}\nexit 1\nfi\ncorrect_password=`awk \'!/^;/&&!/^#/&&$1=="\'${username}\'"{print $2;exit}\' ${passfile}`\nif [ "${correct_password}" = "" ]; then\necho "${time_stamp}: user does not exist: username=\\"${username}\\", password=\\"${password}\\"." >> ${log_file}\nexit 1\nfi\nif [ "${password}" = "${correct_password}" ]; then\necho "${time_stamp}: successful authentication: username=\\"${username}\\"." >> ${log_file}\nexit 0\nfi\necho "${time_stamp}: incorrect password: username=\\"${username}\\", password=\\"${password}\\"." >> ${log_file}\nexit 1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 配置systemd启动脚本\n\n[root@host-172-16-30-22 openvpn]# cat /usr/lib/systemd/system/openvpn.service\n[unit]\ndescription=openvpn\nafter=network.target\n[service]\nenvironmentfile=-/etc/openvpn/openvpn\nexecstart=/opt/apps/openvpn/sbin/openvpn --config /etc/openvpn/server.conf\nrestart=on-failure\ntype=simple\nlimitnofile=65536\n[install]\nwantedby=multi-user.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n[root@host-172-16-30-22 openvpn]# systemctl status openvpn\n● openvpn.service - openvpn\n   loaded: loaded (/usr/lib/systemd/system/openvpn.service; disabled; vendor preset: disabled)\n   active: active (running) since wed 2022-12-28 11:43:13 cst; 6min ago\n main pid: 10319 (openvpn)\n    tasks: 1\n   memory: 1004.0k\n   cgroup: /system.slice/openvpn.service\n           └─10319 /opt/apps/openvpn/sbin/openvpn --config /etc/openvpn/server.conf\n\ndec 28 11:43:13 host-172-16-30-22 systemd[1]: stopped openvpn.\ndec 28 11:43:13 host-172-16-30-22 systemd[1]: started openvpn.\ndec 28 11:43:13 host-172-16-30-22 openvpn[10319]: 2022-12-28 11:43:13 warning: compression for receiving enabled. compression has been used in the past to break encryption. sent packets are not compressed unless "allow-comp...yes" is also set.\nhint: some lines were ellipsized, use -l to show in full.\n[root@host-172-16-30-22 openvpn]# netstat -tunlp\nactive internet connections (only servers)\nproto recv-q send-q local address           foreign address         state       pid/program name    \ntcp        0      0 0.0.0.0:22              0.0.0.0:*               listen      911/sshd            \ntcp        0      0 0.0.0.0:8888            0.0.0.0:*               listen      10319/openvpn       \ntcp6       0      0 :::22                   :::*                    listen      911/sshd            \nudp        0      0 0.0.0.0:68              0.0.0.0:*                           593/dhclient\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 开启内核路由转发功能\n\nvim /etc/sysctl.conf\n添加如下内容\nnet.ipv4.ip_forward = 1\nsysctl -p\n\n\n1\n2\n3\n4\n\n\n# 安装openvpn客户端\n\nwindows和linux下载地址：https://openvpn.net/community-downloads/（可能需要翻墙）\n\n 1. 安装成功后，打开应用，选择导入配置、导入client.ovpn文件\n\n 2. 导入成功后，选择连接即可\n\nmacos安装包：tunnelblick_3.7.8_build_5180.dmg\n\n 1. 下载安装macos客户端 https://tunnelblick.net/downloads.html 选择stable稳定版 tunnelblick 3.7.8\n\n 2. 安装成功后 双击client.ovpn（附件）\n\n 3. 选择连接client 弹窗显示connected 则连接成功\n\nandroid系统 安装包：openvpn-connect-3-0-5.apk\n\n 1. 下载安装客户端 https://openvpn-connect.en.uptodown.com/android 选择最新版本\n\n 2. 安装成功后、点击client.ovpn, 选择使用openvpn connect 打开\n\n 3. 进入app, 点击连接即可\n\nios客户端\n\n1.国内app store屏蔽了openvpn client，暂时不支持\n\n2.国外app store下载后导入openvpn client即可\n\n# 客户端配置\n\nwindows客户端配置\n\nclient\ndev tun\nproto tcp\nremote 220.231.216.147 8888\nresolv-retry infinite\nnobind\npersist-key\npersist-tun\nca ca.crt\ntls-auth ta.key 1\ncipher aes-256-cbc\ncomp-lzo\nverb 3\nauth-user-pass\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\napp端配置\n\n注意：app端配置在windwos主机配置要注意文件格式，不然会报错line is too long\n\nclient\ndev tun\nproto tcp\nremote 220.231.216.147 8888 tcp-client\nresolv-retry infinite\nnobind\npersist-key\npersist-tun\ncipher aes-256-cbc\ncomp-lzo\nverb 3\n<ca>\n-----begin certificate-----\nxxx\n-----end certificate-----\n\n</ca>\n\nkey-direction 1\n\n<tls-auth>\n#\n# 2048 bit openvpn static key\n#\n-----begin openvpn static key v1-----\nxxxx\n-----end openvpn static key v1-----\n\n</tls-auth>\nauth-user-pass\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n',charsets:{cjk:!0}},{title:"实现V2Ray通过CloudFlare自选IP加速",frontmatter:{title:"实现V2Ray通过CloudFlare自选IP加速",date:"2022-12-15T19:05:28.000Z",permalink:"/pages/e9bb88/",categories:["运维","翻墙教程"],tags:[null],readingShow:"top",description:"前言：",meta:[{name:"image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212j30ij0mk75k.jpeg"},{name:"twitter:title",content:"实现V2Ray通过CloudFlare自选IP加速"},{name:"twitter:description",content:"前言："},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212j30ij0mk75k.jpeg"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/02.%E7%BF%BB%E5%A2%99%E6%95%99%E7%A8%8B/01.%E5%AE%9E%E7%8E%B0V2Ray%E9%80%9A%E8%BF%87CloudFlare%E8%87%AA%E9%80%89IP%E5%8A%A0%E9%80%9F.html"},{property:"og:type",content:"article"},{property:"og:title",content:"实现V2Ray通过CloudFlare自选IP加速"},{property:"og:description",content:"前言："},{property:"og:image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212j30ij0mk75k.jpeg"},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/02.%E7%BF%BB%E5%A2%99%E6%95%99%E7%A8%8B/01.%E5%AE%9E%E7%8E%B0V2Ray%E9%80%9A%E8%BF%87CloudFlare%E8%87%AA%E9%80%89IP%E5%8A%A0%E9%80%9F.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T19:05:28.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"实现V2Ray通过CloudFlare自选IP加速"},{itemprop:"description",content:"前言："},{itemprop:"image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212j30ij0mk75k.jpeg"}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/02.%E7%BF%BB%E5%A2%99%E6%95%99%E7%A8%8B/01.%E5%AE%9E%E7%8E%B0V2Ray%E9%80%9A%E8%BF%87CloudFlare%E8%87%AA%E9%80%89IP%E5%8A%A0%E9%80%9F.html",relativePath:"02.生活/03.工具/02.翻墙教程/01.实现V2Ray通过CloudFlare自选IP加速.md",key:"v-df98990e",path:"/pages/e9bb88/",headersStr:null,content:"前言：\n\n无需使用DNSpod和萌精灵等工具，实现自选IP。本文V2ray安装使用v2-ui，带有web面板不容易出错。\n\n一、准备\n\n注册CloudFlare账号\n\n购买顶级域名\n\nVPS\n\n二、安装v2-ui\n\n可参考作者博客教程：https://blog.sprov.xyz/2019/08/03/v2-ui/\n\n一键安装：bash <(curl -Ls https://blog.sprov.xyz/v2-ui.sh)\n\n三、申请SSL证书\n\n直接搜索FreeSSL申请免费SSL证书大把教程\n\n也可参考v2-ui作者教程：在 FreeSSL 上为你的域名申请免费 SSL 证书\n\n四、使用v2-ui配置V2ray\n\n打开v2-ui面板：http://服务器IP:65432\n\n默认用户名密码：admin\n\n添加账号：\n\n参数设置：（端口填写443，图上红字写错了）\n\n\n\n五、配置ClouFlare\n\n先将域名添加到CloudFlare，点击查找教程\n\n添加DNS解析：\n\n\n\n\n\n六、选择适合自己的IP\n\n使用查找适合自己当前网络环境的优选Cloudflare Anycast IP工具：better-cloudflare-ip\n\n七、▲▲▲重点：配置V2ray客户端\n\n这里以电脑版为例，其他版本设置方法相同\n\n先将之前在v2-ui上创建的账号导入V2ray客户端：\n\n\n\n\n\n修改配置：\n\n这里直接将选出来的IP填入服务器地址中，服务器地址填入伪装域名中达到流量走自选IP的效果，而不需要萌精灵等工具。\n\n\n\n小技巧\n\n可以直接将自选IP通过DNS解析记录到一个二级域名，将二级域名直接填入V2ray客户端服务器地址中，也就是填写自选IP的位置。对于多个用户就可以直接更改DNS解析达到更改所有自选IP的效果。如下图：\n\n\n\n\n\n原文链接：\n\nhttps://blog.52learn.top/index.php/archives/16/\n\n----------------------------------------\n\n使用查找适合自己当前网络环境的优选Cloudflare Anycast IP\n\nhttps://github.com/XIU2/CloudflareSpeedTest/releases\n\n百度网盘文件cloudflareST",normalizedContent:"前言：\n\n无需使用dnspod和萌精灵等工具，实现自选ip。本文v2ray安装使用v2-ui，带有web面板不容易出错。\n\n一、准备\n\n注册cloudflare账号\n\n购买顶级域名\n\nvps\n\n二、安装v2-ui\n\n可参考作者博客教程：https://blog.sprov.xyz/2019/08/03/v2-ui/\n\n一键安装：bash <(curl -ls https://blog.sprov.xyz/v2-ui.sh)\n\n三、申请ssl证书\n\n直接搜索freessl申请免费ssl证书大把教程\n\n也可参考v2-ui作者教程：在 freessl 上为你的域名申请免费 ssl 证书\n\n四、使用v2-ui配置v2ray\n\n打开v2-ui面板：http://服务器ip:65432\n\n默认用户名密码：admin\n\n添加账号：\n\n参数设置：（端口填写443，图上红字写错了）\n\n\n\n五、配置clouflare\n\n先将域名添加到cloudflare，点击查找教程\n\n添加dns解析：\n\n\n\n\n\n六、选择适合自己的ip\n\n使用查找适合自己当前网络环境的优选cloudflare anycast ip工具：better-cloudflare-ip\n\n七、▲▲▲重点：配置v2ray客户端\n\n这里以电脑版为例，其他版本设置方法相同\n\n先将之前在v2-ui上创建的账号导入v2ray客户端：\n\n\n\n\n\n修改配置：\n\n这里直接将选出来的ip填入服务器地址中，服务器地址填入伪装域名中达到流量走自选ip的效果，而不需要萌精灵等工具。\n\n\n\n小技巧\n\n可以直接将自选ip通过dns解析记录到一个二级域名，将二级域名直接填入v2ray客户端服务器地址中，也就是填写自选ip的位置。对于多个用户就可以直接更改dns解析达到更改所有自选ip的效果。如下图：\n\n\n\n\n\n原文链接：\n\nhttps://blog.52learn.top/index.php/archives/16/\n\n----------------------------------------\n\n使用查找适合自己当前网络环境的优选cloudflare anycast ip\n\nhttps://github.com/xiu2/cloudflarespeedtest/releases\n\n百度网盘文件cloudflarest",charsets:{cjk:!0}},{title:"mac电脑常用软件",frontmatter:{title:"mac电脑常用软件",date:"2022-12-15T14:59:50.000Z",permalink:"/pages/4ad301/",categories:["生活","工具"],tags:[null],readingShow:"top",description:"任务：notion",meta:[{name:"twitter:title",content:"mac电脑常用软件"},{name:"twitter:description",content:"任务：notion"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/03.mac%E7%94%B5%E8%84%91%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6.html"},{property:"og:type",content:"article"},{property:"og:title",content:"mac电脑常用软件"},{property:"og:description",content:"任务：notion"},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/03.mac%E7%94%B5%E8%84%91%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T14:59:50.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"mac电脑常用软件"},{itemprop:"description",content:"任务：notion"}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/03.mac%E7%94%B5%E8%84%91%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6.html",relativePath:"02.生活/03.工具/03.mac电脑常用软件.md",key:"v-303fed36",path:"/pages/4ad301/",headers:[{level:2,title:"mac电脑常用软件",slug:"mac电脑常用软件",normalizedTitle:"mac电脑常用软件",charIndex:2}],headersStr:"mac电脑常用软件",content:"# mac电脑常用软件\n\n任务：notion\n\n笔记：notability\n\npython开发：vscode\n\n远程windows电脑：micrsoft remote desktop beta\n\n远程ssh linux服务器：termius beta",normalizedContent:"# mac电脑常用软件\n\n任务：notion\n\n笔记：notability\n\npython开发：vscode\n\n远程windows电脑：micrsoft remote desktop beta\n\n远程ssh linux服务器：termius beta",charsets:{cjk:!0}},{title:"工具链接",frontmatter:{title:"工具链接",date:"2022-12-16T14:39:26.000Z",permalink:"/pages/8994e9/",categories:["生活","工具"],tags:[null],readingShow:"top",description:"",meta:[{name:"twitter:title",content:"工具链接"},{name:"twitter:description",content:""},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/04.%E4%B8%AA%E4%BA%BA%E5%B7%A5%E5%85%B7%E9%93%BE%E6%8E%A5.html"},{property:"og:type",content:"article"},{property:"og:title",content:"工具链接"},{property:"og:description",content:""},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/04.%E4%B8%AA%E4%BA%BA%E5%B7%A5%E5%85%B7%E9%93%BE%E6%8E%A5.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-16T14:39:26.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"工具链接"},{itemprop:"description",content:""}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/04.%E4%B8%AA%E4%BA%BA%E5%B7%A5%E5%85%B7%E9%93%BE%E6%8E%A5.html",relativePath:"02.生活/03.工具/04.个人工具链接.md",key:"v-e138df4c",path:"/pages/8994e9/",headers:[{level:3,title:"图片工具",slug:"图片工具",normalizedTitle:"图片工具",charIndex:12}],headersStr:"图片工具",content:"# 个人收藏夹\n\n\n# 图片工具\n\n * 在线图片压缩 - docsmall 在线图片压缩工具,在线图片压缩软件\n\n * ",normalizedContent:"# 个人收藏夹\n\n\n# 图片工具\n\n * 在线图片压缩 - docsmall 在线图片压缩工具,在线图片压缩软件\n\n * ",charsets:{cjk:!0}},{title:"vuepress配置artalk",frontmatter:{title:"vuepress配置artalk",date:"2023-01-11T14:19:31.000Z",permalink:"/pages/468f43/",categories:["生活","工具"],tags:[null],readingShow:"top",description:"Artalk 是一款轻量、安全、易上手的自托管评论系统。",meta:[{name:"twitter:title",content:"vuepress配置artalk"},{name:"twitter:description",content:"Artalk 是一款轻量、安全、易上手的自托管评论系统。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/05.vuepress%E9%85%8D%E7%BD%AEartalk.html"},{property:"og:type",content:"article"},{property:"og:title",content:"vuepress配置artalk"},{property:"og:description",content:"Artalk 是一款轻量、安全、易上手的自托管评论系统。"},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/05.vuepress%E9%85%8D%E7%BD%AEartalk.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-11T14:19:31.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"vuepress配置artalk"},{itemprop:"description",content:"Artalk 是一款轻量、安全、易上手的自托管评论系统。"}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/05.vuepress%E9%85%8D%E7%BD%AEartalk.html",relativePath:"02.生活/03.工具/05.vuepress配置artalk.md",key:"v-19ff8e6a",path:"/pages/468f43/",headers:[{level:4,title:"部署",slug:"部署",normalizedTitle:"部署",charIndex:119},{level:4,title:"vuepress 配置artalk",slug:"vuepress-配置artalk",normalizedTitle:"vuepress 配置artalk",charIndex:376},{level:3,title:"安装依赖",slug:"安装依赖",normalizedTitle:"安装依赖",charIndex:398},{level:3,title:"新建Artalk.vue组件",slug:"新建artalk-vue组件",normalizedTitle:"新建artalk.vue组件",charIndex:432},{level:4,title:"设置评论页面",slug:"设置评论页面",normalizedTitle:"设置评论页面",charIndex:1389}],headersStr:"部署 vuepress 配置artalk 安装依赖 新建Artalk.vue组件 设置评论页面",content:'Artalk 是一款轻量、安全、易上手的自托管评论系统。\n\n前端引入方式参考https://artalk.js.org/guide/backend/fe-control.html\n\n提示：注意配置artalk的CORS等安全措施。\n\n# 部署\n\n可参考https://artalk.js.org/guide/backend/install.html\n\ndocker-compose.yaml\n\nversion: "3"\nservices:\n  artalk:\n    container_name: artalk\n    image: artalk/artalk-go\n    ports:\n      - 23366:23366\n    volumes:\n      - xxx/data:/data\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# vuepress 配置artalk\n\n\n# 安装依赖\n\nyarn add artalk -S\n\n\n1\n\n\n\n# 新建Artalk.vue组件\n\n我的vuepress使用的是vdoing主题，vuepress版本是1.xxx\n\n在vdoing/components下新建Artalk.vue：\n\n<template>\n  <div>\n    \x3c!-- 自定义评论 --\x3e\n    <h2 id="commentArea">评论</h2>\n    <div id="Comments"></div>\n  </div>\n</template>\n\n<script>\nimport "artalk/dist/Artalk.css";\n\nexport default {\n  mounted() {\n    if (typeof window != "undefined") {\n      // 初始化Artalk\n      this.initArtalk();\n    }\n  },\n  methods: {\n    initArtalk() {\n      const ArtalkComponent = () => ({\n        component: import("artalk"),\n      });\n\n      ArtalkComponent().component.then(function (result) {\n        const Artalk = result.default;\n\n        new Artalk({\n          el: "#Comments",\n          pageKey: "", // 页面链接\n          pageTitle: "", // 页面标题\n          server: "https://talk.xuqilong.top", // 后端地址\n          site: "程序员 Life",\n        });\n      });\n    },\n  },\n};\n<\/script>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n# 设置评论页面\n\n然后在所需页面添加artalk评论组件。 例如，在vdoing/components/Page.vue下配置评论：\n\n          <Content class="theme-vdoing-content" />\n          \x3c!-- 自定义评论 --\x3e\n          <Artalk />\n        </div>\n        <slot name="bottom" v-if="isShowSlotB" />\n        <PageEdit />\n\n\n1\n2\n3\n4\n5\n6\n\n\n原文链接：https://xuqilong.top/pages/783df7/#%E6%96%B0%E5%BB%BAartalk-vue%E7%BB%84%E4%BB%B6',normalizedContent:'artalk 是一款轻量、安全、易上手的自托管评论系统。\n\n前端引入方式参考https://artalk.js.org/guide/backend/fe-control.html\n\n提示：注意配置artalk的cors等安全措施。\n\n# 部署\n\n可参考https://artalk.js.org/guide/backend/install.html\n\ndocker-compose.yaml\n\nversion: "3"\nservices:\n  artalk:\n    container_name: artalk\n    image: artalk/artalk-go\n    ports:\n      - 23366:23366\n    volumes:\n      - xxx/data:/data\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# vuepress 配置artalk\n\n\n# 安装依赖\n\nyarn add artalk -s\n\n\n1\n\n\n\n# 新建artalk.vue组件\n\n我的vuepress使用的是vdoing主题，vuepress版本是1.xxx\n\n在vdoing/components下新建artalk.vue：\n\n<template>\n  <div>\n    \x3c!-- 自定义评论 --\x3e\n    <h2 id="commentarea">评论</h2>\n    <div id="comments"></div>\n  </div>\n</template>\n\n<script>\nimport "artalk/dist/artalk.css";\n\nexport default {\n  mounted() {\n    if (typeof window != "undefined") {\n      // 初始化artalk\n      this.initartalk();\n    }\n  },\n  methods: {\n    initartalk() {\n      const artalkcomponent = () => ({\n        component: import("artalk"),\n      });\n\n      artalkcomponent().component.then(function (result) {\n        const artalk = result.default;\n\n        new artalk({\n          el: "#comments",\n          pagekey: "", // 页面链接\n          pagetitle: "", // 页面标题\n          server: "https://talk.xuqilong.top", // 后端地址\n          site: "程序员 life",\n        });\n      });\n    },\n  },\n};\n<\/script>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n# 设置评论页面\n\n然后在所需页面添加artalk评论组件。 例如，在vdoing/components/page.vue下配置评论：\n\n          <content class="theme-vdoing-content" />\n          \x3c!-- 自定义评论 --\x3e\n          <artalk />\n        </div>\n        <slot name="bottom" v-if="isshowslotb" />\n        <pageedit />\n\n\n1\n2\n3\n4\n5\n6\n\n\n原文链接：https://xuqilong.top/pages/783df7/#%e6%96%b0%e5%bb%baartalk-vue%e7%bb%84%e4%bb%b6',charsets:{cjk:!0}},{title:"windows目录实时同步工具",frontmatter:{title:"windows目录实时同步工具",date:"2022-12-15T15:00:54.000Z",permalink:"/pages/112008/",categories:["生活","工具"],tags:[null],readingShow:"top",description:"https://freefilesync.org/",meta:[{name:"twitter:title",content:"windows目录实时同步工具"},{name:"twitter:description",content:"https://freefilesync.org/"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/06.windows%E7%9B%AE%E5%BD%95%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7.html"},{property:"og:type",content:"article"},{property:"og:title",content:"windows目录实时同步工具"},{property:"og:description",content:"https://freefilesync.org/"},{property:"og:url",content:"https://blog.zzppjj.top/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/06.windows%E7%9B%AE%E5%BD%95%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T15:00:54.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"windows目录实时同步工具"},{itemprop:"description",content:"https://freefilesync.org/"}]},regularPath:"/02.%E7%94%9F%E6%B4%BB/03.%E5%B7%A5%E5%85%B7/06.windows%E7%9B%AE%E5%BD%95%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7.html",relativePath:"02.生活/03.工具/06.windows目录实时同步工具.md",key:"v-24ef5f44",path:"/pages/112008/",headers:[{level:2,title:"windows目录实时同步工具",slug:"windows目录实时同步工具",normalizedTitle:"windows目录实时同步工具",charIndex:2}],headersStr:"windows目录实时同步工具",content:"# windows目录实时同步工具\n\nhttps://freefilesync.org/\n\nDsynchronize",normalizedContent:"# windows目录实时同步工具\n\nhttps://freefilesync.org/\n\ndsynchronize",charsets:{cjk:!0}},{title:"监控目录或文件变化",frontmatter:{title:"监控目录或文件变化",date:"2022-12-13T18:01:34.000Z",permalink:"/pages/f8da74/",categories:["编程","python"],tags:[null],readingShow:"top",description:"Watchdog的中文的“看门狗”，有保护的意思。最早引入Watchdog是在单片机系统中，由于单片机的工作环境容易受到外界磁场的干扰，导致程序“跑飞”，造成整个系统无法正常工作，因此，引入了一个“看门狗”，对单片机的运行状态进行实时监测，针对运行故障做一些保护处理，譬如让系统重启。这种Watchdog属于硬件层面，必须有硬件电路的支持。\n>\n> Linux也引入了Watchdog，在Linux内核下，当Watchdog启动后，便设定了一个定时器，如果在超时时间内没有对/dev/Watchdog进行写操作，则会导致系统重启。通过定时器实现的Watchdog属于软件层面。",meta:[{name:"twitter:title",content:"监控目录或文件变化"},{name:"twitter:description",content:"Watchdog的中文的“看门狗”，有保护的意思。最早引入Watchdog是在单片机系统中，由于单片机的工作环境容易受到外界磁场的干扰，导致程序“跑飞”，造成整个系统无法正常工作，因此，引入了一个“看门狗”，对单片机的运行状态进行实时监测，针对运行故障做一些保护处理，譬如让系统重启。这种Watchdog属于硬件层面，必须有硬件电路的支持。\n>\n> Linux也引入了Watchdog，在Linux内核下，当Watchdog启动后，便设定了一个定时器，如果在超时时间内没有对/dev/Watchdog进行写操作，则会导致系统重启。通过定时器实现的Watchdog属于软件层面。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/01.%E7%9B%91%E6%8E%A7%E7%9B%AE%E5%BD%95%E6%88%96%E6%96%87%E4%BB%B6%E5%8F%98%E5%8C%96.html"},{property:"og:type",content:"article"},{property:"og:title",content:"监控目录或文件变化"},{property:"og:description",content:"Watchdog的中文的“看门狗”，有保护的意思。最早引入Watchdog是在单片机系统中，由于单片机的工作环境容易受到外界磁场的干扰，导致程序“跑飞”，造成整个系统无法正常工作，因此，引入了一个“看门狗”，对单片机的运行状态进行实时监测，针对运行故障做一些保护处理，譬如让系统重启。这种Watchdog属于硬件层面，必须有硬件电路的支持。\n>\n> Linux也引入了Watchdog，在Linux内核下，当Watchdog启动后，便设定了一个定时器，如果在超时时间内没有对/dev/Watchdog进行写操作，则会导致系统重启。通过定时器实现的Watchdog属于软件层面。"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/01.%E7%9B%91%E6%8E%A7%E7%9B%AE%E5%BD%95%E6%88%96%E6%96%87%E4%BB%B6%E5%8F%98%E5%8C%96.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-13T18:01:34.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"监控目录或文件变化"},{itemprop:"description",content:"Watchdog的中文的“看门狗”，有保护的意思。最早引入Watchdog是在单片机系统中，由于单片机的工作环境容易受到外界磁场的干扰，导致程序“跑飞”，造成整个系统无法正常工作，因此，引入了一个“看门狗”，对单片机的运行状态进行实时监测，针对运行故障做一些保护处理，譬如让系统重启。这种Watchdog属于硬件层面，必须有硬件电路的支持。\n>\n> Linux也引入了Watchdog，在Linux内核下，当Watchdog启动后，便设定了一个定时器，如果在超时时间内没有对/dev/Watchdog进行写操作，则会导致系统重启。通过定时器实现的Watchdog属于软件层面。"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/01.python/01.%E7%9B%91%E6%8E%A7%E7%9B%AE%E5%BD%95%E6%88%96%E6%96%87%E4%BB%B6%E5%8F%98%E5%8C%96.html",relativePath:"03.编程/01.python/01.监控目录或文件变化.md",key:"v-403a5ccb",path:"/pages/f8da74/",headers:[{level:2,title:"watchdog介绍",slug:"watchdog介绍",normalizedTitle:"watchdog介绍",charIndex:2},{level:2,title:"示例",slug:"示例",normalizedTitle:"示例",charIndex:310}],headersStr:"watchdog介绍 示例",content:'# watchdog介绍\n\n> Watchdog的中文的“看门狗”，有保护的意思。最早引入Watchdog是在单片机系统中，由于单片机的工作环境容易受到外界磁场的干扰，导致程序“跑飞”，造成整个系统无法正常工作，因此，引入了一个“看门狗”，对单片机的运行状态进行实时监测，针对运行故障做一些保护处理，譬如让系统重启。这种Watchdog属于硬件层面，必须有硬件电路的支持。\n> \n> Linux也引入了Watchdog，在Linux内核下，当Watchdog启动后，便设定了一个定时器，如果在超时时间内没有对/dev/Watchdog进行写操作，则会导致系统重启。通过定时器实现的Watchdog属于软件层面。\n\n\n# 示例\n\nfrom watchdog.observers import Observer\nfrom watchdog.events import *\nimport time\n\nclass FileEventHandler(FileSystemEventHandler):\n def __init__(self):\n  FileSystemEventHandler.__init__(self)\n\n def on_moved(self, event):\n  if event.is_directory:\n   print("directory moved from {0} to {1}".format(event.src_path,event.dest_path))\n  else:\n   print("file moved from {0} to {1}".format(event.src_path,event.dest_path))\n\n def on_created(self, event):\n  if event.is_directory:\n   print("directory created:{0}".format(event.src_path))\n  else:\n   print("file created:{0}".format(event.src_path))\n\n def on_deleted(self, event):\n  if event.is_directory:\n   print("directory deleted:{0}".format(event.src_path))\n  else:\n   print("file deleted:{0}".format(event.src_path))\n\n def on_modified(self, event):\n  if event.is_directory:\n   print("directory modified:{0}".format(event.src_path))\n  else:\n   print("file modified:{0}".format(event.src_path))\n\nif __name__ == "__main__":\n observer = Observer()\n event_handler = FileEventHandler()\n observer.schedule(event_handler,r"D:\\code\\dingshirenwu",True)\n observer.start()\n try:\n  while True:\n   time.sleep(1)\n except KeyboardInterrupt:\n  observer.stop()\n observer.join()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n',normalizedContent:'# watchdog介绍\n\n> watchdog的中文的“看门狗”，有保护的意思。最早引入watchdog是在单片机系统中，由于单片机的工作环境容易受到外界磁场的干扰，导致程序“跑飞”，造成整个系统无法正常工作，因此，引入了一个“看门狗”，对单片机的运行状态进行实时监测，针对运行故障做一些保护处理，譬如让系统重启。这种watchdog属于硬件层面，必须有硬件电路的支持。\n> \n> linux也引入了watchdog，在linux内核下，当watchdog启动后，便设定了一个定时器，如果在超时时间内没有对/dev/watchdog进行写操作，则会导致系统重启。通过定时器实现的watchdog属于软件层面。\n\n\n# 示例\n\nfrom watchdog.observers import observer\nfrom watchdog.events import *\nimport time\n\nclass fileeventhandler(filesystemeventhandler):\n def __init__(self):\n  filesystemeventhandler.__init__(self)\n\n def on_moved(self, event):\n  if event.is_directory:\n   print("directory moved from {0} to {1}".format(event.src_path,event.dest_path))\n  else:\n   print("file moved from {0} to {1}".format(event.src_path,event.dest_path))\n\n def on_created(self, event):\n  if event.is_directory:\n   print("directory created:{0}".format(event.src_path))\n  else:\n   print("file created:{0}".format(event.src_path))\n\n def on_deleted(self, event):\n  if event.is_directory:\n   print("directory deleted:{0}".format(event.src_path))\n  else:\n   print("file deleted:{0}".format(event.src_path))\n\n def on_modified(self, event):\n  if event.is_directory:\n   print("directory modified:{0}".format(event.src_path))\n  else:\n   print("file modified:{0}".format(event.src_path))\n\nif __name__ == "__main__":\n observer = observer()\n event_handler = fileeventhandler()\n observer.schedule(event_handler,r"d:\\code\\dingshirenwu",true)\n observer.start()\n try:\n  while true:\n   time.sleep(1)\n except keyboardinterrupt:\n  observer.stop()\n observer.join()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n',charsets:{cjk:!0}},{title:"批量更改文件",frontmatter:{title:"批量更改文件",date:"2022-12-13T18:01:34.000Z",permalink:"/pages/f659cd/",categories:["编程","python"],tags:[null],readingShow:"top",description:"import os\ngodir = os.listdir('F:\\cka认证\\oldboy_go')",meta:[{name:"twitter:title",content:"批量更改文件"},{name:"twitter:description",content:"import os\ngodir = os.listdir('F:\\cka认证\\oldboy_go')"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/02.%E6%89%B9%E9%87%8F%E6%9B%B4%E6%94%B9%E6%96%87%E4%BB%B6.html"},{property:"og:type",content:"article"},{property:"og:title",content:"批量更改文件"},{property:"og:description",content:"import os\ngodir = os.listdir('F:\\cka认证\\oldboy_go')"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/02.%E6%89%B9%E9%87%8F%E6%9B%B4%E6%94%B9%E6%96%87%E4%BB%B6.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-13T18:01:34.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"批量更改文件"},{itemprop:"description",content:"import os\ngodir = os.listdir('F:\\cka认证\\oldboy_go')"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/01.python/02.%E6%89%B9%E9%87%8F%E6%9B%B4%E6%94%B9%E6%96%87%E4%BB%B6.html",relativePath:"03.编程/01.python/02.批量更改文件.md",key:"v-0542148a",path:"/pages/f659cd/",headers:[{level:2,title:"代码示例",slug:"代码示例",normalizedTitle:"代码示例",charIndex:2}],headersStr:"代码示例",content:"# 代码示例\n\nimport os\ngodir = os.listdir('F:\\cka认证\\oldboy_go')\n\nfor i in godir :\n\n    print(i)\n    if i.count('樱花论坛') :\n        filename = i.split('【樱花论坛 www.sakuraaaa.com】',1)[1]\n        print(filename)\n        oldname = 'F:/cka认证/oldboy_go/'+i\n        newname =  'F:/cka认证/oldboy_go/'+filename\n        os.rename(oldname,newname )\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n",normalizedContent:"# 代码示例\n\nimport os\ngodir = os.listdir('f:\\cka认证\\oldboy_go')\n\nfor i in godir :\n\n    print(i)\n    if i.count('樱花论坛') :\n        filename = i.split('【樱花论坛 www.sakuraaaa.com】',1)[1]\n        print(filename)\n        oldname = 'f:/cka认证/oldboy_go/'+i\n        newname =  'f:/cka认证/oldboy_go/'+filename\n        os.rename(oldname,newname )\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n",charsets:{cjk:!0}},{title:"python引用数据库",frontmatter:{title:"python引用数据库",date:"2022-12-13T18:35:51.000Z",permalink:"/pages/a109ec/",categories:["编程","python"],tags:[null],readingShow:"top",description:"headers = {'Content-Type':'application/json;charset=UTF-8'}\n    send_data = json.dumps(data).encode('utf-8')\n    ret = requests.post(url=dingdingurl,data=senddata,headers=headers)",meta:[{name:"twitter:title",content:"python引用数据库"},{name:"twitter:description",content:"headers = {'Content-Type':'application/json;charset=UTF-8'}\n    send_data = json.dumps(data).encode('utf-8')\n    ret = requests.post(url=dingdingurl,data=senddata,headers=headers)"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/03.python%E5%BC%95%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%93.html"},{property:"og:type",content:"article"},{property:"og:title",content:"python引用数据库"},{property:"og:description",content:"headers = {'Content-Type':'application/json;charset=UTF-8'}\n    send_data = json.dumps(data).encode('utf-8')\n    ret = requests.post(url=dingdingurl,data=senddata,headers=headers)"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/03.python%E5%BC%95%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%93.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-13T18:35:51.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"python引用数据库"},{itemprop:"description",content:"headers = {'Content-Type':'application/json;charset=UTF-8'}\n    send_data = json.dumps(data).encode('utf-8')\n    ret = requests.post(url=dingdingurl,data=senddata,headers=headers)"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/01.python/03.python%E5%BC%95%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%93.html",relativePath:"03.编程/01.python/03.python引用数据库.md",key:"v-7246ae2c",path:"/pages/a109ec/",headers:[{level:2,title:"python引用数据库两种方式",slug:"python引用数据库两种方式",normalizedTitle:"python引用数据库两种方式",charIndex:2},{level:4,title:"方式一",slug:"方式一",normalizedTitle:"方式一",charIndex:21},{level:4,title:"方式二",slug:"方式二",normalizedTitle:"方式二",charIndex:1301}],headersStr:"python引用数据库两种方式 方式一 方式二",content:'# python引用数据库两种方式\n\n# 方式一\n\n# -*- coding: UTF-8 -*-\nimport pymysql\nimport requests\nimport json\n#建立连接\nconn = pymysql.connect(host=\'##\', port=3306, database=\'##\', user=\'##\', password=\'####\')\n#拿到游标\ncursor = conn.cursor()\n#执行sql语句\nsql = "SELECT * FROM bh_job_exec WHERE job_cde=\'company\' AND exec_status=\'SUCCESS\' AND JSON_UNQUOTE(JSON_EXTRACT(exec_param, \'$.businessDate\')) = DATE_SUB(CURDATE(),INTERVAL 1 DAY);"\n# print(sql)\ndef sendDing(msg):\n    dingding_url=\'https://oapi.dingtalk.com/robot/send?access_token=xxxxx\'\n    data = {"msgtype": "text","text": {"content": "告警:"+str(msg)}}\n    headers = {\'Content-Type\':\'application/json;charset=UTF-8\'}\n    send_data = json.dumps(data).encode(\'utf-8\')\n    ret = requests.post(url=dingding_url,data=send_data,headers=headers)\n\n\ntry:\n    row = cursor.execute(sql)\n    # print(row)\n    # Python查询Mysql使用 fetchone() 方法获取单条数据, 使用fetchall() 方法获取多条数据。\n    data=cursor.fetchone()\n    #print(data)\n    #进行判断\n    if data:\n        #print("GFSS结算成功")\n        sendDing("GFSS结算成功")\n    else:\n        #print("GFSS结算失败")\n        sendDing("GFSS结算失败")\nexcept:\n    print("Error: unable to fetch data")\nfinally:\n    cursor.close()\n    #关闭数据库\n    conn.close()\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n# 方式二\n\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\nimport pymysql\nimport pickle\nimport os.path,sys\nimport urllib.request\nimport urllib.parse\nimport json\nding_url = "https://oapi.dingtalk.com/robot/send?access_token=xxxx" \ndef get_tabcol_current():\n    tabcol_dic = {}\n    ## 目标库的信息\n    db = pymysql.connect(host=\'xxx\',\n                         port=13306,\n                         user=\'xx\',\n                         password=\'xx\',\n                         database=\'xxx\')\n    cursor = db.cursor()\n \n    sql = "SELECT * FROM `terminal_session` WHERE date_end is Null"\n \n    cursor.execute(sql)\n    results = cursor.fetchall()\n    for row in results:\n      if row[1] in tabcol_dic:\n          tabcol_dic[row[1]].append(row[2])\n      else:\n          tabcol_dic[row[1]]=[row[2]]\n    cursor.close()\n    db.close()\n    # print(tabcol_dic) \n    return tabcol_dic\n    \ndef send_msg_by_ding(message):\n    header = {\n        "Content-Type": "application/json"\n    }\n    data = {\n        "msgtype": "text", \n        "text": {"content": message}\n    }\n    send_data = json.dumps(data).encode(\'utf-8\')\n    req = urllib.request.Request(ding_url, data=send_data, headers=header, method=\'POST\')\n    ret = urllib.request.urlopen(req)\n    if ret.status != 200:\n        print("send message error!")\ndef main():    \n    ret = get_tabcol_current()\n    # print(ret)\n    add_dic = ret\n    # add_dic = list(ret.values())\n    # del_dic = list(ret.keys())\n    message = "堡垒机告警：\\n"\n    if add_dic:\n        for (tab,cols) in add_dic.items():\n            cols.sort()\n            message = message + tab + " 登录了: " + \', \'.join(cols) + "\\n"\n    if add_dic:\n        send_msg_by_ding(message)\n    # print(del_dic)\n    # print(1111)\nif __name__ == \'__main__\':\n    main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n',normalizedContent:'# python引用数据库两种方式\n\n# 方式一\n\n# -*- coding: utf-8 -*-\nimport pymysql\nimport requests\nimport json\n#建立连接\nconn = pymysql.connect(host=\'##\', port=3306, database=\'##\', user=\'##\', password=\'####\')\n#拿到游标\ncursor = conn.cursor()\n#执行sql语句\nsql = "select * from bh_job_exec where job_cde=\'company\' and exec_status=\'success\' and json_unquote(json_extract(exec_param, \'$.businessdate\')) = date_sub(curdate(),interval 1 day);"\n# print(sql)\ndef sendding(msg):\n    dingding_url=\'https://oapi.dingtalk.com/robot/send?access_token=xxxxx\'\n    data = {"msgtype": "text","text": {"content": "告警:"+str(msg)}}\n    headers = {\'content-type\':\'application/json;charset=utf-8\'}\n    send_data = json.dumps(data).encode(\'utf-8\')\n    ret = requests.post(url=dingding_url,data=send_data,headers=headers)\n\n\ntry:\n    row = cursor.execute(sql)\n    # print(row)\n    # python查询mysql使用 fetchone() 方法获取单条数据, 使用fetchall() 方法获取多条数据。\n    data=cursor.fetchone()\n    #print(data)\n    #进行判断\n    if data:\n        #print("gfss结算成功")\n        sendding("gfss结算成功")\n    else:\n        #print("gfss结算失败")\n        sendding("gfss结算失败")\nexcept:\n    print("error: unable to fetch data")\nfinally:\n    cursor.close()\n    #关闭数据库\n    conn.close()\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n# 方式二\n\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\nimport pymysql\nimport pickle\nimport os.path,sys\nimport urllib.request\nimport urllib.parse\nimport json\nding_url = "https://oapi.dingtalk.com/robot/send?access_token=xxxx" \ndef get_tabcol_current():\n    tabcol_dic = {}\n    ## 目标库的信息\n    db = pymysql.connect(host=\'xxx\',\n                         port=13306,\n                         user=\'xx\',\n                         password=\'xx\',\n                         database=\'xxx\')\n    cursor = db.cursor()\n \n    sql = "select * from `terminal_session` where date_end is null"\n \n    cursor.execute(sql)\n    results = cursor.fetchall()\n    for row in results:\n      if row[1] in tabcol_dic:\n          tabcol_dic[row[1]].append(row[2])\n      else:\n          tabcol_dic[row[1]]=[row[2]]\n    cursor.close()\n    db.close()\n    # print(tabcol_dic) \n    return tabcol_dic\n    \ndef send_msg_by_ding(message):\n    header = {\n        "content-type": "application/json"\n    }\n    data = {\n        "msgtype": "text", \n        "text": {"content": message}\n    }\n    send_data = json.dumps(data).encode(\'utf-8\')\n    req = urllib.request.request(ding_url, data=send_data, headers=header, method=\'post\')\n    ret = urllib.request.urlopen(req)\n    if ret.status != 200:\n        print("send message error!")\ndef main():    \n    ret = get_tabcol_current()\n    # print(ret)\n    add_dic = ret\n    # add_dic = list(ret.values())\n    # del_dic = list(ret.keys())\n    message = "堡垒机告警：\\n"\n    if add_dic:\n        for (tab,cols) in add_dic.items():\n            cols.sort()\n            message = message + tab + " 登录了: " + \', \'.join(cols) + "\\n"\n    if add_dic:\n        send_msg_by_ding(message)\n    # print(del_dic)\n    # print(1111)\nif __name__ == \'__main__\':\n    main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n',charsets:{cjk:!0}},{title:"python3给防火墙添加放行",frontmatter:{title:"python3给防火墙添加放行",date:"2022-12-14T14:51:21.000Z",permalink:"/pages/457fb7/",categories:["编程","python"],tags:[null],readingShow:"top",description:"脚本如下",meta:[{name:"twitter:title",content:"python3给防火墙添加放行"},{name:"twitter:description",content:"脚本如下"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/04.python3%E7%BB%99%E9%98%B2%E7%81%AB%E5%A2%99%E6%B7%BB%E5%8A%A0%E6%94%BE%E8%A1%8C.html"},{property:"og:type",content:"article"},{property:"og:title",content:"python3给防火墙添加放行"},{property:"og:description",content:"脚本如下"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/04.python3%E7%BB%99%E9%98%B2%E7%81%AB%E5%A2%99%E6%B7%BB%E5%8A%A0%E6%94%BE%E8%A1%8C.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-14T14:51:21.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"python3给防火墙添加放行"},{itemprop:"description",content:"脚本如下"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/01.python/04.python3%E7%BB%99%E9%98%B2%E7%81%AB%E5%A2%99%E6%B7%BB%E5%8A%A0%E6%94%BE%E8%A1%8C.html",relativePath:"03.编程/01.python/04.python3给防火墙添加放行.md",key:"v-0580956a",path:"/pages/457fb7/",headersStr:null,content:"> 脚本如下\n\nfrom http.server import BaseHTTPRequestHandler\nfrom urllib import parse\nimport subprocess\n\nclass GetHandler(BaseHTTPRequestHandler):\n\n    secret_key = r'Hi93d4cfa5863770f9a0c87d8,b85a4ebc7'\n    #secret_key = r'4AULgSBxBESAt3IaItH6C227ik4,fW5o6xr+1!7CfjJSCTf$5jxV)+w7iImrx@'\n    #secret_key = r'4AULgSBxBESAt3IaItH6C227ik4,fW5o6xr+1!7CfjJSCTf$5jxV)+w7iImrx@'\n    #commands = \"iptables -A INPUT -s {}/32 -p tcp -m state --state NEW -m tcp --dport 443 -j ACCEPT && service iptables save\"\n    commands = r\"\"\"firewall-cmd --add-rich-rule=\"rule family=\"ipv4\" source address=\"{}\" port port=\"3306\" protocol=\"tcp\" accept\" --zone=public --permanent && firew\nall-cmd --reload \"\"\"\n    def process_command(self,client_ip):\n        commands = self.commands.format(client_ip)\n        try:\n            process_result = subprocess.run(commands,stdout=subprocess.PIPE, shell=True, check=True)\n            re_stdout = ''\n            re_stderr = ''\n            if process_result.stdout:\n                re_stdout = str(process_result.stdout,encoding='utf-8')\n            if process_result.stderr:\n                re_stderr = str(process_result.stderr,encoding='utf-8')\n            return (re_stdout, re_stderr)\n        except subprocess.CalledProcessError as e:\n            return \"Process Command Error.\"\n\n        #return subprocess.run(self.commands)\n\n    def do_GET(self):\n       # parsed_path = parse.urlparse(self.path)\n        #parsed_header = parse.urlparse(self.headers)\n        client_ip = self.address_string()\n        #client_ip = self.headers['X-Real-IP']\n        client_request_path = self.path[1:]\n        if client_request_path != self.secret_key:\n            self.send_error(403)\n\n        r_result = self.process_command(client_ip)\n        message = ''.join(r_result)\n        self.send_response(200)\n        self.send_header('Content-Type',\n                         'text/plain; charset=utf-8')\n        self.end_headers()\n        self.wfile.write(message.encode('utf-8'))\n\nif __name__ == '__main__':\n    from http.server import HTTPServer\n    server = HTTPServer(('0.0.0.0', 29999), GetHandler)\n    print('Starting server, use <Ctrl-C> to stop')\n    server.serve_forever()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n",normalizedContent:"> 脚本如下\n\nfrom http.server import basehttprequesthandler\nfrom urllib import parse\nimport subprocess\n\nclass gethandler(basehttprequesthandler):\n\n    secret_key = r'hi93d4cfa5863770f9a0c87d8,b85a4ebc7'\n    #secret_key = r'4aulgsbxbesat3iaith6c227ik4,fw5o6xr+1!7cfjjsctf$5jxv)+w7iimrx@'\n    #secret_key = r'4aulgsbxbesat3iaith6c227ik4,fw5o6xr+1!7cfjjsctf$5jxv)+w7iimrx@'\n    #commands = \"iptables -a input -s {}/32 -p tcp -m state --state new -m tcp --dport 443 -j accept && service iptables save\"\n    commands = r\"\"\"firewall-cmd --add-rich-rule=\"rule family=\"ipv4\" source address=\"{}\" port port=\"3306\" protocol=\"tcp\" accept\" --zone=public --permanent && firew\nall-cmd --reload \"\"\"\n    def process_command(self,client_ip):\n        commands = self.commands.format(client_ip)\n        try:\n            process_result = subprocess.run(commands,stdout=subprocess.pipe, shell=true, check=true)\n            re_stdout = ''\n            re_stderr = ''\n            if process_result.stdout:\n                re_stdout = str(process_result.stdout,encoding='utf-8')\n            if process_result.stderr:\n                re_stderr = str(process_result.stderr,encoding='utf-8')\n            return (re_stdout, re_stderr)\n        except subprocess.calledprocesserror as e:\n            return \"process command error.\"\n\n        #return subprocess.run(self.commands)\n\n    def do_get(self):\n       # parsed_path = parse.urlparse(self.path)\n        #parsed_header = parse.urlparse(self.headers)\n        client_ip = self.address_string()\n        #client_ip = self.headers['x-real-ip']\n        client_request_path = self.path[1:]\n        if client_request_path != self.secret_key:\n            self.send_error(403)\n\n        r_result = self.process_command(client_ip)\n        message = ''.join(r_result)\n        self.send_response(200)\n        self.send_header('content-type',\n                         'text/plain; charset=utf-8')\n        self.end_headers()\n        self.wfile.write(message.encode('utf-8'))\n\nif __name__ == '__main__':\n    from http.server import httpserver\n    server = httpserver(('0.0.0.0', 29999), gethandler)\n    print('starting server, use <ctrl-c> to stop')\n    server.serve_forever()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n",charsets:{cjk:!0}},{title:"python生成部署脚本",frontmatter:{title:"python生成部署脚本",date:"2023-01-06T11:10:56.000Z",permalink:"/pages/d0fcb5/",categories:["编程","python"],tags:[null],readingShow:"top",description:"#!/usr/bin/env python\ncoding=utf-8\n    import os\n    import sys",meta:[{name:"twitter:title",content:"python生成部署脚本"},{name:"twitter:description",content:"#!/usr/bin/env python\ncoding=utf-8\n    import os\n    import sys"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/05.python%E7%94%9F%E6%88%90%E9%83%A8%E7%BD%B2%E8%84%9A%E6%9C%AC.html"},{property:"og:type",content:"article"},{property:"og:title",content:"python生成部署脚本"},{property:"og:description",content:"#!/usr/bin/env python\ncoding=utf-8\n    import os\n    import sys"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/05.python%E7%94%9F%E6%88%90%E9%83%A8%E7%BD%B2%E8%84%9A%E6%9C%AC.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-06T11:10:56.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"python生成部署脚本"},{itemprop:"description",content:"#!/usr/bin/env python\ncoding=utf-8\n    import os\n    import sys"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/01.python/05.python%E7%94%9F%E6%88%90%E9%83%A8%E7%BD%B2%E8%84%9A%E6%9C%AC.html",relativePath:"03.编程/01.python/05.python生成部署脚本.md",key:"v-71ab6464",path:"/pages/d0fcb5/",headersStr:null,content:'#!/usr/bin/env python\n# coding=utf-8\nimport os\nimport sys\n\nappYml = \'\'\'#!/bin/bash\n\nymlName1="docker-compose.yml"\nbuild_compose_yml(){\ncat>"${ymlName1}"<<EOF\nversion: "3"\nservices:\n  gateway:\n    image: xxx/ait0/gm_subscribe:\\${TAG}\n    container_name: "gm_subscribe"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      MQ_URL: ${MQ_URL}\n      GM_ADDRESS: ${GM_ADDRESS}\n      GM_TOKEN: ${GM_TOKEN}\n      GM_SYMBOLS: ${GM_SYMBOLS}\n      USE_FAKE: ${USE_FAKE}\n    entrypoint: python3 ./gm_subscribe.py\n\nnetworks:\n  network:\n    driver: bridge\n\nEOF\n}\n\n\'\'\'\naitoolsappYml = \'\'\'#!/bin/bash\n\nymlName1="docker-compose.yml"\nbuild_compose_yml(){\ncat>"${ymlName1}"<<EOF\nversion: "3"\nservices:\n  gateway:\n    image: xxx/ait0/python_aitools:%s\n    container_name: "python_aitools"\n    restart: always\n    volumes:\n      - ./config.json:/app/config/config.json\n    networks:\n      - network\n\nnetworks:\n  network:\n    driver: bridge\n\nEOF\n}\n\n\'\'\'\naitoolsEnvScript = \'\'\'\nenvName1="config.json"\nbuild_env(){\ncat>"${envName1}"<<EOF\n{\n  "mysql_param": {\n    "quote_db":{\n      "host": "172.16.30.194",\n      "port": 13307,\n      "user": "dev",\n      "passwd": "dev_pwd",\n      "db": "quote"\n    },\n    "sync_db":{\n      "host": "172.16.30.221",\n      "port": 13306,\n      "user": "root",\n      "passwd": "xxx",\n      "db": "quote_test"\n    },\n    "nano_db":{\n      "host": "xxx",\n      "port": 3306,\n      "user": "readonly",\n      "passwd": "xxxx",\n      "db": "marketdata"\n    },\n    "ait0_db":{\n      "host": "172.16.30.194",\n      "port": 13307,\n      "user": "dev",\n      "passwd": "dev_pwd",\n      "db": "ai_server"\n    }\n  },\n\n  "process_threads": 4,\n  "timer_param": {\n    "gm_stock_time": "15:30",\n    "gm_future_time": "16:30",\n    "nano_time": "09:30",\n    "code_time": "08:30",\n    "price_time": 5\n  },\n  "redis_param": {\n    "host": "172.16.30.194",\n    "port": 16379,\n    "password": "xxx"\n  },\n  "check_code_count" : [\n    {\n        "exchange": "SZSE",\n        "min_count": 2000\n    },\n    {\n        "exchange": "SHSE",\n        "min_count": 2000\n    }\n  ],\n  "check_price": [\n    {\n      "exchange": "SZSE",\n      "symbol": "000001"\n    },\n    {\n      "exchange": "SHSE",\n      "symbol": "600000"\n    }\n  ],\n  "other_param" : {\n    "ding_token": "xxx",\n    "quote_url": "http://172.16.30.194:8888/",\n    "quant_token":"xxxx",\n    "quant_addr": "127.0.0.1:7001"\n   }\n}\n\nEOF\n}\n\'\'\'\nEnvScript = \'\'\'\nenvName1=".env"\nbuild_env(){\ncat>"${envName1}"<<EOF\nTAG=%s\nMQ_URL=%s\nGM_ADDRESS=%s\nGM_TOKEN=%s\nGM_SYMBOLS=%s\nUSE_FAKE=%s\nEOF\n}\n\n\'\'\'\n\naitoolsrunScript = \'\'\'\nbuild_deploy(){\n    docker-compose up -d\n}\n\ninit(){\n    docker-compose down -v\n    docker rmi xxx/ait0/python_aitools:%s\n}\n\nmain(){\n    init\n\n    build_compose_yml\n\n    build_env\n    build_deploy\n}\n\nmain\n\'\'\'\n\n\n\nrunScript = \'\'\'\nbuild_deploy(){\n    docker-compose up -d\n}\n\ninit(){\n    docker-compose down -v\n    docker rmi xxx/ait0/gm_subscribe:%s\n}\n\nmain(){\n    init\n\n    build_compose_yml\n\n    build_env\n    build_deploy\n}\n\nmain\n\'\'\'\n\n\ndef ait0_python_deploy():\n    env_dist = os.environ\n    MQ_URL = env_dist.get("MQ_URL", "amqp://guest:guest@172.16.50.24:15672/quote?heartbeat=0")\n    GM_ADDRESS = env_dist.get("GM_ADDRESS", "172.16.50.23:7001")\n    GM_TOKEN = env_dist.get("GM_TOKEN", "xxx")\n    GM_SYMBOLS = env_dist.get("GM_SYMBOLS", "SZSE.300890,SZSE.300409,SHSE.688700,SHSE.603738,SHSE.688689,SHSE.688668,SZSE.300428,SZSE.002011,SZSE.300416,SZSE.300065,SZSE.300447,SZSE.002897,SZSE.002837,SZSE.300648,SZSE.002518,SHSE.603912,SZSE.002567,SHSE.688556,SHSE.603396,SHSE.603477,SHSE.603596,SZSE.002906,SZSE.002466,SHSE.600563,SZSE.002484,SZSE.002812,SZSE.300274,SZSE.300316,SHSE.688598,SHSE.603806,SHSE.605117,SHSE.603606,SZSE.300850,SZSE.002531,SZSE.300327,SZSE.300633,SZSE.002335,SHSE.601636,SZSE.000959,SZSE.000932,SHSE.600782,SHSE.600985,SHSE.600188,SHSE.601088,SHSE.601600,SZSE.000807,SHSE.601899,SHSE.600362,SZSE.002714,SZSE.000977,SHSE.600845,SZSE.002271,SHSE.603208,SZSE.002572,SZSE.000400,SZSE.002028,SZSE.000733,SHSE.600765,SZSE.002179,SZSE.300394,SZSE.300504,SHSE.600702,SHSE.603198,SHSE.603589,SHSE.600559,SHSE.600197,SHSE.601800,SHSE.601669,SHSE.600426,SHSE.600309,SHSE.600230,SZSE.002241,SZSE.000672,SHSE.600585")\n    USE_FAKE = env_dist.get("USE_FAKE", "0")\n    version = env_dist["version_number"]\n\n    # 替换字符串.\n    yml = appYml\n    env1 = EnvScript % (version, MQ_URL, GM_ADDRESS, GM_TOKEN, GM_SYMBOLS, USE_FAKE)\n    run1 = runScript % (version)\n    script = yml + env1 + run1\n\n    # 生成脚本文件.\n    fp = open("ait0_python.sh", "w")\n    fp.write(script)\n    fp.close()\n\ndef aitools_python_deploy():\n    env_dist = os.environ\n    # MQ_URL = env_dist.get("MQ_URL", "amqp://guest:guest@172.16.50.24:15672/quote?heartbeat=0")\n    # GM_ADDRESS = env_dist.get("GM_ADDRESS", "172.16.50.23:7001")\n    # GM_TOKEN = env_dist.get("GM_TOKEN", "xxx")\n    # GM_SYMBOLS = env_dist.get("GM_SYMBOLS", "SZSE.300890,SZSE.300409,SHSE.688700,SHSE.603738,SHSE.688689,SHSE.688668,SZSE.300428,SZSE.002011,SZSE.300416,SZSE.300065,SZSE.300447,SZSE.002897,SZSE.002837,SZSE.300648,SZSE.002518,SHSE.603912,SZSE.002567,SHSE.688556,SHSE.603396,SHSE.603477,SHSE.603596,SZSE.002906,SZSE.002466,SHSE.600563,SZSE.002484,SZSE.002812,SZSE.300274,SZSE.300316,SHSE.688598,SHSE.603806,SHSE.605117,SHSE.603606,SZSE.300850,SZSE.002531,SZSE.300327,SZSE.300633,SZSE.002335,SHSE.601636,SZSE.000959,SZSE.000932,SHSE.600782,SHSE.600985,SHSE.600188,SHSE.601088,SHSE.601600,SZSE.000807,SHSE.601899,SHSE.600362,SZSE.002714,SZSE.000977,SHSE.600845,SZSE.002271,SHSE.603208,SZSE.002572,SZSE.000400,SZSE.002028,SZSE.000733,SHSE.600765,SZSE.002179,SZSE.300394,SZSE.300504,SHSE.600702,SHSE.603198,SHSE.603589,SHSE.600559,SHSE.600197,SHSE.601800,SHSE.601669,SHSE.600426,SHSE.600309,SHSE.600230,SZSE.002241,SZSE.000672,SHSE.600585")\n    # USE_FAKE = env_dist.get("USE_FAKE", "0")\n    version = env_dist["version_number"]\n\n    # 替换字符串.\n    yml = aitoolsappYml % (version)\n    env1 = aitoolsEnvScript\n    run1 = aitoolsrunScript % (version)\n    script = yml + env1 + run1\n\n    # 生成脚本文件.\n    fp = open("aitools_python.sh", "w")\n    fp.write(script)\n    fp.close()\n\n\nif __name__ == "__main__":\n    param = sys.argv[1]\n    names = param.split(".")\n    n = names[len(names)-1]\n    if n == "ait0_python":\n        ait0_python_deploy()\n    elif n == "aitools":\n        aitools_python_deploy()\n    else:\n        print param\n\n\n#!/usr/bin/env python\n# coding=utf-8\n\nimport os\n\nyml = \'\'\'#!/bin/bash\n\nymlName="docker-compose.yml"\nbuild_compose_yml(){\ncat>"${ymlName}"<<EOF\nversion: "3"\nservices:\n  gateway:\n    image: xxx/quote/gateway:\\${TAG}\n    container_name: "gateway"\n    restart: always\n    ports:\n      - 8888:8888\n      - 39101:9101\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    depends_on:\n      - query\n    environment:\n      QUERY_ADDRESS: \\${QUERY_ADDRESS}\n      TRACING_URL: \\${TRACING_URL}\n      TRACING_SAMPLER: \\${TRACING_SAMPLER}\n      QUERY_TABLES_ADDRESS: \\${QUERY_TABLES_ADDRESS}\n  query:\n    image: xxx/quote/query:\\${TAG}\n    container_name: "query"\n    restart: always\n    ports:\n      - 39102:9102\n      - 8081:8081\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      MARKET_TYPE: \\${MARKET_TYPE}\n      MYSQL_SOURCE: \\${MYSQL_SOURCE}\n      REDIS_ADDRESS: \\${REDIS_ADDRESS}\n      REDIS_PASS: \\${REDIS_PASS}\n      TRACING_URL: \\${TRACING_URL}\n      TRACING_SAMPLER: \\${TRACING_SAMPLER}\n\n  storage:\n    image: xxx/quote/storage:\\${TAG}\n    container_name: "storage"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      MYSQL_SOURCE: \\${MYSQL_SOURCE}\n      STORAGE_MODE: \\${STORAGE_MODE}\n      REDIS_ADDRESS: \\${REDIS_ADDRESS}\n      REDIS_PASS: \\${REDIS_PASS}\n      RABBIT_ADDRESS: \\${RABBIT_ADDRESS}\n      PROC_THREADS: \\${PROC_THREADS}\n      TICKS_WRITE_FLAG: \\${TICKS_WRITE_FLAG}\n      TICK_SAVE_SYMBOLS: \\${TICK_SAVE_SYMBOLS}\n      STORAGE_SERVICE_NAME: \\${STORAGE_SERVICE_NAME}\n      STORAGE_SERVICE_PORT: \\${STORAGE_SERVICE_PORT}\n      STORAGE_LOG_PATH: \\${STORAGE_LOG_PATH}\n  probe:\n    image: xxx/probe/probe:v0.7.0\n    container_name: "probe"\n    restart: always\n    networks:\n      - network\n    ports:\n      - 15888:5888\n    command:\n      - -http=gateway:8888,storage:10001\n      - -grpc=query.rpc:query:8081\n  gm:\n    image: xxx/quote/gm:\\${TAG}\n    container_name: "gm"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    ports:\n      - 9159:9159\n    networks:\n      - network\n    environment:\n      RABBIT_ADDRESS: \\${RABBIT_ADDRESS}\n      GM_OPEN_TIME: \\${GM_OPEN_TIME}\n      GM_SYNC_FLAG: \\${GM_SYNC_FLAG}\n      GM_EXCHANGE_PREFIX_FLAG: \\${GM_EXCHANGE_PREFIX_FLAG}\n      GM_QUOTE_COMMODITY: \\${GM_QUOTE_COMMODITY}\n      GM_EXCHANGE_NO: \\${GM_EXCHANGE_NO}\n      GM_TOKEN: \\${GM_TOKEN}\n      GM_STRATEGY_ID: \\${GM_STRATEGY_ID}\n      GM_SERV_ADDR: \\${GM_SERV_ADDR}\n      TRACING_URL: \\${TRACING_URL}\n      TRACING_SAMPLER: \\${TRACING_SAMPLER}\n      TRANFER_ADDRESS: \\${TRANFER_ADDRESS}\n      GM_SERVICE_NAME: \\${GM_SERVICE_NAME_STOCK}\n      SOURCE_GM_PORT: \\${SOURCE_GM_PORT_STOCK}\n      GM_SERVICE_LOG_PATH: \\${GM_SERVICE_LOG_PATH_STOCK}\n      SOURCE_GM_PROM_PORT: \\${SOURCE_GM_PROM_PORT_STOCK}\n      GM_SEC_TYPES: \\${GM_SEC_TYPES_STOCK}\n  querytables:\n    image: xxx/quote/querytables:\\${TAG}\n    container_name: "querytables"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    ports:\n      - 9104:9104\n    networks:\n      - network\n    environment:\n      MYSQL_SOURCE_TABLES: \\${MYSQL_SOURCE_TABLES}\n      TRACING_URL: \\${TRACING_URL}\n      TRACING_SAMPLER: \\${TRACING_SAMPLER}\n  transfer:\n    image: xxx/quote/transfer:\\${TAG}\n    container_name: "transfer"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    ports:\n      - 8082:8082\n      - 19103:9103\n    networks:\n      - network\n    environment:\n      RABBIT_ADDRESS: \\${RABBIT_ADDRESS}\n      TRACING_URL: \\${TRACING_URL}\n      TRACING_SAMPLER: \\${TRACING_SAMPLER}\n  push:\n    image: xxx/quote/push:\\${TAG}\n    container_name: "push"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      RABBIT_ADDRESS: \\${RABBIT_ADDRESS}\n      TRACING_URL: \\${TRACING_URL}\n      TRACING_SAMPLER: \\${TRACING_SAMPLER}\n  websocketv1:\n    image: xxx/quote/websocketv1:\\${TAG}\n    container_name: "websocketv1"\n    restart: always\n    ports:\n      - 8889:8889\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    depends_on:\n      - push\n    environment:\n      STORAGE_MODE: \\${STORAGE_MODE}\n      PUSH_ADDRESS: \\${PUSH_ADDRESS}\n      TRACING_URL: \\${TRACING_URL}\n      TRACING_SAMPLER: \\${TRACING_SAMPLER}\n  tap:\n    image: xxx/quote/tap:\\${TAG}\n    container_name: "tap"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      ESUNNY_QUOTE_IP: \\${ESUNNY_QUOTE_IP}\n      ESUNNY_QUOTE_PORT: \\${ESUNNY_QUOTE_PORT}\n      ESUNNY_QUOTE_AUTHCODE: \\${ESUNNY_QUOTE_AUTHCODE}\n      ESUNNY_QUOTE_USERNAME: \\${ESUNNY_QUOTE_USERNAME}\n      ESUNNY_QUOTE_PASSWD: \\${ESUNNY_QUOTE_PASSWD}\n      ESUNNY_QUOTE_COMMODITY: \\${ESUNNY_QUOTE_COMMODITY}\n      ESUNNY_TRADE_IP: \\${ESUNNY_TRADE_IP}\n      ESUNNY_TRADE_PORT: \\${ESUNNY_TRADE_PORT}\n      ESUNNY_TRADE_AUTHCODE: \\${ESUNNY_TRADE_AUTHCODE}\n      ESUNNY_TRADE_USERNAME: \\${ESUNNY_TRADE_USERNAME}\n      ESUNNY_TRADE_PASSWD: \\${ESUNNY_TRADE_PASSWD}\n      ESUNNY_COMMODITYNO: \\${ESUNNY_COMMODITYNO}\n      ESUNNY_OPEN_TIME: \\${ESUNNY_OPEN_TIME}\n      ESUNNY_SYNC_FLAG: \\${ESUNNY_SYNC_FLAG}\n      ESUNNY_EXCHANGE_PREFIX_FLAG: \\${ESUNNY_EXCHANGE_PREFIX_FLAG}\n      ESUNNY_LOG_PATH: \\${ESUNNY_LOG_PATH}\n      ESUNNY_QUOTE_FLAG: \\${ESUNNY_QUOTE_FLAG}\n      ESUNNY_EXCHANGE_NO: \\${ESUNNY_EXCHANGE_NO}\n      RABBIT_ADDRESS: \\${RABBIT_ADDRESS}\n      TRACING_URL: \\${TRACING_URL}\n      TRACING_SAMPLER: \\${TRACING_SAMPLER}\n      TRANFER_ADDRESS: \\${TRANFER_ADDRESS}\n\n  ths:\n    image: xxx/quote/ths:\\${TAG}\n    container_name: "ths"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    ports:\n      - 9162:9162\n    networks:\n      - network\n    environment:\n      THS_SERVICE_NAME: \\${THS_SERVICE_NAME}\n      SOURCE_THS_PORT: \\${SOURCE_THS_PORT}\n      SOURCE_THS_PROM_PORT: \\${SOURCE_THS_PROM_PORT}\n      RABBIT_ADDRESS: \\${RABBIT_ADDRESS}\n      TRANFER_ADDRESS: \\${TRANFER_ADDRESS}\n      TRACING_URL: \\${TRACING_URL}\n      TRACING_SAMPLER: \\${TRACING_SAMPLER}\n      THS_ACCESS_TOKEN: \\${THS_ACCESS_TOKEN}\n\n  futu:\n    image: xxx/quote/futu:\\${TAG}\n    container_name: "futu"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      FUTU_SERVICE_NAME: \\${FUTU_SERVICE_NAME}\n      SOURCE_FUTU_PORT: \\${SOURCE_FUTU_PORT}\n      RABBIT_ADDRESS: \\${RABBIT_ADDRESS}\n      FUTU_OPEN_TIME: \\${FUTU_OPEN_TIME}\n      FUTU_SYNC_FLAG: \\${FUTU_SYNC_FLAG}\n      FUTU_EXCHANGE_PREFIX_FLAG: \\${FUTU_EXCHANGE_PREFIX_FLAG}\n      FUTU_QUOTE_SYMBOLS: \\${FUTU_QUOTE_SYMBOLS}\n      FUTU_ADDRESS: \\${FUTU_ADDRESS}\n      FUTU_PORT: \\${FUTU_PORT}\n      FUTU_SERVICE_NAME: \\${FUTU_SERVICE_NAME}\n      FUTU_SERVICE_LOG_PATH: \\${FUTU_SERVICE_LOG_PATH}\n      SOURCE_FUTU_PROM_PORT: \\${SOURCE_FUTU_PROM_PORT}\n      TRACING_URL: \\${TRACING_URL}\n      TRACING_SAMPLER: \\${TRACING_SAMPLER}\n      TRANFER_ADDRESS: \\${TRANFER_ADDRESS}\n\n\n\n\nnetworks:\n  network:\n    driver: bridge\nEOF\n}\n\n\'\'\'\n\nenvScript = \'\'\'\nenvName=".env"\nbuild_env(){\ncat>"${envName}"<<EOF\nTAG=%s\nMYSQL_SOURCE="%s:%s@tcp(%s)/quote?charset=utf8mb4&parseTime=true&loc=Local"\nREDIS_ADDRESS="%s"\nREDIS_PASS="%s"\nRABBIT_ADDRESS="amqp://guest:guest@%s/quote"\nQUERY_ADDRESS="query:8081"\nTRACING_URL="%s"\nTRACING_SAMPLER=0.01\nMARKET_TYPE=%s\nSTORAGE_MODE=%s\nPROC_THREADS=8\nGTA_MYSQL_SOURCE="xx:xx@tcp(xx:3306)/RCWALGOPRD1?charset=utf8mb4&parseTime=true&loc=Local"\nGTA_REDIS_ADDRESS="xx:7788"\nGTA_REDIS_PASS="xx"\nTICKS_WRITE_FLAG=0\nGM_OPEN_TIME="06:10"\nGM_SYNC_FLAG=1\nGM_EXCHANGE_PREFIX_FLAG=1\nGM_QUOTE_COMMODITY=""\nGM_EXCHANGE_NO=""\nGM_TOKEN="%s"\nGM_STRATEGY_ID="xx"\nGM_SERV_ADDR="%s"\nMYSQL_SOURCE_TABLES="xx:123456@tcp(xx:3306)/xx?charset=utf8mb4&parseTime=true&loc=Local"\nQUERY_TABLES_ADDRESS="querytables:8083"\nTRANFER_ADDRESS="transfer:8082"\nPUSH_ADDRESS="push:8080"\nGM_SERVICE_NAME_STOCK=%s\nSOURCE_GM_PORT_STOCK=10010\nGM_SERVICE_LOG_PATH_STOCK=%s\nSOURCE_GM_PROM_PORT_STOCK=9159\nGM_SEC_TYPES_STOCK=%s\nESUNNY_QUOTE_IP=%s\nESUNNY_QUOTE_PORT=%s\nESUNNY_QUOTE_AUTHCODE=%s\nESUNNY_QUOTE_USERNAME=%s\nESUNNY_QUOTE_PASSWD=%s\nESUNNY_TRADE_IP=%s\nESUNNY_TRADE_PORT=%s\nESUNNY_TRADE_AUTHCODE=%s\nESUNNY_TRADE_USERNAME=%s\nESUNNY_TRADE_PASSWD=%s\nESUNNY_COMMODITYNO=%s\nESUNNY_OPEN_TIME=%s\nESUNNY_SYNC_FLAG=%s\nESUNNY_QUOTE_COMMODITY=%s\nESUNNY_EXCHANGE_PREFIX_FLAG=%s\nESUNNY_LOG_PATH=%s\nESUNNY_QUOTE_FLAG=%s\nESUNNY_EXCHANGE_NO=%s\nTICK_SAVE_SYMBOLS="SZSE_000001,SHSE_600000"\nTHS_SERVICE_NAME="ths"\n\n\nFUTU_SERVICE_NAME="futu_source"\nSOURCE_FUTU_PORT="10014"\nFUTU_OPEN_TIME="09:00"\nFUTU_SYNC_FLAG=1\nFUTU_EXCHANGE_PREFIX_FLAG=1\nFUTU_QUOTE_SYMBOLS="00001,00002,00003,00004,00005,00006,00011,00012,00016,00017,00019,00023,00027,00066,00083,00101,00135"\nFUTU_ADDRESS="172.16.60.223"\nFUTU_PORT="11111"\nFUTU_SERVICE_LOG_PATH="logs/futu"\nSOURCE_FUTU_PROM_PORT="9159"\n\n\n\nSOURCE_THS_PORT=10013\nSOURCE_THS_PROM_PORT=9162\nTHS_ACCESS_TOKEN="xxx=.eyJ1aWQiOiI2MzMxNDMwNjQifQ==.CA9BA90A3C7259037D07FC3ACDADBD67F3BB61B2DE62DDC6630370F200FEB833"\nSTORAGE_SERVICE_NAME="quote_storage"\nSTORAGE_SERVICE_PORT=10001\nSTORAGE_LOG_PATH="logs/storage"\nEOF\n}\n\nbuild_deploy(){\n    docker-compose up -d\n}\n\ninit(){\n    docker-compose down -v\n    docker images | grep xxx/quote | grep %s | awk \'{print $1 ":" $2}\' | xargs docker rmi\n}\n\nmain(){\n    init\n\n    build_compose_yml\n\n    build_env\n\n    build_deploy\n}\n\nmain\n\'\'\'\n\ndef build_trade_deploy():\n    env_dist = os.environ\n    es_quote_addr = env_dist.get("ESUNNY_QUOTE_ADDR", "xxx")\n    q_addr = es_quote_addr.split(":")\n    q_ip = q_addr[0]\n    q_port = q_addr[1]\n    es_trade_addr = env_dist.get("ESUNNY_TRADE_ADDR", "xxx")\n    t_addr = es_trade_addr.split(":")\n    t_ip = t_addr[0]\n    t_port = t_addr[1]\n    # 替换字符串.\n    env1 = envScript % (env_dist["version_number"], env_dist.get("DATABASE_USER", "dev"), env_dist.get("DATABASE_PWD", "dev_pwd"), env_dist.get("DATABASE_HOST", "172.16.30.194:13306"), \n                       env_dist.get("REDIS_HOST", "172.16.30.193:16379"), env_dist.get("REDIS_PWD", "Gg@xx!!!"), \n                       env_dist.get("RABBITMQ_HOST", "172.16.30.193:15672"), \n                       env_dist.get("TRACING_URL", "http://172.16.30.196:14368/api/traces"), \n                       env_dist.get("MARKET_TYPE", "stock"),\n                       env_dist.get("STORAGE_MODE", "test"), env_dist.get("GM_TOKEN", "xx"), env_dist.get("GM_SERV_ADDR", "172.16.60.223:7001"),\n                       env_dist.get("GM_SERVICE_NAME_STOCK", "gm_stock"),\n                       env_dist.get("GM_SERVICE_LOG_PATH_STOCK", "logs/gm_stock"),\n                       env_dist.get("GM_SEC_TYPES_STOCK", "stock"),\n                       q_ip, q_port, \n                       env_dist.get("ESUNNY_QUOTE_AUTHCODE", "xx"), \n                       env_dist.get("ESUNNY_QUOTE_USERNAME", "ES"), \n                       env_dist.get("ESUNNY_QUOTE_PASSWD", "xx"), \n                       t_ip, t_port, \n                       env_dist.get("ESUNNY_TRADE_AUTHCODE", "xx"), \n                       env_dist.get("ESUNNY_TRADE_USERNAME", "Q576288330"), \n                       env_dist.get("ESUNNY_TRADE_PASSWD", "xx"), \n                       env_dist.get("ESUNNY_COMMODITYNO", "xx"), \n                       env_dist.get("ESUNNY_OPEN_TIME", "04:55"), \n                       env_dist.get("ESUNNY_SYNC_FLAG", "1"),\n                       env_dist.get("ESUNNY_QUOTE_COMMODITY", "HIS,NQ,BO,BP,CL,SV"),\n                       env_dist.get("ESUNNY_EXCHANGE_PREFIX_FLAG", "1"),\n                       env_dist.get("ESUNNY_LOG_PATH", "logs/tap"),\n                       env_dist.get("ESUNNY_QUOTE_FLAG", "0"),\n                       env_dist.get("ESUNNY_EXCHANGE_NO", "x"),\n                       env_dist["version_number"])\n\n    script = yml + env1\n\n    # 生成脚本文件.\n    fp = open("quote_deploy.sh", "w")\n    fp.write(script)\n    fp.close()\n\n\nif __name__ == "__main__":\n    build_trade_deploy()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n',normalizedContent:'#!/usr/bin/env python\n# coding=utf-8\nimport os\nimport sys\n\nappyml = \'\'\'#!/bin/bash\n\nymlname1="docker-compose.yml"\nbuild_compose_yml(){\ncat>"${ymlname1}"<<eof\nversion: "3"\nservices:\n  gateway:\n    image: xxx/ait0/gm_subscribe:\\${tag}\n    container_name: "gm_subscribe"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      mq_url: ${mq_url}\n      gm_address: ${gm_address}\n      gm_token: ${gm_token}\n      gm_symbols: ${gm_symbols}\n      use_fake: ${use_fake}\n    entrypoint: python3 ./gm_subscribe.py\n\nnetworks:\n  network:\n    driver: bridge\n\neof\n}\n\n\'\'\'\naitoolsappyml = \'\'\'#!/bin/bash\n\nymlname1="docker-compose.yml"\nbuild_compose_yml(){\ncat>"${ymlname1}"<<eof\nversion: "3"\nservices:\n  gateway:\n    image: xxx/ait0/python_aitools:%s\n    container_name: "python_aitools"\n    restart: always\n    volumes:\n      - ./config.json:/app/config/config.json\n    networks:\n      - network\n\nnetworks:\n  network:\n    driver: bridge\n\neof\n}\n\n\'\'\'\naitoolsenvscript = \'\'\'\nenvname1="config.json"\nbuild_env(){\ncat>"${envname1}"<<eof\n{\n  "mysql_param": {\n    "quote_db":{\n      "host": "172.16.30.194",\n      "port": 13307,\n      "user": "dev",\n      "passwd": "dev_pwd",\n      "db": "quote"\n    },\n    "sync_db":{\n      "host": "172.16.30.221",\n      "port": 13306,\n      "user": "root",\n      "passwd": "xxx",\n      "db": "quote_test"\n    },\n    "nano_db":{\n      "host": "xxx",\n      "port": 3306,\n      "user": "readonly",\n      "passwd": "xxxx",\n      "db": "marketdata"\n    },\n    "ait0_db":{\n      "host": "172.16.30.194",\n      "port": 13307,\n      "user": "dev",\n      "passwd": "dev_pwd",\n      "db": "ai_server"\n    }\n  },\n\n  "process_threads": 4,\n  "timer_param": {\n    "gm_stock_time": "15:30",\n    "gm_future_time": "16:30",\n    "nano_time": "09:30",\n    "code_time": "08:30",\n    "price_time": 5\n  },\n  "redis_param": {\n    "host": "172.16.30.194",\n    "port": 16379,\n    "password": "xxx"\n  },\n  "check_code_count" : [\n    {\n        "exchange": "szse",\n        "min_count": 2000\n    },\n    {\n        "exchange": "shse",\n        "min_count": 2000\n    }\n  ],\n  "check_price": [\n    {\n      "exchange": "szse",\n      "symbol": "000001"\n    },\n    {\n      "exchange": "shse",\n      "symbol": "600000"\n    }\n  ],\n  "other_param" : {\n    "ding_token": "xxx",\n    "quote_url": "http://172.16.30.194:8888/",\n    "quant_token":"xxxx",\n    "quant_addr": "127.0.0.1:7001"\n   }\n}\n\neof\n}\n\'\'\'\nenvscript = \'\'\'\nenvname1=".env"\nbuild_env(){\ncat>"${envname1}"<<eof\ntag=%s\nmq_url=%s\ngm_address=%s\ngm_token=%s\ngm_symbols=%s\nuse_fake=%s\neof\n}\n\n\'\'\'\n\naitoolsrunscript = \'\'\'\nbuild_deploy(){\n    docker-compose up -d\n}\n\ninit(){\n    docker-compose down -v\n    docker rmi xxx/ait0/python_aitools:%s\n}\n\nmain(){\n    init\n\n    build_compose_yml\n\n    build_env\n    build_deploy\n}\n\nmain\n\'\'\'\n\n\n\nrunscript = \'\'\'\nbuild_deploy(){\n    docker-compose up -d\n}\n\ninit(){\n    docker-compose down -v\n    docker rmi xxx/ait0/gm_subscribe:%s\n}\n\nmain(){\n    init\n\n    build_compose_yml\n\n    build_env\n    build_deploy\n}\n\nmain\n\'\'\'\n\n\ndef ait0_python_deploy():\n    env_dist = os.environ\n    mq_url = env_dist.get("mq_url", "amqp://guest:guest@172.16.50.24:15672/quote?heartbeat=0")\n    gm_address = env_dist.get("gm_address", "172.16.50.23:7001")\n    gm_token = env_dist.get("gm_token", "xxx")\n    gm_symbols = env_dist.get("gm_symbols", "szse.300890,szse.300409,shse.688700,shse.603738,shse.688689,shse.688668,szse.300428,szse.002011,szse.300416,szse.300065,szse.300447,szse.002897,szse.002837,szse.300648,szse.002518,shse.603912,szse.002567,shse.688556,shse.603396,shse.603477,shse.603596,szse.002906,szse.002466,shse.600563,szse.002484,szse.002812,szse.300274,szse.300316,shse.688598,shse.603806,shse.605117,shse.603606,szse.300850,szse.002531,szse.300327,szse.300633,szse.002335,shse.601636,szse.000959,szse.000932,shse.600782,shse.600985,shse.600188,shse.601088,shse.601600,szse.000807,shse.601899,shse.600362,szse.002714,szse.000977,shse.600845,szse.002271,shse.603208,szse.002572,szse.000400,szse.002028,szse.000733,shse.600765,szse.002179,szse.300394,szse.300504,shse.600702,shse.603198,shse.603589,shse.600559,shse.600197,shse.601800,shse.601669,shse.600426,shse.600309,shse.600230,szse.002241,szse.000672,shse.600585")\n    use_fake = env_dist.get("use_fake", "0")\n    version = env_dist["version_number"]\n\n    # 替换字符串.\n    yml = appyml\n    env1 = envscript % (version, mq_url, gm_address, gm_token, gm_symbols, use_fake)\n    run1 = runscript % (version)\n    script = yml + env1 + run1\n\n    # 生成脚本文件.\n    fp = open("ait0_python.sh", "w")\n    fp.write(script)\n    fp.close()\n\ndef aitools_python_deploy():\n    env_dist = os.environ\n    # mq_url = env_dist.get("mq_url", "amqp://guest:guest@172.16.50.24:15672/quote?heartbeat=0")\n    # gm_address = env_dist.get("gm_address", "172.16.50.23:7001")\n    # gm_token = env_dist.get("gm_token", "xxx")\n    # gm_symbols = env_dist.get("gm_symbols", "szse.300890,szse.300409,shse.688700,shse.603738,shse.688689,shse.688668,szse.300428,szse.002011,szse.300416,szse.300065,szse.300447,szse.002897,szse.002837,szse.300648,szse.002518,shse.603912,szse.002567,shse.688556,shse.603396,shse.603477,shse.603596,szse.002906,szse.002466,shse.600563,szse.002484,szse.002812,szse.300274,szse.300316,shse.688598,shse.603806,shse.605117,shse.603606,szse.300850,szse.002531,szse.300327,szse.300633,szse.002335,shse.601636,szse.000959,szse.000932,shse.600782,shse.600985,shse.600188,shse.601088,shse.601600,szse.000807,shse.601899,shse.600362,szse.002714,szse.000977,shse.600845,szse.002271,shse.603208,szse.002572,szse.000400,szse.002028,szse.000733,shse.600765,szse.002179,szse.300394,szse.300504,shse.600702,shse.603198,shse.603589,shse.600559,shse.600197,shse.601800,shse.601669,shse.600426,shse.600309,shse.600230,szse.002241,szse.000672,shse.600585")\n    # use_fake = env_dist.get("use_fake", "0")\n    version = env_dist["version_number"]\n\n    # 替换字符串.\n    yml = aitoolsappyml % (version)\n    env1 = aitoolsenvscript\n    run1 = aitoolsrunscript % (version)\n    script = yml + env1 + run1\n\n    # 生成脚本文件.\n    fp = open("aitools_python.sh", "w")\n    fp.write(script)\n    fp.close()\n\n\nif __name__ == "__main__":\n    param = sys.argv[1]\n    names = param.split(".")\n    n = names[len(names)-1]\n    if n == "ait0_python":\n        ait0_python_deploy()\n    elif n == "aitools":\n        aitools_python_deploy()\n    else:\n        print param\n\n\n#!/usr/bin/env python\n# coding=utf-8\n\nimport os\n\nyml = \'\'\'#!/bin/bash\n\nymlname="docker-compose.yml"\nbuild_compose_yml(){\ncat>"${ymlname}"<<eof\nversion: "3"\nservices:\n  gateway:\n    image: xxx/quote/gateway:\\${tag}\n    container_name: "gateway"\n    restart: always\n    ports:\n      - 8888:8888\n      - 39101:9101\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    depends_on:\n      - query\n    environment:\n      query_address: \\${query_address}\n      tracing_url: \\${tracing_url}\n      tracing_sampler: \\${tracing_sampler}\n      query_tables_address: \\${query_tables_address}\n  query:\n    image: xxx/quote/query:\\${tag}\n    container_name: "query"\n    restart: always\n    ports:\n      - 39102:9102\n      - 8081:8081\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      market_type: \\${market_type}\n      mysql_source: \\${mysql_source}\n      redis_address: \\${redis_address}\n      redis_pass: \\${redis_pass}\n      tracing_url: \\${tracing_url}\n      tracing_sampler: \\${tracing_sampler}\n\n  storage:\n    image: xxx/quote/storage:\\${tag}\n    container_name: "storage"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      mysql_source: \\${mysql_source}\n      storage_mode: \\${storage_mode}\n      redis_address: \\${redis_address}\n      redis_pass: \\${redis_pass}\n      rabbit_address: \\${rabbit_address}\n      proc_threads: \\${proc_threads}\n      ticks_write_flag: \\${ticks_write_flag}\n      tick_save_symbols: \\${tick_save_symbols}\n      storage_service_name: \\${storage_service_name}\n      storage_service_port: \\${storage_service_port}\n      storage_log_path: \\${storage_log_path}\n  probe:\n    image: xxx/probe/probe:v0.7.0\n    container_name: "probe"\n    restart: always\n    networks:\n      - network\n    ports:\n      - 15888:5888\n    command:\n      - -http=gateway:8888,storage:10001\n      - -grpc=query.rpc:query:8081\n  gm:\n    image: xxx/quote/gm:\\${tag}\n    container_name: "gm"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    ports:\n      - 9159:9159\n    networks:\n      - network\n    environment:\n      rabbit_address: \\${rabbit_address}\n      gm_open_time: \\${gm_open_time}\n      gm_sync_flag: \\${gm_sync_flag}\n      gm_exchange_prefix_flag: \\${gm_exchange_prefix_flag}\n      gm_quote_commodity: \\${gm_quote_commodity}\n      gm_exchange_no: \\${gm_exchange_no}\n      gm_token: \\${gm_token}\n      gm_strategy_id: \\${gm_strategy_id}\n      gm_serv_addr: \\${gm_serv_addr}\n      tracing_url: \\${tracing_url}\n      tracing_sampler: \\${tracing_sampler}\n      tranfer_address: \\${tranfer_address}\n      gm_service_name: \\${gm_service_name_stock}\n      source_gm_port: \\${source_gm_port_stock}\n      gm_service_log_path: \\${gm_service_log_path_stock}\n      source_gm_prom_port: \\${source_gm_prom_port_stock}\n      gm_sec_types: \\${gm_sec_types_stock}\n  querytables:\n    image: xxx/quote/querytables:\\${tag}\n    container_name: "querytables"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    ports:\n      - 9104:9104\n    networks:\n      - network\n    environment:\n      mysql_source_tables: \\${mysql_source_tables}\n      tracing_url: \\${tracing_url}\n      tracing_sampler: \\${tracing_sampler}\n  transfer:\n    image: xxx/quote/transfer:\\${tag}\n    container_name: "transfer"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    ports:\n      - 8082:8082\n      - 19103:9103\n    networks:\n      - network\n    environment:\n      rabbit_address: \\${rabbit_address}\n      tracing_url: \\${tracing_url}\n      tracing_sampler: \\${tracing_sampler}\n  push:\n    image: xxx/quote/push:\\${tag}\n    container_name: "push"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      rabbit_address: \\${rabbit_address}\n      tracing_url: \\${tracing_url}\n      tracing_sampler: \\${tracing_sampler}\n  websocketv1:\n    image: xxx/quote/websocketv1:\\${tag}\n    container_name: "websocketv1"\n    restart: always\n    ports:\n      - 8889:8889\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    depends_on:\n      - push\n    environment:\n      storage_mode: \\${storage_mode}\n      push_address: \\${push_address}\n      tracing_url: \\${tracing_url}\n      tracing_sampler: \\${tracing_sampler}\n  tap:\n    image: xxx/quote/tap:\\${tag}\n    container_name: "tap"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      esunny_quote_ip: \\${esunny_quote_ip}\n      esunny_quote_port: \\${esunny_quote_port}\n      esunny_quote_authcode: \\${esunny_quote_authcode}\n      esunny_quote_username: \\${esunny_quote_username}\n      esunny_quote_passwd: \\${esunny_quote_passwd}\n      esunny_quote_commodity: \\${esunny_quote_commodity}\n      esunny_trade_ip: \\${esunny_trade_ip}\n      esunny_trade_port: \\${esunny_trade_port}\n      esunny_trade_authcode: \\${esunny_trade_authcode}\n      esunny_trade_username: \\${esunny_trade_username}\n      esunny_trade_passwd: \\${esunny_trade_passwd}\n      esunny_commodityno: \\${esunny_commodityno}\n      esunny_open_time: \\${esunny_open_time}\n      esunny_sync_flag: \\${esunny_sync_flag}\n      esunny_exchange_prefix_flag: \\${esunny_exchange_prefix_flag}\n      esunny_log_path: \\${esunny_log_path}\n      esunny_quote_flag: \\${esunny_quote_flag}\n      esunny_exchange_no: \\${esunny_exchange_no}\n      rabbit_address: \\${rabbit_address}\n      tracing_url: \\${tracing_url}\n      tracing_sampler: \\${tracing_sampler}\n      tranfer_address: \\${tranfer_address}\n\n  ths:\n    image: xxx/quote/ths:\\${tag}\n    container_name: "ths"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    ports:\n      - 9162:9162\n    networks:\n      - network\n    environment:\n      ths_service_name: \\${ths_service_name}\n      source_ths_port: \\${source_ths_port}\n      source_ths_prom_port: \\${source_ths_prom_port}\n      rabbit_address: \\${rabbit_address}\n      tranfer_address: \\${tranfer_address}\n      tracing_url: \\${tracing_url}\n      tracing_sampler: \\${tracing_sampler}\n      ths_access_token: \\${ths_access_token}\n\n  futu:\n    image: xxx/quote/futu:\\${tag}\n    container_name: "futu"\n    restart: always\n    volumes:\n      - ./logs:/app/logs\n    networks:\n      - network\n    environment:\n      futu_service_name: \\${futu_service_name}\n      source_futu_port: \\${source_futu_port}\n      rabbit_address: \\${rabbit_address}\n      futu_open_time: \\${futu_open_time}\n      futu_sync_flag: \\${futu_sync_flag}\n      futu_exchange_prefix_flag: \\${futu_exchange_prefix_flag}\n      futu_quote_symbols: \\${futu_quote_symbols}\n      futu_address: \\${futu_address}\n      futu_port: \\${futu_port}\n      futu_service_name: \\${futu_service_name}\n      futu_service_log_path: \\${futu_service_log_path}\n      source_futu_prom_port: \\${source_futu_prom_port}\n      tracing_url: \\${tracing_url}\n      tracing_sampler: \\${tracing_sampler}\n      tranfer_address: \\${tranfer_address}\n\n\n\n\nnetworks:\n  network:\n    driver: bridge\neof\n}\n\n\'\'\'\n\nenvscript = \'\'\'\nenvname=".env"\nbuild_env(){\ncat>"${envname}"<<eof\ntag=%s\nmysql_source="%s:%s@tcp(%s)/quote?charset=utf8mb4&parsetime=true&loc=local"\nredis_address="%s"\nredis_pass="%s"\nrabbit_address="amqp://guest:guest@%s/quote"\nquery_address="query:8081"\ntracing_url="%s"\ntracing_sampler=0.01\nmarket_type=%s\nstorage_mode=%s\nproc_threads=8\ngta_mysql_source="xx:xx@tcp(xx:3306)/rcwalgoprd1?charset=utf8mb4&parsetime=true&loc=local"\ngta_redis_address="xx:7788"\ngta_redis_pass="xx"\nticks_write_flag=0\ngm_open_time="06:10"\ngm_sync_flag=1\ngm_exchange_prefix_flag=1\ngm_quote_commodity=""\ngm_exchange_no=""\ngm_token="%s"\ngm_strategy_id="xx"\ngm_serv_addr="%s"\nmysql_source_tables="xx:123456@tcp(xx:3306)/xx?charset=utf8mb4&parsetime=true&loc=local"\nquery_tables_address="querytables:8083"\ntranfer_address="transfer:8082"\npush_address="push:8080"\ngm_service_name_stock=%s\nsource_gm_port_stock=10010\ngm_service_log_path_stock=%s\nsource_gm_prom_port_stock=9159\ngm_sec_types_stock=%s\nesunny_quote_ip=%s\nesunny_quote_port=%s\nesunny_quote_authcode=%s\nesunny_quote_username=%s\nesunny_quote_passwd=%s\nesunny_trade_ip=%s\nesunny_trade_port=%s\nesunny_trade_authcode=%s\nesunny_trade_username=%s\nesunny_trade_passwd=%s\nesunny_commodityno=%s\nesunny_open_time=%s\nesunny_sync_flag=%s\nesunny_quote_commodity=%s\nesunny_exchange_prefix_flag=%s\nesunny_log_path=%s\nesunny_quote_flag=%s\nesunny_exchange_no=%s\ntick_save_symbols="szse_000001,shse_600000"\nths_service_name="ths"\n\n\nfutu_service_name="futu_source"\nsource_futu_port="10014"\nfutu_open_time="09:00"\nfutu_sync_flag=1\nfutu_exchange_prefix_flag=1\nfutu_quote_symbols="00001,00002,00003,00004,00005,00006,00011,00012,00016,00017,00019,00023,00027,00066,00083,00101,00135"\nfutu_address="172.16.60.223"\nfutu_port="11111"\nfutu_service_log_path="logs/futu"\nsource_futu_prom_port="9159"\n\n\n\nsource_ths_port=10013\nsource_ths_prom_port=9162\nths_access_token="xxx=.eyj1awqioii2mzmxndmwnjqifq==.ca9ba90a3c7259037d07fc3acdadbd67f3bb61b2de62ddc6630370f200feb833"\nstorage_service_name="quote_storage"\nstorage_service_port=10001\nstorage_log_path="logs/storage"\neof\n}\n\nbuild_deploy(){\n    docker-compose up -d\n}\n\ninit(){\n    docker-compose down -v\n    docker images | grep xxx/quote | grep %s | awk \'{print $1 ":" $2}\' | xargs docker rmi\n}\n\nmain(){\n    init\n\n    build_compose_yml\n\n    build_env\n\n    build_deploy\n}\n\nmain\n\'\'\'\n\ndef build_trade_deploy():\n    env_dist = os.environ\n    es_quote_addr = env_dist.get("esunny_quote_addr", "xxx")\n    q_addr = es_quote_addr.split(":")\n    q_ip = q_addr[0]\n    q_port = q_addr[1]\n    es_trade_addr = env_dist.get("esunny_trade_addr", "xxx")\n    t_addr = es_trade_addr.split(":")\n    t_ip = t_addr[0]\n    t_port = t_addr[1]\n    # 替换字符串.\n    env1 = envscript % (env_dist["version_number"], env_dist.get("database_user", "dev"), env_dist.get("database_pwd", "dev_pwd"), env_dist.get("database_host", "172.16.30.194:13306"), \n                       env_dist.get("redis_host", "172.16.30.193:16379"), env_dist.get("redis_pwd", "gg@xx!!!"), \n                       env_dist.get("rabbitmq_host", "172.16.30.193:15672"), \n                       env_dist.get("tracing_url", "http://172.16.30.196:14368/api/traces"), \n                       env_dist.get("market_type", "stock"),\n                       env_dist.get("storage_mode", "test"), env_dist.get("gm_token", "xx"), env_dist.get("gm_serv_addr", "172.16.60.223:7001"),\n                       env_dist.get("gm_service_name_stock", "gm_stock"),\n                       env_dist.get("gm_service_log_path_stock", "logs/gm_stock"),\n                       env_dist.get("gm_sec_types_stock", "stock"),\n                       q_ip, q_port, \n                       env_dist.get("esunny_quote_authcode", "xx"), \n                       env_dist.get("esunny_quote_username", "es"), \n                       env_dist.get("esunny_quote_passwd", "xx"), \n                       t_ip, t_port, \n                       env_dist.get("esunny_trade_authcode", "xx"), \n                       env_dist.get("esunny_trade_username", "q576288330"), \n                       env_dist.get("esunny_trade_passwd", "xx"), \n                       env_dist.get("esunny_commodityno", "xx"), \n                       env_dist.get("esunny_open_time", "04:55"), \n                       env_dist.get("esunny_sync_flag", "1"),\n                       env_dist.get("esunny_quote_commodity", "his,nq,bo,bp,cl,sv"),\n                       env_dist.get("esunny_exchange_prefix_flag", "1"),\n                       env_dist.get("esunny_log_path", "logs/tap"),\n                       env_dist.get("esunny_quote_flag", "0"),\n                       env_dist.get("esunny_exchange_no", "x"),\n                       env_dist["version_number"])\n\n    script = yml + env1\n\n    # 生成脚本文件.\n    fp = open("quote_deploy.sh", "w")\n    fp.write(script)\n    fp.close()\n\n\nif __name__ == "__main__":\n    build_trade_deploy()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n',charsets:{cjk:!0}},{title:"python将多个文件内容输出到一个文件中",frontmatter:{title:"python将多个文件内容输出到一个文件中",date:"2023-01-09T17:43:27.000Z",permalink:"/pages/8d0b76/",categories:["编程","python"],tags:[null],readingShow:"top",description:"生成多个文件脚本",meta:[{name:"twitter:title",content:"python将多个文件内容输出到一个文件中"},{name:"twitter:description",content:"生成多个文件脚本"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/06.python%E5%B0%86%E5%A4%9A%E4%B8%AA%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E8%BE%93%E5%87%BA%E5%88%B0%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E4%B8%AD.html"},{property:"og:type",content:"article"},{property:"og:title",content:"python将多个文件内容输出到一个文件中"},{property:"og:description",content:"生成多个文件脚本"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/01.python/06.python%E5%B0%86%E5%A4%9A%E4%B8%AA%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E8%BE%93%E5%87%BA%E5%88%B0%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E4%B8%AD.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-09T17:43:27.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"python将多个文件内容输出到一个文件中"},{itemprop:"description",content:"生成多个文件脚本"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/01.python/06.python%E5%B0%86%E5%A4%9A%E4%B8%AA%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E8%BE%93%E5%87%BA%E5%88%B0%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E4%B8%AD.html",relativePath:"03.编程/01.python/06.python将多个文件内容输出到一个文件中.md",key:"v-2e07d1a0",path:"/pages/8d0b76/",headersStr:null,content:"生成多个文件脚本\n\n#coding=utf-8\n#import os\n#import sys\n\nsql1Script = '''\nuse scrm_%s;\n-- 公司code需替换为相应公司的code\nCREATE OR REPLACE VIEW `scrm_crm_contract` AS SELECT * FROM scrm_jishufuwu.`scrm_crm_contract` WHERE `company_code` = '%s';\n\n-- 更新数据库版本.\nINSERT INTO gf_db_version (MAIN_VERSION, DB_VERSION, SQL_NAME) VALUES ('R0028.000', 'R0028.000.000', 'R0028.000.000.0001.company.sql');\n\n'''\n\n\n\ndef init_sql_execute():\n    db_name=[\"Hitech\", \"Ztltech\", \"Bslm\", \"Yn\"]\n    # 替换字符串.\n    for item in db_name:\n        sql1 = sql1Script % (item, item)\n        script = sql1\n\n    # 生成脚本文件.\n        fp = open(\"saas_sql_\"+item+\".sh\", \"w\")\n        fp.write(script)\n        fp.close()\nif __name__ == \"__main__\":\n    init_sql_execute()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n将多个文件输出到一个文件中\n\n#!/usr/bin/python\n#encoding:utf-8\nimport os\n# 目标文件夹的路径\nfiledir = r'/data/test'\n#获取目标文件的文件名称列表  \nfilenames=os.listdir(filedir)\nf=open(r'/data/test/aa.sh', 'w+')\n\nfor filename in filenames:\n    filepath = filedir + '/' + filename\n    for line in open(filepath):\n        f.writelines(line)\n    f.write('\\n')\nf.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n",normalizedContent:"生成多个文件脚本\n\n#coding=utf-8\n#import os\n#import sys\n\nsql1script = '''\nuse scrm_%s;\n-- 公司code需替换为相应公司的code\ncreate or replace view `scrm_crm_contract` as select * from scrm_jishufuwu.`scrm_crm_contract` where `company_code` = '%s';\n\n-- 更新数据库版本.\ninsert into gf_db_version (main_version, db_version, sql_name) values ('r0028.000', 'r0028.000.000', 'r0028.000.000.0001.company.sql');\n\n'''\n\n\n\ndef init_sql_execute():\n    db_name=[\"hitech\", \"ztltech\", \"bslm\", \"yn\"]\n    # 替换字符串.\n    for item in db_name:\n        sql1 = sql1script % (item, item)\n        script = sql1\n\n    # 生成脚本文件.\n        fp = open(\"saas_sql_\"+item+\".sh\", \"w\")\n        fp.write(script)\n        fp.close()\nif __name__ == \"__main__\":\n    init_sql_execute()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n将多个文件输出到一个文件中\n\n#!/usr/bin/python\n#encoding:utf-8\nimport os\n# 目标文件夹的路径\nfiledir = r'/data/test'\n#获取目标文件的文件名称列表  \nfilenames=os.listdir(filedir)\nf=open(r'/data/test/aa.sh', 'w+')\n\nfor filename in filenames:\n    filepath = filedir + '/' + filename\n    for line in open(filepath):\n        f.writelines(line)\n    f.write('\\n')\nf.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n",charsets:{cjk:!0}},{title:"进程pid判断脚本",frontmatter:{title:"进程pid判断脚本",date:"2022-12-14T14:30:33.000Z",permalink:"/pages/cae02d/",categories:["编程","shell"],tags:[null],readingShow:"top",description:"业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；",meta:[{name:"twitter:title",content:"进程pid判断脚本"},{name:"twitter:description",content:"业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/01.%E8%BF%9B%E7%A8%8Bpid%E5%88%A4%E6%96%AD%E8%84%9A%E6%9C%AC.html"},{property:"og:type",content:"article"},{property:"og:title",content:"进程pid判断脚本"},{property:"og:description",content:"业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/01.%E8%BF%9B%E7%A8%8Bpid%E5%88%A4%E6%96%AD%E8%84%9A%E6%9C%AC.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-14T14:30:33.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"进程pid判断脚本"},{itemprop:"description",content:"业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/02.shell/01.%E8%BF%9B%E7%A8%8Bpid%E5%88%A4%E6%96%AD%E8%84%9A%E6%9C%AC.html",relativePath:"03.编程/02.shell/01.进程pid判断脚本.md",key:"v-6c2001b3",path:"/pages/cae02d/",headersStr:null,content:"# zabbix监控进程变动\n\n> 业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；\n\n在每台服务器/etc/zabbix/zabbix_agentd.conf设置路径：此例只需要piddiff.sh\n\nUserParameter=checkpid,sh /usr/local/script/piddiff.sh\n\n/etc/zabbix/sh下面存放脚本\n\nstorage为业务监控的id取值根据业务需求\n\n#!/bin/bash\nonl_ok=1\nonl_cored=3\ndir=/etc/zabbix/sh\nif [[ ! -f \"$dir/old.txt\" ]];then\n    ps aux | grep storage | grep -v grep |grep -v probe | grep root | awk '{print $2,$11}' > $dir/old.txt\nelse\n    sleep 1s\nfi\n    ps aux | grep storage | grep -v grep |grep -v probe | grep root | awk '{print $2,$11}' > $dir/now.txt\nif ! diff -q $dir/old.txt $dir/now.txt > /dev/null;then\n    echo $onl_cored\n    diff -c $dir/old.txt $dir/now.txt > $dir/`date \"+%Y%m%d%H%M\"`_diff.txt\n    cat $dir/now.txt > $dir/old.txt\nelse\n    echo $onl_ok\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n",normalizedContent:"# zabbix监控进程变动\n\n> 业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；\n\n在每台服务器/etc/zabbix/zabbix_agentd.conf设置路径：此例只需要piddiff.sh\n\nuserparameter=checkpid,sh /usr/local/script/piddiff.sh\n\n/etc/zabbix/sh下面存放脚本\n\nstorage为业务监控的id取值根据业务需求\n\n#!/bin/bash\nonl_ok=1\nonl_cored=3\ndir=/etc/zabbix/sh\nif [[ ! -f \"$dir/old.txt\" ]];then\n    ps aux | grep storage | grep -v grep |grep -v probe | grep root | awk '{print $2,$11}' > $dir/old.txt\nelse\n    sleep 1s\nfi\n    ps aux | grep storage | grep -v grep |grep -v probe | grep root | awk '{print $2,$11}' > $dir/now.txt\nif ! diff -q $dir/old.txt $dir/now.txt > /dev/null;then\n    echo $onl_cored\n    diff -c $dir/old.txt $dir/now.txt > $dir/`date \"+%y%m%d%h%m\"`_diff.txt\n    cat $dir/now.txt > $dir/old.txt\nelse\n    echo $onl_ok\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n",charsets:{cjk:!0}},{title:"日志切割脚本",frontmatter:{title:"日志切割脚本",date:"2022-12-15T12:32:31.000Z",permalink:"/pages/3b937e/",categories:["编程","shell"],tags:[null],readingShow:"top",description:"脚本如下",meta:[{name:"twitter:title",content:"日志切割脚本"},{name:"twitter:description",content:"脚本如下"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/02.%E6%97%A5%E5%BF%97%E5%88%87%E5%89%B2%E8%84%9A%E6%9C%AC.html"},{property:"og:type",content:"article"},{property:"og:title",content:"日志切割脚本"},{property:"og:description",content:"脚本如下"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/02.%E6%97%A5%E5%BF%97%E5%88%87%E5%89%B2%E8%84%9A%E6%9C%AC.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T12:32:31.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"日志切割脚本"},{itemprop:"description",content:"脚本如下"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/02.shell/02.%E6%97%A5%E5%BF%97%E5%88%87%E5%89%B2%E8%84%9A%E6%9C%AC.html",relativePath:"03.编程/02.shell/02.日志切割脚本.md",key:"v-6ba31c1d",path:"/pages/3b937e/",headersStr:null,content:'# 日志切割脚本\n\n脚本如下\n\n注：安装zip命令\n\n#!/bin/bash\nloglist=`ls -l /data/applications/*.log |awk -F \'/\' \'{print $NF}\'`\nlogdate=`date "+%Y%m%d"`\nfor logname in $loglist\n  do\n  zip -r /data/applications/logs/"$logname"-"$logdate".zip /data/applications/$logname\n  echo "" > /data/applications/$logname\ndone\nfind /data/applications/logs/* -mtime +60 -exec rm {} \\;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n',normalizedContent:'# 日志切割脚本\n\n脚本如下\n\n注：安装zip命令\n\n#!/bin/bash\nloglist=`ls -l /data/applications/*.log |awk -f \'/\' \'{print $nf}\'`\nlogdate=`date "+%y%m%d"`\nfor logname in $loglist\n  do\n  zip -r /data/applications/logs/"$logname"-"$logdate".zip /data/applications/$logname\n  echo "" > /data/applications/$logname\ndone\nfind /data/applications/logs/* -mtime +60 -exec rm {} \\;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n',charsets:{cjk:!0}},{title:"设置跳板机脚本",frontmatter:{title:"设置跳板机脚本",date:"2022-12-15T12:41:13.000Z",permalink:"/pages/69b699/",categories:["编程","shell"],tags:[null],readingShow:"top",description:"发送公钥到对应的机器",meta:[{name:"twitter:title",content:"设置跳板机脚本"},{name:"twitter:description",content:"发送公钥到对应的机器"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/03.%E8%AE%BE%E7%BD%AE%E8%B7%B3%E6%9D%BF%E6%9C%BA%E8%84%9A%E6%9C%AC.html"},{property:"og:type",content:"article"},{property:"og:title",content:"设置跳板机脚本"},{property:"og:description",content:"发送公钥到对应的机器"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/03.%E8%AE%BE%E7%BD%AE%E8%B7%B3%E6%9D%BF%E6%9C%BA%E8%84%9A%E6%9C%AC.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T12:41:13.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"设置跳板机脚本"},{itemprop:"description",content:"发送公钥到对应的机器"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/02.shell/03.%E8%AE%BE%E7%BD%AE%E8%B7%B3%E6%9D%BF%E6%9C%BA%E8%84%9A%E6%9C%AC.html",relativePath:"03.编程/02.shell/03.设置跳板机脚本.md",key:"v-a840ca1e",path:"/pages/69b699/",headers:[{level:2,title:"脚本如下",slug:"脚本如下",normalizedTitle:"脚本如下",charIndex:2}],headersStr:"脚本如下",content:"# 脚本如下\n\n发送公钥到对应的机器\n\n目录下创建ip\n\n192.168.100.32 | medicalinscore | ecsProd-201707141750\n192.168.100.33 | medicalinscore | ecsProd-201707141751\n192.168.100.197 | medicalinscore | ecsProd-medical-14-1\n192.168.100.195 | medicalinscore | ecsProd-medical-14-2\n192.168.100.226 | medicalinscore | ecsProd-medicalinscore-5\n192.168.100.227 | medicalinscore | ecsProd-medicalinscore-6\n192.168.100.225 | medicalinscore | ecsProd-medicalinscore-7\n192.168.100.224 | medicalinscore | ecsProd-medicalinscore-8\n\n192.168.100.31 | medicalinsprod | ecsProd-201707141752\n192.168.100.30 | medicalinsprod | ecsProd-201707141753\n192.168.100.196 | medicalinsprod | ecsProd-medical-14-3\n192.168.100.198 | medicalinsprod | ecsProd-medical-14-4\n192.168.100.229 | medicalinsprod | ecsProd-medicalinsprod-5\n192.168.100.228 | medicalinsprod | ecsProd-medicalinsprod-6\n\n192.168.100.75 | medicalinsmng | ecsProd-medicalinsmng-01\n192.168.100.74 | medicalinsmng | ecsProd-medicalinsmng-02\n192.168.100.208 | medicalinsmng-boot | ecsProd-medicalinsmng-boot-1\n192.168.100.209 | medicalinsmng-boot | ecsProd-medicalinsmng-boot-2\n\n\n设置跳板机脚本\n\n#!/bin/bash\necho \"develop environment\"\nfor i in `seq  $(cat ip | wc -l )`\ndo read line\necho $i\"|\" $line\ndone <ip\necho \"please input the number for you choice machine\"\n\nread a\nhostname=`sed -n ''$a'p' ip |awk '{ print $2 }'`\nip=`sed -n ''$a'p' ip |awk '{ print $1 }'`\necho \"you while login this machine\"${hostname}\nssh -l log $ip\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n",normalizedContent:"# 脚本如下\n\n发送公钥到对应的机器\n\n目录下创建ip\n\n192.168.100.32 | medicalinscore | ecsprod-201707141750\n192.168.100.33 | medicalinscore | ecsprod-201707141751\n192.168.100.197 | medicalinscore | ecsprod-medical-14-1\n192.168.100.195 | medicalinscore | ecsprod-medical-14-2\n192.168.100.226 | medicalinscore | ecsprod-medicalinscore-5\n192.168.100.227 | medicalinscore | ecsprod-medicalinscore-6\n192.168.100.225 | medicalinscore | ecsprod-medicalinscore-7\n192.168.100.224 | medicalinscore | ecsprod-medicalinscore-8\n\n192.168.100.31 | medicalinsprod | ecsprod-201707141752\n192.168.100.30 | medicalinsprod | ecsprod-201707141753\n192.168.100.196 | medicalinsprod | ecsprod-medical-14-3\n192.168.100.198 | medicalinsprod | ecsprod-medical-14-4\n192.168.100.229 | medicalinsprod | ecsprod-medicalinsprod-5\n192.168.100.228 | medicalinsprod | ecsprod-medicalinsprod-6\n\n192.168.100.75 | medicalinsmng | ecsprod-medicalinsmng-01\n192.168.100.74 | medicalinsmng | ecsprod-medicalinsmng-02\n192.168.100.208 | medicalinsmng-boot | ecsprod-medicalinsmng-boot-1\n192.168.100.209 | medicalinsmng-boot | ecsprod-medicalinsmng-boot-2\n\n\n设置跳板机脚本\n\n#!/bin/bash\necho \"develop environment\"\nfor i in `seq  $(cat ip | wc -l )`\ndo read line\necho $i\"|\" $line\ndone <ip\necho \"please input the number for you choice machine\"\n\nread a\nhostname=`sed -n ''$a'p' ip |awk '{ print $2 }'`\nip=`sed -n ''$a'p' ip |awk '{ print $1 }'`\necho \"you while login this machine\"${hostname}\nssh -l log $ip\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n",charsets:{cjk:!0}},{title:"编写启动、停止、重启的脚本",frontmatter:{title:"编写启动、停止、重启的脚本",date:"2022-12-21T18:07:20.000Z",permalink:"/pages/fe0782/",categories:["编程","shell"],tags:[null],readingShow:"top",description:'PROFTPD="/usr/local/proftpd/sbin/proftpd"\nPROCONF="/usr/local/proftpd/etc/proftpd.conf"\nPROPID="/usr/local/proftpd/var/proftpd.pid"\nRETVAL=0\nprog="ProFTPd"',meta:[{name:"twitter:title",content:"编写启动、停止、重启的脚本"},{name:"twitter:description",content:'PROFTPD="/usr/local/proftpd/sbin/proftpd"\nPROCONF="/usr/local/proftpd/etc/proftpd.conf"\nPROPID="/usr/local/proftpd/var/proftpd.pid"\nRETVAL=0\nprog="ProFTPd"'},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/04.%E7%BC%96%E5%86%99%E5%90%AF%E5%8A%A8%E3%80%81%E5%81%9C%E6%AD%A2%E3%80%81%E9%87%8D%E5%90%AF%E7%9A%84%E8%84%9A%E6%9C%AC.html"},{property:"og:type",content:"article"},{property:"og:title",content:"编写启动、停止、重启的脚本"},{property:"og:description",content:'PROFTPD="/usr/local/proftpd/sbin/proftpd"\nPROCONF="/usr/local/proftpd/etc/proftpd.conf"\nPROPID="/usr/local/proftpd/var/proftpd.pid"\nRETVAL=0\nprog="ProFTPd"'},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/04.%E7%BC%96%E5%86%99%E5%90%AF%E5%8A%A8%E3%80%81%E5%81%9C%E6%AD%A2%E3%80%81%E9%87%8D%E5%90%AF%E7%9A%84%E8%84%9A%E6%9C%AC.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-21T18:07:20.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"编写启动、停止、重启的脚本"},{itemprop:"description",content:'PROFTPD="/usr/local/proftpd/sbin/proftpd"\nPROCONF="/usr/local/proftpd/etc/proftpd.conf"\nPROPID="/usr/local/proftpd/var/proftpd.pid"\nRETVAL=0\nprog="ProFTPd"'}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/02.shell/04.%E7%BC%96%E5%86%99%E5%90%AF%E5%8A%A8%E3%80%81%E5%81%9C%E6%AD%A2%E3%80%81%E9%87%8D%E5%90%AF%E7%9A%84%E8%84%9A%E6%9C%AC.html",relativePath:"03.编程/02.shell/04.编写启动、停止、重启的脚本.md",key:"v-8d36e636",path:"/pages/fe0782/",headersStr:null,content:'#!/bin/bash    \n# ProFTPd Settings \nPROFTPD="/usr/local/proftpd/sbin/proftpd" \nPROCONF="/usr/local/proftpd/etc/proftpd.conf" \nPROPID="/usr/local/proftpd/var/proftpd.pid" \nRETVAL=0 \nprog="ProFTPd" \n   \nstart() { \n    echo -n $"Starting $prog... " \n    $PROFTPD -c $PROCONF \n    if [ "$?" = 0 ] ; then \n        echo " done" \n    else \n        echo " failed" \n    fi \n} \n   \nstop() { \n    echo -n $"Stopping $prog...  " \n    if [ ! -e $PROPID ]; then \n        echo -n $"$prog is not running." \n        exit 1 \n    fi \n    kill `cat $PROPID` \n    if [ "$?" = 0 ] ; then \n        echo " done" \n    else \n        echo " failed" \n    fi \n} \n   \nrestart(){ \n    echo $"Restarting $prog..." \n    $0 stop \n    sleep 2 \n    $0 start \n} \n   \nstatus(){ \n    if [ -e $PROPID ]; then \n        echo $"$prog is running." \n    else \n        echo $"$prog is not running." \n    fi \n} \n   \ncase "$1" in \n    start) \n        start \n        ;; \n    stop) \n        stop \n        ;; \n    restart) \n        restart \n        ;; \n    status) \n        status \n        ;; \n  *) \n        echo $"Usage: $0 {start|stop|restart|status}" \nesac \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n',normalizedContent:'#!/bin/bash    \n# proftpd settings \nproftpd="/usr/local/proftpd/sbin/proftpd" \nproconf="/usr/local/proftpd/etc/proftpd.conf" \npropid="/usr/local/proftpd/var/proftpd.pid" \nretval=0 \nprog="proftpd" \n   \nstart() { \n    echo -n $"starting $prog... " \n    $proftpd -c $proconf \n    if [ "$?" = 0 ] ; then \n        echo " done" \n    else \n        echo " failed" \n    fi \n} \n   \nstop() { \n    echo -n $"stopping $prog...  " \n    if [ ! -e $propid ]; then \n        echo -n $"$prog is not running." \n        exit 1 \n    fi \n    kill `cat $propid` \n    if [ "$?" = 0 ] ; then \n        echo " done" \n    else \n        echo " failed" \n    fi \n} \n   \nrestart(){ \n    echo $"restarting $prog..." \n    $0 stop \n    sleep 2 \n    $0 start \n} \n   \nstatus(){ \n    if [ -e $propid ]; then \n        echo $"$prog is running." \n    else \n        echo $"$prog is not running." \n    fi \n} \n   \ncase "$1" in \n    start) \n        start \n        ;; \n    stop) \n        stop \n        ;; \n    restart) \n        restart \n        ;; \n    status) \n        status \n        ;; \n  *) \n        echo $"usage: $0 {start|stop|restart|status}" \nesac \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n',charsets:{cjk:!0}},{title:"mysql数据库备份的三种方式",frontmatter:{title:"mysql数据库备份的三种方式",date:"2023-01-04T14:22:29.000Z",permalink:"/pages/93dcc7/",categories:["编程","shell"],tags:[null],readingShow:"top",description:"Mysqldump是mysql自带工具。备份出来的文件是一个可以直接倒入的sql脚本。该sql文件中实际上包含了多个CREATE 和INSERT语句，使用这些语句可以重新创建表和插入数据。",meta:[{name:"twitter:title",content:"mysql数据库备份的三种方式"},{name:"twitter:description",content:"Mysqldump是mysql自带工具。备份出来的文件是一个可以直接倒入的sql脚本。该sql文件中实际上包含了多个CREATE 和INSERT语句，使用这些语句可以重新创建表和插入数据。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/05.mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F.html"},{property:"og:type",content:"article"},{property:"og:title",content:"mysql数据库备份的三种方式"},{property:"og:description",content:"Mysqldump是mysql自带工具。备份出来的文件是一个可以直接倒入的sql脚本。该sql文件中实际上包含了多个CREATE 和INSERT语句，使用这些语句可以重新创建表和插入数据。"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/05.mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-04T14:22:29.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"mysql数据库备份的三种方式"},{itemprop:"description",content:"Mysqldump是mysql自带工具。备份出来的文件是一个可以直接倒入的sql脚本。该sql文件中实际上包含了多个CREATE 和INSERT语句，使用这些语句可以重新创建表和插入数据。"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/02.shell/05.mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F.html",relativePath:"03.编程/02.shell/05.mysql数据库备份的三种方式.md",key:"v-08986d40",path:"/pages/93dcc7/",headersStr:null,content:' 1. mysqldump备份\n\n> Mysqldump是mysql自带工具。备份出来的文件是一个可以直接倒入的sql脚本。该sql文件中实际上包含了多个CREATE 和INSERT语句，使用这些语句可以重新创建表和插入数据。\n\n#!/bin/bash\naiserver_bak()\n{\nuser="root"\npasswd="xxxx"\ndb_name="ai_server"\nbackup_path="/data/mysql-backup/ai_server"\ndate=$(date +"%Y%m%d%H%M%S")\numask 177\n/usr/bin/mysqldump -u${user} -p${passwd} ${db_name} |gzip > ${backup_path}/${db_name}_mysql_${date}.sql.gz\nfind ${backup_path} -name "mysql_*.sql.gz" -type f -mtime +10 -exec rm -rf {} \\;\n}\naiserver_bak\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n 2. mysqlbackup备份\n\n> Mysqlbackup是物理备份的方式。对innodb的表空间进行物理复制，但是，它是记录LSN点的。在备份过程中，新增加的输入直接写入备份文件的ibbackup_logfile中。同时记录最后的LSN点。还原的时候，检测对比ibbackup_logfile文件里面与表空间里面的差值，使ibbackup_logfile里面的数据进入事务日志或表空间。\n\n#!/bin/bash\n#热备，备份数据库目录\nBACKUP_DIR=/var/backups/mysql/backups\nBACKUP_PASS=xxx\nBACKUP_USER=root\n\nDATE_DAY=$(date +"%Y-%m-%d")\nDATE_HOUR=$(date +"%H")\n\n#EMAIL_RECIPIENT=dba_zhang@163.com\n\n#/usr/local/mysql/bin/mysqlbackup --port=3306 --protocol=tcp --user=$BACKUP_USER --password=$BACKUP_PASS --with-timestamp --backup-dir=$BACKUP_DIR backup-and-apply-log\n/var/lib/mysql/meb/meb-4.1.3-linux-glibc2.12-x86-64bit/bin/mysqlbackup --port=3306 --protocol=tcp --user=$BACKUP_USER --password=$BACKUP_PASS --with-timestamp --backup-dir=$BACKUP_DIR --compress --compress-level=9 backup-and-apply-log\n\nsleep 100s\n\ntar -zcvf ${BACKUP_DIR}/meb-bk${DATE_DAY}${DATE_HOUR}.tar.gz ${BACKUP_DIR}/${DATE_DAY}*\n\nsleep 100s\n\nrm -rf ${BACKUP_DIR}/${DATE_DAY}*\n\nNO_OF_COMPLETE_OK_MESSAGES=$(cat $BACKUP_DIR/${DATE_DAY}_${DATE_HOUR}*/meta/MEB_${DATE_DAY}.${DATE_HOUR}*.log | grep "mysqlbackup completed OK" | wc -l)\n\n# Note that the string "mysqlbackup completed OK" must occur 2 times in the log in order for the backup to be OK\nif [ $NO_OF_COMPLETE_OK_MESSAGES -eq 2 ]; then\n        # Backup successful, find backup directory\n        echo "Backup succeeded"\n        exit 0\n#else\n#        echo "MySQL backup failed, please check logfile" | mail -s "ERROR: MySQL Backup Failed!" ${EMAIL_RECIPIENT}\n#        exit 1\nfi\nfind $BACKUP_DIR -mtime +17 -exec rm -f {} \\;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n 3. innobackupex备份\n\n> Percona XtraBackup是一款基于MySQL的热备份的开源实用程序，它可以备份5.1到5.7版本上InnoDB,XtraDB,MyISAM存储引擎的表， Xtrabackup有两个主要的工具：xtrabackup、innobackupex 。\n> （1）xtrabackup只能备份InnoDB和XtraDB两种数据表，而不能备份MyISAM数据表 　　\n> （2）innobackupex则封装了xtrabackup，是一个脚本封装，所以能同时备份处理innodb和myisam，但在处理myisam时需要加一个读锁。\n\n#!/bin/bash\n\n#### -- default value -- ####\n# mysql参数\nMYSQL_CNF=/etc/my.cnf\nMYSQL_USER="mysqlbackup"\nMYSQL_PASS="xxx"\nMYSQL_HOST=\'localhost\'\n\n# 备份参数\nlogtime=`date +%x\' \'%T`\nfullbase=/data/mysql_master_backup/full\nlogbase=/data/mysql_master_backup/logs\n#DBlist=`mysql -u${MYSQL_USER} -p${MYSQL_PASS} -e "show databases" | egrep -v $exclude`\ninnobackupex=/usr/bin/innobackupex\n\n#### -- 目录创建 -- ####\n[ -d ${fullbase} ] || mkdir -p ${fullbase}\n[ -d ${logbase} ] || mkdir -p ${logbase}\n\n\n####备份时间、大小记录\n\nlog_file="/data/mysql_master_backup/logs/backup_record_"$(date +%Y%m%d%H%M%S)".log"\n\n#### -- main funcation -- ####\n# 全备\n_FULLBackup()\n{\ndtime=`date +%Y-%m-%d`\n$innobackupex --defaults-file=$MYSQL_CNF --user=$MYSQL_USER --password=$MYSQL_PASS --host=$MYSQL_HOST $fullbase > $logbase/${dtime}.log 2>&1\nresult=$?\nif [ $result -eq 0 ];then\n        bkbase=`ls -d ${fullbase}/${dtime}* | sort -r | head -n 1`\n        bksize=`du -sh ${bkbase} | awk \'{print $1}\'`\n        echo -e "[$logtime]\\tStatus: successful, Backdir: $bkbase, size: $bksize" | tee -a  ${logbase}/full.log\nelse\n        echo -e "[$logtime]\\tStatus: failed, Reason: check you log $logbase/${dtime}.log" | tee -a  ${logbase}/backup.log\nfi\n}\n\n# run\nbackup_timer_start=`date "+%Y-%m-%d %H:%M:%S"`\necho "backup_timer_start   :    $backup_timer_start" >>$log_file\n_FULLBackup\nbackup_timer_end=`date "+%Y-%m-%d %H:%M:%S"`\necho "backup_timer_end     :    $backup_timer_end" >>$log_file\nbackup_duration=`echo $(($(date +%s -d "${backup_timer_end}") - $(date +%s -d "${backup_timer_start}"))) | awk \'{t=split("60 s 60 m 24 h 999 d",a);for(n=1;n<t;n+=2){if($1==0)break;s=$1%a[n]a[n+1] s;$1=int($1/a[n])}print s}\'`\necho "total backup time    :    $backup_duration"  >>$log_file\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n',normalizedContent:' 1. mysqldump备份\n\n> mysqldump是mysql自带工具。备份出来的文件是一个可以直接倒入的sql脚本。该sql文件中实际上包含了多个create 和insert语句，使用这些语句可以重新创建表和插入数据。\n\n#!/bin/bash\naiserver_bak()\n{\nuser="root"\npasswd="xxxx"\ndb_name="ai_server"\nbackup_path="/data/mysql-backup/ai_server"\ndate=$(date +"%y%m%d%h%m%s")\numask 177\n/usr/bin/mysqldump -u${user} -p${passwd} ${db_name} |gzip > ${backup_path}/${db_name}_mysql_${date}.sql.gz\nfind ${backup_path} -name "mysql_*.sql.gz" -type f -mtime +10 -exec rm -rf {} \\;\n}\naiserver_bak\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n 2. mysqlbackup备份\n\n> mysqlbackup是物理备份的方式。对innodb的表空间进行物理复制，但是，它是记录lsn点的。在备份过程中，新增加的输入直接写入备份文件的ibbackup_logfile中。同时记录最后的lsn点。还原的时候，检测对比ibbackup_logfile文件里面与表空间里面的差值，使ibbackup_logfile里面的数据进入事务日志或表空间。\n\n#!/bin/bash\n#热备，备份数据库目录\nbackup_dir=/var/backups/mysql/backups\nbackup_pass=xxx\nbackup_user=root\n\ndate_day=$(date +"%y-%m-%d")\ndate_hour=$(date +"%h")\n\n#email_recipient=dba_zhang@163.com\n\n#/usr/local/mysql/bin/mysqlbackup --port=3306 --protocol=tcp --user=$backup_user --password=$backup_pass --with-timestamp --backup-dir=$backup_dir backup-and-apply-log\n/var/lib/mysql/meb/meb-4.1.3-linux-glibc2.12-x86-64bit/bin/mysqlbackup --port=3306 --protocol=tcp --user=$backup_user --password=$backup_pass --with-timestamp --backup-dir=$backup_dir --compress --compress-level=9 backup-and-apply-log\n\nsleep 100s\n\ntar -zcvf ${backup_dir}/meb-bk${date_day}${date_hour}.tar.gz ${backup_dir}/${date_day}*\n\nsleep 100s\n\nrm -rf ${backup_dir}/${date_day}*\n\nno_of_complete_ok_messages=$(cat $backup_dir/${date_day}_${date_hour}*/meta/meb_${date_day}.${date_hour}*.log | grep "mysqlbackup completed ok" | wc -l)\n\n# note that the string "mysqlbackup completed ok" must occur 2 times in the log in order for the backup to be ok\nif [ $no_of_complete_ok_messages -eq 2 ]; then\n        # backup successful, find backup directory\n        echo "backup succeeded"\n        exit 0\n#else\n#        echo "mysql backup failed, please check logfile" | mail -s "error: mysql backup failed!" ${email_recipient}\n#        exit 1\nfi\nfind $backup_dir -mtime +17 -exec rm -f {} \\;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n 3. innobackupex备份\n\n> percona xtrabackup是一款基于mysql的热备份的开源实用程序，它可以备份5.1到5.7版本上innodb,xtradb,myisam存储引擎的表， xtrabackup有两个主要的工具：xtrabackup、innobackupex 。\n> （1）xtrabackup只能备份innodb和xtradb两种数据表，而不能备份myisam数据表 　　\n> （2）innobackupex则封装了xtrabackup，是一个脚本封装，所以能同时备份处理innodb和myisam，但在处理myisam时需要加一个读锁。\n\n#!/bin/bash\n\n#### -- default value -- ####\n# mysql参数\nmysql_cnf=/etc/my.cnf\nmysql_user="mysqlbackup"\nmysql_pass="xxx"\nmysql_host=\'localhost\'\n\n# 备份参数\nlogtime=`date +%x\' \'%t`\nfullbase=/data/mysql_master_backup/full\nlogbase=/data/mysql_master_backup/logs\n#dblist=`mysql -u${mysql_user} -p${mysql_pass} -e "show databases" | egrep -v $exclude`\ninnobackupex=/usr/bin/innobackupex\n\n#### -- 目录创建 -- ####\n[ -d ${fullbase} ] || mkdir -p ${fullbase}\n[ -d ${logbase} ] || mkdir -p ${logbase}\n\n\n####备份时间、大小记录\n\nlog_file="/data/mysql_master_backup/logs/backup_record_"$(date +%y%m%d%h%m%s)".log"\n\n#### -- main funcation -- ####\n# 全备\n_fullbackup()\n{\ndtime=`date +%y-%m-%d`\n$innobackupex --defaults-file=$mysql_cnf --user=$mysql_user --password=$mysql_pass --host=$mysql_host $fullbase > $logbase/${dtime}.log 2>&1\nresult=$?\nif [ $result -eq 0 ];then\n        bkbase=`ls -d ${fullbase}/${dtime}* | sort -r | head -n 1`\n        bksize=`du -sh ${bkbase} | awk \'{print $1}\'`\n        echo -e "[$logtime]\\tstatus: successful, backdir: $bkbase, size: $bksize" | tee -a  ${logbase}/full.log\nelse\n        echo -e "[$logtime]\\tstatus: failed, reason: check you log $logbase/${dtime}.log" | tee -a  ${logbase}/backup.log\nfi\n}\n\n# run\nbackup_timer_start=`date "+%y-%m-%d %h:%m:%s"`\necho "backup_timer_start   :    $backup_timer_start" >>$log_file\n_fullbackup\nbackup_timer_end=`date "+%y-%m-%d %h:%m:%s"`\necho "backup_timer_end     :    $backup_timer_end" >>$log_file\nbackup_duration=`echo $(($(date +%s -d "${backup_timer_end}") - $(date +%s -d "${backup_timer_start}"))) | awk \'{t=split("60 s 60 m 24 h 999 d",a);for(n=1;n<t;n+=2){if($1==0)break;s=$1%a[n]a[n+1] s;$1=int($1/a[n])}print s}\'`\necho "total backup time    :    $backup_duration"  >>$log_file\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n',charsets:{cjk:!0}},{title:"jenkins编译服务脚本",frontmatter:{title:"jenkins编译服务脚本",date:"2023-01-06T11:01:07.000Z",permalink:"/pages/2019f8/",categories:["编程","shell"],tags:[null],readingShow:"top",description:'gittreestate="clean"\nbuild_tag=""',meta:[{name:"twitter:title",content:"jenkins编译服务脚本"},{name:"twitter:description",content:'gittreestate="clean"\nbuild_tag=""'},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/06.jenkins%E7%BC%96%E8%AF%91%E6%9C%8D%E5%8A%A1%E8%84%9A%E6%9C%AC.html"},{property:"og:type",content:"article"},{property:"og:title",content:"jenkins编译服务脚本"},{property:"og:description",content:'gittreestate="clean"\nbuild_tag=""'},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/06.jenkins%E7%BC%96%E8%AF%91%E6%9C%8D%E5%8A%A1%E8%84%9A%E6%9C%AC.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-06T11:01:07.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"jenkins编译服务脚本"},{itemprop:"description",content:'gittreestate="clean"\nbuild_tag=""'}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/02.shell/06.jenkins%E7%BC%96%E8%AF%91%E6%9C%8D%E5%8A%A1%E8%84%9A%E6%9C%AC.html",relativePath:"03.编程/02.shell/06.jenkins编译服务脚本.md",key:"v-38a8ed18",path:"/pages/2019f8/",headersStr:null,content:'#!/bin/bash\n\nproject_code=$product_code\nbuild_path=/var/jenkins_home/workspace/$JOB_NAME\ntime=`date +\'%Y-%m-%dT%H:%M:%SZ\'`\ngit_tree_state="clean"\nbuild_tag=""\n\nfunction run_build(){\n    name=${JOB_BASE_NAME##*.}\n    if [[ $name = "service_cpp_quote" ]] || [[ $name = "service_new_trade" ]]  || [[ $name = "service_new_trade_gg" ]];then\n        # C++需要先编译.\n        mapping_path=/workspace\n        docker_images=xxx/cpp/dev_base_tools:1.1.1\n        build_command="cd /workspace && rm -rf build && mkdir build && cd build && cmake .. && make -j4 install"\n        sudo docker run -v /opt/apps/jenkins/jenkins_home/workspace/$JOB_BASE_NAME:$mapping_path --rm $docker_images sh -c "$build_command"\n        cd $build_path/build/dist\n    elif [[ $JOB_BASE_NAME = "AIRM.db" ]]; then\n        time=`date "+%Y%m%d%H%M%S"`\n        cd $build_path\n        sudo mkdir -p $JOB_BASE_NAME\n        versionlib_path=/data/Vsersion_Lib/$project_code\n\t\tsudo mkdir -p $versionlib_path\n        sudo cp -rf `ls |grep $"sql"` $JOB_BASE_NAME/\n        sudo tar -zcf  "$JOB_BASE_NAME"_"$time"_"$COMMIT_ID".tar.gz $JOB_BASE_NAME/\n        sudo cp "$JOB_BASE_NAME"_"$time"_"$COMMIT_ID".tar.gz $versionlib_path/\n        sudo rm -f "$JOB_BASE_NAME"_"$time"_"$COMMIT_ID".tar.gz\n        sudo rm -rf $JOB_BASE_NAME/\n        echo "$JOB_BASE_NAME"_"$time"_"$COMMIT_ID".tar.gz\n\n        return 0\n    elif [[ $JOB_BASE_NAME = "AIRM.gateway" ]]; then\n        build_tag="--build-arg VERSION=$version_number --build-arg GIT_COMMIT=$COMMIT_ID --build-arg GIT_TREE_STATE=$git_tree_state --build-arg BUILD_DATE=$time"\n    elif [[ $JOB_BASE_NAME = "AIRM.service_go_trade" ]]; then\n        build_tag="--build-arg VERSION=$version_number --build-arg GIT_COMMIT=$COMMIT_ID --build-arg GIT_TREE_STATE=$git_tree_state --build-arg BUILD_DATE=$time"\n        docker_names=("new_gateway" "query" "trade")\n        for docker_name in ${docker_names[@]}\n        do\n            cp script/docker/$docker_name/Dockerfile .\n            sudo docker tag xxx/$project_code/$docker_name:$version_number xxx/$project_code/$docker_name:$version_number.$BUILD_NUMBER\n            sudo docker rmi xxx/$project_code/$docker_name:$version_number\n            sudo docker build -t xxx/$project_code/$docker_name:$version_number $build_tag .\n            rm -f Dockerfile\n            sudo docker push xxx/$project_code/$docker_name:$version_number\n        done\n\n        return 0\n    else\n        cd $build_path\n    fi\n\n    # 打包.\n    docker_name=${name}\n    sudo docker tag xxx/$project_code/$docker_name:$version_number xxx/$project_code/$docker_name:$version_number.$BUILD_NUMBER\n    sudo docker rmi xxx/$project_code/$docker_name:$version_number\n    sudo docker build -t xxx/$project_code/$docker_name:$version_number $build_tag .\n    sudo docker push xxx/$project_code/$docker_name:$version_number\n}\n\nfunction checkBuild(){\n    product=$1\n    version=$2\n    if [[ -z $product ]] || [[ -z $version ]];then\n        # 参数为空.\n        return 1\n    fi\n    info_path=http://172.16.30.217:9110/version/status\n    value=`curl -XGET "$info_path?product_code=$product_code&version_number=$version_number" |awk -F\'lock_status":\' \'{print $2}\'|awk -F, \'{print $1}\'`\n    if [[ $value -eq 0 ]];then\n        # 可以正常构建.\n        return 0\n    else\n        # 版本已锁定.\n        return 3\n    fi\n}\n\nres1=$(checkBuild $product_code $version_number)\nres2=`echo $?`\nif [[ $res2 -eq 1 ]];then\n    echo "版本号没有输入"\n    exit $res2\nelif [[ $res2 -eq 3 ]];then\n    echo "对应的版本已锁定"\n    exit $res2\nelse\n    run_build\nfi\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n',normalizedContent:'#!/bin/bash\n\nproject_code=$product_code\nbuild_path=/var/jenkins_home/workspace/$job_name\ntime=`date +\'%y-%m-%dt%h:%m:%sz\'`\ngit_tree_state="clean"\nbuild_tag=""\n\nfunction run_build(){\n    name=${job_base_name##*.}\n    if [[ $name = "service_cpp_quote" ]] || [[ $name = "service_new_trade" ]]  || [[ $name = "service_new_trade_gg" ]];then\n        # c++需要先编译.\n        mapping_path=/workspace\n        docker_images=xxx/cpp/dev_base_tools:1.1.1\n        build_command="cd /workspace && rm -rf build && mkdir build && cd build && cmake .. && make -j4 install"\n        sudo docker run -v /opt/apps/jenkins/jenkins_home/workspace/$job_base_name:$mapping_path --rm $docker_images sh -c "$build_command"\n        cd $build_path/build/dist\n    elif [[ $job_base_name = "airm.db" ]]; then\n        time=`date "+%y%m%d%h%m%s"`\n        cd $build_path\n        sudo mkdir -p $job_base_name\n        versionlib_path=/data/vsersion_lib/$project_code\n\t\tsudo mkdir -p $versionlib_path\n        sudo cp -rf `ls |grep $"sql"` $job_base_name/\n        sudo tar -zcf  "$job_base_name"_"$time"_"$commit_id".tar.gz $job_base_name/\n        sudo cp "$job_base_name"_"$time"_"$commit_id".tar.gz $versionlib_path/\n        sudo rm -f "$job_base_name"_"$time"_"$commit_id".tar.gz\n        sudo rm -rf $job_base_name/\n        echo "$job_base_name"_"$time"_"$commit_id".tar.gz\n\n        return 0\n    elif [[ $job_base_name = "airm.gateway" ]]; then\n        build_tag="--build-arg version=$version_number --build-arg git_commit=$commit_id --build-arg git_tree_state=$git_tree_state --build-arg build_date=$time"\n    elif [[ $job_base_name = "airm.service_go_trade" ]]; then\n        build_tag="--build-arg version=$version_number --build-arg git_commit=$commit_id --build-arg git_tree_state=$git_tree_state --build-arg build_date=$time"\n        docker_names=("new_gateway" "query" "trade")\n        for docker_name in ${docker_names[@]}\n        do\n            cp script/docker/$docker_name/dockerfile .\n            sudo docker tag xxx/$project_code/$docker_name:$version_number xxx/$project_code/$docker_name:$version_number.$build_number\n            sudo docker rmi xxx/$project_code/$docker_name:$version_number\n            sudo docker build -t xxx/$project_code/$docker_name:$version_number $build_tag .\n            rm -f dockerfile\n            sudo docker push xxx/$project_code/$docker_name:$version_number\n        done\n\n        return 0\n    else\n        cd $build_path\n    fi\n\n    # 打包.\n    docker_name=${name}\n    sudo docker tag xxx/$project_code/$docker_name:$version_number xxx/$project_code/$docker_name:$version_number.$build_number\n    sudo docker rmi xxx/$project_code/$docker_name:$version_number\n    sudo docker build -t xxx/$project_code/$docker_name:$version_number $build_tag .\n    sudo docker push xxx/$project_code/$docker_name:$version_number\n}\n\nfunction checkbuild(){\n    product=$1\n    version=$2\n    if [[ -z $product ]] || [[ -z $version ]];then\n        # 参数为空.\n        return 1\n    fi\n    info_path=http://172.16.30.217:9110/version/status\n    value=`curl -xget "$info_path?product_code=$product_code&version_number=$version_number" |awk -f\'lock_status":\' \'{print $2}\'|awk -f, \'{print $1}\'`\n    if [[ $value -eq 0 ]];then\n        # 可以正常构建.\n        return 0\n    else\n        # 版本已锁定.\n        return 3\n    fi\n}\n\nres1=$(checkbuild $product_code $version_number)\nres2=`echo $?`\nif [[ $res2 -eq 1 ]];then\n    echo "版本号没有输入"\n    exit $res2\nelif [[ $res2 -eq 3 ]];then\n    echo "对应的版本已锁定"\n    exit $res2\nelse\n    run_build\nfi\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n',charsets:{cjk:!0}},{title:"app编译脚本",frontmatter:{title:"app编译脚本",date:"2023-01-06T11:06:30.000Z",permalink:"/pages/fd3678/",categories:["编程","shell"],tags:[null],readingShow:"top",description:"commit_num=git rev-parse --short HEAD\nproject_code=saas\nproductcode=$projectcode\nversionlibpath=/data/VsersionLib/$project_code\ndocker_images=\nmapping_path=\ndevbuildpath=",meta:[{name:"twitter:title",content:"app编译脚本"},{name:"twitter:description",content:"commit_num=git rev-parse --short HEAD\nproject_code=saas\nproductcode=$projectcode\nversionlibpath=/data/VsersionLib/$project_code\ndocker_images=\nmapping_path=\ndevbuildpath="},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/07.app%E7%BC%96%E8%AF%91%E8%84%9A%E6%9C%AC.html"},{property:"og:type",content:"article"},{property:"og:title",content:"app编译脚本"},{property:"og:description",content:"commit_num=git rev-parse --short HEAD\nproject_code=saas\nproductcode=$projectcode\nversionlibpath=/data/VsersionLib/$project_code\ndocker_images=\nmapping_path=\ndevbuildpath="},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/07.app%E7%BC%96%E8%AF%91%E8%84%9A%E6%9C%AC.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-06T11:06:30.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"app编译脚本"},{itemprop:"description",content:"commit_num=git rev-parse --short HEAD\nproject_code=saas\nproductcode=$projectcode\nversionlibpath=/data/VsersionLib/$project_code\ndocker_images=\nmapping_path=\ndevbuildpath="}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/02.shell/07.app%E7%BC%96%E8%AF%91%E8%84%9A%E6%9C%AC.html",relativePath:"03.编程/02.shell/07.app编译脚本.md",key:"v-45252678",path:"/pages/fd3678/",headers:[{level:4,title:"android编译",slug:"android编译",normalizedTitle:"android编译",charIndex:2},{level:4,title:"ios编译",slug:"ios编译",normalizedTitle:"ios编译",charIndex:5429}],headersStr:"android编译 ios编译",content:'# android编译\n\n#!/bin/bash\n\n\ntime=`date "+%Y%m%d%H%M%S"`\ncommit_num=`git rev-parse --short HEAD`\nproject_code=saas\nproduct_code=$project_code\nversionlib_path=/data/Vsersion_Lib/$project_code\ndocker_images=\nmapping_path=\ndev_build_path=\n\nrun_build(){\n    sudo docker run -v /data/jenkins/jenkins_home/workspace/saas/$JOB_BASE_NAME:$mapping_path -v /data/gradle_cache/.gradle:/home/cirrus/.gradle --env GRADLE_USER_HOME=/home/cirrus/.gradle --workdir /build --rm $docker_images sh -c "$build_command"\n    if [ "$1" == "app_dev" ];then\n        echo "开始dev打包"\n        cd $dev_build_path\n        sudo mkdir -p $versionlib_path\n        sudo cp -rf app-release.apk $versionlib_path/"$JOB_BASE_NAME"_dev_"$time"_"$commit_num".apk\n        dev_new_package_name="$JOB_BASE_NAME"_dev_"$time"_"$commit_num".apk\n        echo $dev_new_package_name\n        # echo "开始stest打包"\n        # cd $stest_build_path\n        # sudo cp -rf app-stest-release.apk $versionlib_path/"$JOB_BASE_NAME"_test_"$time"_"$commit_num".apk\n        # test_new_package_name="$JOB_BASE_NAME"_test_"$time"_"$commit_num".apk\n        # echo $test_new_package_name\n        # echo "开始pro打包"\n        # cd $pro_build_path\n        # sudo cp -rf app-pro-release.apk $versionlib_path/"$JOB_BASE_NAME"_pro_"$time"_"$commit_num".apk\n        # pro_new_package_name="$JOB_BASE_NAME"_pro_"$time"_"$commit_num".apk\n        # echo $pro_new_package_name\n        curl -H "Content-Type:application/json" -XPOST http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$project_code"\'","version_number":"\'"$version_number"\'","new_package_name":"\'"$dev_new_package_name"\'"}\'\n        # curl -H "Content-Type:application/json" -XPOST http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$project_code"\'","version_number":"\'"$version_number"\'","new_package_name":"\'"$test_new_package_name"\'"}\'\n        # curl -H "Content-Type:application/json" -XPOST http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$project_code"\'","version_number":"\'"$version_number"\'","new_package_name":"\'"$pro_new_package_name"\'"}\' \n    elif [ "$1" == "app_test" ];then\n        echo "开始test打包"\n        cd $test_build_path\n        sudo mkdir -p $versionlib_path\n        sudo cp -rf app-release.apk $versionlib_path/"$JOB_BASE_NAME"_test_"$time"_"$commit_num".apk\n        test_new_package_name="$JOB_BASE_NAME"_test_"$time"_"$commit_num".apk\n        echo $test_new_package_name\n        curl -H "Content-Type:application/json" -XPOST http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$project_code"\'","version_number":"\'"$version_number"\'","new_package_name":"\'"$test_new_package_name"\'"}\'\n    elif [ "$1" == "app_release" ];then\n        echo "开始release打包"\n        cd $release_build_path\n        sudo mkdir -p $versionlib_path\n        sudo cp -rf app-release.apk $versionlib_path/"$JOB_BASE_NAME"_pro_"$time"_"$commit_num".apk\n        pro_new_package_name="$JOB_BASE_NAME"_pro_"$time"_"$commit_num".apk\n        echo $pro_new_package_name\n        curl -H "Content-Type:application/json" -XPOST http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$project_code"\'","version_number":"\'"$version_number"\'","new_package_name":"\'"$pro_new_package_name"\'"}\'               \n\n    else\n        exit 1\n    fi\n\n}\n\nrun_build1(){\nif [ "$1" == "android_dev" ];then\n        docker_images=xxx/library/flutter:latest\n        mapping_path=/build\n        dev_build_path=/var/jenkins_home/workspace/$JOB_NAME/build/app/outputs/flutter-apk\n        build_command="flutter clean && flutter packages get && flutter build apk --no-sound-null-safety --dart-define=APP_ENV=dev"\n        run_build app_dev\nelif [ "$1" == "android_test" ];then\n        docker_images=xxx/library/flutter:latest\n        mapping_path=/build\n        test_build_path=/var/jenkins_home/workspace/$JOB_NAME/build/app/outputs/flutter-apk\n        build_command="flutter clean && flutter packages get && flutter build apk --no-sound-null-safety --dart-define=APP_ENV=test"\n        run_build app_test\nelif [ "$1" == "android_release" ];then\n        docker_images=xxx/library/flutter:latest\n        mapping_path=/build\n        release_build_path=/var/jenkins_home/workspace/$JOB_NAME/build/app/outputs/flutter-apk\n        build_command="flutter clean && flutter packages get && flutter build apk --no-sound-null-safety --dart-define=APP_ENV=release"\n        run_build app_release\n\n\nelse\n        exit 2\nfi\n}\nfunction checkBuild(){\n    product=$1\n    version=$2\n    if [[ -z $product ]] || [[ -z $version ]];then\n        # 参数为空.\n        return 1\n    fi\n    info_path=http://172.16.30.217:9110/version/status\n    value=`curl -XGET "$info_path?product_code=$product_code&version_number=$version_number" |awk -F\'lock_status":\' \'{print $2}\'|awk -F, \'{print $1}\'`\n    if [[ $value -eq 0 ]];then\n        # 可以正常构建.\n        return 0\n    else\n        # 版本已锁定.\n        return 3\n    fi\n}\n\nres1=$(checkBuild $product_code $version_number)\nres2=`echo $?`\nif [[ $res2 -eq 1 ]];then\n    echo "版本号没有输入"\n    exit $res2\nelif [[ $res2 -eq 3 ]];then\n    echo "对应的版本已锁定"\n    exit $res2\nelse\n    run_build1 $1\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n\n\n# ios编译\n\n#flutter build ipa --build-name=0.0.6 --build-number=15 --release --dart-define=APP_ENV=dev --no-sound-null-safety\n#flutter build ipa --release --dart-define=APP_ENV=test --no-sound-null-safety\n#open ./build/ios/archive/Runner.xcarchive\n#flutter build apk --no-sound-null-safety --dart-define=APP_ENV=test\n\n#flutter build ipa --release --dart-define=APP_ENV=dev --no-sound-null-safety\n#xcodebuild -exportArchive -exportOptionsPlist ExportOptionsDevelopment.plist -archivePath $PWD/build/ios/archive/Runner.xcarchive -exportPath $PWD/build/ios/ipa/ -allowProvisioningUpdates\n#cd ./ios && xcodebuild -workspace Runner.xcworkspace -scheme Runner -sdk iphoneos -configuration Release archive -archivePath $PWD/build/Runner.xcarchive && cd ..\n#cd ./ios && fastlane ios build_dev && cd ..\n\n#flutter build ipa --release --dart-define=APP_ENV=test --no-sound-null-safety\n#cd ./ios && fastlane ios build_test && cd ..\n\n\n#flutter build ios --release --dart-define=APP_ENV=dev --no-sound-null-safety\n#cd ./ios && fastlane ios build_dev && cd ..\n#\n#flutter build ios --release --dart-define=APP_ENV=test --no-sound-null-safety\n#cd ./ios && fastlane ios build_test && cd ..\n#\ntime=`date "+%Y%m%d%H%M%S"`\ncommit_num=`git rev-parse --short HEAD`\nproduct_code=saas\nchoice_env=$1\nversion_number=$2\nexport LANG=en_US.UTF-8\n#JOB_NAME=scrm.ios\n\nrun_build(){\n    if [ "$1" == "app_dev" ];then\n        fir publish ./build/ios/outputs/saas_dev.ipa -c "jenkins自动打包上传" > /opt/jenkins/logs/saas/saas_dev.log\n        time=`date "+%Y%m%d%H%M%S"`\n        aa=`cat /opt/jenkins/logs/saas/saas_dev.log | grep Published | awk \'{print $9}\' | awk -F\'/\' \'{print $4}\'`\n        bb=`cat /opt/jenkins/logs/saas/saas_dev.log | grep Release | awk \'{print $10}\'`\n        new_package_name="$JOB_BASE_NAME"_dev_"$time"-"$commit_num"-"$aa"?release_id="$bb"\n        echo $new_package_name\n        echo $version_number\n        curl -H "Content-Type:application/json" -XPOST http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$product_code"\'","\'"version_number"\'":"\'"$version_number"\'","new_package_name":"\'"$new_package_name"\'"}\'\n    elif [ "$1" == "app_test" ];then\n        fir publish ./build/ios/outputs/saas_test.ipa -c "jenkins自动打包上传" > /opt/jenkins/logs/saas/saas_test.log\n        time=`date "+%Y%m%d%H%M%S"`\n        aa=`cat /opt/jenkins/logs/saas/saas_test.log | grep Published | awk \'{print $9}\' | awk -F\'/\' \'{print $4}\'`\n        bb=`cat /opt/jenkins/logs/saas/saas_test.log | grep Release | awk \'{print $10}\'`\n        new_package_name="$JOB_BASE_NAME"_test_"$time"-"$commit_num"-"$aa"?release_id="$bb"\n        echo $new_package_name\n        curl -H "Content-Type:application/json" -XPOST http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$product_code"\'","\'"version_number"\'":"\'"$version_number"\'","new_package_name":"\'"$new_package_name"\'"}\'\n    elif [ "$1" == "app_release" ];then\n        fir publish ./build/ios/outputs/saas_release.ipa -c "jenkins自动打包上传" > /opt/jenkins/logs/saas/saas_pro.log\n        time=`date "+%Y%m%d%H%M%S"`\n        aa=`cat /opt/jenkins/logs/saas/saas_pro.log | grep Published | awk \'{print $9}\' | awk -F\'/\' \'{print $4}\'`\n        bb=`cat /opt/jenkins/logs/saas/saas_pro.log | grep Release | awk \'{print $10}\'`\n        new_package_name="$JOB_BASE_NAME"_pro_"$time"-"$commit_num"-"$aa"?release_id="$bb"\n        echo $new_package_name\n        curl -H "Content-Type:application/json" -XPOST http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$product_code"\'","\'"version_number"\'":"\'"$version_number"\'","new_package_name":"\'"$new_package_name"\'"}\'\n    else\n       exit 1\n    fi\n}\n\n\nclean_build(){\n  flutter doctor\n  flutter clean\n  rm -Rf ios/Pods\n  rm -Rf ios/.symlinks\n  rm -Rf ios/Flutter/Flutter.framework\n  rm -Rf ios/Flutter/Flutter.podspec\n  rm ios/Podfile\n}\n\nrun_build1(){\nif [ $choice_env == "ios_dev" ];then\n     clean_build\n     flutter build ipa --export-options-plist ./ios/ExportOptionsDevelopment.plist --release --dart-define=APP_ENV=dev --no-sound-null-safety\n     mkdir -p ./build/ios/outputs\n     cp -r ./build/ios/ipa/scrm_app.ipa ./build/ios/outputs/saas_dev.ipa\n     run_build app_dev\nelif [ $choice_env == "ios_test" ];then\n     clean_build\n     flutter build ipa --export-options-plist ./ios/ExportOptionsDevelopment.plist --release --dart-define=APP_ENV=test --no-sound-null-safety\n     mkdir -p ./build/ios/outputs\n     cp -r ./build/ios/ipa/scrm_app.ipa ./build/ios/outputs/saas_test.ipa\n     run_build app_test\nelif [ $choice_env == "ios_release" ];then\n     clean_build\n     flutter build ipa --export-options-plist ./ios/ExportOptionsDevelopment.plist --release --dart-define=APP_ENV=release --no-sound-null-safety\n     mkdir -p ./build/ios/outputs\n     cp -r ./build/ios/ipa/scrm_app.ipa ./build/ios/outputs/saas_release.ipa\n     run_build app_release\nelif [ $choice_env == "ios_appstore" ];then\n     cd ./ios && fastlane ios sign_xcarchive_and_publish\nelse\n     exit 2\nfi\n}\nfunction checkBuild(){\n    product=$1\n    version=$2\n    if [[ -z $product ]] || [[ -z $version ]];then\n        # 参数为空.\n        return 1\n    fi\n    info_path=http://172.16.30.217:9110/version/status\n    value=`curl -XGET "$info_path?product_code=$product_code&version_number=$version_number" |awk -F\'lock_status":\' \'{print $2}\'|awk -F, \'{print $1}\'`\n    if [[ $value -eq 0 ]];then\n        # 可以正常构建.\n        return 0\n    else\n        # 版本已锁定.\n        return 3\n    fi\n}\n\nres1=$(checkBuild $product_code $version_number)\nres2=`echo $?`\nif [[ $res2 -eq 1 ]];then\n    echo "版本号没有输入"\n    exit $res2\nelif [[ $res2 -eq 3 ]];then\n    echo "对应的版本已锁定"\n    exit $res2\nelse\n    run_build1\nfi\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n',normalizedContent:'# android编译\n\n#!/bin/bash\n\n\ntime=`date "+%y%m%d%h%m%s"`\ncommit_num=`git rev-parse --short head`\nproject_code=saas\nproduct_code=$project_code\nversionlib_path=/data/vsersion_lib/$project_code\ndocker_images=\nmapping_path=\ndev_build_path=\n\nrun_build(){\n    sudo docker run -v /data/jenkins/jenkins_home/workspace/saas/$job_base_name:$mapping_path -v /data/gradle_cache/.gradle:/home/cirrus/.gradle --env gradle_user_home=/home/cirrus/.gradle --workdir /build --rm $docker_images sh -c "$build_command"\n    if [ "$1" == "app_dev" ];then\n        echo "开始dev打包"\n        cd $dev_build_path\n        sudo mkdir -p $versionlib_path\n        sudo cp -rf app-release.apk $versionlib_path/"$job_base_name"_dev_"$time"_"$commit_num".apk\n        dev_new_package_name="$job_base_name"_dev_"$time"_"$commit_num".apk\n        echo $dev_new_package_name\n        # echo "开始stest打包"\n        # cd $stest_build_path\n        # sudo cp -rf app-stest-release.apk $versionlib_path/"$job_base_name"_test_"$time"_"$commit_num".apk\n        # test_new_package_name="$job_base_name"_test_"$time"_"$commit_num".apk\n        # echo $test_new_package_name\n        # echo "开始pro打包"\n        # cd $pro_build_path\n        # sudo cp -rf app-pro-release.apk $versionlib_path/"$job_base_name"_pro_"$time"_"$commit_num".apk\n        # pro_new_package_name="$job_base_name"_pro_"$time"_"$commit_num".apk\n        # echo $pro_new_package_name\n        curl -h "content-type:application/json" -xpost http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$project_code"\'","version_number":"\'"$version_number"\'","new_package_name":"\'"$dev_new_package_name"\'"}\'\n        # curl -h "content-type:application/json" -xpost http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$project_code"\'","version_number":"\'"$version_number"\'","new_package_name":"\'"$test_new_package_name"\'"}\'\n        # curl -h "content-type:application/json" -xpost http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$project_code"\'","version_number":"\'"$version_number"\'","new_package_name":"\'"$pro_new_package_name"\'"}\' \n    elif [ "$1" == "app_test" ];then\n        echo "开始test打包"\n        cd $test_build_path\n        sudo mkdir -p $versionlib_path\n        sudo cp -rf app-release.apk $versionlib_path/"$job_base_name"_test_"$time"_"$commit_num".apk\n        test_new_package_name="$job_base_name"_test_"$time"_"$commit_num".apk\n        echo $test_new_package_name\n        curl -h "content-type:application/json" -xpost http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$project_code"\'","version_number":"\'"$version_number"\'","new_package_name":"\'"$test_new_package_name"\'"}\'\n    elif [ "$1" == "app_release" ];then\n        echo "开始release打包"\n        cd $release_build_path\n        sudo mkdir -p $versionlib_path\n        sudo cp -rf app-release.apk $versionlib_path/"$job_base_name"_pro_"$time"_"$commit_num".apk\n        pro_new_package_name="$job_base_name"_pro_"$time"_"$commit_num".apk\n        echo $pro_new_package_name\n        curl -h "content-type:application/json" -xpost http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$project_code"\'","version_number":"\'"$version_number"\'","new_package_name":"\'"$pro_new_package_name"\'"}\'               \n\n    else\n        exit 1\n    fi\n\n}\n\nrun_build1(){\nif [ "$1" == "android_dev" ];then\n        docker_images=xxx/library/flutter:latest\n        mapping_path=/build\n        dev_build_path=/var/jenkins_home/workspace/$job_name/build/app/outputs/flutter-apk\n        build_command="flutter clean && flutter packages get && flutter build apk --no-sound-null-safety --dart-define=app_env=dev"\n        run_build app_dev\nelif [ "$1" == "android_test" ];then\n        docker_images=xxx/library/flutter:latest\n        mapping_path=/build\n        test_build_path=/var/jenkins_home/workspace/$job_name/build/app/outputs/flutter-apk\n        build_command="flutter clean && flutter packages get && flutter build apk --no-sound-null-safety --dart-define=app_env=test"\n        run_build app_test\nelif [ "$1" == "android_release" ];then\n        docker_images=xxx/library/flutter:latest\n        mapping_path=/build\n        release_build_path=/var/jenkins_home/workspace/$job_name/build/app/outputs/flutter-apk\n        build_command="flutter clean && flutter packages get && flutter build apk --no-sound-null-safety --dart-define=app_env=release"\n        run_build app_release\n\n\nelse\n        exit 2\nfi\n}\nfunction checkbuild(){\n    product=$1\n    version=$2\n    if [[ -z $product ]] || [[ -z $version ]];then\n        # 参数为空.\n        return 1\n    fi\n    info_path=http://172.16.30.217:9110/version/status\n    value=`curl -xget "$info_path?product_code=$product_code&version_number=$version_number" |awk -f\'lock_status":\' \'{print $2}\'|awk -f, \'{print $1}\'`\n    if [[ $value -eq 0 ]];then\n        # 可以正常构建.\n        return 0\n    else\n        # 版本已锁定.\n        return 3\n    fi\n}\n\nres1=$(checkbuild $product_code $version_number)\nres2=`echo $?`\nif [[ $res2 -eq 1 ]];then\n    echo "版本号没有输入"\n    exit $res2\nelif [[ $res2 -eq 3 ]];then\n    echo "对应的版本已锁定"\n    exit $res2\nelse\n    run_build1 $1\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n\n\n# ios编译\n\n#flutter build ipa --build-name=0.0.6 --build-number=15 --release --dart-define=app_env=dev --no-sound-null-safety\n#flutter build ipa --release --dart-define=app_env=test --no-sound-null-safety\n#open ./build/ios/archive/runner.xcarchive\n#flutter build apk --no-sound-null-safety --dart-define=app_env=test\n\n#flutter build ipa --release --dart-define=app_env=dev --no-sound-null-safety\n#xcodebuild -exportarchive -exportoptionsplist exportoptionsdevelopment.plist -archivepath $pwd/build/ios/archive/runner.xcarchive -exportpath $pwd/build/ios/ipa/ -allowprovisioningupdates\n#cd ./ios && xcodebuild -workspace runner.xcworkspace -scheme runner -sdk iphoneos -configuration release archive -archivepath $pwd/build/runner.xcarchive && cd ..\n#cd ./ios && fastlane ios build_dev && cd ..\n\n#flutter build ipa --release --dart-define=app_env=test --no-sound-null-safety\n#cd ./ios && fastlane ios build_test && cd ..\n\n\n#flutter build ios --release --dart-define=app_env=dev --no-sound-null-safety\n#cd ./ios && fastlane ios build_dev && cd ..\n#\n#flutter build ios --release --dart-define=app_env=test --no-sound-null-safety\n#cd ./ios && fastlane ios build_test && cd ..\n#\ntime=`date "+%y%m%d%h%m%s"`\ncommit_num=`git rev-parse --short head`\nproduct_code=saas\nchoice_env=$1\nversion_number=$2\nexport lang=en_us.utf-8\n#job_name=scrm.ios\n\nrun_build(){\n    if [ "$1" == "app_dev" ];then\n        fir publish ./build/ios/outputs/saas_dev.ipa -c "jenkins自动打包上传" > /opt/jenkins/logs/saas/saas_dev.log\n        time=`date "+%y%m%d%h%m%s"`\n        aa=`cat /opt/jenkins/logs/saas/saas_dev.log | grep published | awk \'{print $9}\' | awk -f\'/\' \'{print $4}\'`\n        bb=`cat /opt/jenkins/logs/saas/saas_dev.log | grep release | awk \'{print $10}\'`\n        new_package_name="$job_base_name"_dev_"$time"-"$commit_num"-"$aa"?release_id="$bb"\n        echo $new_package_name\n        echo $version_number\n        curl -h "content-type:application/json" -xpost http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$product_code"\'","\'"version_number"\'":"\'"$version_number"\'","new_package_name":"\'"$new_package_name"\'"}\'\n    elif [ "$1" == "app_test" ];then\n        fir publish ./build/ios/outputs/saas_test.ipa -c "jenkins自动打包上传" > /opt/jenkins/logs/saas/saas_test.log\n        time=`date "+%y%m%d%h%m%s"`\n        aa=`cat /opt/jenkins/logs/saas/saas_test.log | grep published | awk \'{print $9}\' | awk -f\'/\' \'{print $4}\'`\n        bb=`cat /opt/jenkins/logs/saas/saas_test.log | grep release | awk \'{print $10}\'`\n        new_package_name="$job_base_name"_test_"$time"-"$commit_num"-"$aa"?release_id="$bb"\n        echo $new_package_name\n        curl -h "content-type:application/json" -xpost http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$product_code"\'","\'"version_number"\'":"\'"$version_number"\'","new_package_name":"\'"$new_package_name"\'"}\'\n    elif [ "$1" == "app_release" ];then\n        fir publish ./build/ios/outputs/saas_release.ipa -c "jenkins自动打包上传" > /opt/jenkins/logs/saas/saas_pro.log\n        time=`date "+%y%m%d%h%m%s"`\n        aa=`cat /opt/jenkins/logs/saas/saas_pro.log | grep published | awk \'{print $9}\' | awk -f\'/\' \'{print $4}\'`\n        bb=`cat /opt/jenkins/logs/saas/saas_pro.log | grep release | awk \'{print $10}\'`\n        new_package_name="$job_base_name"_pro_"$time"-"$commit_num"-"$aa"?release_id="$bb"\n        echo $new_package_name\n        curl -h "content-type:application/json" -xpost http://172.16.30.217:9110/version/modify -d \'{"product_code":"\'"$product_code"\'","\'"version_number"\'":"\'"$version_number"\'","new_package_name":"\'"$new_package_name"\'"}\'\n    else\n       exit 1\n    fi\n}\n\n\nclean_build(){\n  flutter doctor\n  flutter clean\n  rm -rf ios/pods\n  rm -rf ios/.symlinks\n  rm -rf ios/flutter/flutter.framework\n  rm -rf ios/flutter/flutter.podspec\n  rm ios/podfile\n}\n\nrun_build1(){\nif [ $choice_env == "ios_dev" ];then\n     clean_build\n     flutter build ipa --export-options-plist ./ios/exportoptionsdevelopment.plist --release --dart-define=app_env=dev --no-sound-null-safety\n     mkdir -p ./build/ios/outputs\n     cp -r ./build/ios/ipa/scrm_app.ipa ./build/ios/outputs/saas_dev.ipa\n     run_build app_dev\nelif [ $choice_env == "ios_test" ];then\n     clean_build\n     flutter build ipa --export-options-plist ./ios/exportoptionsdevelopment.plist --release --dart-define=app_env=test --no-sound-null-safety\n     mkdir -p ./build/ios/outputs\n     cp -r ./build/ios/ipa/scrm_app.ipa ./build/ios/outputs/saas_test.ipa\n     run_build app_test\nelif [ $choice_env == "ios_release" ];then\n     clean_build\n     flutter build ipa --export-options-plist ./ios/exportoptionsdevelopment.plist --release --dart-define=app_env=release --no-sound-null-safety\n     mkdir -p ./build/ios/outputs\n     cp -r ./build/ios/ipa/scrm_app.ipa ./build/ios/outputs/saas_release.ipa\n     run_build app_release\nelif [ $choice_env == "ios_appstore" ];then\n     cd ./ios && fastlane ios sign_xcarchive_and_publish\nelse\n     exit 2\nfi\n}\nfunction checkbuild(){\n    product=$1\n    version=$2\n    if [[ -z $product ]] || [[ -z $version ]];then\n        # 参数为空.\n        return 1\n    fi\n    info_path=http://172.16.30.217:9110/version/status\n    value=`curl -xget "$info_path?product_code=$product_code&version_number=$version_number" |awk -f\'lock_status":\' \'{print $2}\'|awk -f, \'{print $1}\'`\n    if [[ $value -eq 0 ]];then\n        # 可以正常构建.\n        return 0\n    else\n        # 版本已锁定.\n        return 3\n    fi\n}\n\nres1=$(checkbuild $product_code $version_number)\nres2=`echo $?`\nif [[ $res2 -eq 1 ]];then\n    echo "版本号没有输入"\n    exit $res2\nelif [[ $res2 -eq 3 ]];then\n    echo "对应的版本已锁定"\n    exit $res2\nelse\n    run_build1\nfi\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n',charsets:{cjk:!0}},{title:"常用shell脚本",frontmatter:{title:"常用shell脚本",date:"2023-02-24T15:30:36.000Z",permalink:"/pages/8fdac1/",categories:["编程","shell"],tags:[null],readingShow:"top",description:"response=$(curl -sL -o /dev/null -w %{http_code} https://baidu.com)\nif [[ $response -ge 200 && $response -le 299 ]] ;then\n    echo 'check point success'\nelse\n    echo 'check point fail'\nfi\n`",meta:[{name:"twitter:title",content:"常用shell脚本"},{name:"twitter:description",content:"response=$(curl -sL -o /dev/null -w %{http_code} https://baidu.com)\nif [[ $response -ge 200 && $response -le 299 ]] ;then\n    echo 'check point success'\nelse\n    echo 'check point fail'\nfi\n`"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/08.%E5%B8%B8%E7%94%A8shell%E8%84%9A%E6%9C%AC.html"},{property:"og:type",content:"article"},{property:"og:title",content:"常用shell脚本"},{property:"og:description",content:"response=$(curl -sL -o /dev/null -w %{http_code} https://baidu.com)\nif [[ $response -ge 200 && $response -le 299 ]] ;then\n    echo 'check point success'\nelse\n    echo 'check point fail'\nfi\n`"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/08.%E5%B8%B8%E7%94%A8shell%E8%84%9A%E6%9C%AC.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-24T15:30:36.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"常用shell脚本"},{itemprop:"description",content:"response=$(curl -sL -o /dev/null -w %{http_code} https://baidu.com)\nif [[ $response -ge 200 && $response -le 299 ]] ;then\n    echo 'check point success'\nelse\n    echo 'check point fail'\nfi\n`"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/02.shell/08.%E5%B8%B8%E7%94%A8shell%E8%84%9A%E6%9C%AC.html",relativePath:"03.编程/02.shell/08.常用shell脚本.md",key:"v-c86447e2",path:"/pages/8fdac1/",headersStr:null,content:'# 常用shell脚本\n\n#!/bin/bash\nresponse=$(curl -sL -o /dev/null -w %{http_code} https://baidu.com)\nif [[ $response -ge 200 && $response -le 299 ]] ;then \n    echo \'check point success\'\nelse \n    echo \'check point fail\'\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 2、读取文件中的配置到变量中\n\n#!/bin/bash\n# 配置文件中的配置项格式为key1=value1，一行一个配置项\nwhile read line;do\n    eval "$line"\ndone < /etc/openvpn/server/smtp.conf\necho $key1\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3、根据 console 输入条件执行\n\n#!/bin/bash\necho "选择以下功能:"\necho "   0) 功能0"\necho "   1) 功能1"\necho "   2) 功能2"\necho "   3) 功能3"\necho "   4) 功能4"\nread -p "功能选项[4]: " option\nuntil [[ -z "$option" || "$option" =~ ^[0-4]$ ]]; do\n    read -p "$option为无效的选项，请重新输入功能选项: " option\ndone\ncase "$option" in\n    0) \n        echo "功能0已执行!" \n    ;;\n        1)\n            echo "功能1已执行!" \n      exit\n    ;;\n        2)\n            echo "功能2已执行!" \n            exit\n        ;;\n        3)\n            echo "功能3已执行!"  \n            exit\n        ;;\n        # 默认选项\n        4|"")\n            echo "功能4已执行!" \n            exit\n        ;;\nesac\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# 4、将指定输出内容写入文件\n\n{\necho "hahh"\necho "lalal"\n} > /tmp/test\n\n\n1\n2\n3\n4\n\n\n\n# 5、判断变量是否存在或为空\n\nif [ -z ${var+x} ]; then \n    echo "var is unset"; \nelse \n    echo "var is set to \'$var\'"; \nfi\n\n\n1\n2\n3\n4\n5\n\n\n参考：https://stackoverflow.com/questions/3601515/how-to-check-if-a-variable-is-set-in-bash\n\n\n#\n\n\n# 6、换算秒为分钟、小时\n\n#!/bin/bash\n\na=60100\nswap_seconds ()\n{\n    SEC=$1\n    (( SEC < 60 )) && echo -e "持续时间: $SEC秒\\c"\n    (( SEC >= 60 && SEC < 3600 )) && echo -e "持续时间: $(( SEC / 60 ))分钟$(( SEC % 60 ))秒\\c"\n    (( SEC > 3600 )) && echo -e "持续时间: $(( SEC / 3600 ))小时$(( (SEC % 3600) / 60 ))分钟$(( (SEC % 3600) % 60 ))秒\\c"\n}\n\nb=`swap_seconds $a`\necho $b\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n输出\n\n持续时间: 16小时41分钟40秒\n\n\n1\n\n\n\n# 7、脚本命令行参数的传递与判断\n\n#!/bin/bash\n\nmain() {\n    if [[ $# == 1 ]]; then\n        case $1 in\n        "-h")\n            echo "脚本使用方法: "\n            echo "  ./gitlab-pipeline.sh git仓库名1 git仓库名2 ... tag名(tag命名规则为: *-v加数字)"\n            exit\n            ;;\n        "--help")\n            echo "脚本使用方法: "\n            echo "  ./gitlab-pipeline.sh git仓库名1 git仓库名2 ... tag名(tag命名规则为: *-v加数字)"\n            exit\n            ;;\n        *)\n            echo "参数错误！"\n            exit\n            ;;\n        esac\n    fi\n}\n\nmain $*\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 8、检测 docker 容器的启动状态\n\n# 第一步：判断镜像是否存在\nif [ `docker images --format {{.Repository}}:{{.Tag}} |grep -Fx 192.168.1.7:32772/applications/$CI_PROJECT_NAME:${CI_COMMIT_SHORT_SHA};echo $?` -eq 0 ];then\n   docker pull 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ;\nfi;\n# 第二步：判断是否已经有重名的容器在运行或者处在其他状态。重名的，先删掉，在启动；不重名的直接启动\nif [ `docker ps -a --format {{.Names}} |grep -Fx $CI_PROJECT_NAME > /dev/null ;echo $?` -eq 0 ] ;then\n   docker rm -f $CI_PROJECT_NAME ;\n   docker run -d --name $CI_PROJECT_NAME -p 30088:8080 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ;\nelse\n   docker run -d --name $CI_PROJECT_NAME -p 30088:8080 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ;\nfi;\n# 第三步：循环五次判断容器的监控检测是否处于什么状态。健康状态就直接退出循环，不健康显示健康检查日志，正在启动的直接显示。处于其他状态的直接显示状态\nn=0;\nwhile true ;do\n  container_state=`docker inspect --format=\'{{json .State.Health.Status}}\' $CI_PROJECT_NAME`;\n  case $container_state in\n    \'"starting"\' )\n      echo "应用容器正在启动！";\n    ;;\n    \'"healthy"\' )\n      echo "应用容器已启动，状态健康！";\n    break;\n    ;;\n    \'"unhealthy"\' )\n      echo "应用容器健康检测失败！";\n      docker inspect --format=\'{{json .State.Health.Log}}\' $CI_PROJECT_NAME;\n    ;;\n    * )\n      echo "未知的状态:$container_state";\n    ;;\n  esac;\n  sleep 1s;\n  n=$(($n+1));\n  if [ $n -eq 5 ];\n     then break ;\n  fi ;\ndone\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n\n# 9、检查常见系统命令是否安装\n\ncheck_command() {\n    if ! command -v ifconfig >/dev/null 2>&1; then\n        echo -e "\\033[31mifconfig命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y net-tools >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y net-tools >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y net-tools >/dev/null 2>&1\n        fi\n    elif ! command -v ip >/dev/null 2>&1; then\n        echo -e "\\033[31mip命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y iproute2 >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y iproute2 >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y iproute2 >/dev/null 2>&1\n        fi\n    elif ! command -v curl >/dev/null 2>&1; then\n        echo -e "\\033[31mcurl命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y curl >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y curl >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y curl >/dev/null 2>&1\n        fi\n    elif ! command -v wget >/dev/null 2>&1; then\n        echo -e "\\033[31mawk命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y wget >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y wget >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y wget >/dev/null 2>&1\n        fi\n    elif ! command -v tail >/dev/null 2>&1; then\n        echo -e "\\033[31mcoreutils命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y coreutils >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y coreutils >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y coreutils >/dev/null 2>&1\n        fi\n    elif ! command -v sed >/dev/null 2>&1; then\n        echo -e "\\033[31msed命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y sed >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y sed >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y sed >/dev/null 2>&1\n        fi\n    elif ! command -v grep >/dev/null 2>&1; then\n        echo -e "\\033[31mgrep命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y grep >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y grep >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y grep >/dev/null 2>&1\n        fi\n    fi\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n\n\n\n# 10、检查系统网络\n\ncheck_network() {\n    check_command\n    ping -c 4 114.114.114.114 >/dev/null\n    if [ ! $? -eq 0 ]; then\n        echo -e "\\033[31mIP地址无法ping通，请检查网络连接！！！\\033[0m"\n        exit\n    fi\n    ping -c 4 www.baidu.com >/dev/null\n    if [ ! $? -eq 0 ]; then\n        echo -e "\\033[31m域名无法Ping通，请检查DNS配置！！！\\033[0m"\n        exit\n    fi\n    curl -s --retry 2 --connect-timeout 2 www.baidu.com >/dev/null\n    if [ ! $? -eq 0 ]; then\n        echo -e "\\033[31m域名无法Ping通，请检查DNS配置！！！\\033[0m"\n        exit\n    fi\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 11、发送钉钉通知\n\nfunction sendDingDing(){\n  Ding_Webhook_Token=\'钉钉机器人的WebHook Token\'\n  Ding_Webhook_keyword="可用作消息标题"\n  send_result=$(curl -sS \\\n    -XPOST https://oapi.dingtalk.com/robot/send?access_token="$Ding_Webhook_Token" \\\n    -H \'Content-Type: application/json\' \\\n    -d \'{"msgtype": "markdown","markdown": {"title": "\'$Ding_Webhook_Keyword\'","text": "\'"$*"\'"},"at": {"isAtAll": true}}\')\n  if [ $(echo $send_result | jq -r \'.errcode\') -eq 0 ]; then\n    echo "发送成功:  \\n" $*" "\n  else\n    echo "发送失败:  \\n" $*"\\n"\n    echo $send_result\n  fi\n}\n\nsendDingDing  "$(date +"%F %H:%M:%S") 需要告警的消息"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n.zshrc\n\nfunction sendDingDing() {\n  setopt no_nomatch\n  Ding_Webhook_Token=\'钉钉机器人的WebHook Token\'\n  Ding_Webhook_Keyword=\'可用作消息标题\'\n  send_result=$(curl -sS \\\n    -XPOST https://oapi.dingtalk.com/robot/send?access_token="$Ding_Webhook_Token" \\\n    -H \'Content-Type: application/json\' \\\n    -d \'{"msgtype": "markdown","markdown": {"title": "\'$Ding_Webhook_Keyword\'","text": "\'"$*"\'"},"at": {"isAtAll": true}}\')\n  if [ $(echo $send_result | jq -r \'.errcode\') -eq 0 ]; then\n    echo "发送成功:  \\n" $*" "\n  else\n    echo "发送失败:  \\n" $*"\\n"\n    echo $send_result\n  fi\n}\nfunction kfdd() {\n  sendDingDing "$(date +"%F %H:%M:%S") "$*" "\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 12、清理保持备份个数\n\n清理指定目录北备份文件，并保持指定个数。\n\nfunction clean_backups (){\n  if [ $# -gt 2 ];then\n    echo "参数错误"\n    exit 1\n  else\n    BAK_HOME=$1\n    keepNum=$2\n    fileNum=$(find ${BAK_HOME} -maxdepth 1 -type d -ctime -1 ! -path $BAK_HOME |wc -l)\n    if [ $fileNum -gt $keepNum ] ;then\n      find ${BAK_HOME} -maxdepth 1 -type d -ctime -1 ! -path $BAK_HOME | head -n `expr $fileNum - $keepNum` | xargs rm -rf\n    fi\n  fi\n}\n# 调用\nclean_backups "./backups" 2 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n----------------------------------------\n\n原文链接：常用shell脚本 | Gaoyufu \'s blog',normalizedContent:'# 常用shell脚本\n\n#!/bin/bash\nresponse=$(curl -sl -o /dev/null -w %{http_code} https://baidu.com)\nif [[ $response -ge 200 && $response -le 299 ]] ;then \n    echo \'check point success\'\nelse \n    echo \'check point fail\'\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 2、读取文件中的配置到变量中\n\n#!/bin/bash\n# 配置文件中的配置项格式为key1=value1，一行一个配置项\nwhile read line;do\n    eval "$line"\ndone < /etc/openvpn/server/smtp.conf\necho $key1\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3、根据 console 输入条件执行\n\n#!/bin/bash\necho "选择以下功能:"\necho "   0) 功能0"\necho "   1) 功能1"\necho "   2) 功能2"\necho "   3) 功能3"\necho "   4) 功能4"\nread -p "功能选项[4]: " option\nuntil [[ -z "$option" || "$option" =~ ^[0-4]$ ]]; do\n    read -p "$option为无效的选项，请重新输入功能选项: " option\ndone\ncase "$option" in\n    0) \n        echo "功能0已执行!" \n    ;;\n        1)\n            echo "功能1已执行!" \n      exit\n    ;;\n        2)\n            echo "功能2已执行!" \n            exit\n        ;;\n        3)\n            echo "功能3已执行!"  \n            exit\n        ;;\n        # 默认选项\n        4|"")\n            echo "功能4已执行!" \n            exit\n        ;;\nesac\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n# 4、将指定输出内容写入文件\n\n{\necho "hahh"\necho "lalal"\n} > /tmp/test\n\n\n1\n2\n3\n4\n\n\n\n# 5、判断变量是否存在或为空\n\nif [ -z ${var+x} ]; then \n    echo "var is unset"; \nelse \n    echo "var is set to \'$var\'"; \nfi\n\n\n1\n2\n3\n4\n5\n\n\n参考：https://stackoverflow.com/questions/3601515/how-to-check-if-a-variable-is-set-in-bash\n\n\n#\n\n\n# 6、换算秒为分钟、小时\n\n#!/bin/bash\n\na=60100\nswap_seconds ()\n{\n    sec=$1\n    (( sec < 60 )) && echo -e "持续时间: $sec秒\\c"\n    (( sec >= 60 && sec < 3600 )) && echo -e "持续时间: $(( sec / 60 ))分钟$(( sec % 60 ))秒\\c"\n    (( sec > 3600 )) && echo -e "持续时间: $(( sec / 3600 ))小时$(( (sec % 3600) / 60 ))分钟$(( (sec % 3600) % 60 ))秒\\c"\n}\n\nb=`swap_seconds $a`\necho $b\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n输出\n\n持续时间: 16小时41分钟40秒\n\n\n1\n\n\n\n# 7、脚本命令行参数的传递与判断\n\n#!/bin/bash\n\nmain() {\n    if [[ $# == 1 ]]; then\n        case $1 in\n        "-h")\n            echo "脚本使用方法: "\n            echo "  ./gitlab-pipeline.sh git仓库名1 git仓库名2 ... tag名(tag命名规则为: *-v加数字)"\n            exit\n            ;;\n        "--help")\n            echo "脚本使用方法: "\n            echo "  ./gitlab-pipeline.sh git仓库名1 git仓库名2 ... tag名(tag命名规则为: *-v加数字)"\n            exit\n            ;;\n        *)\n            echo "参数错误！"\n            exit\n            ;;\n        esac\n    fi\n}\n\nmain $*\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 8、检测 docker 容器的启动状态\n\n# 第一步：判断镜像是否存在\nif [ `docker images --format {{.repository}}:{{.tag}} |grep -fx 192.168.1.7:32772/applications/$ci_project_name:${ci_commit_short_sha};echo $?` -eq 0 ];then\n   docker pull 192.168.1.7:32772/applications/$ci_project_name:$ci_commit_short_sha ;\nfi;\n# 第二步：判断是否已经有重名的容器在运行或者处在其他状态。重名的，先删掉，在启动；不重名的直接启动\nif [ `docker ps -a --format {{.names}} |grep -fx $ci_project_name > /dev/null ;echo $?` -eq 0 ] ;then\n   docker rm -f $ci_project_name ;\n   docker run -d --name $ci_project_name -p 30088:8080 192.168.1.7:32772/applications/$ci_project_name:$ci_commit_short_sha ;\nelse\n   docker run -d --name $ci_project_name -p 30088:8080 192.168.1.7:32772/applications/$ci_project_name:$ci_commit_short_sha ;\nfi;\n# 第三步：循环五次判断容器的监控检测是否处于什么状态。健康状态就直接退出循环，不健康显示健康检查日志，正在启动的直接显示。处于其他状态的直接显示状态\nn=0;\nwhile true ;do\n  container_state=`docker inspect --format=\'{{json .state.health.status}}\' $ci_project_name`;\n  case $container_state in\n    \'"starting"\' )\n      echo "应用容器正在启动！";\n    ;;\n    \'"healthy"\' )\n      echo "应用容器已启动，状态健康！";\n    break;\n    ;;\n    \'"unhealthy"\' )\n      echo "应用容器健康检测失败！";\n      docker inspect --format=\'{{json .state.health.log}}\' $ci_project_name;\n    ;;\n    * )\n      echo "未知的状态:$container_state";\n    ;;\n  esac;\n  sleep 1s;\n  n=$(($n+1));\n  if [ $n -eq 5 ];\n     then break ;\n  fi ;\ndone\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n\n# 9、检查常见系统命令是否安装\n\ncheck_command() {\n    if ! command -v ifconfig >/dev/null 2>&1; then\n        echo -e "\\033[31mifconfig命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y net-tools >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y net-tools >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y net-tools >/dev/null 2>&1\n        fi\n    elif ! command -v ip >/dev/null 2>&1; then\n        echo -e "\\033[31mip命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y iproute2 >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y iproute2 >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y iproute2 >/dev/null 2>&1\n        fi\n    elif ! command -v curl >/dev/null 2>&1; then\n        echo -e "\\033[31mcurl命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y curl >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y curl >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y curl >/dev/null 2>&1\n        fi\n    elif ! command -v wget >/dev/null 2>&1; then\n        echo -e "\\033[31mawk命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y wget >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y wget >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y wget >/dev/null 2>&1\n        fi\n    elif ! command -v tail >/dev/null 2>&1; then\n        echo -e "\\033[31mcoreutils命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y coreutils >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y coreutils >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y coreutils >/dev/null 2>&1\n        fi\n    elif ! command -v sed >/dev/null 2>&1; then\n        echo -e "\\033[31msed命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y sed >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y sed >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y sed >/dev/null 2>&1\n        fi\n    elif ! command -v grep >/dev/null 2>&1; then\n        echo -e "\\033[31mgrep命令不存在，正在下载安装！\\033[0m"\n        if os="ubuntu"; then\n            apt install -y grep >/dev/null 2>&1\n        elif os="centos"; then\n            yum install -y grep >/dev/null 2>&1\n        elif os="fedora"; then\n            dnf install -y grep >/dev/null 2>&1\n        fi\n    fi\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n\n\n\n# 10、检查系统网络\n\ncheck_network() {\n    check_command\n    ping -c 4 114.114.114.114 >/dev/null\n    if [ ! $? -eq 0 ]; then\n        echo -e "\\033[31mip地址无法ping通，请检查网络连接！！！\\033[0m"\n        exit\n    fi\n    ping -c 4 www.baidu.com >/dev/null\n    if [ ! $? -eq 0 ]; then\n        echo -e "\\033[31m域名无法ping通，请检查dns配置！！！\\033[0m"\n        exit\n    fi\n    curl -s --retry 2 --connect-timeout 2 www.baidu.com >/dev/null\n    if [ ! $? -eq 0 ]; then\n        echo -e "\\033[31m域名无法ping通，请检查dns配置！！！\\033[0m"\n        exit\n    fi\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 11、发送钉钉通知\n\nfunction senddingding(){\n  ding_webhook_token=\'钉钉机器人的webhook token\'\n  ding_webhook_keyword="可用作消息标题"\n  send_result=$(curl -ss \\\n    -xpost https://oapi.dingtalk.com/robot/send?access_token="$ding_webhook_token" \\\n    -h \'content-type: application/json\' \\\n    -d \'{"msgtype": "markdown","markdown": {"title": "\'$ding_webhook_keyword\'","text": "\'"$*"\'"},"at": {"isatall": true}}\')\n  if [ $(echo $send_result | jq -r \'.errcode\') -eq 0 ]; then\n    echo "发送成功:  \\n" $*" "\n  else\n    echo "发送失败:  \\n" $*"\\n"\n    echo $send_result\n  fi\n}\n\nsenddingding  "$(date +"%f %h:%m:%s") 需要告警的消息"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n.zshrc\n\nfunction senddingding() {\n  setopt no_nomatch\n  ding_webhook_token=\'钉钉机器人的webhook token\'\n  ding_webhook_keyword=\'可用作消息标题\'\n  send_result=$(curl -ss \\\n    -xpost https://oapi.dingtalk.com/robot/send?access_token="$ding_webhook_token" \\\n    -h \'content-type: application/json\' \\\n    -d \'{"msgtype": "markdown","markdown": {"title": "\'$ding_webhook_keyword\'","text": "\'"$*"\'"},"at": {"isatall": true}}\')\n  if [ $(echo $send_result | jq -r \'.errcode\') -eq 0 ]; then\n    echo "发送成功:  \\n" $*" "\n  else\n    echo "发送失败:  \\n" $*"\\n"\n    echo $send_result\n  fi\n}\nfunction kfdd() {\n  senddingding "$(date +"%f %h:%m:%s") "$*" "\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 12、清理保持备份个数\n\n清理指定目录北备份文件，并保持指定个数。\n\nfunction clean_backups (){\n  if [ $# -gt 2 ];then\n    echo "参数错误"\n    exit 1\n  else\n    bak_home=$1\n    keepnum=$2\n    filenum=$(find ${bak_home} -maxdepth 1 -type d -ctime -1 ! -path $bak_home |wc -l)\n    if [ $filenum -gt $keepnum ] ;then\n      find ${bak_home} -maxdepth 1 -type d -ctime -1 ! -path $bak_home | head -n `expr $filenum - $keepnum` | xargs rm -rf\n    fi\n  fi\n}\n# 调用\nclean_backups "./backups" 2 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n----------------------------------------\n\n原文链接：常用shell脚本 | gaoyufu \'s blog',charsets:{cjk:!0}},{title:"字符串的截取拼接",frontmatter:{title:"字符串的截取拼接",date:"2023-02-24T17:28:59.000Z",permalink:"/pages/f5c376/",categories:["编程","shell"],tags:[null],readingShow:"top",description:"一、字符串的截取",meta:[{name:"twitter:title",content:"字符串的截取拼接"},{name:"twitter:description",content:"一、字符串的截取"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/09.%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%88%AA%E5%8F%96%E6%8B%BC%E6%8E%A5.html"},{property:"og:type",content:"article"},{property:"og:title",content:"字符串的截取拼接"},{property:"og:description",content:"一、字符串的截取"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/09.%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%88%AA%E5%8F%96%E6%8B%BC%E6%8E%A5.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-24T17:28:59.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"字符串的截取拼接"},{itemprop:"description",content:"一、字符串的截取"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/02.shell/09.%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%88%AA%E5%8F%96%E6%8B%BC%E6%8E%A5.html",relativePath:"03.编程/02.shell/09.字符串的截取拼接.md",key:"v-1ae891a8",path:"/pages/f5c376/",headers:[{level:2,title:"1、# 号从左边开始，删除第一次匹配到条件的左边字符，保留右边字符",slug:"_1、-号从左边开始-删除第一次匹配到条件的左边字符-保留右边字符",normalizedTitle:"1、# 号从左边开始，删除第一次匹配到条件的左边字符，保留右边字符",charIndex:1507},{level:2,title:"2、## 号从左边开始，删除最后一次匹配到条件的左边字符，保留右边字符",slug:"_2、-号从左边开始-删除最后一次匹配到条件的左边字符-保留右边字符",normalizedTitle:"2、## 号从左边开始，删除最后一次匹配到条件的左边字符，保留右边字符",charIndex:1678},{level:2,title:"3、%号从右边开始，删除第一次匹配到条件的右边内容，保留左边字符（不保留匹配条件）",slug:"_3、-号从右边开始-删除第一次匹配到条件的右边内容-保留左边字符-不保留匹配条件",normalizedTitle:"3、%号从右边开始，删除第一次匹配到条件的右边内容，保留左边字符（不保留匹配条件）",charIndex:1842},{level:2,title:"4、%% 号从右边开始，删除最后一次匹配到条件的右边内容，保留左边字符（不保留匹配条件）",slug:"_4、-号从右边开始-删除最后一次匹配到条件的右边内容-保留左边字符-不保留匹配条件",normalizedTitle:"4、%% 号从右边开始，删除最后一次匹配到条件的右边内容，保留左边字符（不保留匹配条件）",charIndex:2004},{level:2,title:"5、从左边第几个字符开始，及字符的个数",slug:"_5、从左边第几个字符开始-及字符的个数",normalizedTitle:"5、从左边第几个字符开始，及字符的个数",charIndex:2160},{level:2,title:"6、从左边第几个字符开始，一直到结束",slug:"_6、从左边第几个字符开始-一直到结束",normalizedTitle:"6、从左边第几个字符开始，一直到结束",charIndex:2287},{level:2,title:"7、从右边第几个字符开始，向右截取 length 个字符。",slug:"_7、从右边第几个字符开始-向右截取-length-个字符。",normalizedTitle:"7、从右边第几个字符开始，向右截取 length 个字符。",charIndex:2448},{level:2,title:"8、从右边第几个字符开始，一直到结束",slug:"_8、从右边第几个字符开始-一直到结束",normalizedTitle:"8、从右边第几个字符开始，一直到结束",charIndex:2587},{level:2,title:"9、截取字符串中的ip",slug:"_9、截取字符串中的ip",normalizedTitle:"9、截取字符串中的ip",charIndex:2716},{level:2,title:"10、提取字符串中的数字",slug:"_10、提取字符串中的数字",normalizedTitle:"10、提取字符串中的数字",charIndex:2905}],headersStr:"1、# 号从左边开始，删除第一次匹配到条件的左边字符，保留右边字符 2、## 号从左边开始，删除最后一次匹配到条件的左边字符，保留右边字符 3、%号从右边开始，删除第一次匹配到条件的右边内容，保留左边字符（不保留匹配条件） 4、%% 号从右边开始，删除最后一次匹配到条件的右边内容，保留左边字符（不保留匹配条件） 5、从左边第几个字符开始，及字符的个数 6、从左边第几个字符开始，一直到结束 7、从右边第几个字符开始，向右截取 length 个字符。 8、从右边第几个字符开始，一直到结束 9、截取字符串中的ip 10、提取字符串中的数字",content:'一、字符串的截取\n\n表达式                                       含义\n${#string}                                $string的字符个数\n${string:position}                        在$string中, 从位置$position开始提取子串\n${string:position:length}                 在$string中, 从位置position开始提取长度为length的子串\n${string#substring}                       从 变量$string的开头, 删除最短匹配$substring的子串\n${string##substring}                      从 变量$string的开头, 删除最长匹配$substring的子串\n${string%substring}                       从 变量$string的结尾, 删除最短匹配$substring的子串\n${string%%substring}                      从 变量$string的结尾, 删除最长匹配$substring的子串\n${string/substring/replacement}           使用$replacement, 来代替第一个匹配的$substring\n${string//substring/replacement}          使用$replacement, 代替所有匹配的$substring\n${string/#substring/replacement}          如果$string的前缀匹配substring, 那么就用replacement来代替匹配到的$substring\n${string/%substring/replacement}          如果$string的后缀匹配substring, 那么就用replacement来代替匹配到的$substring\nexpr match "$string" \'$substring\'         匹配$string开头的$substring* 的长度\nexpr "$string" : \'$substring\'             匹 配$string开头的$substring* 的长度\nexpr index "$string" $substring           在$string中匹配到的$substring的第一个字符出现的位置\nexpr substr $string $position $length     在$string中 从位置position开始提取长度为length的子串\nexpr match "$string" \'\\($substring\\)\'     从$string的 开头位置提取$substring*\nexpr "$string" : \'\\($substring\\)\'         从$string的 开头位置提取$substring*\nexpr match "$string" \'.*\\($substring\\)\'   从$string的 结尾提取$substring*\nexpr "$string" : \'.*\\($substring\\)\'       从$string的 结尾提取$substring*\n\n\n# 1、# 号从左边开始，删除第一次匹配到条件的左边字符，保留右边字符\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"\nb=${a#*/};echo $b\n# 结果：openshift/origin-metrics-cassandra:v3.9\n\n\n1\n2\n3\n\n\n\n# 2、## 号从左边开始，删除最后一次匹配到条件的左边字符，保留右边字符\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"\nb=${a##*/};echo $b\n# 结果：origin-metrics-cassandra:v3.9\n\n\n1\n2\n3\n\n\n\n# 3、%号从右边开始，删除第一次匹配到条件的右边内容，保留左边字符（不保留匹配条件）\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a%/*};echo $b\n# 结果：docker.io/openshift\n\n\n1\n2\n3\n\n\n\n# 4、%% 号从右边开始，删除最后一次匹配到条件的右边内容，保留左边字符（不保留匹配条件）\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a%%/*};echo $b\n# 结果：docker.io\n\n\n1\n2\n3\n\n\n\n# 5、从左边第几个字符开始，及字符的个数\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a:0:5};echo $b\n# 结果：docke\n\n\n1\n2\n3\n\n\n\n# 6、从左边第几个字符开始，一直到结束\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a:7};echo $b\n# 结果：io/openshift/origin-metrics-cassandra:v3.9\n\n\n1\n2\n3\n\n\n\n# 7、从右边第几个字符开始，向右截取 length 个字符。\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a:0-8:5};echo $b\n# 结果：dra:v\n\n\n1\n2\n3\n\n\n\n# 8、从右边第几个字符开始，一直到结束\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a:0-8};echo $b\n# 结果：dra:v3.9\n\n\n1\n2\n3\n\n\n\n# 9、截取字符串中的ip\n\n# 样本: a="当前 IP：123.456.789.172  来自于：中国 上海 上海  联通"\nb=${a//[!0-9.]/};echo $b\n\n或者\necho $a | grep -o -E "[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]*"\n\n# 结果：123.456.789.172\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 10、提取字符串中的数字\n\necho "test-v1.1.0" | tr -cd \'[0-9.]\'    # 输出1.1.0\naa="test-v1.1.0" | echo ${aa//[!0-9.]/} # 输出1.1.0\n\n\n1\n2\n',normalizedContent:'一、字符串的截取\n\n表达式                                       含义\n${#string}                                $string的字符个数\n${string:position}                        在$string中, 从位置$position开始提取子串\n${string:position:length}                 在$string中, 从位置position开始提取长度为length的子串\n${string#substring}                       从 变量$string的开头, 删除最短匹配$substring的子串\n${string##substring}                      从 变量$string的开头, 删除最长匹配$substring的子串\n${string%substring}                       从 变量$string的结尾, 删除最短匹配$substring的子串\n${string%%substring}                      从 变量$string的结尾, 删除最长匹配$substring的子串\n${string/substring/replacement}           使用$replacement, 来代替第一个匹配的$substring\n${string//substring/replacement}          使用$replacement, 代替所有匹配的$substring\n${string/#substring/replacement}          如果$string的前缀匹配substring, 那么就用replacement来代替匹配到的$substring\n${string/%substring/replacement}          如果$string的后缀匹配substring, 那么就用replacement来代替匹配到的$substring\nexpr match "$string" \'$substring\'         匹配$string开头的$substring* 的长度\nexpr "$string" : \'$substring\'             匹 配$string开头的$substring* 的长度\nexpr index "$string" $substring           在$string中匹配到的$substring的第一个字符出现的位置\nexpr substr $string $position $length     在$string中 从位置position开始提取长度为length的子串\nexpr match "$string" \'\\($substring\\)\'     从$string的 开头位置提取$substring*\nexpr "$string" : \'\\($substring\\)\'         从$string的 开头位置提取$substring*\nexpr match "$string" \'.*\\($substring\\)\'   从$string的 结尾提取$substring*\nexpr "$string" : \'.*\\($substring\\)\'       从$string的 结尾提取$substring*\n\n\n# 1、# 号从左边开始，删除第一次匹配到条件的左边字符，保留右边字符\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"\nb=${a#*/};echo $b\n# 结果：openshift/origin-metrics-cassandra:v3.9\n\n\n1\n2\n3\n\n\n\n# 2、## 号从左边开始，删除最后一次匹配到条件的左边字符，保留右边字符\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"\nb=${a##*/};echo $b\n# 结果：origin-metrics-cassandra:v3.9\n\n\n1\n2\n3\n\n\n\n# 3、%号从右边开始，删除第一次匹配到条件的右边内容，保留左边字符（不保留匹配条件）\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a%/*};echo $b\n# 结果：docker.io/openshift\n\n\n1\n2\n3\n\n\n\n# 4、%% 号从右边开始，删除最后一次匹配到条件的右边内容，保留左边字符（不保留匹配条件）\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a%%/*};echo $b\n# 结果：docker.io\n\n\n1\n2\n3\n\n\n\n# 5、从左边第几个字符开始，及字符的个数\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a:0:5};echo $b\n# 结果：docke\n\n\n1\n2\n3\n\n\n\n# 6、从左边第几个字符开始，一直到结束\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a:7};echo $b\n# 结果：io/openshift/origin-metrics-cassandra:v3.9\n\n\n1\n2\n3\n\n\n\n# 7、从右边第几个字符开始，向右截取 length 个字符。\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a:0-8:5};echo $b\n# 结果：dra:v\n\n\n1\n2\n3\n\n\n\n# 8、从右边第几个字符开始，一直到结束\n\n# 样本: a="docker.io/openshift/origin-metrics-cassandra:v3.9"   \nb=${a:0-8};echo $b\n# 结果：dra:v3.9\n\n\n1\n2\n3\n\n\n\n# 9、截取字符串中的ip\n\n# 样本: a="当前 ip：123.456.789.172  来自于：中国 上海 上海  联通"\nb=${a//[!0-9.]/};echo $b\n\n或者\necho $a | grep -o -e "[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]*"\n\n# 结果：123.456.789.172\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 10、提取字符串中的数字\n\necho "test-v1.1.0" | tr -cd \'[0-9.]\'    # 输出1.1.0\naa="test-v1.1.0" | echo ${aa//[!0-9.]/} # 输出1.1.0\n\n\n1\n2\n',charsets:{cjk:!0}},{title:"shell基础",frontmatter:{title:"shell基础",date:"2023-02-24T17:31:50.000Z",permalink:"/pages/665355/",categories:["编程","shell"],tags:[null],readingShow:"top",description:'计算机只能认识（识别）机器语言(0和1)，如（11000000 这种）。但是，我们的程序猿们不能直接去写01这样的代码，所以，要想将程序猿所开发的代码在计算机上运行，就必须找"人"（工具）来翻译成机器语言，这个"人"(工具)就是我们常常所说的编译器或者解释器。',meta:[{name:"image",content:"http://pic.zzppjj.top/LightPicture/2023/02/87fe91cf99897c92.png"},{name:"twitter:title",content:"shell基础"},{name:"twitter:description",content:'计算机只能认识（识别）机器语言(0和1)，如（11000000 这种）。但是，我们的程序猿们不能直接去写01这样的代码，所以，要想将程序猿所开发的代码在计算机上运行，就必须找"人"（工具）来翻译成机器语言，这个"人"(工具)就是我们常常所说的编译器或者解释器。'},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://pic.zzppjj.top/LightPicture/2023/02/87fe91cf99897c92.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/10.shell%E5%9F%BA%E7%A1%80.html"},{property:"og:type",content:"article"},{property:"og:title",content:"shell基础"},{property:"og:description",content:'计算机只能认识（识别）机器语言(0和1)，如（11000000 这种）。但是，我们的程序猿们不能直接去写01这样的代码，所以，要想将程序猿所开发的代码在计算机上运行，就必须找"人"（工具）来翻译成机器语言，这个"人"(工具)就是我们常常所说的编译器或者解释器。'},{property:"og:image",content:"http://pic.zzppjj.top/LightPicture/2023/02/87fe91cf99897c92.png"},{property:"og:url",content:"https://blog.zzppjj.top/03.%E7%BC%96%E7%A8%8B/02.shell/10.shell%E5%9F%BA%E7%A1%80.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-24T17:31:50.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"shell基础"},{itemprop:"description",content:'计算机只能认识（识别）机器语言(0和1)，如（11000000 这种）。但是，我们的程序猿们不能直接去写01这样的代码，所以，要想将程序猿所开发的代码在计算机上运行，就必须找"人"（工具）来翻译成机器语言，这个"人"(工具)就是我们常常所说的编译器或者解释器。'},{itemprop:"image",content:"http://pic.zzppjj.top/LightPicture/2023/02/87fe91cf99897c92.png"}]},regularPath:"/03.%E7%BC%96%E7%A8%8B/02.shell/10.shell%E5%9F%BA%E7%A1%80.html",relativePath:"03.编程/02.shell/10.shell基础.md",key:"v-0aaf2f47",path:"/pages/665355/",headers:[{level:2,title:"1. 编程语言分类",slug:"_1-编程语言分类",normalizedTitle:"1. 编程语言分类",charIndex:148},{level:2,title:"2. shell简介",slug:"_2-shell简介",normalizedTitle:"2. shell简介",charIndex:500},{level:2,title:"3. shell脚本",slug:"_3-shell脚本",normalizedTitle:"3. shell脚本",charIndex:889},{level:3,title:"1、什么是shell脚本？",slug:"_1、什么是shell脚本",normalizedTitle:"1、什么是shell脚本？",charIndex:904},{level:3,title:"2、 什么时候用到脚本?",slug:"_2、-什么时候用到脚本",normalizedTitle:"2、 什么时候用到脚本?",charIndex:1024},{level:3,title:"3、 shell脚本能干啥?",slug:"_3、-shell脚本能干啥",normalizedTitle:"3、 shell脚本能干啥?",charIndex:1086},{level:3,title:"4、如何学习shell脚本？",slug:"_4、如何学习shell脚本",normalizedTitle:"4、如何学习shell脚本？",charIndex:1216},{level:3,title:"5、 学习shell脚本的秘诀",slug:"_5、-学习shell脚本的秘诀",normalizedTitle:"5、 学习shell脚本的秘诀",charIndex:1319},{level:3,title:"6、 shell脚本的基本写法",slug:"_6、-shell脚本的基本写法",normalizedTitle:"6、 shell脚本的基本写法",charIndex:1366},{level:3,title:"7、shell脚本的执行方法",slug:"_7、shell脚本的执行方法",normalizedTitle:"7、shell脚本的执行方法",charIndex:1742},{level:2,title:"二、变量的定义",slug:"二、变量的定义",normalizedTitle:"二、变量的定义",charIndex:3263},{level:2,title:"1. 变量是什么？",slug:"_1-变量是什么",normalizedTitle:"1. 变量是什么？",charIndex:3275},{level:2,title:"2. 什么时候需要定义变量？",slug:"_2-什么时候需要定义变量",normalizedTitle:"2. 什么时候需要定义变量？",charIndex:3322},{level:2,title:"3.变量如何定义？",slug:"_3-变量如何定义",normalizedTitle:"3.变量如何定义？",charIndex:3462},{level:2,title:"4. 变量的定义规则",slug:"_4-变量的定义规则",normalizedTitle:"4. 变量的定义规则",charIndex:3826},{level:3,title:"1、变量名区分大小写",slug:"_1、变量名区分大小写",normalizedTitle:"1、变量名区分大小写",charIndex:3875},{level:3,title:"2、 变量名不能有特殊符号",slug:"_2、-变量名不能有特殊符号",normalizedTitle:"2、 变量名不能有特殊符号",charIndex:3958},{level:3,title:"3、 变量名不能以数字开头",slug:"_3、-变量名不能以数字开头",normalizedTitle:"3、 变量名不能以数字开头",charIndex:4259},{level:3,title:"4、等号两边不能有任何空格",slug:"_4、等号两边不能有任何空格",normalizedTitle:"4、等号两边不能有任何空格",charIndex:4376},{level:3,title:"5、 变量名尽量做到见名知意",slug:"_5、-变量名尽量做到见名知意",normalizedTitle:"5、 变量名尽量做到见名知意",charIndex:4552},{level:2,title:"5. 变量的定义方式有哪些？",slug:"_5-变量的定义方式有哪些",normalizedTitle:"5. 变量的定义方式有哪些？",charIndex:4690},{level:3,title:"1、基本方式",slug:"_1、基本方式",normalizedTitle:"1、基本方式",charIndex:4709},{level:3,title:"2、 命令执行结果赋值给变量",slug:"_2、-命令执行结果赋值给变量",normalizedTitle:"2、 命令执行结果赋值给变量",charIndex:4905},{level:3,title:"3、 交互式定义变量(read)",slug:"_3、-交互式定义变量-read",normalizedTitle:"3、 交互式定义变量(read)",charIndex:5024},{level:3,title:"4、定义有类型的变量(declare)",slug:"_4、定义有类型的变量-declare",normalizedTitle:"4、定义有类型的变量(declare)",charIndex:5488},{level:2,title:"6. 变量的分类",slug:"_6-变量的分类",normalizedTitle:"6. 变量的分类",charIndex:6018},{level:3,title:"1、本地变量",slug:"_1、本地变量",normalizedTitle:"1、本地变量",charIndex:6031},{level:3,title:"2、 环境变量",slug:"_2、-环境变量",normalizedTitle:"2、 环境变量",charIndex:7121},{level:3,title:"3、 全局变量",slug:"_3、-全局变量",normalizedTitle:"3、 全局变量",charIndex:7528},{level:3,title:"4、系统变量",slug:"_4、系统变量",normalizedTitle:"4、系统变量",charIndex:8182},{level:2,title:"1. 四则运算符号",slug:"_1-四则运算符号",normalizedTitle:"1. 四则运算符号",charIndex:9167},{level:2,title:"2.了解i++和++i",slug:"_2-了解i-和-i",normalizedTitle:"2.了解i++和++i",charIndex:9468},{level:2,title:"1. 数组定义",slug:"_1-数组定义",normalizedTitle:"1. 数组定义",charIndex:9754},{level:3,title:"1、数组分类",slug:"_1、数组分类",normalizedTitle:"1、数组分类",charIndex:9766},{level:3,title:"2、 普通数组定义",slug:"_2、-普通数组定义",normalizedTitle:"2、 普通数组定义",charIndex:9835},{level:3,title:"3、 数组的读取",slug:"_3、-数组的读取",normalizedTitle:"3、 数组的读取",charIndex:10166},{level:3,title:"4、关联数组定义",slug:"_4、关联数组定义",normalizedTitle:"4、关联数组定义",charIndex:10466},{level:4,title:"1、首先声明关联数组",slug:"_1、首先声明关联数组",normalizedTitle:"1、首先声明关联数组",charIndex:10478},{level:4,title:"2、数组赋值",slug:"_2、数组赋值",normalizedTitle:"2、数组赋值",charIndex:10595},{level:2,title:"2. 其他变量定义",slug:"_2-其他变量定义",normalizedTitle:"2. 其他变量定义",charIndex:11508}],headersStr:"1. 编程语言分类 2. shell简介 3. shell脚本 1、什么是shell脚本？ 2、 什么时候用到脚本? 3、 shell脚本能干啥? 4、如何学习shell脚本？ 5、 学习shell脚本的秘诀 6、 shell脚本的基本写法 7、shell脚本的执行方法 二、变量的定义 1. 变量是什么？ 2. 什么时候需要定义变量？ 3.变量如何定义？ 4. 变量的定义规则 1、变量名区分大小写 2、 变量名不能有特殊符号 3、 变量名不能以数字开头 4、等号两边不能有任何空格 5、 变量名尽量做到见名知意 5. 变量的定义方式有哪些？ 1、基本方式 2、 命令执行结果赋值给变量 3、 交互式定义变量(read) 4、定义有类型的变量(declare) 6. 变量的分类 1、本地变量 2、 环境变量 3、 全局变量 4、系统变量 1. 四则运算符号 2.了解i++和++i 1. 数组定义 1、数组分类 2、 普通数组定义 3、 数组的读取 4、关联数组定义 1、首先声明关联数组 2、数组赋值 2. 其他变量定义",content:'# 一、SHELL介绍\n\n计算机只能认识（识别）机器语言(0和1)，如（11000000 这种）。但是，我们的程序猿们不能直接去写01这样的代码，所以，要想将程序猿所开发的代码在计算机上运行，就必须找"人"（工具）来翻译成机器语言，这个"人"(工具)就是我们常常所说的编译器或者解释器。\n\n\n# 1. 编程语言分类\n\n * 编译型语言：\n   \n   程序在执行之前需要一个专门的编译过程，把程序编译成为机器语言文件，运行时不需要重新翻译，直接使用编译的结果就行了。程序执行效率高，依赖编译器，跨平台性差些。如C、C++\n\n * 解释型语言：\n   \n   程序不需要编译，程序在运行时由解释器翻译成机器语言，每执行一次都要翻译一次。因此效率比较低。比如Python/JavaScript/ Perl /ruby/Shell等都是解释型语言。\n\n * 总结\n\n编译型语言比解释型语言速度较快，但是不如解释型语言跨平台性好。如果做底层开发或者大型应用程序或者操作系开发一般都用编译型语言；如果是一些服务器脚本及一些辅助的接口，对速度要求不高、对各个平台的兼容性有要求的话则一般都用解释型语言。\n\n\n# 2. shell简介\n\n总结：\n\n * shell就是人机交互的一个桥梁\n\n * shell的种类```bash\n\n# cat /etc/shells \n/bin/sh                #是bash的一个快捷方式\n/bin/bash            #bash是大多数Linux默认的shell，包含的功能几乎可以涵盖shell所有的功能\n/sbin/nologin        #表示非交互，不能登录操作系统\n/bin/dash            #小巧，高效，功能相比少一些\n\n/bin/csh            #具有C语言风格的一种shell，具有许多特性，但也有一些缺陷\n/bin/tcsh            #是csh的增强版，完全兼容csh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n思考：终端和shell有什么关系？\n\n\n\n\n# 3. shell脚本\n\n\n# 1、什么是shell脚本？\n\n * 一句话概括简单来说就是将需要执行的命令保存到文本中，按照顺序执行。它是解释型的，意味着不需要编译。\n\n * 准确叙述\n\n若干命令 + 脚本的基本格式 + 脚本特定语法 + 思想= shell脚本\n\n\n# 2、 什么时候用到脚本?\n\n重复化、复杂化的工作，通过把工作的命令写成脚本，以后仅仅需要执行脚本就能完成这些工作。\n\n\n# 3、 shell脚本能干啥?\n\n①自动化软件部署 LAMP/LNMP/Tomcat...\n\n②自动化管理 系统初始化脚本、批量更改主机密码、推送公钥...\n\n③自动化分析处理 统计网站访问量④自动化备份 数据库备份、日志转储...\n\n⑤自动化监控脚本\n\n\n# 4、如何学习shell脚本？\n\n 1. 尽可能记忆更多的命令(记忆命令使用功能和场景)2. 掌握脚本的标准的格式（指定魔法字节、使用标准的执行方式运行脚本）3. 必须熟悉掌握脚本的基本语法（重点)\n\n\n# 5、 学习shell脚本的秘诀\n\n多看（看懂）——>模仿（多练）——>多思考（多写）\n\n\n# 6、 shell脚本的基本写法\n\n1）脚本第一行，魔法字符**#!**指定解释器【必写】\n\n#!/bin/bash 表示以下内容使用bash解释器解析\n\n注意： 如果直接将解释器路径写死在脚本里，可能在某些系统就会存在找不到解释器的兼容性问题，所以可以使用:#!/bin/env 解释器 #!/bin/env bash\n\n2）脚本第二部分，注释(#号)说明，对脚本的基本信息进行描述【可选】\n\n#!/bin/env bash\n\n# 以下内容是对脚本的基本信息的描述\n# Name: 名字\n# Desc:描述describe\n# Path:存放路径\n# Usage:用法\n# Update:更新时间\n\n#下面就是脚本的具体内容\ncommands\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n3）脚本第三部分，脚本要实现的具体代码内容\n\n\n# 7、shell脚本的执行方法\n\n编写人生第一个shell脚本\n# cat example.sh\n#!/bin/env bash\n\n# 以下内容是对脚本的基本信息的描述\n# Name: example.sh\n# Desc: num1\n# Path: /tmp/shell/example.sh\n# Usage:/tmp/shell/example.sh\n# Update:2019-05-05\n\necho "hello Aluo"\necho "hello world"\n\n# pwd\n/tmp/shell\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n1、以路径方式执行，包括相对路径和绝对路径两种方式。\n\n1) 相对路径\n# cd /tmp/shell\n# chmod +x example.sh\n# ./example.sh\n\n2) 绝对路径\n# chmod +x /tmp/shell/example.sh\n# /tmp/shell/example.sh\n\n说明： ./ 代表在当前的工作目录下执行example.sh。如果不加上 ./，bash可能会因找不到相应example.sh而报错，因为目前的工作目录（/tmp/shell）可能不在执行程序默认的搜索路径之列,也就是说，不在环境变量     PASH的内容之中。查看PATH的内容可用echo $PASH命令。现在的/tmp/shell就不在环境变量PASH中的，所     以必须加上./才可执行。\n路径方式执行要求脚本文件有可执行权限，所以需要事先设定脚本文件的执行权限。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n2、直接在命令行指定解释器执行，使用 bash 或 sh 命令执行。\n\n1) bash\n# cd /tmp/shell\n# bash example.sh\n# bash -x example.sh \n+ echo \'hello Aluo\'\nhello Aluo\n+ echo \'hello world\'\nhello world\n\n----------------\n-x:一般用于排错，查看脚本的执行过程\n-n:用来查看脚本的语法是否有问题\n------------\n\n2) sh\n# cd /tmp/shell\n# sh example.sh\n\n说明：使用bash或sh命令执行可以不必事先设定脚本文件的执行权限，甚至都不用写shell文件中的第一行（指定bash路径），因为这种方式是将example.sh作为参数传给sh(bash)命令来执行的，这时不是example.sh自己来执行，而是被人家调用执行，所以不要执行权限。那么不用指定bash路径自然也好理解了。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n3、在当前 shell 环境中执行\n\n1) source\n# cd /tmp/shell\n# source example.sh     # 或source /tmp/shell/example.sh\n\n2) .\n# cd /tmp/shell\n# . example.sh          # 或. /tmp/shell/example.sh\n\n说明：前面两种方法执行shell脚本时都是在当前shell（称为父shell）中开启的一个子shell环境中去执行，shell脚本执行完后子shell环境随即关闭，然后又回到父shell中。而方法3则是在当前shell中执行的。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 二、变量的定义\n\n\n# 1. 变量是什么？\n\n一句话概括：变量是用来临时保存数据的，该数据是可以变化的数据。\n\n\n# 2. 什么时候需要定义变量？\n\n * 如果某个内容需要多次使用，并且在代码中重复出现，那么可以用变量代表该内容。这样在修改内容的时候，仅仅需要修改变量的值。\n * 在代码运作的过程中，可能会把某些命令的执行结果保存起来，后续代码需要使用这些结果，就可以直接使用这个变量。\n\n\n# 3.变量如何定义？\n\n变量名=变量值\n\n变量名：用来临时保存数据的\n\n变量值：就是临时的可变化的数据\n\n# A=hello            定义变量A\n# echo $A            调用变量A，要给钱的，不是人民币是美元"$"\nhello\n# echo ${A}            还可以这样调用，不管你的姿势多优雅，总之要给钱\nhello\n# A=world            因为是变量所以可以变，移情别恋是常事\n# echo $A            不管你是谁，只要调用就要给钱\nworld\n# unset A            不跟你玩了，取消变量\n# echo $A            从此，我单身了，你可以给我介绍任何人\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 4. 变量的定义规则\n\n虽然可以给变量（变量名）赋予任何值；但是，对于变量名也是要求的！\n\n\n# 1、变量名区分大小写\n\n# A=hello\n# a=world\n# echo $A\nhello\n# echo $a\nworld\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 2、 变量名不能有特殊符号\n\n# *A=hello\n-bash: *A=hello: command not found\n# ?A=hello\n-bash: ?A=hello: command not found\n# @A=hello\n-bash: @A=hello: command not found\n\n特别说明：对于有空格的字符串给变量赋值时，要用引号引起来\n# A=hello world\n-bash: world: command not found\n# A="hello world"\n# A=\'hello world\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 3、 变量名不能以数字开头\n\n# 1A=hello\n-bash: 1A=hello: command not found\n# A1=hello\n\n注意：不能以数字开头并不代表变量名中不能包含数字呦。\n\n\n1\n2\n3\n4\n5\n\n\n\n# 4、等号两边不能有任何空格\n\n# A =123\n-bash: A: command not found\n# A= 123\n-bash: 123: command not found\n# A = 123\n-bash: A: command not found\n# A=123\n# echo $A\n123\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 5、 变量名尽量做到见名知意\n\nNTP_IP=10.1.1.1\nDIR=/u01/app1\nTMP_FILE=/var/log/1.log\n...\n\n说明：一般变量名使用大写（小写也可以），不要同一个脚本中变量全是a,b,c等不容易阅读\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 5. 变量的定义方式有哪些？\n\n\n# 1、基本方式\n\n> 直接赋值给一个变量\n\n# A=1234567\n# echo $A\n1234567\n# echo ${A:2:4}        表示从A变量中第3个字符开始截取，截取4个字符\n3456\n\n说明：\n$变量名 和 ${变量名}的异同\n相同点：都可以调用变量\n不同点：${变量名}可以只截取变量的一部分，而$变量名不可以\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 2、 命令执行结果赋值给变量\n\n# B=`date +%F`\n# echo $B\n2019-04-16\n# C=$(uname -r)\n# echo $C\n2.6.32-696.el6.x86_64\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3、 交互式定义变量(read)\n\n**目的：**让用户自己给变量赋值，比较灵活。\n\n语法：read [选项] 变量名\n\n常见选项：\n\n选项   释义\n-p   定义提示用户的信息\n-n   定义字符数（限制变量值的长度）\n-s   不显示（不显示用户输入的内容）\n-t   定义超时时间，默认单位为秒（限制用户输入变量值的超时时间）\n\n举例说明：\n\n用法1：用户自己定义变量值\n# read name\nharry\n# echo $name\nharry\n# read -p "Input your name:" name\nInput your name:tom\n# echo $name\ntom\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n用法2：变量值来自文件\n\n# cat 1.txt \n10.1.1.1 255.255.255.0\n\n# read ip mask < 1.txt \n# echo $ip\n10.1.1.1\n# echo $mask\n255.255.255.0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 4、定义有类型的变量(declare)\n\n目的： 给变量做一些限制，固定变量的类型，比如：整型、只读\n\n用法：declare 选项 变量名=变量值\n\n常用选项：\n\n选项   释义              举例\n-i   将变量看成整数         declare -i A=123\n-r   定义只读变量          declare -r B=hello\n-a   定义普通数组；查看普通数组   \n-A   定义关联数组；查看关联数组   \n-x   将变量通过环境导出       declare -x AAA=123456 等于 export AAA=123456\n\n举例说明：\n\n# declare -i A=123\n# echo $A\n123\n# A=hello\n# echo $A\n0\n\n# declare -r B=hello\n# echo $B\nhello\n# B=world\n-bash: B: readonly variable\n# unset B\n-bash: unset: B: cannot unset: readonly variable\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 6. 变量的分类\n\n\n# 1、本地变量\n\n * 本地变量：当前用户自定义的变量。当前进程中有效，其他进程及当前进程的子进程无效。\n\n# ps                     # 当前bash的进程号为29105\n  PID TTY          TIME CMD\n29105 pts/2    00:00:00 bash\n29354 pts/2    00:00:00 ps\n# echo $$                # 当前bash的进程号为29105\n29105\n# name=aluo\n# echo $name\naluo\n# /bin/bash               # 再开启一个bash\n# ps\n  PID TTY          TIME CMD\n29105 pts/2    00:00:00 bash\n29355 pts/2    00:00:00 bash\n29366 pts/2    00:00:00 ps\n# ps auxf | grep bash     # 进程号为29355的bash是进程号为29105的bash的子进程\nroot     28884  0.0  0.1 115680  2204 pts/1    Ss   10:37   0:00  \nroot     29027  0.0  0.1 115680  2132 pts/1    S    12:32   0:00 \nroot     29039  0.0  0.1 115680  2156 pts/1    S+   12:32   0:00 \nroot     29105  0.0  0.1 115680  2260 pts/2    Ss   13:30   0:00 \nroot     29355  0.0  0.1 115680  2124 pts/2    S    15:15   0:00\nroot     29368  0.0  0.0 112812   972 pts/2    S+   15:15   0:00\n# echo $$                 # 打印当前所在进程的进程号\n29355\n# echo $name              # 子进程中无法调用父进程中定义的name变量\n\n# exit\nexit\n# echo $name\naluo\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 2、 环境变量\n\n * 环境变量\n   \n   当前进程有效，并且能够被子进程调用。\n   \n   * env查看当前用户的环境变量\n   * set查询当前用户的所有变量(临时变量与环境变量)\n   * export 变量名=变量值 或者 变量名=变量值；export 变量名 或者 declare -x 变量名=变量值\n\n# export A=hello        # 临时将一个本地变量（临时变量）变成环境变量\n# env|grep ^A\nA=hello\n\n永久生效：\nvim /etc/profile 或者 ~/.bashrc\nexport A=hello\n或者\nA=hello\nexport A\n\n说明：\n系统中有一个变量PATH，环境变量\nexport PATH=/usr/local/mysql/bin:$PATH\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 3、 全局变量\n\n * 全局变量：全局所有的用户和程序都能调用，且继承，新建的用户也默认能调用.\n * 解读相关配置文件\n\n文件名               说明                    备注\n~/.bashrc         当前用户的bash信息，用户登录时读取   局部。定义别名、umask、函数等\n~/.bash_profile   当前用户的环境变量信息，用户登录时读取   局部。\n~/.bash_logout    当前用户退出当前shell时最后读取    局部。定义用户退出时执行的程序等\n~/.bash_history   当前用户的历史命令             局部。history -w保存历史记录 history -c清空历史记录\n/etc/bashrc       全局的bash信息             全局。所有用户都生效\n/etc/profile      全局环境变量信息              全局。系统和所有用户都生效\n\n**说明：**以上文件修改后，都需要重新source让其生效或者退出重新登录。\n\n * 用户登录系统读取相关文件的顺序\n   1. /etc/profile\n   2. $HOME/.bash_profile\n   3. $HOME/.bashrc\n   4. /etc/bashrc\n   5. $HOME/.bash_logout\n\n> 说明：本地变量、环境变量、全局变量是根据变量的作用域来区分的。\n\n\n# 4、系统变量\n\n * 系统变量(内置bash中变量) ： shell本身已经固定好了它的名字和作用.\n\n内置变量       含义\n$?         上一条命令执行后返回的状态；状态值为0表示执行正常，非0表示执行异常或错误\n$0         当前执行的程序或脚本名\n$#         脚本后面接的参数的个数\n$*         脚本后面所有参数，参数当成一个整体输出，每一个变量参数之间以空格隔开\n$@         脚本后面所有参数，参数是独立的，也是全部输出\n1~9        脚本后面的位置参数，$1表示第1个位置参数，依次类推\n{10}~{n}   扩展位置参数,第10个位置变量必须用{}大括号括起来(2位数字以上扩起来)\n$$         当前所在进程的进程号，如echo $$\n$！         后台运行的最后一个进程号 (当前终端）\n!$         调用最后一条命令历史中的参数\n\n * 进一步了解位置参数$1~${n}\n\n#!/bin/bash\n#了解shell内置变量中的位置参数含义\necho "\\$0 = $0"\necho "\\$# = $#"\necho "\\$* = $*"\necho "\\$@ = $@"\necho "\\$1 = $1" \necho "\\$2 = $2" \necho "\\$3 = $3" \necho "\\$11 = ${11}" \necho "\\$12 = ${12}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * 进一步了解*和@的区别\n\n$*：表示将变量看成一个整体 $@：表示变量是独立的\n\n#!/bin/bash\nfor i in "$@"\ndo\necho $i\ndone\n\necho "======我是分割线======="\n\nfor i in "$*"\ndo\necho $i\ndone\n\n# bash 3.sh a b c\na\nb\nc\n======我是分割线=======\na b c\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 三、简单四则运算\n\n算术运算：默认情况下，shell就只能支持简单的整数运算\n\n运算内容：加(+)、减(-)、乘(*)、除(/)、求余数（%）、幂（**）\n\n\n# 1. 四则运算符号\n\n表达式      举例                           说明\n$(( ))   echo $((1+1))                \n$[ ]     echo $[10-5]                 \nexpr     expr 10 / 5                  expr程序中，数值与运算符之间需要空格隔开，乘(*)运算符需要用转义符( \\ )转义，expr不能做幂运算\nlet      n=1;let n+=1 等价于 let n=n+1   let n*=2 等价于let n=n*2，不能使用let n**=2\n\n\n# 2.了解i++和++i\n\n * 对变量的值的影响\n\n# i=1\n# let i++\n# echo $i\n2\n# j=1\n# let ++j\n# echo $j\n2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * 对表达式的值的影响\n\n# unset i j\n# i=1;j=1\n# let x=i++         先赋值，再运算\n# let y=++j         先运算，再赋值\n# echo $i\n2\n# echo $j\n2\n# echo $x\n1\n# echo $y\n2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 四、扩展补充\n\n\n# 1. 数组定义\n\n\n# 1、数组分类\n\n * 普通数组：只能使用整数作为数组索引(元素的下标)\n * 关联数组：可以使用字符串作为数组索引(元素的下标)\n\n\n# 2、 普通数组定义\n\n * 一次赋予一个值\n\n数组名[索引下标]=值\narray[0]=v1\narray[1]=v2\narray[2]=v3\narray[3]=v4\n\n\n1\n2\n3\n4\n5\n\n * 一次赋予多个值\n\n数组名=(值1 值2 值3 ...)\narray=(var1 var2 var3 var4)\n\narray1=(`cat /etc/passwd`)            将文件中每一行赋值给array1数组\narray2=(`ls /root`)\narray3=(harry amy jack "Miss Hou")\narray4=(1 2 3 4 "hello world" [10]=linux)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 3、 数组的读取\n\n${数组名[元素下标]}\n\necho ${array[0]}            获取数组里第一个元素\necho ${array[*]}            获取数组里的所有元素\necho ${#array[*]}            获取数组里所有元素个数\necho ${!array[@]}            获取数组元素的索引下标\necho ${array[@]:1:2}        访问指定的元素；1代表从下标为1的元素开始获取；2代表获取后面几个元素\n\n查看普通数组信息：\n# declare -a\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 4、关联数组定义\n\n# 1、首先声明关联数组\n\ndeclare -A asso_array1\n\ndeclare -A asso_array1\ndeclare -A asso_array2\ndeclare -A asso_array3\n\n\n1\n2\n3\n\n\n# 2、数组赋值\n\n * 一次赋一个值\n\n数组名[索引or下标]=变量值\n# asso_array1[linux]=one\n# asso_array1[java]=two\n# asso_array1[php]=three\n\n\n1\n2\n3\n4\n\n * 一次赋多个值\n\n# asso_array2=([name1]=harry [name2]=jack [name3]=amy [name4]="Miss Hou")\n\n\n1\n\n * 查看关联数组\n\n# declare -A\ndeclare -A asso_array1=\'([php]="three" [java]="two" [linux]="one" )\'\ndeclare -A asso_array2=\'([name3]="amy" [name2]="jack" [name1]="harry" [name4]="Miss Hou" )\'\n\n\n1\n2\n3\n\n * 获取关联数组值\n\n# echo ${asso_array1[linux]}\none\n# echo ${asso_array1[php]}\nthree\n# echo ${asso_array1[*]}\nthree two one\n# echo ${!asso_array1[*]}\nphp java linux\n# echo ${#asso_array1[*]}\n3\n# echo ${#asso_array2[*]}\n4\n# echo ${!asso_array2[*]}\nname3 name2 name1 name4\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n * 其他定义方式\n\n# declare -A books\n# let books[linux]++\n# declare -A|grep books\ndeclare -A books=\'([linux]="1" )\'\n# let books[linux]++\n# declare -A|grep books\ndeclare -A books=\'([linux]="2" )\'\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 2. 其他变量定义\n\n * 取出一个目录下的目录和文件：dirname和 basename\n\n# A=/root/Desktop/shell/mem.txt \n# echo $A\n/root/Desktop/shell/mem.txt\n# dirname $A   取出目录\n/root/Desktop/shell\n# basename $A  取出文件\nmem.txt\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * -变量"内容"的删除和替换\n\n一个“%”代表从右往左删除\n两个“%%”代表从右往左去掉最多\n一个“#”代表从左往右去掉删除\n两个“##”代表从左往右去掉最多\n\n举例说明：\n# url=www.taobao.com\n# echo ${#url}             获取变量的长度\n# echo ${url#*.}\n# echo ${url##*.}\n# echo ${url%.*}\n# echo ${url%%.*}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n * 以下了解\n\n替换：/ 和 //\n 1015  echo ${url/ao/AO}  用AO代替ao（从左往右第一个）\n 1017  echo ${url//ao/AO}   贪婪替换（替代所有）\n\n替代： - 和 :-  +和:+\n 1019  echo ${abc-123}\n 1020  abc=hello\n 1021  echo ${abc-444}\n 1022  echo $abc\n 1024  abc=\n 1025  echo ${abc-222}\n\n${变量名-新的变量值} 或者 ${变量名=新的变量值}\n变量没有被赋值：会使用“新的变量值“ 替代\n变量有被赋值（包括空值）： 不会被替代\n\n 1062  echo ${ABC:-123}\n 1063  ABC=HELLO\n 1064  echo ${ABC:-123}\n 1065  ABC=\n 1066  echo ${ABC:-123}\n\n${变量名:-新的变量值} 或者 ${变量名:=新的变量值}\n变量没有被赋值或者赋空值：会使用“新的变量值“ 替代\n变量有被赋值： 不会被替代\n\n 1116  echo ${abc=123}\n 1118  echo ${abc:=123}\n\n# unset abc\n# echo ${abc:+123}\n\n# abc=hello\n# echo ${abc:+123}\n123\n# abc=\n# echo ${abc:+123}\n\n${变量名+新的变量值}\n变量没有被赋值或者赋空值：不会使用“新的变量值“ 替代\n变量有被赋值： 会被替代\n# unset abc\n# echo ${abc+123}\n\n# abc=hello\n# echo ${abc+123}\n123\n# abc=\n# echo ${abc+123}\n123\n${变量名:+新的变量值}\n变量没有被赋值：不会使用“新的变量值“ 替代\n变量有被赋值（包括空值）： 会被替代\n\n# unset abc\n# echo ${abc?123}\n-bash: abc: 123\n\n# abc=hello\n# echo ${abc?123}\nhello\n# abc=\n# echo ${abc?123}\n\n${变量名?新的变量值}\n变量没有被赋值:提示错误信息\n变量被赋值（包括空值）：不会使用“新的变量值“ 替代\n\n# unset abc\n# echo ${abc:?123}\n-bash: abc: 123\n# abc=hello\n# echo ${abc:?123}\nhello\n# abc=\n# echo ${abc:?123}\n-bash: abc: 123\n\n${变量名:?新的变量值}\n变量没有被赋值或者赋空值时:提示错误信息\n变量被赋值：不会使用“新的变量值“ 替代\n\n说明：?主要是当变量没有赋值提示错误信息的，没有赋值功能\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n',normalizedContent:'# 一、shell介绍\n\n计算机只能认识（识别）机器语言(0和1)，如（11000000 这种）。但是，我们的程序猿们不能直接去写01这样的代码，所以，要想将程序猿所开发的代码在计算机上运行，就必须找"人"（工具）来翻译成机器语言，这个"人"(工具)就是我们常常所说的编译器或者解释器。\n\n\n# 1. 编程语言分类\n\n * 编译型语言：\n   \n   程序在执行之前需要一个专门的编译过程，把程序编译成为机器语言文件，运行时不需要重新翻译，直接使用编译的结果就行了。程序执行效率高，依赖编译器，跨平台性差些。如c、c++\n\n * 解释型语言：\n   \n   程序不需要编译，程序在运行时由解释器翻译成机器语言，每执行一次都要翻译一次。因此效率比较低。比如python/javascript/ perl /ruby/shell等都是解释型语言。\n\n * 总结\n\n编译型语言比解释型语言速度较快，但是不如解释型语言跨平台性好。如果做底层开发或者大型应用程序或者操作系开发一般都用编译型语言；如果是一些服务器脚本及一些辅助的接口，对速度要求不高、对各个平台的兼容性有要求的话则一般都用解释型语言。\n\n\n# 2. shell简介\n\n总结：\n\n * shell就是人机交互的一个桥梁\n\n * shell的种类```bash\n\n# cat /etc/shells \n/bin/sh                #是bash的一个快捷方式\n/bin/bash            #bash是大多数linux默认的shell，包含的功能几乎可以涵盖shell所有的功能\n/sbin/nologin        #表示非交互，不能登录操作系统\n/bin/dash            #小巧，高效，功能相比少一些\n\n/bin/csh            #具有c语言风格的一种shell，具有许多特性，但也有一些缺陷\n/bin/tcsh            #是csh的增强版，完全兼容csh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n思考：终端和shell有什么关系？\n\n\n\n\n# 3. shell脚本\n\n\n# 1、什么是shell脚本？\n\n * 一句话概括简单来说就是将需要执行的命令保存到文本中，按照顺序执行。它是解释型的，意味着不需要编译。\n\n * 准确叙述\n\n若干命令 + 脚本的基本格式 + 脚本特定语法 + 思想= shell脚本\n\n\n# 2、 什么时候用到脚本?\n\n重复化、复杂化的工作，通过把工作的命令写成脚本，以后仅仅需要执行脚本就能完成这些工作。\n\n\n# 3、 shell脚本能干啥?\n\n①自动化软件部署 lamp/lnmp/tomcat...\n\n②自动化管理 系统初始化脚本、批量更改主机密码、推送公钥...\n\n③自动化分析处理 统计网站访问量④自动化备份 数据库备份、日志转储...\n\n⑤自动化监控脚本\n\n\n# 4、如何学习shell脚本？\n\n 1. 尽可能记忆更多的命令(记忆命令使用功能和场景)2. 掌握脚本的标准的格式（指定魔法字节、使用标准的执行方式运行脚本）3. 必须熟悉掌握脚本的基本语法（重点)\n\n\n# 5、 学习shell脚本的秘诀\n\n多看（看懂）——>模仿（多练）——>多思考（多写）\n\n\n# 6、 shell脚本的基本写法\n\n1）脚本第一行，魔法字符**#!**指定解释器【必写】\n\n#!/bin/bash 表示以下内容使用bash解释器解析\n\n注意： 如果直接将解释器路径写死在脚本里，可能在某些系统就会存在找不到解释器的兼容性问题，所以可以使用:#!/bin/env 解释器 #!/bin/env bash\n\n2）脚本第二部分，注释(#号)说明，对脚本的基本信息进行描述【可选】\n\n#!/bin/env bash\n\n# 以下内容是对脚本的基本信息的描述\n# name: 名字\n# desc:描述describe\n# path:存放路径\n# usage:用法\n# update:更新时间\n\n#下面就是脚本的具体内容\ncommands\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n3）脚本第三部分，脚本要实现的具体代码内容\n\n\n# 7、shell脚本的执行方法\n\n编写人生第一个shell脚本\n# cat example.sh\n#!/bin/env bash\n\n# 以下内容是对脚本的基本信息的描述\n# name: example.sh\n# desc: num1\n# path: /tmp/shell/example.sh\n# usage:/tmp/shell/example.sh\n# update:2019-05-05\n\necho "hello aluo"\necho "hello world"\n\n# pwd\n/tmp/shell\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n1、以路径方式执行，包括相对路径和绝对路径两种方式。\n\n1) 相对路径\n# cd /tmp/shell\n# chmod +x example.sh\n# ./example.sh\n\n2) 绝对路径\n# chmod +x /tmp/shell/example.sh\n# /tmp/shell/example.sh\n\n说明： ./ 代表在当前的工作目录下执行example.sh。如果不加上 ./，bash可能会因找不到相应example.sh而报错，因为目前的工作目录（/tmp/shell）可能不在执行程序默认的搜索路径之列,也就是说，不在环境变量     pash的内容之中。查看path的内容可用echo $pash命令。现在的/tmp/shell就不在环境变量pash中的，所     以必须加上./才可执行。\n路径方式执行要求脚本文件有可执行权限，所以需要事先设定脚本文件的执行权限。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n2、直接在命令行指定解释器执行，使用 bash 或 sh 命令执行。\n\n1) bash\n# cd /tmp/shell\n# bash example.sh\n# bash -x example.sh \n+ echo \'hello aluo\'\nhello aluo\n+ echo \'hello world\'\nhello world\n\n----------------\n-x:一般用于排错，查看脚本的执行过程\n-n:用来查看脚本的语法是否有问题\n------------\n\n2) sh\n# cd /tmp/shell\n# sh example.sh\n\n说明：使用bash或sh命令执行可以不必事先设定脚本文件的执行权限，甚至都不用写shell文件中的第一行（指定bash路径），因为这种方式是将example.sh作为参数传给sh(bash)命令来执行的，这时不是example.sh自己来执行，而是被人家调用执行，所以不要执行权限。那么不用指定bash路径自然也好理解了。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n3、在当前 shell 环境中执行\n\n1) source\n# cd /tmp/shell\n# source example.sh     # 或source /tmp/shell/example.sh\n\n2) .\n# cd /tmp/shell\n# . example.sh          # 或. /tmp/shell/example.sh\n\n说明：前面两种方法执行shell脚本时都是在当前shell（称为父shell）中开启的一个子shell环境中去执行，shell脚本执行完后子shell环境随即关闭，然后又回到父shell中。而方法3则是在当前shell中执行的。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 二、变量的定义\n\n\n# 1. 变量是什么？\n\n一句话概括：变量是用来临时保存数据的，该数据是可以变化的数据。\n\n\n# 2. 什么时候需要定义变量？\n\n * 如果某个内容需要多次使用，并且在代码中重复出现，那么可以用变量代表该内容。这样在修改内容的时候，仅仅需要修改变量的值。\n * 在代码运作的过程中，可能会把某些命令的执行结果保存起来，后续代码需要使用这些结果，就可以直接使用这个变量。\n\n\n# 3.变量如何定义？\n\n变量名=变量值\n\n变量名：用来临时保存数据的\n\n变量值：就是临时的可变化的数据\n\n# a=hello            定义变量a\n# echo $a            调用变量a，要给钱的，不是人民币是美元"$"\nhello\n# echo ${a}            还可以这样调用，不管你的姿势多优雅，总之要给钱\nhello\n# a=world            因为是变量所以可以变，移情别恋是常事\n# echo $a            不管你是谁，只要调用就要给钱\nworld\n# unset a            不跟你玩了，取消变量\n# echo $a            从此，我单身了，你可以给我介绍任何人\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 4. 变量的定义规则\n\n虽然可以给变量（变量名）赋予任何值；但是，对于变量名也是要求的！\n\n\n# 1、变量名区分大小写\n\n# a=hello\n# a=world\n# echo $a\nhello\n# echo $a\nworld\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 2、 变量名不能有特殊符号\n\n# *a=hello\n-bash: *a=hello: command not found\n# ?a=hello\n-bash: ?a=hello: command not found\n# @a=hello\n-bash: @a=hello: command not found\n\n特别说明：对于有空格的字符串给变量赋值时，要用引号引起来\n# a=hello world\n-bash: world: command not found\n# a="hello world"\n# a=\'hello world\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 3、 变量名不能以数字开头\n\n# 1a=hello\n-bash: 1a=hello: command not found\n# a1=hello\n\n注意：不能以数字开头并不代表变量名中不能包含数字呦。\n\n\n1\n2\n3\n4\n5\n\n\n\n# 4、等号两边不能有任何空格\n\n# a =123\n-bash: a: command not found\n# a= 123\n-bash: 123: command not found\n# a = 123\n-bash: a: command not found\n# a=123\n# echo $a\n123\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 5、 变量名尽量做到见名知意\n\nntp_ip=10.1.1.1\ndir=/u01/app1\ntmp_file=/var/log/1.log\n...\n\n说明：一般变量名使用大写（小写也可以），不要同一个脚本中变量全是a,b,c等不容易阅读\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 5. 变量的定义方式有哪些？\n\n\n# 1、基本方式\n\n> 直接赋值给一个变量\n\n# a=1234567\n# echo $a\n1234567\n# echo ${a:2:4}        表示从a变量中第3个字符开始截取，截取4个字符\n3456\n\n说明：\n$变量名 和 ${变量名}的异同\n相同点：都可以调用变量\n不同点：${变量名}可以只截取变量的一部分，而$变量名不可以\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 2、 命令执行结果赋值给变量\n\n# b=`date +%f`\n# echo $b\n2019-04-16\n# c=$(uname -r)\n# echo $c\n2.6.32-696.el6.x86_64\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3、 交互式定义变量(read)\n\n**目的：**让用户自己给变量赋值，比较灵活。\n\n语法：read [选项] 变量名\n\n常见选项：\n\n选项   释义\n-p   定义提示用户的信息\n-n   定义字符数（限制变量值的长度）\n-s   不显示（不显示用户输入的内容）\n-t   定义超时时间，默认单位为秒（限制用户输入变量值的超时时间）\n\n举例说明：\n\n用法1：用户自己定义变量值\n# read name\nharry\n# echo $name\nharry\n# read -p "input your name:" name\ninput your name:tom\n# echo $name\ntom\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n用法2：变量值来自文件\n\n# cat 1.txt \n10.1.1.1 255.255.255.0\n\n# read ip mask < 1.txt \n# echo $ip\n10.1.1.1\n# echo $mask\n255.255.255.0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 4、定义有类型的变量(declare)\n\n目的： 给变量做一些限制，固定变量的类型，比如：整型、只读\n\n用法：declare 选项 变量名=变量值\n\n常用选项：\n\n选项   释义              举例\n-i   将变量看成整数         declare -i a=123\n-r   定义只读变量          declare -r b=hello\n-a   定义普通数组；查看普通数组   \n-a   定义关联数组；查看关联数组   \n-x   将变量通过环境导出       declare -x aaa=123456 等于 export aaa=123456\n\n举例说明：\n\n# declare -i a=123\n# echo $a\n123\n# a=hello\n# echo $a\n0\n\n# declare -r b=hello\n# echo $b\nhello\n# b=world\n-bash: b: readonly variable\n# unset b\n-bash: unset: b: cannot unset: readonly variable\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 6. 变量的分类\n\n\n# 1、本地变量\n\n * 本地变量：当前用户自定义的变量。当前进程中有效，其他进程及当前进程的子进程无效。\n\n# ps                     # 当前bash的进程号为29105\n  pid tty          time cmd\n29105 pts/2    00:00:00 bash\n29354 pts/2    00:00:00 ps\n# echo $$                # 当前bash的进程号为29105\n29105\n# name=aluo\n# echo $name\naluo\n# /bin/bash               # 再开启一个bash\n# ps\n  pid tty          time cmd\n29105 pts/2    00:00:00 bash\n29355 pts/2    00:00:00 bash\n29366 pts/2    00:00:00 ps\n# ps auxf | grep bash     # 进程号为29355的bash是进程号为29105的bash的子进程\nroot     28884  0.0  0.1 115680  2204 pts/1    ss   10:37   0:00  \nroot     29027  0.0  0.1 115680  2132 pts/1    s    12:32   0:00 \nroot     29039  0.0  0.1 115680  2156 pts/1    s+   12:32   0:00 \nroot     29105  0.0  0.1 115680  2260 pts/2    ss   13:30   0:00 \nroot     29355  0.0  0.1 115680  2124 pts/2    s    15:15   0:00\nroot     29368  0.0  0.0 112812   972 pts/2    s+   15:15   0:00\n# echo $$                 # 打印当前所在进程的进程号\n29355\n# echo $name              # 子进程中无法调用父进程中定义的name变量\n\n# exit\nexit\n# echo $name\naluo\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# 2、 环境变量\n\n * 环境变量\n   \n   当前进程有效，并且能够被子进程调用。\n   \n   * env查看当前用户的环境变量\n   * set查询当前用户的所有变量(临时变量与环境变量)\n   * export 变量名=变量值 或者 变量名=变量值；export 变量名 或者 declare -x 变量名=变量值\n\n# export a=hello        # 临时将一个本地变量（临时变量）变成环境变量\n# env|grep ^a\na=hello\n\n永久生效：\nvim /etc/profile 或者 ~/.bashrc\nexport a=hello\n或者\na=hello\nexport a\n\n说明：\n系统中有一个变量path，环境变量\nexport path=/usr/local/mysql/bin:$path\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 3、 全局变量\n\n * 全局变量：全局所有的用户和程序都能调用，且继承，新建的用户也默认能调用.\n * 解读相关配置文件\n\n文件名               说明                    备注\n~/.bashrc         当前用户的bash信息，用户登录时读取   局部。定义别名、umask、函数等\n~/.bash_profile   当前用户的环境变量信息，用户登录时读取   局部。\n~/.bash_logout    当前用户退出当前shell时最后读取    局部。定义用户退出时执行的程序等\n~/.bash_history   当前用户的历史命令             局部。history -w保存历史记录 history -c清空历史记录\n/etc/bashrc       全局的bash信息             全局。所有用户都生效\n/etc/profile      全局环境变量信息              全局。系统和所有用户都生效\n\n**说明：**以上文件修改后，都需要重新source让其生效或者退出重新登录。\n\n * 用户登录系统读取相关文件的顺序\n   1. /etc/profile\n   2. $home/.bash_profile\n   3. $home/.bashrc\n   4. /etc/bashrc\n   5. $home/.bash_logout\n\n> 说明：本地变量、环境变量、全局变量是根据变量的作用域来区分的。\n\n\n# 4、系统变量\n\n * 系统变量(内置bash中变量) ： shell本身已经固定好了它的名字和作用.\n\n内置变量       含义\n$?         上一条命令执行后返回的状态；状态值为0表示执行正常，非0表示执行异常或错误\n$0         当前执行的程序或脚本名\n$#         脚本后面接的参数的个数\n$*         脚本后面所有参数，参数当成一个整体输出，每一个变量参数之间以空格隔开\n$@         脚本后面所有参数，参数是独立的，也是全部输出\n1~9        脚本后面的位置参数，$1表示第1个位置参数，依次类推\n{10}~{n}   扩展位置参数,第10个位置变量必须用{}大括号括起来(2位数字以上扩起来)\n$$         当前所在进程的进程号，如echo $$\n$！         后台运行的最后一个进程号 (当前终端）\n!$         调用最后一条命令历史中的参数\n\n * 进一步了解位置参数$1~${n}\n\n#!/bin/bash\n#了解shell内置变量中的位置参数含义\necho "\\$0 = $0"\necho "\\$# = $#"\necho "\\$* = $*"\necho "\\$@ = $@"\necho "\\$1 = $1" \necho "\\$2 = $2" \necho "\\$3 = $3" \necho "\\$11 = ${11}" \necho "\\$12 = ${12}"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * 进一步了解*和@的区别\n\n$*：表示将变量看成一个整体 $@：表示变量是独立的\n\n#!/bin/bash\nfor i in "$@"\ndo\necho $i\ndone\n\necho "======我是分割线======="\n\nfor i in "$*"\ndo\necho $i\ndone\n\n# bash 3.sh a b c\na\nb\nc\n======我是分割线=======\na b c\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 三、简单四则运算\n\n算术运算：默认情况下，shell就只能支持简单的整数运算\n\n运算内容：加(+)、减(-)、乘(*)、除(/)、求余数（%）、幂（**）\n\n\n# 1. 四则运算符号\n\n表达式      举例                           说明\n$(( ))   echo $((1+1))                \n$[ ]     echo $[10-5]                 \nexpr     expr 10 / 5                  expr程序中，数值与运算符之间需要空格隔开，乘(*)运算符需要用转义符( \\ )转义，expr不能做幂运算\nlet      n=1;let n+=1 等价于 let n=n+1   let n*=2 等价于let n=n*2，不能使用let n**=2\n\n\n# 2.了解i++和++i\n\n * 对变量的值的影响\n\n# i=1\n# let i++\n# echo $i\n2\n# j=1\n# let ++j\n# echo $j\n2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * 对表达式的值的影响\n\n# unset i j\n# i=1;j=1\n# let x=i++         先赋值，再运算\n# let y=++j         先运算，再赋值\n# echo $i\n2\n# echo $j\n2\n# echo $x\n1\n# echo $y\n2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 四、扩展补充\n\n\n# 1. 数组定义\n\n\n# 1、数组分类\n\n * 普通数组：只能使用整数作为数组索引(元素的下标)\n * 关联数组：可以使用字符串作为数组索引(元素的下标)\n\n\n# 2、 普通数组定义\n\n * 一次赋予一个值\n\n数组名[索引下标]=值\narray[0]=v1\narray[1]=v2\narray[2]=v3\narray[3]=v4\n\n\n1\n2\n3\n4\n5\n\n * 一次赋予多个值\n\n数组名=(值1 值2 值3 ...)\narray=(var1 var2 var3 var4)\n\narray1=(`cat /etc/passwd`)            将文件中每一行赋值给array1数组\narray2=(`ls /root`)\narray3=(harry amy jack "miss hou")\narray4=(1 2 3 4 "hello world" [10]=linux)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 3、 数组的读取\n\n${数组名[元素下标]}\n\necho ${array[0]}            获取数组里第一个元素\necho ${array[*]}            获取数组里的所有元素\necho ${#array[*]}            获取数组里所有元素个数\necho ${!array[@]}            获取数组元素的索引下标\necho ${array[@]:1:2}        访问指定的元素；1代表从下标为1的元素开始获取；2代表获取后面几个元素\n\n查看普通数组信息：\n# declare -a\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 4、关联数组定义\n\n# 1、首先声明关联数组\n\ndeclare -a asso_array1\n\ndeclare -a asso_array1\ndeclare -a asso_array2\ndeclare -a asso_array3\n\n\n1\n2\n3\n\n\n# 2、数组赋值\n\n * 一次赋一个值\n\n数组名[索引or下标]=变量值\n# asso_array1[linux]=one\n# asso_array1[java]=two\n# asso_array1[php]=three\n\n\n1\n2\n3\n4\n\n * 一次赋多个值\n\n# asso_array2=([name1]=harry [name2]=jack [name3]=amy [name4]="miss hou")\n\n\n1\n\n * 查看关联数组\n\n# declare -a\ndeclare -a asso_array1=\'([php]="three" [java]="two" [linux]="one" )\'\ndeclare -a asso_array2=\'([name3]="amy" [name2]="jack" [name1]="harry" [name4]="miss hou" )\'\n\n\n1\n2\n3\n\n * 获取关联数组值\n\n# echo ${asso_array1[linux]}\none\n# echo ${asso_array1[php]}\nthree\n# echo ${asso_array1[*]}\nthree two one\n# echo ${!asso_array1[*]}\nphp java linux\n# echo ${#asso_array1[*]}\n3\n# echo ${#asso_array2[*]}\n4\n# echo ${!asso_array2[*]}\nname3 name2 name1 name4\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n * 其他定义方式\n\n# declare -a books\n# let books[linux]++\n# declare -a|grep books\ndeclare -a books=\'([linux]="1" )\'\n# let books[linux]++\n# declare -a|grep books\ndeclare -a books=\'([linux]="2" )\'\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 2. 其他变量定义\n\n * 取出一个目录下的目录和文件：dirname和 basename\n\n# a=/root/desktop/shell/mem.txt \n# echo $a\n/root/desktop/shell/mem.txt\n# dirname $a   取出目录\n/root/desktop/shell\n# basename $a  取出文件\nmem.txt\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * -变量"内容"的删除和替换\n\n一个“%”代表从右往左删除\n两个“%%”代表从右往左去掉最多\n一个“#”代表从左往右去掉删除\n两个“##”代表从左往右去掉最多\n\n举例说明：\n# url=www.taobao.com\n# echo ${#url}             获取变量的长度\n# echo ${url#*.}\n# echo ${url##*.}\n# echo ${url%.*}\n# echo ${url%%.*}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n * 以下了解\n\n替换：/ 和 //\n 1015  echo ${url/ao/ao}  用ao代替ao（从左往右第一个）\n 1017  echo ${url//ao/ao}   贪婪替换（替代所有）\n\n替代： - 和 :-  +和:+\n 1019  echo ${abc-123}\n 1020  abc=hello\n 1021  echo ${abc-444}\n 1022  echo $abc\n 1024  abc=\n 1025  echo ${abc-222}\n\n${变量名-新的变量值} 或者 ${变量名=新的变量值}\n变量没有被赋值：会使用“新的变量值“ 替代\n变量有被赋值（包括空值）： 不会被替代\n\n 1062  echo ${abc:-123}\n 1063  abc=hello\n 1064  echo ${abc:-123}\n 1065  abc=\n 1066  echo ${abc:-123}\n\n${变量名:-新的变量值} 或者 ${变量名:=新的变量值}\n变量没有被赋值或者赋空值：会使用“新的变量值“ 替代\n变量有被赋值： 不会被替代\n\n 1116  echo ${abc=123}\n 1118  echo ${abc:=123}\n\n# unset abc\n# echo ${abc:+123}\n\n# abc=hello\n# echo ${abc:+123}\n123\n# abc=\n# echo ${abc:+123}\n\n${变量名+新的变量值}\n变量没有被赋值或者赋空值：不会使用“新的变量值“ 替代\n变量有被赋值： 会被替代\n# unset abc\n# echo ${abc+123}\n\n# abc=hello\n# echo ${abc+123}\n123\n# abc=\n# echo ${abc+123}\n123\n${变量名:+新的变量值}\n变量没有被赋值：不会使用“新的变量值“ 替代\n变量有被赋值（包括空值）： 会被替代\n\n# unset abc\n# echo ${abc?123}\n-bash: abc: 123\n\n# abc=hello\n# echo ${abc?123}\nhello\n# abc=\n# echo ${abc?123}\n\n${变量名?新的变量值}\n变量没有被赋值:提示错误信息\n变量被赋值（包括空值）：不会使用“新的变量值“ 替代\n\n# unset abc\n# echo ${abc:?123}\n-bash: abc: 123\n# abc=hello\n# echo ${abc:?123}\nhello\n# abc=\n# echo ${abc:?123}\n-bash: abc: 123\n\n${变量名:?新的变量值}\n变量没有被赋值或者赋空值时:提示错误信息\n变量被赋值：不会使用“新的变量值“ 替代\n\n说明：?主要是当变量没有赋值提示错误信息的，没有赋值功能\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n',charsets:{cjk:!0}},{title:"rsync用法及参数详解",frontmatter:{title:"rsync用法及参数详解",categories:["rsync"],tags:["rsync"],date:"2022-12-09T20:51:06.000Z",permalink:"/pages/98cac7/",feed:{enable:!0},description:"Logrotate入门了解及生产实践",readingShow:"top",meta:[{name:"twitter:title",content:"rsync用法及参数详解"},{name:"twitter:description",content:"Logrotate入门了解及生产实践"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/01.rsync/01.rsync%E7%94%A8%E6%B3%95%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3.html"},{property:"og:type",content:"article"},{property:"og:title",content:"rsync用法及参数详解"},{property:"og:description",content:"Logrotate入门了解及生产实践"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/01.rsync/01.rsync%E7%94%A8%E6%B3%95%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:51:06.000Z"},{property:"article:tag",content:"rsync"},{itemprop:"name",content:"rsync用法及参数详解"},{itemprop:"description",content:"Logrotate入门了解及生产实践"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/01.linux/01.rsync/01.rsync%E7%94%A8%E6%B3%95%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3.html",relativePath:"04.运维/01.linux/01.rsync/01.rsync用法及参数详解.md",key:"v-11090b27",path:"/pages/98cac7/",headers:[{level:2,title:"rsync",slug:"rsync",normalizedTitle:"rsync",charIndex:2}],headersStr:"rsync",content:'# rsync\n\nrsync的目的是实现本地主机和远程主机上的文件同步(包括本地推到远程，远程拉到本地两种同步方式)，也可以实现本地不同路径下文件的同步，但不能实现远程路径1到远程路径2之间的同步(scp可以实现)。\n\n不考虑rsync的实现细节，就文件同步而言，涉及了源文件和目标文件的概念，还涉及了以哪边文件为同步基准。例如，想让目标主机上的文件和本地文件保持同步，则是以本地文件为同步基准，将本地文件作为源文件推送到目标主机上。反之，如果想让本地主机上的文件和目标主机上的文件保持同步，则目标主机上的文件为同步基准，实现方式是将目标主机上的文件作为源文件拉取到本地。当然，要保持本地的两个文件相互同步，rsync也一样能实现，这就像Linux中cp命令一样，以本地某文件作为源，另一文件作为目标文件，但请注意，虽然rsync和cp能达到相同的目的，但它们的实现方式是不一样的。\n\n既然是文件同步，在同步过程中必然会涉及到源和目标两文件之间版本控制的问题，例如是否要删除源主机上没有但目标上多出来的文件，目标文件比源文件更新(newer than source)时是否仍要保持同步，遇到软链接时是拷贝软链接本身还是拷贝软链接所指向的文件，目标文件已存在时是否要先对其做个备份等等。\n\nrsync同步过程中由两部分模式组成：决定哪些文件需要同步的检查模式以及文件同步时的同步模式。\n\n(1).检查模式是指按照指定规则来检查哪些文件需要被同步，例如哪些文件是明确被排除不传输的。**默认情况下，rsync使用"quick check"算法快速检查源文件和目标文件的大小、mtime(修改时间)是否一致，如果不一致则需要传输。**当然，也可以通过在rsync命令行中指定某些选项来改变quick check的检查模式，比如"--size-only"选项表示"quick check"将仅检查文件大小不同的文件作为待传输文件。rsync支持非常多的选项，其中检查模式的自定义性是非常有弹性的。\n\n(2).同步模式是指在文件确定要被同步后，在同步过程发生之前要做哪些额外工作。例如上文所说的是否要先删除源主机上没有但目标主机上有的文件，是否要先备份已存在的目标文件，是否要追踪链接文件等额外操作。rsync也提供非常多的选项使得同步模式变得更具弹性。\n\n相对来说，为rsync手动指定同步模式的选项更常见一些，只有在有特殊需求时才指定检查模式，因为大多数检查模式选项都可能会影响rsync的性能。\n\nrsync 的命令格式（用法）为：\n\n1）本地使用：\n\n\n\nrsync [OPTION...] SRC... [DEST]\n\n\n\n\n\n\n\n2）通过远程 Shell 使用：\n\n\n\n拉: rsync [OPTION...] [USER@]HOST:SRC... [DEST]\n\n\n\n推: rsync [OPTION...] SRC... [USER@]HOST:DEST\n\n\n\n\n\n\n\n3）访问 rsync 服务器:\n\n\n\n拉: rsync [OPTION...] [USER@]HOST::SRC... [DEST]\n\n\n\n推: rsync [OPTION...] SRC... [USER@]HOST::DEST\n\n\n\n拉: rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST]\n\n\n\n推: rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n其中：\n\n * SRC: 是要复制的源位置\n\n * DEST: 是复制目标位置\n\n * 若本地登录用户与远程主机上的用户一致，可以省略 USER@\n\n * 使用远程 shell 同步时，主机名与资源之间使用单个冒号“:”作为分隔符\n\n * 使用 rsync 服务器同步时，主机名与资源之间使用两个冒号“::”作为分隔符\n\n * 当访问 rsync 服务器时也可以使用 rsync:// URL\n\n * “拉”复制是指从远程主机复制文件到本地主机\n\n * “推”复制是指从本地主机复制文件到远程主机\n\n * 当进行“拉”复制时，若指定一个 SRC 且省略 DEST，则只列出资源而不进行复制\n   \n   下面列出常用选项：\n   \n   选项                     说明\n   -a, ––archive          归档模式，表示以递归方式传输文件，并保持所有文件属性，等价于 -rlptgoD (注意不包括 -H)\n   -r, ––recursive        对子目录以递归模式处理\n   -l, ––links            保持符号链接文件\n   -H, ––hard-links       保持硬链接文件\n   -p, ––perms            保持文件权限\n   -t, ––times            保持文件时间信息\n   -g, ––group            保持文件属组信息\n   -o, ––owner            保持文件属主信息 (super-user only)\n   -D                     保持设备文件和特殊文件 (super-user only)\n   -z, ––compress         在传输文件时进行压缩处理\n   ––exclude=PATTERN      指定排除一个不需要传输的文件匹配模式\n   ––exclude-from=FILE    从 FILE 中读取排除规则\n   ––include=PATTERN      指定需要传输的文件匹配模式\n   ––include-from=FILE    从 FILE 中读取包含规则\n   ––copy-unsafe-links    拷贝指向SRC路径目录树以外的链接文件\n   ––safe-links           忽略指向SRC路径目录树以外的链接文件（默认）\n   ––existing             仅仅更新那些已经存在于接收端的文件，而不备份那些新创建的文件\n   ––ignore-existing      忽略那些已经存在于接收端的文件，仅备份那些新创建的文件\n   -b, ––backup           当有变化时，对目标目录中的旧版文件进行备份\n   ––backup-dir=DIR       与 -b 结合使用，将备份的文件存到 DIR 目录中\n   ––link-dest=DIR        当文件未改变时基于 DIR 创建硬链接文件\n   ––delete               删除那些接收端还有而发送端已经不存在的文件\n   ––delete-before        接收者在传输之前进行删除操作 (默认)\n   ––delete-during        接收者在传输过程中进行删除操作\n   ––delete-after         接收者在传输之后进行删除操作\n   ––delete-excluded      在接收方同时删除被排除的文件\n   -e, ––rsh=COMMAND      指定替代 rsh 的 shell 程序\n   ––ignore-errors        即使出现 I/O 错误也进行删除\n   ––partial              保留那些因故没有完全传输的文件，以是加快随后的再次传输\n   ––progress             在传输时显示传输过程\n   -P                     等价于 ––partial ––progress\n   ––delay-updates        将正在更新的文件先保存到一个临时目录（默认为 “.~tmp~”），待传输完毕再更新目标文件\n   -v, ––verbose          详细输出模式\n   -q, ––quiet            精简输出模式\n   -h, ––human-readable   输出文件大小使用易读的单位（如，K，M等）\n   -n, ––dry-run          显示哪些文件将被传输\n   ––list-only            仅仅列出文件而不进行复制\n   ––rsyncpath=PROGRAM    指定远程服务器上的 rsync 命令所在路径\n   ––password-file=FILE   从 FILE 中读取口令，以避免在终端上输入口令，通常在 cron 中连接 rsync 服务器时使用\n   -4, ––ipv4             使用 IPv4\n   -6, ––ipv6             使用 IPv6\n   ––version              打印版本信息\n   ––help                 显示帮助信息\n\n * 若使用普通用户身份运行 rsync 命令，同步后的文件的属主将改变为这个普通用户身份。\n\n * 若使用超级用户身份运行 rsync 命令，同步后的文件的属主将保持原来的用户身份。\n\n摘自：\n\nhttps://www.cnblogs.com/noxy/p/8986164.html\n\nhttp://www.cnblogs.com/f-ck-need-u/p/7220009.html\n\nhttps://blog.csdn.net/woshiji594167/article/details/83860993',normalizedContent:'# rsync\n\nrsync的目的是实现本地主机和远程主机上的文件同步(包括本地推到远程，远程拉到本地两种同步方式)，也可以实现本地不同路径下文件的同步，但不能实现远程路径1到远程路径2之间的同步(scp可以实现)。\n\n不考虑rsync的实现细节，就文件同步而言，涉及了源文件和目标文件的概念，还涉及了以哪边文件为同步基准。例如，想让目标主机上的文件和本地文件保持同步，则是以本地文件为同步基准，将本地文件作为源文件推送到目标主机上。反之，如果想让本地主机上的文件和目标主机上的文件保持同步，则目标主机上的文件为同步基准，实现方式是将目标主机上的文件作为源文件拉取到本地。当然，要保持本地的两个文件相互同步，rsync也一样能实现，这就像linux中cp命令一样，以本地某文件作为源，另一文件作为目标文件，但请注意，虽然rsync和cp能达到相同的目的，但它们的实现方式是不一样的。\n\n既然是文件同步，在同步过程中必然会涉及到源和目标两文件之间版本控制的问题，例如是否要删除源主机上没有但目标上多出来的文件，目标文件比源文件更新(newer than source)时是否仍要保持同步，遇到软链接时是拷贝软链接本身还是拷贝软链接所指向的文件，目标文件已存在时是否要先对其做个备份等等。\n\nrsync同步过程中由两部分模式组成：决定哪些文件需要同步的检查模式以及文件同步时的同步模式。\n\n(1).检查模式是指按照指定规则来检查哪些文件需要被同步，例如哪些文件是明确被排除不传输的。**默认情况下，rsync使用"quick check"算法快速检查源文件和目标文件的大小、mtime(修改时间)是否一致，如果不一致则需要传输。**当然，也可以通过在rsync命令行中指定某些选项来改变quick check的检查模式，比如"--size-only"选项表示"quick check"将仅检查文件大小不同的文件作为待传输文件。rsync支持非常多的选项，其中检查模式的自定义性是非常有弹性的。\n\n(2).同步模式是指在文件确定要被同步后，在同步过程发生之前要做哪些额外工作。例如上文所说的是否要先删除源主机上没有但目标主机上有的文件，是否要先备份已存在的目标文件，是否要追踪链接文件等额外操作。rsync也提供非常多的选项使得同步模式变得更具弹性。\n\n相对来说，为rsync手动指定同步模式的选项更常见一些，只有在有特殊需求时才指定检查模式，因为大多数检查模式选项都可能会影响rsync的性能。\n\nrsync 的命令格式（用法）为：\n\n1）本地使用：\n\n\n\nrsync [option...] src... [dest]\n\n\n\n\n\n\n\n2）通过远程 shell 使用：\n\n\n\n拉: rsync [option...] [user@]host:src... [dest]\n\n\n\n推: rsync [option...] src... [user@]host:dest\n\n\n\n\n\n\n\n3）访问 rsync 服务器:\n\n\n\n拉: rsync [option...] [user@]host::src... [dest]\n\n\n\n推: rsync [option...] src... [user@]host::dest\n\n\n\n拉: rsync [option...] rsync://[user@]host[:port]/src... [dest]\n\n\n\n推: rsync [option...] src... rsync://[user@]host[:port]/dest\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n其中：\n\n * src: 是要复制的源位置\n\n * dest: 是复制目标位置\n\n * 若本地登录用户与远程主机上的用户一致，可以省略 user@\n\n * 使用远程 shell 同步时，主机名与资源之间使用单个冒号“:”作为分隔符\n\n * 使用 rsync 服务器同步时，主机名与资源之间使用两个冒号“::”作为分隔符\n\n * 当访问 rsync 服务器时也可以使用 rsync:// url\n\n * “拉”复制是指从远程主机复制文件到本地主机\n\n * “推”复制是指从本地主机复制文件到远程主机\n\n * 当进行“拉”复制时，若指定一个 src 且省略 dest，则只列出资源而不进行复制\n   \n   下面列出常用选项：\n   \n   选项                     说明\n   -a, ––archive          归档模式，表示以递归方式传输文件，并保持所有文件属性，等价于 -rlptgod (注意不包括 -h)\n   -r, ––recursive        对子目录以递归模式处理\n   -l, ––links            保持符号链接文件\n   -h, ––hard-links       保持硬链接文件\n   -p, ––perms            保持文件权限\n   -t, ––times            保持文件时间信息\n   -g, ––group            保持文件属组信息\n   -o, ––owner            保持文件属主信息 (super-user only)\n   -d                     保持设备文件和特殊文件 (super-user only)\n   -z, ––compress         在传输文件时进行压缩处理\n   ––exclude=pattern      指定排除一个不需要传输的文件匹配模式\n   ––exclude-from=file    从 file 中读取排除规则\n   ––include=pattern      指定需要传输的文件匹配模式\n   ––include-from=file    从 file 中读取包含规则\n   ––copy-unsafe-links    拷贝指向src路径目录树以外的链接文件\n   ––safe-links           忽略指向src路径目录树以外的链接文件（默认）\n   ––existing             仅仅更新那些已经存在于接收端的文件，而不备份那些新创建的文件\n   ––ignore-existing      忽略那些已经存在于接收端的文件，仅备份那些新创建的文件\n   -b, ––backup           当有变化时，对目标目录中的旧版文件进行备份\n   ––backup-dir=dir       与 -b 结合使用，将备份的文件存到 dir 目录中\n   ––link-dest=dir        当文件未改变时基于 dir 创建硬链接文件\n   ––delete               删除那些接收端还有而发送端已经不存在的文件\n   ––delete-before        接收者在传输之前进行删除操作 (默认)\n   ––delete-during        接收者在传输过程中进行删除操作\n   ––delete-after         接收者在传输之后进行删除操作\n   ––delete-excluded      在接收方同时删除被排除的文件\n   -e, ––rsh=command      指定替代 rsh 的 shell 程序\n   ––ignore-errors        即使出现 i/o 错误也进行删除\n   ––partial              保留那些因故没有完全传输的文件，以是加快随后的再次传输\n   ––progress             在传输时显示传输过程\n   -p                     等价于 ––partial ––progress\n   ––delay-updates        将正在更新的文件先保存到一个临时目录（默认为 “.~tmp~”），待传输完毕再更新目标文件\n   -v, ––verbose          详细输出模式\n   -q, ––quiet            精简输出模式\n   -h, ––human-readable   输出文件大小使用易读的单位（如，k，m等）\n   -n, ––dry-run          显示哪些文件将被传输\n   ––list-only            仅仅列出文件而不进行复制\n   ––rsyncpath=program    指定远程服务器上的 rsync 命令所在路径\n   ––password-file=file   从 file 中读取口令，以避免在终端上输入口令，通常在 cron 中连接 rsync 服务器时使用\n   -4, ––ipv4             使用 ipv4\n   -6, ––ipv6             使用 ipv6\n   ––version              打印版本信息\n   ––help                 显示帮助信息\n\n * 若使用普通用户身份运行 rsync 命令，同步后的文件的属主将改变为这个普通用户身份。\n\n * 若使用超级用户身份运行 rsync 命令，同步后的文件的属主将保持原来的用户身份。\n\n摘自：\n\nhttps://www.cnblogs.com/noxy/p/8986164.html\n\nhttp://www.cnblogs.com/f-ck-need-u/p/7220009.html\n\nhttps://blog.csdn.net/woshiji594167/article/details/83860993',charsets:{cjk:!0}},{title:"rsync服务实现推送，拉取",frontmatter:{title:"rsync服务实现推送，拉取",categories:["rsync"],tags:["rsync"],date:"2022-12-09T20:51:06.000Z",permalink:"/pages/c7f1bc/",feed:{enable:!0},description:"Logrotate入门了解及生产实践",readingShow:"top",meta:[{name:"image",content:"http://s3.51cto.com/wyfs02/M02/8A/01/wKioL1gkBdaDE-dQAAAE5R3eKYo767.png"},{name:"twitter:title",content:"rsync服务实现推送，拉取"},{name:"twitter:description",content:"Logrotate入门了解及生产实践"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://s3.51cto.com/wyfs02/M02/8A/01/wKioL1gkBdaDE-dQAAAE5R3eKYo767.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/01.rsync/02.rsync%E6%9C%8D%E5%8A%A1%E5%AE%9E%E7%8E%B0%E6%8E%A8%E9%80%81%EF%BC%8C%E6%8B%89%E5%8F%96.html"},{property:"og:type",content:"article"},{property:"og:title",content:"rsync服务实现推送，拉取"},{property:"og:description",content:"Logrotate入门了解及生产实践"},{property:"og:image",content:"http://s3.51cto.com/wyfs02/M02/8A/01/wKioL1gkBdaDE-dQAAAE5R3eKYo767.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/01.rsync/02.rsync%E6%9C%8D%E5%8A%A1%E5%AE%9E%E7%8E%B0%E6%8E%A8%E9%80%81%EF%BC%8C%E6%8B%89%E5%8F%96.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:51:06.000Z"},{property:"article:tag",content:"rsync"},{itemprop:"name",content:"rsync服务实现推送，拉取"},{itemprop:"description",content:"Logrotate入门了解及生产实践"},{itemprop:"image",content:"http://s3.51cto.com/wyfs02/M02/8A/01/wKioL1gkBdaDE-dQAAAE5R3eKYo767.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/01.linux/01.rsync/02.rsync%E6%9C%8D%E5%8A%A1%E5%AE%9E%E7%8E%B0%E6%8E%A8%E9%80%81%EF%BC%8C%E6%8B%89%E5%8F%96.html",relativePath:"04.运维/01.linux/01.rsync/02.rsync服务实现推送，拉取.md",key:"v-2f584880",path:"/pages/c7f1bc/",headersStr:null,content:'# rsync服务实现推送，拉取\n\nhttps://www.csdn.net/tags/OtTacg3sMzk5Mi1ibG9n.html\n\n1.简介\n\nrsync是一款远程数据同步工具，一个Rsync server能够同时备份多个客户端数据，需要scp，ssh，daemon的支持，默认端口为873。\n\nrsync + crond 可以实现数据定时同步，rsync + inotify可以实现数据的实时同步。\n\n工作中的Rsync服务最好以只读方式提供要备份的数据，避免造成误操作。\n\n*2.实验环境介绍*\n\n两台CentOS6.3 x64 测试机，一台server，一台client。系统已经默认安装了rsync软件。\n\n\n\n*3.Rsync命令格式及命令参数*\n\n\n\n参数介绍：\n\n-a 归档模式，表示以归档方式传输文件，并保持所有文件属性\n\n-v 详细模式输出\n\n-z 对备份的文件在传输时进行压缩处理\n\n--delete 无差异同步\n\n*4.Rsync的三种工作模式*\n\n1）本地的拷贝和删除\n\n# rsync avz /etc/hosts /tmp 将文件hosts拷贝到/tmp目录中。与cp命令相似，但区别在rsync可以自己比较两个文件，实现增量备份\n\n\n\n# rsync avz delete /tmp/ /opt/ /tmp/目录为空，加上 --delete参数，相当于 rm 命令\n\n--delete可以理解为：本地有远端有，本地没有删除远端有的\n\n\n\n2）remote shell\n\nrsync远程"推"和"拉"\n\n（推）# rsync avzP e \'ssh p 22\' /tmp/ root@192.168.87.138:/tmp/aaa\n\n把本地的 /tmp/ 目录中的文件复制到192.168.87.138的 /tmp/aaa 目录下，通过22端口\n\n\n\n（拉）# rsync avzP e \'ssh p 22\' root@192.168.87.138:/tmp/ /tmp\n\n把192.168.87.138的 /tmp/ 目录文件复制到本地的 /tmp 目录中\n\n\n\n# rsync -avzP -e \'ssh -p 22\' /tmp*/* root@192.168.87.133:/tmp\n\n# rsync -avzP -e \'ssh -p 22\' root@192.168.87.133:/tmp*/* /tmp\n\n标红的斜杠，有这个就是推送指定文件夹的全部内容，没有这个就是推送整个目录\n\n3）daemon （配置Rsync服务端步骤）\n\n首先确认系统中安装了rsync版本及其版本号\n\n# rsync --version查看当前rsync版本\n\n# rpm qa rsync\n\n\n\nrsync的配置文件默认不存在，需要手工创建。路径为 /etc/rsyncd.conf\n\n编辑 /etc/rsyncd.conf 文件，内容如下。复制粘贴保存退出即可。\n\n\n\n#Rsync server\n\n#yuci\n\nuid = rsync\n\ngid = rsync\n\nuse chroot = no\n\nmax connections = 2000\n\ntimeout = 600\n\npid file = /var/run/rsyncd.pid\n\nlock file = /var/run/rsync.lock\n\nlog file = /var/log/rsyncd.log\n\nignore errors\n\nread only = false\n\nlist = false\n\nhosts allow = 192.168.0.0/24\n\nhosts deny = 0.0.0.0/32\n\nauth users = rsync_backup\n\nsecrets file = /etc/rsync.password\n\n###################################\n\n[backup]\n\ncomment = www by yuci\n\npath = /backup\n\n创建rsync用户及共享的目录 /backup（创建完后检查一下，养成好习惯）\n\n\n\n如上编辑的文件，secrets file = /etc/rsync.password 为密码文件，可以实现两台机器互信。因为是密码所以需要更改权限。\n\nrsync_backup 是上面配置文件的 auth users = rsync_backup "123456"为连接时需要验证的密码\n\n\n\n# rsync --daemon 启动rsync服务\n\n# netstat lntup | grep rsync 查看rsync是否正常运行在873端口\n\n# ps ef | grep rsync\n\n# echo "rsync --daemon" >> /etc/rc.local 加入开机自启动\n\n# cat /etc/rc.local\n\n\n\n\n\n\n\n到此为止，服务器端的配置已经完成。接下来配置客户端\n\n# echo "123456" >> /etc/rsync.password\n\n只需要密码即可，因为在连接命令中已经有了 rsync_backup用户\n\n# chmod 600 /etc/rsync.password\n\n跟服务器的密码文件相同，需要修改权限\n\n\n\n客户端只需要这简单的两步就可以了。\n\n*5.测试环境搭建是否成功*\n\n现在要将客户端的 /tmp/ 目录中的文件，推送到服务端的 /backup 目录中，将 /backup 目录清空，在 /tmp/ 目录中随便创建几个文件\n\n没有报错推送成功，下图选中的backup是要对应 /etc/rsyncd.conf 中的模块命令\n\n\n\n与上图的 ::backup 对应\n\n\n\n回到服务器端检查，测试成功\n\n\n\n本文转自 mlwzby 51CTO博客，原文链接:http://blog.51cto.com/aby028/1871433',normalizedContent:'# rsync服务实现推送，拉取\n\nhttps://www.csdn.net/tags/ottacg3smzk5mi1ibg9n.html\n\n1.简介\n\nrsync是一款远程数据同步工具，一个rsync server能够同时备份多个客户端数据，需要scp，ssh，daemon的支持，默认端口为873。\n\nrsync + crond 可以实现数据定时同步，rsync + inotify可以实现数据的实时同步。\n\n工作中的rsync服务最好以只读方式提供要备份的数据，避免造成误操作。\n\n*2.实验环境介绍*\n\n两台centos6.3 x64 测试机，一台server，一台client。系统已经默认安装了rsync软件。\n\n\n\n*3.rsync命令格式及命令参数*\n\n\n\n参数介绍：\n\n-a 归档模式，表示以归档方式传输文件，并保持所有文件属性\n\n-v 详细模式输出\n\n-z 对备份的文件在传输时进行压缩处理\n\n--delete 无差异同步\n\n*4.rsync的三种工作模式*\n\n1）本地的拷贝和删除\n\n# rsync avz /etc/hosts /tmp 将文件hosts拷贝到/tmp目录中。与cp命令相似，但区别在rsync可以自己比较两个文件，实现增量备份\n\n\n\n# rsync avz delete /tmp/ /opt/ /tmp/目录为空，加上 --delete参数，相当于 rm 命令\n\n--delete可以理解为：本地有远端有，本地没有删除远端有的\n\n\n\n2）remote shell\n\nrsync远程"推"和"拉"\n\n（推）# rsync avzp e \'ssh p 22\' /tmp/ root@192.168.87.138:/tmp/aaa\n\n把本地的 /tmp/ 目录中的文件复制到192.168.87.138的 /tmp/aaa 目录下，通过22端口\n\n\n\n（拉）# rsync avzp e \'ssh p 22\' root@192.168.87.138:/tmp/ /tmp\n\n把192.168.87.138的 /tmp/ 目录文件复制到本地的 /tmp 目录中\n\n\n\n# rsync -avzp -e \'ssh -p 22\' /tmp*/* root@192.168.87.133:/tmp\n\n# rsync -avzp -e \'ssh -p 22\' root@192.168.87.133:/tmp*/* /tmp\n\n标红的斜杠，有这个就是推送指定文件夹的全部内容，没有这个就是推送整个目录\n\n3）daemon （配置rsync服务端步骤）\n\n首先确认系统中安装了rsync版本及其版本号\n\n# rsync --version查看当前rsync版本\n\n# rpm qa rsync\n\n\n\nrsync的配置文件默认不存在，需要手工创建。路径为 /etc/rsyncd.conf\n\n编辑 /etc/rsyncd.conf 文件，内容如下。复制粘贴保存退出即可。\n\n\n\n#rsync server\n\n#yuci\n\nuid = rsync\n\ngid = rsync\n\nuse chroot = no\n\nmax connections = 2000\n\ntimeout = 600\n\npid file = /var/run/rsyncd.pid\n\nlock file = /var/run/rsync.lock\n\nlog file = /var/log/rsyncd.log\n\nignore errors\n\nread only = false\n\nlist = false\n\nhosts allow = 192.168.0.0/24\n\nhosts deny = 0.0.0.0/32\n\nauth users = rsync_backup\n\nsecrets file = /etc/rsync.password\n\n###################################\n\n[backup]\n\ncomment = www by yuci\n\npath = /backup\n\n创建rsync用户及共享的目录 /backup（创建完后检查一下，养成好习惯）\n\n\n\n如上编辑的文件，secrets file = /etc/rsync.password 为密码文件，可以实现两台机器互信。因为是密码所以需要更改权限。\n\nrsync_backup 是上面配置文件的 auth users = rsync_backup "123456"为连接时需要验证的密码\n\n\n\n# rsync --daemon 启动rsync服务\n\n# netstat lntup | grep rsync 查看rsync是否正常运行在873端口\n\n# ps ef | grep rsync\n\n# echo "rsync --daemon" >> /etc/rc.local 加入开机自启动\n\n# cat /etc/rc.local\n\n\n\n\n\n\n\n到此为止，服务器端的配置已经完成。接下来配置客户端\n\n# echo "123456" >> /etc/rsync.password\n\n只需要密码即可，因为在连接命令中已经有了 rsync_backup用户\n\n# chmod 600 /etc/rsync.password\n\n跟服务器的密码文件相同，需要修改权限\n\n\n\n客户端只需要这简单的两步就可以了。\n\n*5.测试环境搭建是否成功*\n\n现在要将客户端的 /tmp/ 目录中的文件，推送到服务端的 /backup 目录中，将 /backup 目录清空，在 /tmp/ 目录中随便创建几个文件\n\n没有报错推送成功，下图选中的backup是要对应 /etc/rsyncd.conf 中的模块命令\n\n\n\n与上图的 ::backup 对应\n\n\n\n回到服务器端检查，测试成功\n\n\n\n本文转自 mlwzby 51cto博客，原文链接:http://blog.51cto.com/aby028/1871433',charsets:{cjk:!0}},{title:"centos7搭建dns,bind配置",frontmatter:{title:"centos7搭建dns,bind配置",date:"2022-12-15T19:18:37.000Z",permalink:"/pages/78c801/",categories:["运维","dns"],tags:[null],feed:{enable:!0},description:"Logrotate入门了解及生产实践",readingShow:"top",meta:[{name:"image",content:"http://pic.zzppjj.top/LightPicture/2023/01/c61083c823f2ed94.png"},{name:"twitter:title",content:"centos7搭建dns,bind配置"},{name:"twitter:description",content:"Logrotate入门了解及生产实践"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://pic.zzppjj.top/LightPicture/2023/01/c61083c823f2ed94.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/02.dns/01.centos7%E6%90%AD%E5%BB%BAdns,bind%E9%85%8D%E7%BD%AE.html"},{property:"og:type",content:"article"},{property:"og:title",content:"centos7搭建dns,bind配置"},{property:"og:description",content:"Logrotate入门了解及生产实践"},{property:"og:image",content:"http://pic.zzppjj.top/LightPicture/2023/01/c61083c823f2ed94.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/02.dns/01.centos7%E6%90%AD%E5%BB%BAdns,bind%E9%85%8D%E7%BD%AE.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T19:18:37.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"centos7搭建dns,bind配置"},{itemprop:"description",content:"Logrotate入门了解及生产实践"},{itemprop:"image",content:"http://pic.zzppjj.top/LightPicture/2023/01/c61083c823f2ed94.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/01.linux/02.dns/01.centos7%E6%90%AD%E5%BB%BAdns,bind%E9%85%8D%E7%BD%AE.html",relativePath:"04.运维/01.linux/02.dns/01.centos7搭建dns,bind配置.md",key:"v-52cf205e",path:"/pages/78c801/",headersStr:null,content:'虽然可以修改操作系统下的hosts文件劫持dns解析，但是如果在多台计算机之间测试，就得一行行的复制到多个计算机中。更有甚者，如果测试接口地址是测试域名，在未越狱的iphone中就没办法测试了，未越狱的iphone修改不了hosts。而且hosts只支持A记录，没办法设置MX，PTR，CNAME……搭建一个dns服务器这些问题就解决了。\n\n\n\n1.安装\n\nyum install bind-chroot\n\n2.设置开机启动\n\nsystemctl enable named-chroot\n\n3.配置bind\n\nvim /etc/named.conf\n\noptions {\n    listen-on port 53 { any; };\n    listen-on-v6 port 53 { ::1; };\n    directory     "/var/named";\n    dump-file     "/var/named/data/cache_dump.db";\n    statistics-file "/var/named/data/named_stats.txt";\n    memstatistics-file "/var/named/data/named_mem_stats.txt";\n    allow-query     { any; };\n\n    /* \n     - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.\n     - If you are building a RECURSIVE (caching) DNS server, you need to enable \n       recursion. \n     - If your recursive DNS server has a public IP address, you MUST enable access \n       control to limit queries to your legitimate users. Failing to do so will\n       cause your server to become part of large scale DNS amplification \n       attacks. Implementing BCP38 within your network would greatly\n       reduce such attack surface \n    */\n    recursion yes;\n    forwarders { 223.5.5.5; };\n    dnssec-enable no;\n    dnssec-validation no;\n\n    /* Path to ISC DLV key */\n    bindkeys-file "/etc/named.iscdlv.key";\n\n    managed-keys-directory "/var/named/dynamic";\n\n    pid-file "/run/named/named.pid";\n    session-keyfile "/run/named/session.key";\n};\n\nlogging {\n        channel default_debug {\n                file "data/named.run";\n                severity dynamic;\n        };\n};\n\nzone "." IN {\n    type hint;\n    file "named.ca";\n};\n\nzone "a.com" IN {\n    type master;\n    file "a.com.zone";\n};\n\nzone "0.168.192.in-addr.arpa" IN {\n    type master;\n    file "192.168.0.zone";\n};\n\ninclude "/etc/named.rfc1912.zones";\ninclude "/etc/named.root.key";\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n默认配置修改这两项:\n\nlisten-on port 53 { any; }表示监听任何ip对53端口的请求\n\nallow-query { any; }表示接收任何来源查询dns记录\n\nzone "a.com" IN { type master; file "a.com.zone"; };\n\n此段增加一个a.com域名的解析，具体解析规则在/var/named/a.com.zone里。\n\nzone "0.168.192.in-addr.arpa" IN { type master; file "192.168.0.zone"; };\n\n此段增加一个反向解析，即根据ip查域名（不需要的话可以不设置）\n\n/var/named/a.com.zone文件内容，请注意named用户有读的权限\n\n$TTL 1D @ IN SOA @ root.a.com. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS @ A 192.168.0.111 www A 192.168.0.112 @ MX 10 mx.a.com. AAAA ::1\n\n此段设置了\n\na.com的ip为192.168.0.111，\n\nwww.a.com的ip解析到192.168.0.112,\n\na.com的mx记录为mx.a.com\n\n/var/named/192.168.0.zone文件内容，请注意named用户需要有读的权限\n\n$TTL 86400 @ IN SOA localhost a.com. ( 2014031101 2H 10M 7D 1D ) IN NS localhost. 111 IN PTR a.com 112 IN PTR www.a.com.\n\n此段设置了反查记录，即\n\n192.168.0.111查询后得到的域名是a.com\n\n192.168.0.222查询后得到的域名是www.a.com\n\n最后，启动bind\n\nsystemctl start named-chroot\n\n基本的DNS服务器搭建完成，并已经设置了一个a.com的域名解析，来测试一下。\n\nwindows右键网卡图标，打开网络和共享中心，更改适配器设置，右键网卡，属性，Internet 协议版本4(TCP/IPv4)，勾选使用下面的DNS服务器地址，首选DNS服务器填入服务器ip,类nix系统编辑/etc/resolv.conf修改nameserver为服务器ip。115.28.142.187为我的dns服务器IP，下面的结果都是基于115.28.142.187dns服务器返回的结果.\n\n查询a.com的dns记录\n\na.com A记录\n\n查询www.a.com的dns记录\n\nwww.a.com A记录\n\n查询a.com的MX记录\n\na.com MX记录\n\n查询www.a.com的PTR记录\n\nwww.a.com PTR记录\n\n转载请注明：飞嗨 » CentOS 7搭建DNS服务器，bind安装配置\n\n例子域名配置：\n\n$TTL 1D\n\n@ IN SOA ns.vcenter.com. root (\n\n0 ; serial\n\n1D ; refresh\n\n1H ; retry\n\n1W ; expire\n\n3H ) ; minimum\n\nNS ns.vcenter.com.\n\nns A 172.16.30.243\n\n@ A 172.16.30.211\n\nwww A 172.16.30.211',normalizedContent:'虽然可以修改操作系统下的hosts文件劫持dns解析，但是如果在多台计算机之间测试，就得一行行的复制到多个计算机中。更有甚者，如果测试接口地址是测试域名，在未越狱的iphone中就没办法测试了，未越狱的iphone修改不了hosts。而且hosts只支持a记录，没办法设置mx，ptr，cname……搭建一个dns服务器这些问题就解决了。\n\n\n\n1.安装\n\nyum install bind-chroot\n\n2.设置开机启动\n\nsystemctl enable named-chroot\n\n3.配置bind\n\nvim /etc/named.conf\n\noptions {\n    listen-on port 53 { any; };\n    listen-on-v6 port 53 { ::1; };\n    directory     "/var/named";\n    dump-file     "/var/named/data/cache_dump.db";\n    statistics-file "/var/named/data/named_stats.txt";\n    memstatistics-file "/var/named/data/named_mem_stats.txt";\n    allow-query     { any; };\n\n    /* \n     - if you are building an authoritative dns server, do not enable recursion.\n     - if you are building a recursive (caching) dns server, you need to enable \n       recursion. \n     - if your recursive dns server has a public ip address, you must enable access \n       control to limit queries to your legitimate users. failing to do so will\n       cause your server to become part of large scale dns amplification \n       attacks. implementing bcp38 within your network would greatly\n       reduce such attack surface \n    */\n    recursion yes;\n    forwarders { 223.5.5.5; };\n    dnssec-enable no;\n    dnssec-validation no;\n\n    /* path to isc dlv key */\n    bindkeys-file "/etc/named.iscdlv.key";\n\n    managed-keys-directory "/var/named/dynamic";\n\n    pid-file "/run/named/named.pid";\n    session-keyfile "/run/named/session.key";\n};\n\nlogging {\n        channel default_debug {\n                file "data/named.run";\n                severity dynamic;\n        };\n};\n\nzone "." in {\n    type hint;\n    file "named.ca";\n};\n\nzone "a.com" in {\n    type master;\n    file "a.com.zone";\n};\n\nzone "0.168.192.in-addr.arpa" in {\n    type master;\n    file "192.168.0.zone";\n};\n\ninclude "/etc/named.rfc1912.zones";\ninclude "/etc/named.root.key";\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n默认配置修改这两项:\n\nlisten-on port 53 { any; }表示监听任何ip对53端口的请求\n\nallow-query { any; }表示接收任何来源查询dns记录\n\nzone "a.com" in { type master; file "a.com.zone"; };\n\n此段增加一个a.com域名的解析，具体解析规则在/var/named/a.com.zone里。\n\nzone "0.168.192.in-addr.arpa" in { type master; file "192.168.0.zone"; };\n\n此段增加一个反向解析，即根据ip查域名（不需要的话可以不设置）\n\n/var/named/a.com.zone文件内容，请注意named用户有读的权限\n\n$ttl 1d @ in soa @ root.a.com. ( 0 ; serial 1d ; refresh 1h ; retry 1w ; expire 3h ) ; minimum ns @ a 192.168.0.111 www a 192.168.0.112 @ mx 10 mx.a.com. aaaa ::1\n\n此段设置了\n\na.com的ip为192.168.0.111，\n\nwww.a.com的ip解析到192.168.0.112,\n\na.com的mx记录为mx.a.com\n\n/var/named/192.168.0.zone文件内容，请注意named用户需要有读的权限\n\n$ttl 86400 @ in soa localhost a.com. ( 2014031101 2h 10m 7d 1d ) in ns localhost. 111 in ptr a.com 112 in ptr www.a.com.\n\n此段设置了反查记录，即\n\n192.168.0.111查询后得到的域名是a.com\n\n192.168.0.222查询后得到的域名是www.a.com\n\n最后，启动bind\n\nsystemctl start named-chroot\n\n基本的dns服务器搭建完成，并已经设置了一个a.com的域名解析，来测试一下。\n\nwindows右键网卡图标，打开网络和共享中心，更改适配器设置，右键网卡，属性，internet 协议版本4(tcp/ipv4)，勾选使用下面的dns服务器地址，首选dns服务器填入服务器ip,类nix系统编辑/etc/resolv.conf修改nameserver为服务器ip。115.28.142.187为我的dns服务器ip，下面的结果都是基于115.28.142.187dns服务器返回的结果.\n\n查询a.com的dns记录\n\na.com a记录\n\n查询www.a.com的dns记录\n\nwww.a.com a记录\n\n查询a.com的mx记录\n\na.com mx记录\n\n查询www.a.com的ptr记录\n\nwww.a.com ptr记录\n\n转载请注明：飞嗨 » centos 7搭建dns服务器，bind安装配置\n\n例子域名配置：\n\n$ttl 1d\n\n@ in soa ns.vcenter.com. root (\n\n0 ; serial\n\n1d ; refresh\n\n1h ; retry\n\n1w ; expire\n\n3h ) ; minimum\n\nns ns.vcenter.com.\n\nns a 172.16.30.243\n\n@ a 172.16.30.211\n\nwww a 172.16.30.211',charsets:{cjk:!0}},{title:"sed命令在文本每行,行尾或行首添加字符",frontmatter:{title:"sed命令在文本每行,行尾或行首添加字符",date:"2022-12-21T20:56:09.000Z",permalink:"/pages/4baba0/",categories:["运维","linux","sed、awk、grep、find四剑客"],tags:[null],feed:{enable:!0},description:"Logrotate入门了解及生产实践",readingShow:"top",meta:[{name:"image",content:"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMwMDIuY25ibG9ncy5jb20vaW1hZ2VzLzIwMTEvMzIyODMzLzIwMTEwODE5MDk0NDQwMTMucG5n?x-oss-process=image/format,png"},{name:"twitter:title",content:"sed命令在文本每行,行尾或行首添加字符"},{name:"twitter:description",content:"Logrotate入门了解及生产实践"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMwMDIuY25ibG9ncy5jb20vaW1hZ2VzLzIwMTEvMzIyODMzLzIwMTEwODE5MDk0NDQwMTMucG5n?x-oss-process=image/format,png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/03.sed%E3%80%81awk%E3%80%81grep%E3%80%81find%E5%9B%9B%E5%89%91%E5%AE%A2/01.sed%E5%91%BD%E4%BB%A4%E5%9C%A8%E6%96%87%E6%9C%AC%E6%AF%8F%E8%A1%8C,%E8%A1%8C%E5%B0%BE%E6%88%96%E8%A1%8C%E9%A6%96%E6%B7%BB%E5%8A%A0%E5%AD%97%E7%AC%A6.html"},{property:"og:type",content:"article"},{property:"og:title",content:"sed命令在文本每行,行尾或行首添加字符"},{property:"og:description",content:"Logrotate入门了解及生产实践"},{property:"og:image",content:"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMwMDIuY25ibG9ncy5jb20vaW1hZ2VzLzIwMTEvMzIyODMzLzIwMTEwODE5MDk0NDQwMTMucG5n?x-oss-process=image/format,png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/03.sed%E3%80%81awk%E3%80%81grep%E3%80%81find%E5%9B%9B%E5%89%91%E5%AE%A2/01.sed%E5%91%BD%E4%BB%A4%E5%9C%A8%E6%96%87%E6%9C%AC%E6%AF%8F%E8%A1%8C,%E8%A1%8C%E5%B0%BE%E6%88%96%E8%A1%8C%E9%A6%96%E6%B7%BB%E5%8A%A0%E5%AD%97%E7%AC%A6.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-21T20:56:09.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"sed命令在文本每行,行尾或行首添加字符"},{itemprop:"description",content:"Logrotate入门了解及生产实践"},{itemprop:"image",content:"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMwMDIuY25ibG9ncy5jb20vaW1hZ2VzLzIwMTEvMzIyODMzLzIwMTEwODE5MDk0NDQwMTMucG5n?x-oss-process=image/format,png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/01.linux/03.sed%E3%80%81awk%E3%80%81grep%E3%80%81find%E5%9B%9B%E5%89%91%E5%AE%A2/01.sed%E5%91%BD%E4%BB%A4%E5%9C%A8%E6%96%87%E6%9C%AC%E6%AF%8F%E8%A1%8C,%E8%A1%8C%E5%B0%BE%E6%88%96%E8%A1%8C%E9%A6%96%E6%B7%BB%E5%8A%A0%E5%AD%97%E7%AC%A6.html",relativePath:"04.运维/01.linux/03.sed、awk、grep、find四剑客/01.sed命令在文本每行,行尾或行首添加字符.md",key:"v-44ab5907",path:"/pages/4baba0/",headersStr:null,content:'用sed命令在行首或行尾添加字符的命令有以下几种：\n假设处理的文本为test.file\n\n\n\n在每行的头添加字符，比如"HEAD"，命令如下：\n\nsed "s/^/HEAD&/g" test.file\n\n\n1\n\n\n在每行的行尾添加字符，比如“TAIL”，命令如下：\n\nsed "s/$/&TAIL/g" test.file\n\n\n1\n\n\n运行结果如下图：\n\n\n\n几点说明：\n\n 1. "^"代表行首，"$"代表行尾\n\n 2. \'s/$/&TAIL/g\'中的字符g代表每行出现的字符全部替换，如果想在特定字符处添加，g就有用了，否则只会替换每行第一个，而不继续往后找了\n\n例：\n\n\n\n 3. 如果想导出文件，在命令末尾加"> outfile_name"；如果想在原文件上更改，添加选项"-i"，如（这里的-i，可以理解为其他命令执行后的结果重定向到原文件，所以-n p等参数会影响-i的效果\n\n\n\n 4. 也可以把两条命令和在一起，在test.file的每一行的行头和行尾分别添加字符"HEAD"、“TAIL”，命令：\n\nsed "/./{s/^/HEAD&/;s/$/&TAIL/}" test.file\n\n\n1\n',normalizedContent:'用sed命令在行首或行尾添加字符的命令有以下几种：\n假设处理的文本为test.file\n\n\n\n在每行的头添加字符，比如"head"，命令如下：\n\nsed "s/^/head&/g" test.file\n\n\n1\n\n\n在每行的行尾添加字符，比如“tail”，命令如下：\n\nsed "s/$/&tail/g" test.file\n\n\n1\n\n\n运行结果如下图：\n\n\n\n几点说明：\n\n 1. "^"代表行首，"$"代表行尾\n\n 2. \'s/$/&tail/g\'中的字符g代表每行出现的字符全部替换，如果想在特定字符处添加，g就有用了，否则只会替换每行第一个，而不继续往后找了\n\n例：\n\n\n\n 3. 如果想导出文件，在命令末尾加"> outfile_name"；如果想在原文件上更改，添加选项"-i"，如（这里的-i，可以理解为其他命令执行后的结果重定向到原文件，所以-n p等参数会影响-i的效果\n\n\n\n 4. 也可以把两条命令和在一起，在test.file的每一行的行头和行尾分别添加字符"head"、“tail”，命令：\n\nsed "/./{s/^/head&/;s/$/&tail/}" test.file\n\n\n1\n',charsets:{cjk:!0}},{title:"LVM管理",frontmatter:{title:"LVM管理",date:"2022-12-15T17:38:10.000Z",permalink:"/pages/5f261c/",categories:["运维","linux系统"],tags:[null],feed:{enable:!0},description:"LVM是在磁盘分区和文件系统之间添加的一个逻辑层，来为文件系统屏蔽下层磁盘分区布局，提供一个抽象的盘卷，在盘卷上建立文件系统。管理员利用LVM可以在磁盘不用重新分区的情况下动态调整文件系统的大小，并且利用LVM管理的文件系统可以跨越磁盘，当服务器添加了新的磁盘后，管理员不必将原有的文件移动到新的磁盘上，而是通过LVM可以直接扩展文件系统跨越磁盘",readingShow:"top",meta:[{name:"image",content:"http://pic.zzppjj.top/LightPicture/2023/01/5a41ef21aac7ec6c.jpg"},{name:"twitter:title",content:"LVM管理"},{name:"twitter:description",content:"LVM是在磁盘分区和文件系统之间添加的一个逻辑层，来为文件系统屏蔽下层磁盘分区布局，提供一个抽象的盘卷，在盘卷上建立文件系统。管理员利用LVM可以在磁盘不用重新分区的情况下动态调整文件系统的大小，并且利用LVM管理的文件系统可以跨越磁盘，当服务器添加了新的磁盘后，管理员不必将原有的文件移动到新的磁盘上，而是通过LVM可以直接扩展文件系统跨越磁盘"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://pic.zzppjj.top/LightPicture/2023/01/5a41ef21aac7ec6c.jpg"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/04.LVM%E7%AE%A1%E7%90%86.html"},{property:"og:type",content:"article"},{property:"og:title",content:"LVM管理"},{property:"og:description",content:"LVM是在磁盘分区和文件系统之间添加的一个逻辑层，来为文件系统屏蔽下层磁盘分区布局，提供一个抽象的盘卷，在盘卷上建立文件系统。管理员利用LVM可以在磁盘不用重新分区的情况下动态调整文件系统的大小，并且利用LVM管理的文件系统可以跨越磁盘，当服务器添加了新的磁盘后，管理员不必将原有的文件移动到新的磁盘上，而是通过LVM可以直接扩展文件系统跨越磁盘"},{property:"og:image",content:"http://pic.zzppjj.top/LightPicture/2023/01/5a41ef21aac7ec6c.jpg"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/04.LVM%E7%AE%A1%E7%90%86.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T17:38:10.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"LVM管理"},{itemprop:"description",content:"LVM是在磁盘分区和文件系统之间添加的一个逻辑层，来为文件系统屏蔽下层磁盘分区布局，提供一个抽象的盘卷，在盘卷上建立文件系统。管理员利用LVM可以在磁盘不用重新分区的情况下动态调整文件系统的大小，并且利用LVM管理的文件系统可以跨越磁盘，当服务器添加了新的磁盘后，管理员不必将原有的文件移动到新的磁盘上，而是通过LVM可以直接扩展文件系统跨越磁盘"},{itemprop:"image",content:"http://pic.zzppjj.top/LightPicture/2023/01/5a41ef21aac7ec6c.jpg"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/01.linux/04.LVM%E7%AE%A1%E7%90%86.html",relativePath:"04.运维/01.linux/04.LVM管理.md",key:"v-e8e0b9ea",path:"/pages/5f261c/",headers:[{level:2,title:"lvm工作原理",slug:"lvm工作原理",normalizedTitle:"lvm工作原理",charIndex:2},{level:2,title:"LVM常用的术语",slug:"lvm常用的术语",normalizedTitle:"lvm常用的术语",charIndex:363},{level:2,title:"LVM优点",slug:"lvm优点",normalizedTitle:"lvm优点",charIndex:906},{level:2,title:"创建LVM的基本步骤",slug:"创建lvm的基本步骤",normalizedTitle:"创建lvm的基本步骤",charIndex:1297},{level:2,title:"LVM删除",slug:"lvm删除",normalizedTitle:"lvm删除",charIndex:1635},{level:2,title:"底层存储更换新建PV pvcreate /dev/sdc1",slug:"底层存储更换新建pv-pvcreate-dev-sdc1",normalizedTitle:"底层存储更换新建pv pvcreate /dev/sdc1",charIndex:1782},{level:2,title:"扩容操作",slug:"扩容操作",normalizedTitle:"扩容操作",charIndex:1938},{level:2,title:"大于2T的扩容使用",slug:"大于2t的扩容使用",normalizedTitle:"大于2t的扩容使用",charIndex:2153}],headersStr:"lvm工作原理 LVM常用的术语 LVM优点 创建LVM的基本步骤 LVM删除 底层存储更换新建PV pvcreate /dev/sdc1 扩容操作 大于2T的扩容使用",content:'# lvm工作原理\n\nLVM是在磁盘分区和文件系统之间添加的一个逻辑层，来为文件系统屏蔽下层磁盘分区布局，提供一个抽象的盘卷，在盘卷上建立文件系统。管理员利用LVM可以在磁盘不用重新分区的情况下动态调整文件系统的大小，并且利用LVM管理的文件系统可以跨越磁盘，当服务器添加了新的磁盘后，管理员不必将原有的文件移动到新的磁盘上，而是通过LVM可以直接扩展文件系统跨越磁盘\n\n它就是通过将底层的物理硬盘封装起来，然后以逻辑卷的方式呈现给上层应用。在LVM中，其通过对底层的硬盘进行封装，当我们对底层的物理硬盘进行操作时，其不再是针对于分区进行操作，而是通过一个叫做逻辑卷的东西来对其进行底层的磁盘管理操作。\n\n逻辑卷管理的核心在于如何处理我们系统中安装的硬盘及其分区，对于我们的逻辑卷管理器来说，把硬盘当作物理卷来看\n\n\n\n\n# LVM常用的术语\n\n先来了解一下常用的名词解释\n\n * 物理存储介质（The physical media）\n\n这里指系统的存储设备文件，可是磁盘分区，整个磁盘，RAID阵列或SAN磁盘，设备必须初始化为LVM物理卷，才能与LVM结合使用\n\n * 物理卷PV（physical volume）\n\n物理卷就是指硬盘分区或从逻辑上与磁盘分区具有同样功能的设备(如RAID)，是LVM的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与LVM相关的管理参数 （注册物理设备以便在卷组中使用）,创建物理卷它可以用硬盘分区，也可以用硬盘本身；\n\n * 卷组VG（Volume Group）\n\nLVM卷组类似于非LVM系统中的物理硬盘，一个LVM卷组由一个或多个物理卷组成 \n\n * 逻辑卷LV（logical volume）\n\n类似于非LVM系统中的硬盘分区，LV建立在VG之上，可以在LV之上建立文件系统\n\n * PE（physical extents）\n\nPV中可以分配的最少存储单元，PE的大小是可以指定的，默认为4MB\n\n * LE（logical extent）\n\nLV中可以分配的最少存储单元，在同一个卷组中，LE的大小和PE是相同的，并且一一对应\n\n\n# LVM优点\n\n * 使用卷组，使多个硬盘空间看起来像是一个大的硬盘\n * 使用逻辑卷，可以跨多个硬盘空间的分区 sdb1 sdb2 sdc1 sdd2 sdf\n * 在使用逻辑卷时，它可以在空间不足时动态调整它的大小\n * 在调整逻辑卷大小时，不需要考虑逻辑卷在硬盘上的位置，不用担心没有可用的连续空间\n * 可以在线对LV,VG 进行创建，删除，调整大小等操作。LVM上的文件系统也需要重新调整大小。\n * 可以将正在使用中的硬盘迁移到别的硬盘上(emc--500G DELL---1.2T)\n * 允许创建快照，可以用来保存文件系统的备份。只能使用一次\n\n> 解释：\n> \n> LVM是软件的卷管理方式，而RAID是磁盘保护的方法。对于重要的业务来说，一般是同时存在。RAID用来保护物理的磁盘不会因为故障而中断业务。LVM用来实现对卷的良性的管理，更好的利用磁盘资源\n\n\n# 创建LVM的基本步骤\n\n 1. 先做物理卷 pvcreate /dev/sdb1\n\n 2. 把不同的物理卷加入到卷组当中 vgcreate datavg /dev/sdb1\n\n 3. 创建逻辑卷 lvcreate\n\n 4. 格式化文件系统 mkfs.xfs\n\n 5. 创建挂载点并进行挂载 mount\n\n> lvcreate -l +100%FREE 指定全部大小\n\n对ext类型文件系统扩容： resize2fs\n\n对xfs类型文件系统扩容： xfs_growfs\n\nresize2fs和xfs_growfs 两者的区别是传递的参数不一样的，xfs_growfs是采用的挂载点；resize2fs是逻辑卷名称，而且resize2fs命令不能对xfs类型文件系统使用\n\n\n# LVM删除\n\n创建: pvcreate ---\x3e vgcreate ---\x3e lvcrdeate ---\x3e mkfs.xfs lv ----\x3e mount\n\n删除： umount ----\x3e lvremove lv -----\x3e vgremove vg -----\x3e pvremove\n\n\n# 底层存储更换新建PV pvcreate /dev/sdc1\n\n扩容VG vgextend datavg /dev/sdc1\n\n移动数据 pvmove /dev/sdb5 /dev/sdc1\n\nVG缩小 vgreduce datavg /dev/sdb5\n\n删除小盘 pvremove /dev/sdb5\n\n\n# 扩容操作\n\n1.fdisk /dev/vdc    //给新加的硬盘设备/dev/vdc创建分区(注意设备名称略有差异，步骤同下)\n2.分别按n、p、1、t、8e、wr\n3.pvcreate /dev/vdc1\n4.vgextend vg0 /dev/vdd1\n5.lvextend -l +100%FREE /dev/vg0/lv0\n6. xfs_growfs  /dev/vg0/lv0\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 大于2T的扩容使用\n\n1. 使用parted方式格式化磁盘\n1.1 查看磁盘情况\n[root@jylhlog2 ~]# fdisk -l\nDisk /dev/sdd: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sdb: 42.9 GB, 42949672960 bytes, 83886080 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sdc: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sde: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sdf: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sda: 536.9 GB, 536870912000 bytes, 1048576000 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk label type: dos\nDisk identifier: 0x0008ce60\nDevice Boot Start End Blocks Id System\n/dev/sda1 * 2048 2099199 1048576 83 Linux\n/dev/sda2 2099200 1048567807 523234304 8e Linux LVM\nDisk /dev/mapper/vg_root-lv_root: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_swap: 68.7 GB, 68719476736 bytes, 134217728 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_data: 209.4 GB, 209371267072 bytes, 408928256 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_home: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-oracle: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_grid: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sdg: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n[root@jylhlog2 ~]#\n1.2 使用parted格式化/dev/sdg分区\n[root@jylhlog2 ~]# parted /dev/sdg\nGNU Parted 3.1\nUsing /dev/sdg\nWelcome to GNU Parted! Type ‘help’ to view a list of commands.\n(parted) mklabel gpt ------将MBR分区形式转为GPT分区形式\n(parted) mkpart primary xfs 0 -1 ------设置分区的开始位置和结束位置\nWarning: The resulting partition is not properly aligned for best performance.\nIgnore/Cancel? Ignore\n(parted) print ------查看分区\nModel: VMware Virtual disk (scsi)\nDisk /dev/sdg: 2199GB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags:\nNumber Start End Size File system Name Flags\n1 17.4kB 2199GB 2199GB primary\n(parted) toggle 1 lvm ------使用toggle 更改硬盘类型\n(parted) print ------查看分区\nModel: VMware Virtual disk (scsi)\nDisk /dev/sdg: 2199GB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags:\nNumber Start End Size File system Name Flags\n1 17.4kB 2199GB 2199GB primary lvm\n(parted) quit\nInformation: You may need to update /etc/fstab.\n[root@jylhlog2 ~]#\n1.3 查看格式化后的分区情况\n[root@jylhlog2 ~]# fdisk -l\nDisk /dev/sdd: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sdb: 42.9 GB, 42949672960 bytes, 83886080 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sdc: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sde: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sdf: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sda: 536.9 GB, 536870912000 bytes, 1048576000 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk label type: dos\nDisk identifier: 0x0008ce60\nDevice Boot Start End Blocks Id System\n/dev/sda1 * 2048 2099199 1048576 83 Linux\n/dev/sda2 2099200 1048567807 523234304 8e Linux LVM\nDisk /dev/mapper/vg_root-lv_root: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_swap: 68.7 GB, 68719476736 bytes, 134217728 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_data: 209.4 GB, 209371267072 bytes, 408928256 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_home: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-oracle: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_grid: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nWARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.\nDisk /dev/sdg: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk label type: gpt\nDisk identifier: 9C5E039C-2C6C-4585-8EAB-7B2DBA949516\n# Start End Size Type Name\n1 34 4294965343 2T Linux LVM primary\n[root@jylhlog2 ~]#\n1.4 使系统重读分区表\n[root@jylhlog2 ~]# partprobe\nWarning: Unable to open /dev/sr0 read-write (Read-only file system). /dev/sr0 has been opened read-only.\n[root@jylhlog2 ~]#\n[root@jylhlog2 ~]# fdisk -l\nDisk /dev/sdd: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sdb: 42.9 GB, 42949672960 bytes, 83886080 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sdc: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sde: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sdf: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/sda: 536.9 GB, 536870912000 bytes, 1048576000 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk label type: dos\nDisk identifier: 0x0008ce60\nDevice Boot Start End Blocks Id System\n/dev/sda1 * 2048 2099199 1048576 83 Linux\n/dev/sda2 2099200 1048567807 523234304 8e Linux LVM\nDisk /dev/mapper/vg_root-lv_root: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_swap: 68.7 GB, 68719476736 bytes, 134217728 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_data: 209.4 GB, 209371267072 bytes, 408928256 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_home: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-oracle: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk /dev/mapper/vg_root-lv_grid: 64.4 GB, 64424509440 bytes, 125829120 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nWARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.\nDisk /dev/sdg: 2199.0 GB, 2199023255552 bytes, 4294967296 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk label type: gpt\nDisk identifier: 9C5E039C-2C6C-4585-8EAB-7B2DBA949516\n# Start End Size Type Name\n1 34 4294965343 2T Linux LVM primary\n[root@jylhlog2 ~]#\n2. 创建pv、扩容vg、扩容lv、扩容xfs分区\n2.1 查看目前分区情况\n[root@jylhlog2 ~]# df -h\nFilesystem Size Used Avail Use% Mounted on\n/dev/mapper/vg_root-lv_root 60G 6.2G 54G 11% /\ndevtmpfs 63G 0 63G 0% /dev\ntmpfs 63G 640M 63G 1% /dev/shm\ntmpfs 63G 32M 63G 1% /run\ntmpfs 63G 0 63G 0% /sys/fs/cgroup\n/dev/sda1 1014M 179M 836M 18% /boot\n/dev/mapper/vg_root-lv_grid 60G 40G 21G 66% /grid\n/dev/mapper/vg_root-lv_data 195G 595M 195G 1% /data\n/dev/mapper/vg_root-oracle 60G 22G 38G 37% /oracle\n/dev/mapper/vg_root-lv_home 60G 16G 45G 27% /home\ntmpfs 13G 12K 13G 1% /run/user/42\ntmpfs 13G 0 13G 0% /run/user/1000\n[root@jylhlog2 ~]#\n2.2 查看目前lv情况\n[root@jylhlog2 ~]# lvs\nLV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert\nlv_data vg_root -wi-ao---- 194.99g\nlv_grid vg_root -wi-ao---- 60.00g\nlv_home vg_root -wi-ao---- 60.00g\nlv_root vg_root -wi-ao---- 60.00g\nlv_swap vg_root -wi-ao---- 64.00g\noracle vg_root -wi-ao---- 60.00g\n[root@jylhlog2 ~]#\n2.3 查看目前vg情况\n[root@jylhlog2 ~]# vgs\nVG #PV #LV #SN Attr VSize VFree\nvg_root 1 6 0 wz–n- 498.99g 0\n[root@jylhlog2 ~]#\n2.4 创建pv\n[root@jylhlog2 ~]# pvcreate /dev/sdg1\nPhysical volume “/dev/sdg1” successfully created.\n[root@jylhlog2 ~]#\n2.5 扩容vg\n[root@jylhlog2 ~]# vgextend vg_root /dev/sdg1\nVolume group “vg_root” successfully extended\n[root@jylhlog2 ~]#\n2.6 查看vg扩容情况\n[root@jylhlog2 ~]# vgs\nVG #PV #LV #SN Attr VSize VFree\nvg_root 2 6 0 wz–n- <2.49t <2.00t\n[root@jylhlog2 ~]#\n2.7 查看扩容后vg大小情况\n[root@jylhlog2 ~]# vgdisplay\n— Volume group —\nVG Name vg_root\nSystem ID\nFormat lvm2\nMetadata Areas 2\nMetadata Sequence No 8\nVG Access read/write\nVG Status resizable\nMAX LV 0\nCur LV 6\nOpen LV 6\nMax PV 0\nCur PV 2\nAct PV 2\nVG Size <2.49 TiB\nPE Size 4.00 MiB\nTotal PE 652029\nAlloc PE / Size 127742 / 498.99 GiB\nFree PE / Size 524287 / <2.00 TiB\nVG UUID bV0qQR-Vhfg-ynTF-dbza-qJD1-ao3k-l02WiP\n[root@jylhlog2 ~]#\n2.8 扩容lv\n[root@jylhlog2 ~]# lvextend -L +2097148M /dev/mapper/vg_root-lv_data\nSize of logical volume vg_root/lv_data changed from 194.99 GiB (49918 extents) to 2.19 TiB (574205 extents).\nLogical volume vg_root/lv_data successfully resized.\n[root@jylhlog2 ~]#\n2.9 查看容量变化，发现文件系统大小没有变化\n[root@jylhlog2 ~]# df -h\nFilesystem Size Used Avail Use% Mounted on\n/dev/mapper/vg_root-lv_root 60G 6.2G 54G 11% /\ndevtmpfs 63G 0 63G 0% /dev\ntmpfs 63G 640M 63G 1% /dev/shm\ntmpfs 63G 32M 63G 1% /run\ntmpfs 63G 0 63G 0% /sys/fs/cgroup\n/dev/sda1 1014M 179M 836M 18% /boot\n/dev/mapper/vg_root-lv_grid 60G 40G 21G 66% /grid\n/dev/mapper/vg_root-lv_data 195G 595M 195G 1% /data\n/dev/mapper/vg_root-oracle 60G 22G 38G 37% /oracle\n/dev/mapper/vg_root-lv_home 60G 16G 45G 27% /home\ntmpfs 13G 12K 13G 1% /run/user/42\ntmpfs 13G 0 13G 0% /run/user/1000\n[root@jylhlog2 ~]#\n2.10 使用xfs_growfs同步磁盘信息\n[root@jylhlog2 ~]# xfs_growfs /dev/mapper/vg_root-lv_data\nmeta-data=/dev/mapper/vg_root-lv_data isize=512 agcount=4, agsize=12779008 blks\n​ = sectsz=512 attr=2, projid32bit=1\n​ = crc=1 finobt=0 spinodes=0\ndata = bsize=4096 blocks=51116032, imaxpct=25\n​ = sunit=0 swidth=0 blks\nnaming =version 2 bsize=4096 ascii-ci=0 ftype=1\nlog =internal bsize=4096 blocks=24959, version=2\n​ = sectsz=512 sunit=0 blks, lazy-count=1\nrealtime =none extsz=4096 blocks=0, rtextents=0\ndata blocks changed from 51116032 to 587985920\n[root@jylhlog2 ~]#\n2.11 查看容量变化，发现文件系统已扩容成功\n[root@jylhlog2 ~]# df -h\nFilesystem Size Used Avail Use% Mounted on\n/dev/mapper/vg_root-lv_root 60G 6.2G 54G 11% /\ndevtmpfs 63G 0 63G 0% /dev\ntmpfs 63G 640M 63G 1% /dev/shm\ntmpfs 63G 32M 63G 1% /run\ntmpfs 63G 0 63G 0% /sys/fs/cgroup\n/dev/sda1 1014M 179M 836M 18% /boot\n/dev/mapper/vg_root-lv_grid 60G 40G 21G 66% /grid\n/dev/mapper/vg_root-lv_data 2.2T 597M 2.2T 1% /data\n/dev/mapper/vg_root-oracle 60G 22G 38G 37% /oracle\n/dev/mapper/vg_root-lv_home 60G 16G 45G 27% /home\ntmpfs 13G 12K 13G 1% /run/user/42\ntmpfs 13G 0 13G 0% /run/user/1000\n[root@jylhlog2 ~]#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n\n\n> 如果sdb1是一个磁盘阵列，而这个磁盘阵列使用年代太久，我们必须移出怎么办？\n> \n> 移动数据： [root@test1 ~]# pvmove /dev/sdb1 /dev/sdb3 #将sdb1上数据移到新增加sdb3 pv 上 /dev/sdb1: Moved: 23.53% /dev/sdb1: Moved: 76.47% /dev/sdb1: Moved: 100.00% [root@test1 ~]# vgreduce vg01 /dev/sdb1 #移完数据再移出 Removed "/dev/sdb1" from volume group "vg01" [root@test1 ~]# pvs PV VG Fmt Attr PSize PFree /dev/sdb1 lvm2 --- 1.00g 1.00g /dev/sdb2 vg02 lvm2 a-- 1008.00m 1008.00m /dev/sdb3 vg01 lvm2 a-- 1020.00m 952.00m #vg01中只有sdb3了',normalizedContent:'# lvm工作原理\n\nlvm是在磁盘分区和文件系统之间添加的一个逻辑层，来为文件系统屏蔽下层磁盘分区布局，提供一个抽象的盘卷，在盘卷上建立文件系统。管理员利用lvm可以在磁盘不用重新分区的情况下动态调整文件系统的大小，并且利用lvm管理的文件系统可以跨越磁盘，当服务器添加了新的磁盘后，管理员不必将原有的文件移动到新的磁盘上，而是通过lvm可以直接扩展文件系统跨越磁盘\n\n它就是通过将底层的物理硬盘封装起来，然后以逻辑卷的方式呈现给上层应用。在lvm中，其通过对底层的硬盘进行封装，当我们对底层的物理硬盘进行操作时，其不再是针对于分区进行操作，而是通过一个叫做逻辑卷的东西来对其进行底层的磁盘管理操作。\n\n逻辑卷管理的核心在于如何处理我们系统中安装的硬盘及其分区，对于我们的逻辑卷管理器来说，把硬盘当作物理卷来看\n\n\n\n\n# lvm常用的术语\n\n先来了解一下常用的名词解释\n\n * 物理存储介质（the physical media）\n\n这里指系统的存储设备文件，可是磁盘分区，整个磁盘，raid阵列或san磁盘，设备必须初始化为lvm物理卷，才能与lvm结合使用\n\n * 物理卷pv（physical volume）\n\n物理卷就是指硬盘分区或从逻辑上与磁盘分区具有同样功能的设备(如raid)，是lvm的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与lvm相关的管理参数 （注册物理设备以便在卷组中使用）,创建物理卷它可以用硬盘分区，也可以用硬盘本身；\n\n * 卷组vg（volume group）\n\nlvm卷组类似于非lvm系统中的物理硬盘，一个lvm卷组由一个或多个物理卷组成 \n\n * 逻辑卷lv（logical volume）\n\n类似于非lvm系统中的硬盘分区，lv建立在vg之上，可以在lv之上建立文件系统\n\n * pe（physical extents）\n\npv中可以分配的最少存储单元，pe的大小是可以指定的，默认为4mb\n\n * le（logical extent）\n\nlv中可以分配的最少存储单元，在同一个卷组中，le的大小和pe是相同的，并且一一对应\n\n\n# lvm优点\n\n * 使用卷组，使多个硬盘空间看起来像是一个大的硬盘\n * 使用逻辑卷，可以跨多个硬盘空间的分区 sdb1 sdb2 sdc1 sdd2 sdf\n * 在使用逻辑卷时，它可以在空间不足时动态调整它的大小\n * 在调整逻辑卷大小时，不需要考虑逻辑卷在硬盘上的位置，不用担心没有可用的连续空间\n * 可以在线对lv,vg 进行创建，删除，调整大小等操作。lvm上的文件系统也需要重新调整大小。\n * 可以将正在使用中的硬盘迁移到别的硬盘上(emc--500g dell---1.2t)\n * 允许创建快照，可以用来保存文件系统的备份。只能使用一次\n\n> 解释：\n> \n> lvm是软件的卷管理方式，而raid是磁盘保护的方法。对于重要的业务来说，一般是同时存在。raid用来保护物理的磁盘不会因为故障而中断业务。lvm用来实现对卷的良性的管理，更好的利用磁盘资源\n\n\n# 创建lvm的基本步骤\n\n 1. 先做物理卷 pvcreate /dev/sdb1\n\n 2. 把不同的物理卷加入到卷组当中 vgcreate datavg /dev/sdb1\n\n 3. 创建逻辑卷 lvcreate\n\n 4. 格式化文件系统 mkfs.xfs\n\n 5. 创建挂载点并进行挂载 mount\n\n> lvcreate -l +100%free 指定全部大小\n\n对ext类型文件系统扩容： resize2fs\n\n对xfs类型文件系统扩容： xfs_growfs\n\nresize2fs和xfs_growfs 两者的区别是传递的参数不一样的，xfs_growfs是采用的挂载点；resize2fs是逻辑卷名称，而且resize2fs命令不能对xfs类型文件系统使用\n\n\n# lvm删除\n\n创建: pvcreate ---\x3e vgcreate ---\x3e lvcrdeate ---\x3e mkfs.xfs lv ----\x3e mount\n\n删除： umount ----\x3e lvremove lv -----\x3e vgremove vg -----\x3e pvremove\n\n\n# 底层存储更换新建pv pvcreate /dev/sdc1\n\n扩容vg vgextend datavg /dev/sdc1\n\n移动数据 pvmove /dev/sdb5 /dev/sdc1\n\nvg缩小 vgreduce datavg /dev/sdb5\n\n删除小盘 pvremove /dev/sdb5\n\n\n# 扩容操作\n\n1.fdisk /dev/vdc    //给新加的硬盘设备/dev/vdc创建分区(注意设备名称略有差异，步骤同下)\n2.分别按n、p、1、t、8e、wr\n3.pvcreate /dev/vdc1\n4.vgextend vg0 /dev/vdd1\n5.lvextend -l +100%free /dev/vg0/lv0\n6. xfs_growfs  /dev/vg0/lv0\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 大于2t的扩容使用\n\n1. 使用parted方式格式化磁盘\n1.1 查看磁盘情况\n[root@jylhlog2 ~]# fdisk -l\ndisk /dev/sdd: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sdb: 42.9 gb, 42949672960 bytes, 83886080 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sdc: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sde: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sdf: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sda: 536.9 gb, 536870912000 bytes, 1048576000 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk label type: dos\ndisk identifier: 0x0008ce60\ndevice boot start end blocks id system\n/dev/sda1 * 2048 2099199 1048576 83 linux\n/dev/sda2 2099200 1048567807 523234304 8e linux lvm\ndisk /dev/mapper/vg_root-lv_root: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_swap: 68.7 gb, 68719476736 bytes, 134217728 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_data: 209.4 gb, 209371267072 bytes, 408928256 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_home: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-oracle: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_grid: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sdg: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\n[root@jylhlog2 ~]#\n1.2 使用parted格式化/dev/sdg分区\n[root@jylhlog2 ~]# parted /dev/sdg\ngnu parted 3.1\nusing /dev/sdg\nwelcome to gnu parted! type ‘help’ to view a list of commands.\n(parted) mklabel gpt ------将mbr分区形式转为gpt分区形式\n(parted) mkpart primary xfs 0 -1 ------设置分区的开始位置和结束位置\nwarning: the resulting partition is not properly aligned for best performance.\nignore/cancel? ignore\n(parted) print ------查看分区\nmodel: vmware virtual disk (scsi)\ndisk /dev/sdg: 2199gb\nsector size (logical/physical): 512b/512b\npartition table: gpt\ndisk flags:\nnumber start end size file system name flags\n1 17.4kb 2199gb 2199gb primary\n(parted) toggle 1 lvm ------使用toggle 更改硬盘类型\n(parted) print ------查看分区\nmodel: vmware virtual disk (scsi)\ndisk /dev/sdg: 2199gb\nsector size (logical/physical): 512b/512b\npartition table: gpt\ndisk flags:\nnumber start end size file system name flags\n1 17.4kb 2199gb 2199gb primary lvm\n(parted) quit\ninformation: you may need to update /etc/fstab.\n[root@jylhlog2 ~]#\n1.3 查看格式化后的分区情况\n[root@jylhlog2 ~]# fdisk -l\ndisk /dev/sdd: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sdb: 42.9 gb, 42949672960 bytes, 83886080 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sdc: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sde: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sdf: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sda: 536.9 gb, 536870912000 bytes, 1048576000 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk label type: dos\ndisk identifier: 0x0008ce60\ndevice boot start end blocks id system\n/dev/sda1 * 2048 2099199 1048576 83 linux\n/dev/sda2 2099200 1048567807 523234304 8e linux lvm\ndisk /dev/mapper/vg_root-lv_root: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_swap: 68.7 gb, 68719476736 bytes, 134217728 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_data: 209.4 gb, 209371267072 bytes, 408928256 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_home: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-oracle: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_grid: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\nwarning: fdisk gpt support is currently new, and therefore in an experimental phase. use at your own discretion.\ndisk /dev/sdg: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk label type: gpt\ndisk identifier: 9c5e039c-2c6c-4585-8eab-7b2dba949516\n# start end size type name\n1 34 4294965343 2t linux lvm primary\n[root@jylhlog2 ~]#\n1.4 使系统重读分区表\n[root@jylhlog2 ~]# partprobe\nwarning: unable to open /dev/sr0 read-write (read-only file system). /dev/sr0 has been opened read-only.\n[root@jylhlog2 ~]#\n[root@jylhlog2 ~]# fdisk -l\ndisk /dev/sdd: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sdb: 42.9 gb, 42949672960 bytes, 83886080 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sdc: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sde: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sdf: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/sda: 536.9 gb, 536870912000 bytes, 1048576000 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk label type: dos\ndisk identifier: 0x0008ce60\ndevice boot start end blocks id system\n/dev/sda1 * 2048 2099199 1048576 83 linux\n/dev/sda2 2099200 1048567807 523234304 8e linux lvm\ndisk /dev/mapper/vg_root-lv_root: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_swap: 68.7 gb, 68719476736 bytes, 134217728 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_data: 209.4 gb, 209371267072 bytes, 408928256 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_home: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-oracle: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk /dev/mapper/vg_root-lv_grid: 64.4 gb, 64424509440 bytes, 125829120 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\nwarning: fdisk gpt support is currently new, and therefore in an experimental phase. use at your own discretion.\ndisk /dev/sdg: 2199.0 gb, 2199023255552 bytes, 4294967296 sectors\nunits = sectors of 1 * 512 = 512 bytes\nsector size (logical/physical): 512 bytes / 512 bytes\ni/o size (minimum/optimal): 512 bytes / 512 bytes\ndisk label type: gpt\ndisk identifier: 9c5e039c-2c6c-4585-8eab-7b2dba949516\n# start end size type name\n1 34 4294965343 2t linux lvm primary\n[root@jylhlog2 ~]#\n2. 创建pv、扩容vg、扩容lv、扩容xfs分区\n2.1 查看目前分区情况\n[root@jylhlog2 ~]# df -h\nfilesystem size used avail use% mounted on\n/dev/mapper/vg_root-lv_root 60g 6.2g 54g 11% /\ndevtmpfs 63g 0 63g 0% /dev\ntmpfs 63g 640m 63g 1% /dev/shm\ntmpfs 63g 32m 63g 1% /run\ntmpfs 63g 0 63g 0% /sys/fs/cgroup\n/dev/sda1 1014m 179m 836m 18% /boot\n/dev/mapper/vg_root-lv_grid 60g 40g 21g 66% /grid\n/dev/mapper/vg_root-lv_data 195g 595m 195g 1% /data\n/dev/mapper/vg_root-oracle 60g 22g 38g 37% /oracle\n/dev/mapper/vg_root-lv_home 60g 16g 45g 27% /home\ntmpfs 13g 12k 13g 1% /run/user/42\ntmpfs 13g 0 13g 0% /run/user/1000\n[root@jylhlog2 ~]#\n2.2 查看目前lv情况\n[root@jylhlog2 ~]# lvs\nlv vg attr lsize pool origin data% meta% move log cpy%sync convert\nlv_data vg_root -wi-ao---- 194.99g\nlv_grid vg_root -wi-ao---- 60.00g\nlv_home vg_root -wi-ao---- 60.00g\nlv_root vg_root -wi-ao---- 60.00g\nlv_swap vg_root -wi-ao---- 64.00g\noracle vg_root -wi-ao---- 60.00g\n[root@jylhlog2 ~]#\n2.3 查看目前vg情况\n[root@jylhlog2 ~]# vgs\nvg #pv #lv #sn attr vsize vfree\nvg_root 1 6 0 wz–n- 498.99g 0\n[root@jylhlog2 ~]#\n2.4 创建pv\n[root@jylhlog2 ~]# pvcreate /dev/sdg1\nphysical volume “/dev/sdg1” successfully created.\n[root@jylhlog2 ~]#\n2.5 扩容vg\n[root@jylhlog2 ~]# vgextend vg_root /dev/sdg1\nvolume group “vg_root” successfully extended\n[root@jylhlog2 ~]#\n2.6 查看vg扩容情况\n[root@jylhlog2 ~]# vgs\nvg #pv #lv #sn attr vsize vfree\nvg_root 2 6 0 wz–n- <2.49t <2.00t\n[root@jylhlog2 ~]#\n2.7 查看扩容后vg大小情况\n[root@jylhlog2 ~]# vgdisplay\n— volume group —\nvg name vg_root\nsystem id\nformat lvm2\nmetadata areas 2\nmetadata sequence no 8\nvg access read/write\nvg status resizable\nmax lv 0\ncur lv 6\nopen lv 6\nmax pv 0\ncur pv 2\nact pv 2\nvg size <2.49 tib\npe size 4.00 mib\ntotal pe 652029\nalloc pe / size 127742 / 498.99 gib\nfree pe / size 524287 / <2.00 tib\nvg uuid bv0qqr-vhfg-yntf-dbza-qjd1-ao3k-l02wip\n[root@jylhlog2 ~]#\n2.8 扩容lv\n[root@jylhlog2 ~]# lvextend -l +2097148m /dev/mapper/vg_root-lv_data\nsize of logical volume vg_root/lv_data changed from 194.99 gib (49918 extents) to 2.19 tib (574205 extents).\nlogical volume vg_root/lv_data successfully resized.\n[root@jylhlog2 ~]#\n2.9 查看容量变化，发现文件系统大小没有变化\n[root@jylhlog2 ~]# df -h\nfilesystem size used avail use% mounted on\n/dev/mapper/vg_root-lv_root 60g 6.2g 54g 11% /\ndevtmpfs 63g 0 63g 0% /dev\ntmpfs 63g 640m 63g 1% /dev/shm\ntmpfs 63g 32m 63g 1% /run\ntmpfs 63g 0 63g 0% /sys/fs/cgroup\n/dev/sda1 1014m 179m 836m 18% /boot\n/dev/mapper/vg_root-lv_grid 60g 40g 21g 66% /grid\n/dev/mapper/vg_root-lv_data 195g 595m 195g 1% /data\n/dev/mapper/vg_root-oracle 60g 22g 38g 37% /oracle\n/dev/mapper/vg_root-lv_home 60g 16g 45g 27% /home\ntmpfs 13g 12k 13g 1% /run/user/42\ntmpfs 13g 0 13g 0% /run/user/1000\n[root@jylhlog2 ~]#\n2.10 使用xfs_growfs同步磁盘信息\n[root@jylhlog2 ~]# xfs_growfs /dev/mapper/vg_root-lv_data\nmeta-data=/dev/mapper/vg_root-lv_data isize=512 agcount=4, agsize=12779008 blks\n​ = sectsz=512 attr=2, projid32bit=1\n​ = crc=1 finobt=0 spinodes=0\ndata = bsize=4096 blocks=51116032, imaxpct=25\n​ = sunit=0 swidth=0 blks\nnaming =version 2 bsize=4096 ascii-ci=0 ftype=1\nlog =internal bsize=4096 blocks=24959, version=2\n​ = sectsz=512 sunit=0 blks, lazy-count=1\nrealtime =none extsz=4096 blocks=0, rtextents=0\ndata blocks changed from 51116032 to 587985920\n[root@jylhlog2 ~]#\n2.11 查看容量变化，发现文件系统已扩容成功\n[root@jylhlog2 ~]# df -h\nfilesystem size used avail use% mounted on\n/dev/mapper/vg_root-lv_root 60g 6.2g 54g 11% /\ndevtmpfs 63g 0 63g 0% /dev\ntmpfs 63g 640m 63g 1% /dev/shm\ntmpfs 63g 32m 63g 1% /run\ntmpfs 63g 0 63g 0% /sys/fs/cgroup\n/dev/sda1 1014m 179m 836m 18% /boot\n/dev/mapper/vg_root-lv_grid 60g 40g 21g 66% /grid\n/dev/mapper/vg_root-lv_data 2.2t 597m 2.2t 1% /data\n/dev/mapper/vg_root-oracle 60g 22g 38g 37% /oracle\n/dev/mapper/vg_root-lv_home 60g 16g 45g 27% /home\ntmpfs 13g 12k 13g 1% /run/user/42\ntmpfs 13g 0 13g 0% /run/user/1000\n[root@jylhlog2 ~]#\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n\n\n> 如果sdb1是一个磁盘阵列，而这个磁盘阵列使用年代太久，我们必须移出怎么办？\n> \n> 移动数据： [root@test1 ~]# pvmove /dev/sdb1 /dev/sdb3 #将sdb1上数据移到新增加sdb3 pv 上 /dev/sdb1: moved: 23.53% /dev/sdb1: moved: 76.47% /dev/sdb1: moved: 100.00% [root@test1 ~]# vgreduce vg01 /dev/sdb1 #移完数据再移出 removed "/dev/sdb1" from volume group "vg01" [root@test1 ~]# pvs pv vg fmt attr psize pfree /dev/sdb1 lvm2 --- 1.00g 1.00g /dev/sdb2 vg02 lvm2 a-- 1008.00m 1008.00m /dev/sdb3 vg01 lvm2 a-- 1020.00m 952.00m #vg01中只有sdb3了',charsets:{cjk:!0}},{title:"sudo权限规划",frontmatter:{title:"sudo权限规划",date:"2022-12-15T18:49:07.000Z",permalink:"/pages/0bac05/",categories:["运维","linux系统"],tags:[null],feed:{enable:!0},description:"sudo权限规划",readingShow:"top",meta:[{name:"twitter:title",content:"sudo权限规划"},{name:"twitter:description",content:"sudo权限规划"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/05.sudo%E6%9D%83%E9%99%90%E8%A7%84%E5%88%92.html"},{property:"og:type",content:"article"},{property:"og:title",content:"sudo权限规划"},{property:"og:description",content:"sudo权限规划"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/05.sudo%E6%9D%83%E9%99%90%E8%A7%84%E5%88%92.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T18:49:07.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"sudo权限规划"},{itemprop:"description",content:"sudo权限规划"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/01.linux/05.sudo%E6%9D%83%E9%99%90%E8%A7%84%E5%88%92.html",relativePath:"04.运维/01.linux/05.sudo权限规划.md",key:"v-ba2529e4",path:"/pages/0bac05/",headersStr:null,content:'一 问题简述\n\n随着公司的服务器越来越多,人员流动性也开始与日俱增,以往管理服务器的陈旧思想（root权限随意给出，开发、测试、运维共用同一账号）应当摒弃,公司需要有更好更完善的权限体系,经过多轮沟通和协商,公司一致决定重新整理规划权限体系。运维主管明确指出权限存在的问题,并需要解决以往的root权限泛滥问题。\n\n我作为本次权限规划的发起人,我首先了解到公司服务器权限现状后,经过多次与相关员工及领导沟通,确认了公司存在的部分问题:运维部基本入职离职流程中存在一些账号问题，如：离职不报备,系统权限不回收、账号密码过于简单化等。root权限随意给开发及测试。\n\n以上问题无疑给公司的服务器及数据安全造成了不小的隐患。因此下文将详解此次关于权限划分的方案。\n\n公司有多个部门使用我们提供的linux服务器以及开通的账号:安全权限没有进行合理规划,因此我提出更加安全的账号管理方式:①领用账号权限流程，②命令执行以sudo授权形式。\n\n优势: 它可以对账号进行详细的权限分层划分,给服务器带来了更好的安全保障，公司有级别不同的运维人员,我们需要对其权限整理划分,根据职责能力我们规划权限为初级、中级、高级。而其他部门，如开发、测试等部门采取服务器账号权限流程。这样有利于权限最小化控制，避免因权限滥用导致服务运行不稳定，配置随意修改，不规范操作等安全隐患。为后续日志审计等溯源，分析奠定基础。\n\n二 权限规划表及技术人员配备情况\n\n1.权限规划表\n\n①运维组权限规划表\n\n       \n级别     权限\n初级运维   查看系统、网络、服务、进程状态信息：\n       /bin/hostname,/sbin/ifconfig,/bin/netstat,/sbin/route,/sbin/ip,/bin/ping,\n       /usr/bin/sar,/usr/bin/free,\n        \n       /usr/bin/vmstat,/usr/bin/mpstat,/usr/bin/iostat,/usr/sbin/iotop,/usr/bin/top,/bin/ps\n高级运维   查看和修改系统、网格、服务、进程状态配置信息，软件包管理，存储管理\n       \n       /bin/hostname,/sbin/ifconfig,/bin/netstat,/sbin/route,/sbin/ip,/usr/bin/sar,\n       /usr/bin/free, /usr/bin/vmstat,\n       /usr/bin/mpstat,/usr/bin/iostat,/usr/sbin/iotop,/usr/bin/top,\n       /bin/ps,/sbin/iptables,/etc/init.d/network,/bin/nice,/bin/kill,/usr/bin/kill,\n       /usr/bin/killall,/bin/rpm,/usr/bin/up2date,/usr/bin/yum,/sbin/fdisk,/sbin/sfdisk,\n       /sbin/parted,/sbin/partprobe,/bin/mount,/bin/umount\n运维经理   超级用户所有权限\n       \n         ALL\n\n②开发组权限模板\n\n       \n级别     权限\n初级开发   root的查看权限，对应服务查看日志的权限\n       \n         /usr/bin/tail /app/log*,/bin/grep\n       /app/log*,/bin/cat,/bin/ls\n高级开发   root的查看权限，对应服务查看日志的权限，重启对应服务的权限\n       \n       /sbin/service,/sbin/chkconfig,/usr/bin/tail\n       /app/log*,/bin/grep   /app/log*,/bin/cat,/bin/ls, /bin/sh\n       ~/scripts/deploy.sh\n开发经理   项目所在服务器的ALL权限，不能修改root密码\n       \n         ALL,!/usr/bin/passwd root,!/usr/sbin/visudo,!/usr/bin/vim\n         /etc/sudoers\n\n③测试组权限模板\n\n     \n级别   权限\n测试   普通用户的权限\n     \n     不加入sudo列表\n\n④DBA组权限模板\n\n        \n级别      权限\n初级DBA   普通用户的权限\n        \n        不加入sudo列表\n高级DBA   项目所在数据库服务器的ALL权限\n        \n          ALL,!/usr/bin/passwd root,!/usr/sbin/visudo,!/usr/bin/vim\n          /etc/sudoers\n\n⑤网络组权限模板\n\n       \n级别     权限\n初级网工   普通用户的权限\n       \n       不加入sudo列表\n高级网工   项目所在服务器的有关网络配置的权限\n       \n        \n       /sbin/route,/sbin/ifconfig,/bin/ping,/sbin/dhclient,/usr/bin/net,\n       /sbin/ip,/sbin/iptables,/usr/bin/rfcomm,/usr/bin/wvdial,/sbin/iwconfig,/sbin/mii-tool,/bin/cat\n       /var/log/,/usr/bin/vim   /etc/sysconfig/network-scripts/\n\n2.公司目前的技术人员配备情况\n\n运维组：5个初级运维，2个高级运维，1个运维经理\n\n开发组：3个初级开发人员，1个高级开发，1个开发经理\n\n测试组：2个测试工程师（测试组不加入sudo）\n\nDBA组：3个初级DBA（初级DBA不加入sudo），1个高级DBA\n\n网络组：2个初级网工（初级DBA不加入sudo），1个高级网工\n\n三 命令讲解\n\n创建用户\n\nuseradd yw && echo "123456" | passwd --stdin yw\n\n创建yw用户,然后输出一个123456交passwd作为yw用户的密码\n\nfor n in seq 21 25;do useradd user$n;echo "user$necho user$n|md5sum|cut -c4-8"|passwd --stdin user$n;done\n\n用for循环来创建用户账号和密码: seq 21 25创建5个数字（21，22，23，24，25），n 就代表这5个数字，后面的创建用户接着user$n就是以user开头接n，\n\n然后 echo 输出用户名的md5，使用cut 取出4-8块的字符交给passwd --stdin来为每位用户配置不同的密码,他们的密码都是用户名的md5值的4至8位字符串。\n\n删除用户\n\nuserdel yw\n\n删除yw用户\n\nfor n in seq 21 25;do userdel -r user$n;done\n\n用for循环删除用户\n\n查看自己的sudo权限：sudo -l\n\n……\n\n……\n\n四 实施命令脚本\n\n举例：在公司时根据实际情况编写用户账号密码及相应权限，此处根据上述权限规划表为例。\n\n1.创建用户组和用户：\n\n#运维组、开发组、测试组、DBA组、网络组\n\ngroupadd chujiyunwei -g 1100\n\ngroupadd gaojiyunwei -g 1110\n\ngroupadd yunweijingli -g 1120\n\ngroupadd chujikaifa -g 1200\n\ngroupadd gaojikaifa -g 1210\n\ngroupadd kaifajingli -g 1220\n\ngroupadd ceshi -g 1300\n\ngroupadd chujidba -g 1400\n\ngroupadd gaojidba -g 1410\n\ngroupadd chujinetwork -g 1500\n\ngroupadd gaojinetwork -g 1510\n\n#-----------------------------------------------------------------------------\n\n#运维组：5个初级运维，2个高级运维，1个运维经理\n\n#-----------------------------------------------------------------------------\n\nfor n in seq 1 5;do useradd chujiyunwei$n -g 1100;echo "123456" | passwd --stdin chuji$n;done\n\n#创建5个初级运维账户并配置了密码\n\nfor n in seq 1 2;do useradd gaojiyunwei$n -g 1110;echo "123456" | passwd --stdin gaoji$n;done\n\n#创建2个高级运维的用户和密码\n\nuseradd yunweijingli -g 1120 && echo 123456 | passwd --stdin yunweijingli\n\n#创建1个运维经理的账号和密码\n\n#-----------------------------------------------------------------------------\n\n#开发组：3个初级开发人员，1个高级开发，1个开发经理\n\n#-----------------------------------------------------------------------------\n\nfor n in seq 1 3;do useradd chujikaifa$n -g 1200;echo "123456" | passwd --stdin chujikaifa$n;done\n\n#创建3个初级开发账户并配置了密码\n\nuseradd gaojikaifa1 -g 1210 && echo "123456" | passwd --stdin gaojikaifa1\n\n#创建1个高级开发的用户和密码\n\nuseradd kaifajingli -g 1220 && echo 123456 | passwd --stdin kaifajingli\n\n#创建1个开发经理的账号和密码\n\n#-----------------------------------------------------------------------------\n\n#测试组：2个测试\n\n#-----------------------------------------------------------------------------\n\nfor n in seq 1 2;do useradd ceshi$n -g 1300;echo "123456" | passwd --stdin ceshi$n;done\n\n#创建2个测试账号和密码\n\n#-----------------------------------------------------------------------------\n\n#DBA组：3个初级DBA，1个高级DBA\n\n#-----------------------------------------------------------------------------\n\nfor n in seq 1 3;do useradd chujidba$n -g 1400;echo "123456" | passwd --stdin chujidba$n;done\n\n#创建3个初级dba 账号和密码\n\nuseradd gaojidba1 -g 1410&& echo 123456 | passwd --stdin gaojidba1\n\n#创建1个高级dba账号和密码\n\n#-----------------------------------------------------------------------------\n\n#网工组：2个初级网工，1个高级网工\n\n#-----------------------------------------------------------------------------\n\nfor n in seq 1 2;do useradd chujinetwork$n -g 1500;echo "123456" | passwd --stdin chujinetwork$n;done\n\n#创建2个初级网工\n\nuseradd gaojinetwork1 -g 1510 && echo 123456 | passwd --stdin gaojinetwork1\n\n#创建1个高级网工\n\n2.配置/etc/sudoers文件\n\ncat >>/etc/sudoers<<EOF\n\n#配置用户组别名:\n\nUser_Alias CHUJIYUNWEI = %chujiyunwei\n\nUser_Alias GAOJIYUNWEI = %gaojiyunwei\n\nUser_Alias YUNWEIJINGLI = %yunweijingli\n\nUser_Alias CHUJIKAIFA = %chujikaifa\n\nUser_Alias GAOJIKAIFA = %gaojikaifa\n\nUser_Alias KAIFAJINGLI = %kaifajingli\n\nUser_Alias CESHI= %ceshi\n\nUser_Alias CHUJIDBA = %chujidba\n\nUser_Alias GAOJIDBA = %gaojidba\n\nUser_Alias CHUJINETWORK = %chujinetwork\n\nUser_Alias GAOJINETWORK = %gaojinetwork\n\n#配置命令别名：\n\nCmnd_Alias CHUJIYUNWEI_CMD =  /bin/hostname,/sbin/ifconfig,/bin/netstat,/sbin/route,/sbin/ip,/bin/ping,/usr/bin/sar,/usr/bin/free, /usr/bin/vmstat,/usr/bin/mpstat,/usr/bin/iostat,/usr/sbin/iotop,/usr/bin/top,/bin/ps\n\nCmnd_Alias GAOJIYUNWEI_CMD =  /bin/hostname,/sbin/ifconfig,/bin/netstat,/sbin/route,/sbin/ip,/usr/bin/sar,/usr/bin/free,/usr/bin/vmstat,/usr/bin/mpstat,/usr/bin/iostat,/usr/sbin/iotop,/usr/bin/top,/bin/ps,/sbin/iptables,/etc/init.d/network,/bin/nice,/bin/kill,/usr/bin/kill,/usr/bin/killall,/bin/rpm,/usr/bin/up2date,/usr/bin/yum,/sbin/fdisk,/sbin/sfdisk,/sbin/parted,/sbin/partprobe,/bin/mount,/bin/umount\n\nCmnd_Alias YUNWEIJINGLI_CMD =  ALL\n\nCmnd_Alias CHUJIKAIFA_CMD =  /usr/bin/tail /app/log*,/bin/grep /app/log*,/bin/cat,/bin/ls\n\nCmnd_Alias GAOJIKAIFA_CMD =  /sbin/service,/sbin/chkconfig,/usr/bin/tail /app/log*,/bin/grep /app/log*,/bin/cat,/bin/ls, /bin/sh ~/scripts/deploy.sh\n\nCmnd_Alias KAIFAJINGLI_CMD =  ALL,!/usr/bin/passwd root,!/usr/sbin/visudo,!/usr/bin/vim /etc/sudoers\n\nCmnd_Alias CESHI_CMD =  /usr/bin/uname\n\nCmnd_Alias CHUJIDBA_CMD =  /usr/bin/uname\n\nCmnd_Alias GAOJIDBA_CMD =  ALL,!/usr/bin/passwd root,!/usr/sbin/visudo,!/usr/bin/vim /etc/sudoers\n\nCmnd_Alias CHUJINETWORK_CMD =  /usr/bin/uname\n\nCmnd_Alias GAOJINETWORK_CMD =  /sbin/route,/sbin/ifconfig,/bin/ping,/sbin/dhclient,/usr/bin/net,/sbin/ip,/sbin/iptables,/usr/bin/rfcomm,/usr/bin/wvdial,/sbin/iwconfig,/sbin/mii-tool,/bin/cat /var/log/,/usr/bin/vim /etc/sysconfig/network-scripts/\n\n#用户组能获取到的权限\n\nRunas_Alias CHUJIYUNWEI = root\n\nRunas_Alias GAOJIYUNWEI = root\n\nRunas_Alias YUNWEIJINGLI = root\n\nRunas_Alias CHUJIKAIFA = root\n\nRunas_Alias GAOJIKAIFA = root\n\nRunas_Alias KAIFAJINGLI = root\n\nRunas_Alias CESHI = root\n\nRunas_Alias CHUJIDBA = root\n\nRunas_Alias GAOJIDBA = root\n\nRunas_Alias CHUJINETWORK = root\n\nRunas_Alias GAOJINETWORK = root\n\n#用户组权限对应关系\n\nCHUJIYUNWEI ALL=(CHUJIYUNWEI) CHUJIYUNWEI_CMD\n\nGAOJIYUNWEI ALL=(GAOJIYUNWEI) GAOJIYUNWEI_CMD\n\nYUNWEIJINGLI ALL=(YUNWEIJINGLI) YUNWEIJINGLI_CMD\n\nCHUJIKAIFA ALL=(CHUJIKAIFA) CHUJIKAIFA_CMD\n\nGAOJIKAIFA ALL=(GAOJIKAIFA) GAOJIKAIFA_CMD\n\nKAIFAJINGLI ALL=(KAIFAJINGLI) KAIFAJINGLI_CMD\n\nCESHI ALL=(CESHI) CESHI_CMD\n\nCHUJIDBA ALL=(CHUJIDBA) CHUJIDBA_CMD\n\nGAOJIDBA ALL=(GAOJIDBA) GAOJIDBA_CMD\n\nCHUJINETWORK ALL=(CHUJINETWORK) CHUJINETWORK_CMD\n\nGAOJINETWORK ALL=(GAOJINETWORK) GAOJINETWORK_CMD\n\nEOF\n\n五 权限领用流程\n\n1.   领用流程\n\n① 领用人发起权限领用流程，填写权限配置变更申请表，交由运维人员审核。\n\n② 运维人员审核没有问题告知运维主管，由运维主管开放权限。\n\n2.配置更变申请表\n\n                                                                                            \n配置变更申请表                                                                                     \n*以下申请部门填写                                                                          填表日期：    \n申请部门                                                                                        \n申 请 人                                                                              联系方式     \n申请测试日期                                                                   年  月  日   申请实施日期   \n实施部门                                                                                        \n实施联系人                                                                              联系方式     \n变更目的：                                                                                       \n变更内容：（如内容较多，请另附页说明）                                                                         \n申请部门                                        口 一般变更    口 重大变更    口 紧急变更                      \n\n意   见\n签章：                  日期：      年    月    日                                                   \n*以下为运维单位填写                                                                                  \n运维单位                                        口 一般变更    口 重大变更    口 紧急变更                      \n\n意   见\n口 同意申请实施日期     口 请在以下日期执行：     年  月  日                                                      \n签章：                  日期：      年    月    日                                                   \n*以下为实施人填写                                                                                   \n运维单位                                                                                        \n\n处理结果\n签章：                  日期：      年    月    日                                                   \n\n注： 1、该申请需附带需求报告、开发及变更计划、变更方案操作步骤详解、用户测试报告、源代码或脚本等的纸质文件，及操作所涉及到的电子文件用以存档。\n\n2、变更方案需包括变更步骤详解、变更成功标志、回滚方案等。\n\n（例子）\n\ngroupadd loguser -g 1003\n\nuseradd loguser -g 1003 && echo "############" | passwd --stdin loguser\n\ncat >>/etc/sudoers<<EOF\n\nUser_Alias LOGUSER = %loguser\n\nCmnd_Alias LOGUSER_CMD = /sbin/service,/sbin/chkconfig,/usr/bin/tail,/bin/grep,/bin/cat,/bin/ls, /bin/sh,/bin/vim,/bin/less,/usr/bin/docker\n\nRunas_Alias LOGUSER = root\n\nLOGUSER ALL=(LOGUSER) NOPASSWD:LOGUSER_CMD\n\nEOF',normalizedContent:'一 问题简述\n\n随着公司的服务器越来越多,人员流动性也开始与日俱增,以往管理服务器的陈旧思想（root权限随意给出，开发、测试、运维共用同一账号）应当摒弃,公司需要有更好更完善的权限体系,经过多轮沟通和协商,公司一致决定重新整理规划权限体系。运维主管明确指出权限存在的问题,并需要解决以往的root权限泛滥问题。\n\n我作为本次权限规划的发起人,我首先了解到公司服务器权限现状后,经过多次与相关员工及领导沟通,确认了公司存在的部分问题:运维部基本入职离职流程中存在一些账号问题，如：离职不报备,系统权限不回收、账号密码过于简单化等。root权限随意给开发及测试。\n\n以上问题无疑给公司的服务器及数据安全造成了不小的隐患。因此下文将详解此次关于权限划分的方案。\n\n公司有多个部门使用我们提供的linux服务器以及开通的账号:安全权限没有进行合理规划,因此我提出更加安全的账号管理方式:①领用账号权限流程，②命令执行以sudo授权形式。\n\n优势: 它可以对账号进行详细的权限分层划分,给服务器带来了更好的安全保障，公司有级别不同的运维人员,我们需要对其权限整理划分,根据职责能力我们规划权限为初级、中级、高级。而其他部门，如开发、测试等部门采取服务器账号权限流程。这样有利于权限最小化控制，避免因权限滥用导致服务运行不稳定，配置随意修改，不规范操作等安全隐患。为后续日志审计等溯源，分析奠定基础。\n\n二 权限规划表及技术人员配备情况\n\n1.权限规划表\n\n①运维组权限规划表\n\n       \n级别     权限\n初级运维   查看系统、网络、服务、进程状态信息：\n       /bin/hostname,/sbin/ifconfig,/bin/netstat,/sbin/route,/sbin/ip,/bin/ping,\n       /usr/bin/sar,/usr/bin/free,\n        \n       /usr/bin/vmstat,/usr/bin/mpstat,/usr/bin/iostat,/usr/sbin/iotop,/usr/bin/top,/bin/ps\n高级运维   查看和修改系统、网格、服务、进程状态配置信息，软件包管理，存储管理\n       \n       /bin/hostname,/sbin/ifconfig,/bin/netstat,/sbin/route,/sbin/ip,/usr/bin/sar,\n       /usr/bin/free, /usr/bin/vmstat,\n       /usr/bin/mpstat,/usr/bin/iostat,/usr/sbin/iotop,/usr/bin/top,\n       /bin/ps,/sbin/iptables,/etc/init.d/network,/bin/nice,/bin/kill,/usr/bin/kill,\n       /usr/bin/killall,/bin/rpm,/usr/bin/up2date,/usr/bin/yum,/sbin/fdisk,/sbin/sfdisk,\n       /sbin/parted,/sbin/partprobe,/bin/mount,/bin/umount\n运维经理   超级用户所有权限\n       \n         all\n\n②开发组权限模板\n\n       \n级别     权限\n初级开发   root的查看权限，对应服务查看日志的权限\n       \n         /usr/bin/tail /app/log*,/bin/grep\n       /app/log*,/bin/cat,/bin/ls\n高级开发   root的查看权限，对应服务查看日志的权限，重启对应服务的权限\n       \n       /sbin/service,/sbin/chkconfig,/usr/bin/tail\n       /app/log*,/bin/grep   /app/log*,/bin/cat,/bin/ls, /bin/sh\n       ~/scripts/deploy.sh\n开发经理   项目所在服务器的all权限，不能修改root密码\n       \n         all,!/usr/bin/passwd root,!/usr/sbin/visudo,!/usr/bin/vim\n         /etc/sudoers\n\n③测试组权限模板\n\n     \n级别   权限\n测试   普通用户的权限\n     \n     不加入sudo列表\n\n④dba组权限模板\n\n        \n级别      权限\n初级dba   普通用户的权限\n        \n        不加入sudo列表\n高级dba   项目所在数据库服务器的all权限\n        \n          all,!/usr/bin/passwd root,!/usr/sbin/visudo,!/usr/bin/vim\n          /etc/sudoers\n\n⑤网络组权限模板\n\n       \n级别     权限\n初级网工   普通用户的权限\n       \n       不加入sudo列表\n高级网工   项目所在服务器的有关网络配置的权限\n       \n        \n       /sbin/route,/sbin/ifconfig,/bin/ping,/sbin/dhclient,/usr/bin/net,\n       /sbin/ip,/sbin/iptables,/usr/bin/rfcomm,/usr/bin/wvdial,/sbin/iwconfig,/sbin/mii-tool,/bin/cat\n       /var/log/,/usr/bin/vim   /etc/sysconfig/network-scripts/\n\n2.公司目前的技术人员配备情况\n\n运维组：5个初级运维，2个高级运维，1个运维经理\n\n开发组：3个初级开发人员，1个高级开发，1个开发经理\n\n测试组：2个测试工程师（测试组不加入sudo）\n\ndba组：3个初级dba（初级dba不加入sudo），1个高级dba\n\n网络组：2个初级网工（初级dba不加入sudo），1个高级网工\n\n三 命令讲解\n\n创建用户\n\nuseradd yw && echo "123456" | passwd --stdin yw\n\n创建yw用户,然后输出一个123456交passwd作为yw用户的密码\n\nfor n in seq 21 25;do useradd user$n;echo "user$necho user$n|md5sum|cut -c4-8"|passwd --stdin user$n;done\n\n用for循环来创建用户账号和密码: seq 21 25创建5个数字（21，22，23，24，25），n 就代表这5个数字，后面的创建用户接着user$n就是以user开头接n，\n\n然后 echo 输出用户名的md5，使用cut 取出4-8块的字符交给passwd --stdin来为每位用户配置不同的密码,他们的密码都是用户名的md5值的4至8位字符串。\n\n删除用户\n\nuserdel yw\n\n删除yw用户\n\nfor n in seq 21 25;do userdel -r user$n;done\n\n用for循环删除用户\n\n查看自己的sudo权限：sudo -l\n\n……\n\n……\n\n四 实施命令脚本\n\n举例：在公司时根据实际情况编写用户账号密码及相应权限，此处根据上述权限规划表为例。\n\n1.创建用户组和用户：\n\n#运维组、开发组、测试组、dba组、网络组\n\ngroupadd chujiyunwei -g 1100\n\ngroupadd gaojiyunwei -g 1110\n\ngroupadd yunweijingli -g 1120\n\ngroupadd chujikaifa -g 1200\n\ngroupadd gaojikaifa -g 1210\n\ngroupadd kaifajingli -g 1220\n\ngroupadd ceshi -g 1300\n\ngroupadd chujidba -g 1400\n\ngroupadd gaojidba -g 1410\n\ngroupadd chujinetwork -g 1500\n\ngroupadd gaojinetwork -g 1510\n\n#-----------------------------------------------------------------------------\n\n#运维组：5个初级运维，2个高级运维，1个运维经理\n\n#-----------------------------------------------------------------------------\n\nfor n in seq 1 5;do useradd chujiyunwei$n -g 1100;echo "123456" | passwd --stdin chuji$n;done\n\n#创建5个初级运维账户并配置了密码\n\nfor n in seq 1 2;do useradd gaojiyunwei$n -g 1110;echo "123456" | passwd --stdin gaoji$n;done\n\n#创建2个高级运维的用户和密码\n\nuseradd yunweijingli -g 1120 && echo 123456 | passwd --stdin yunweijingli\n\n#创建1个运维经理的账号和密码\n\n#-----------------------------------------------------------------------------\n\n#开发组：3个初级开发人员，1个高级开发，1个开发经理\n\n#-----------------------------------------------------------------------------\n\nfor n in seq 1 3;do useradd chujikaifa$n -g 1200;echo "123456" | passwd --stdin chujikaifa$n;done\n\n#创建3个初级开发账户并配置了密码\n\nuseradd gaojikaifa1 -g 1210 && echo "123456" | passwd --stdin gaojikaifa1\n\n#创建1个高级开发的用户和密码\n\nuseradd kaifajingli -g 1220 && echo 123456 | passwd --stdin kaifajingli\n\n#创建1个开发经理的账号和密码\n\n#-----------------------------------------------------------------------------\n\n#测试组：2个测试\n\n#-----------------------------------------------------------------------------\n\nfor n in seq 1 2;do useradd ceshi$n -g 1300;echo "123456" | passwd --stdin ceshi$n;done\n\n#创建2个测试账号和密码\n\n#-----------------------------------------------------------------------------\n\n#dba组：3个初级dba，1个高级dba\n\n#-----------------------------------------------------------------------------\n\nfor n in seq 1 3;do useradd chujidba$n -g 1400;echo "123456" | passwd --stdin chujidba$n;done\n\n#创建3个初级dba 账号和密码\n\nuseradd gaojidba1 -g 1410&& echo 123456 | passwd --stdin gaojidba1\n\n#创建1个高级dba账号和密码\n\n#-----------------------------------------------------------------------------\n\n#网工组：2个初级网工，1个高级网工\n\n#-----------------------------------------------------------------------------\n\nfor n in seq 1 2;do useradd chujinetwork$n -g 1500;echo "123456" | passwd --stdin chujinetwork$n;done\n\n#创建2个初级网工\n\nuseradd gaojinetwork1 -g 1510 && echo 123456 | passwd --stdin gaojinetwork1\n\n#创建1个高级网工\n\n2.配置/etc/sudoers文件\n\ncat >>/etc/sudoers<<eof\n\n#配置用户组别名:\n\nuser_alias chujiyunwei = %chujiyunwei\n\nuser_alias gaojiyunwei = %gaojiyunwei\n\nuser_alias yunweijingli = %yunweijingli\n\nuser_alias chujikaifa = %chujikaifa\n\nuser_alias gaojikaifa = %gaojikaifa\n\nuser_alias kaifajingli = %kaifajingli\n\nuser_alias ceshi= %ceshi\n\nuser_alias chujidba = %chujidba\n\nuser_alias gaojidba = %gaojidba\n\nuser_alias chujinetwork = %chujinetwork\n\nuser_alias gaojinetwork = %gaojinetwork\n\n#配置命令别名：\n\ncmnd_alias chujiyunwei_cmd =  /bin/hostname,/sbin/ifconfig,/bin/netstat,/sbin/route,/sbin/ip,/bin/ping,/usr/bin/sar,/usr/bin/free, /usr/bin/vmstat,/usr/bin/mpstat,/usr/bin/iostat,/usr/sbin/iotop,/usr/bin/top,/bin/ps\n\ncmnd_alias gaojiyunwei_cmd =  /bin/hostname,/sbin/ifconfig,/bin/netstat,/sbin/route,/sbin/ip,/usr/bin/sar,/usr/bin/free,/usr/bin/vmstat,/usr/bin/mpstat,/usr/bin/iostat,/usr/sbin/iotop,/usr/bin/top,/bin/ps,/sbin/iptables,/etc/init.d/network,/bin/nice,/bin/kill,/usr/bin/kill,/usr/bin/killall,/bin/rpm,/usr/bin/up2date,/usr/bin/yum,/sbin/fdisk,/sbin/sfdisk,/sbin/parted,/sbin/partprobe,/bin/mount,/bin/umount\n\ncmnd_alias yunweijingli_cmd =  all\n\ncmnd_alias chujikaifa_cmd =  /usr/bin/tail /app/log*,/bin/grep /app/log*,/bin/cat,/bin/ls\n\ncmnd_alias gaojikaifa_cmd =  /sbin/service,/sbin/chkconfig,/usr/bin/tail /app/log*,/bin/grep /app/log*,/bin/cat,/bin/ls, /bin/sh ~/scripts/deploy.sh\n\ncmnd_alias kaifajingli_cmd =  all,!/usr/bin/passwd root,!/usr/sbin/visudo,!/usr/bin/vim /etc/sudoers\n\ncmnd_alias ceshi_cmd =  /usr/bin/uname\n\ncmnd_alias chujidba_cmd =  /usr/bin/uname\n\ncmnd_alias gaojidba_cmd =  all,!/usr/bin/passwd root,!/usr/sbin/visudo,!/usr/bin/vim /etc/sudoers\n\ncmnd_alias chujinetwork_cmd =  /usr/bin/uname\n\ncmnd_alias gaojinetwork_cmd =  /sbin/route,/sbin/ifconfig,/bin/ping,/sbin/dhclient,/usr/bin/net,/sbin/ip,/sbin/iptables,/usr/bin/rfcomm,/usr/bin/wvdial,/sbin/iwconfig,/sbin/mii-tool,/bin/cat /var/log/,/usr/bin/vim /etc/sysconfig/network-scripts/\n\n#用户组能获取到的权限\n\nrunas_alias chujiyunwei = root\n\nrunas_alias gaojiyunwei = root\n\nrunas_alias yunweijingli = root\n\nrunas_alias chujikaifa = root\n\nrunas_alias gaojikaifa = root\n\nrunas_alias kaifajingli = root\n\nrunas_alias ceshi = root\n\nrunas_alias chujidba = root\n\nrunas_alias gaojidba = root\n\nrunas_alias chujinetwork = root\n\nrunas_alias gaojinetwork = root\n\n#用户组权限对应关系\n\nchujiyunwei all=(chujiyunwei) chujiyunwei_cmd\n\ngaojiyunwei all=(gaojiyunwei) gaojiyunwei_cmd\n\nyunweijingli all=(yunweijingli) yunweijingli_cmd\n\nchujikaifa all=(chujikaifa) chujikaifa_cmd\n\ngaojikaifa all=(gaojikaifa) gaojikaifa_cmd\n\nkaifajingli all=(kaifajingli) kaifajingli_cmd\n\nceshi all=(ceshi) ceshi_cmd\n\nchujidba all=(chujidba) chujidba_cmd\n\ngaojidba all=(gaojidba) gaojidba_cmd\n\nchujinetwork all=(chujinetwork) chujinetwork_cmd\n\ngaojinetwork all=(gaojinetwork) gaojinetwork_cmd\n\neof\n\n五 权限领用流程\n\n1.   领用流程\n\n① 领用人发起权限领用流程，填写权限配置变更申请表，交由运维人员审核。\n\n② 运维人员审核没有问题告知运维主管，由运维主管开放权限。\n\n2.配置更变申请表\n\n                                                                                            \n配置变更申请表                                                                                     \n*以下申请部门填写                                                                          填表日期：    \n申请部门                                                                                        \n申 请 人                                                                              联系方式     \n申请测试日期                                                                   年  月  日   申请实施日期   \n实施部门                                                                                        \n实施联系人                                                                              联系方式     \n变更目的：                                                                                       \n变更内容：（如内容较多，请另附页说明）                                                                         \n申请部门                                        口 一般变更    口 重大变更    口 紧急变更                      \n\n意   见\n签章：                  日期：      年    月    日                                                   \n*以下为运维单位填写                                                                                  \n运维单位                                        口 一般变更    口 重大变更    口 紧急变更                      \n\n意   见\n口 同意申请实施日期     口 请在以下日期执行：     年  月  日                                                      \n签章：                  日期：      年    月    日                                                   \n*以下为实施人填写                                                                                   \n运维单位                                                                                        \n\n处理结果\n签章：                  日期：      年    月    日                                                   \n\n注： 1、该申请需附带需求报告、开发及变更计划、变更方案操作步骤详解、用户测试报告、源代码或脚本等的纸质文件，及操作所涉及到的电子文件用以存档。\n\n2、变更方案需包括变更步骤详解、变更成功标志、回滚方案等。\n\n（例子）\n\ngroupadd loguser -g 1003\n\nuseradd loguser -g 1003 && echo "############" | passwd --stdin loguser\n\ncat >>/etc/sudoers<<eof\n\nuser_alias loguser = %loguser\n\ncmnd_alias loguser_cmd = /sbin/service,/sbin/chkconfig,/usr/bin/tail,/bin/grep,/bin/cat,/bin/ls, /bin/sh,/bin/vim,/bin/less,/usr/bin/docker\n\nrunas_alias loguser = root\n\nloguser all=(loguser) nopasswd:loguser_cmd\n\neof',charsets:{cjk:!0}},{title:"linux修改网卡为eth0的两种方法",frontmatter:{title:"linux修改网卡为eth0的两种方法",date:"2022-12-21T14:34:00.000Z",permalink:"/pages/c17655/",categories:["运维","linux"],tags:[null],feed:{enable:!0},description:"sudo权限规划",readingShow:"top",meta:[{name:"image",content:"https://img-blog.csdnimg.cn/20210719151032530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjUxNjkyMg==,size_16,color_FFFFFF,t_70"},{name:"twitter:title",content:"linux修改网卡为eth0的两种方法"},{name:"twitter:description",content:"sudo权限规划"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://img-blog.csdnimg.cn/20210719151032530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjUxNjkyMg==,size_16,color_FFFFFF,t_70"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/06.linux%E4%BF%AE%E6%94%B9%E7%BD%91%E5%8D%A1%E4%B8%BAeth0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95.html"},{property:"og:type",content:"article"},{property:"og:title",content:"linux修改网卡为eth0的两种方法"},{property:"og:description",content:"sudo权限规划"},{property:"og:image",content:"https://img-blog.csdnimg.cn/20210719151032530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjUxNjkyMg==,size_16,color_FFFFFF,t_70"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/06.linux%E4%BF%AE%E6%94%B9%E7%BD%91%E5%8D%A1%E4%B8%BAeth0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-21T14:34:00.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"linux修改网卡为eth0的两种方法"},{itemprop:"description",content:"sudo权限规划"},{itemprop:"image",content:"https://img-blog.csdnimg.cn/20210719151032530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjUxNjkyMg==,size_16,color_FFFFFF,t_70"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/01.linux/06.linux%E4%BF%AE%E6%94%B9%E7%BD%91%E5%8D%A1%E4%B8%BAeth0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95.html",relativePath:"04.运维/01.linux/06.linux修改网卡为eth0的两种方法.md",key:"v-2c290406",path:"/pages/c17655/",headers:[{level:4,title:"在系统安装时",slug:"在系统安装时",normalizedTitle:"在系统安装时",charIndex:2},{level:4,title:"系统安装完成后",slug:"系统安装完成后",normalizedTitle:"系统安装完成后",charIndex:80}],headersStr:"在系统安装时 系统安装完成后",content:'# 在系统安装时\n\n 1. 如下图，系统安装时，直接按下"tab"按键，进入编辑界面，添加如下信息："net.ifnames=0 biosdevname=0",系统安装完成后默认为eth0。\n\n\n\n# 系统安装完成后\n\n 1. 修改网卡配置文件DEVICE配置的值：\n    \n    vim /etc/sysconfig/network-scripts/ifcfg-ens33 ... DEVICE=eth0 ...\n\n 2. 重新命名网卡文件\n    \n    mv /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-eth0\n\n 3. 由于centos7采用grub2 引导，还需要对 grub2 进行修改，编辑 /etc/default/grub 配置文件,在"GRUB_CMDLINE_LINUX "这个参数后面加入 "net.ifnames=0 biosdevname=0"配置：\n    \n    vim /etc/default/grub ... GRUB_CMDLINE_LINUX="rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet net.ifnames=0 biosdevname=0" ...\n\n 4. 最后，用 grub2-mkconfig 命令重新生成GRUB配置并更新内核,重启系统即可：\n    \n    grub2-mkconfig -o /boot/grub2/grub.cfg reboot',normalizedContent:'# 在系统安装时\n\n 1. 如下图，系统安装时，直接按下"tab"按键，进入编辑界面，添加如下信息："net.ifnames=0 biosdevname=0",系统安装完成后默认为eth0。\n\n\n\n# 系统安装完成后\n\n 1. 修改网卡配置文件device配置的值：\n    \n    vim /etc/sysconfig/network-scripts/ifcfg-ens33 ... device=eth0 ...\n\n 2. 重新命名网卡文件\n    \n    mv /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-eth0\n\n 3. 由于centos7采用grub2 引导，还需要对 grub2 进行修改，编辑 /etc/default/grub 配置文件,在"grub_cmdline_linux "这个参数后面加入 "net.ifnames=0 biosdevname=0"配置：\n    \n    vim /etc/default/grub ... grub_cmdline_linux="rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet net.ifnames=0 biosdevname=0" ...\n\n 4. 最后，用 grub2-mkconfig 命令重新生成grub配置并更新内核,重启系统即可：\n    \n    grub2-mkconfig -o /boot/grub2/grub.cfg reboot',charsets:{cjk:!0}},{title:"Logrotate入门了解及生产实践",frontmatter:{title:"Logrotate入门了解及生产实践",date:"2023-01-11T15:19:54.000Z",permalink:"/pages/a8c469/",categories:["运维","linux"],tags:[null],feed:{enable:!0},description:"Logrotate入门了解及生产实践",readingShow:"top",meta:[{name:"twitter:title",content:"Logrotate入门了解及生产实践"},{name:"twitter:description",content:"Logrotate入门了解及生产实践"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/07.Logrotate%E5%85%A5%E9%97%A8%E4%BA%86%E8%A7%A3%E5%8F%8A%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5.html"},{property:"og:type",content:"article"},{property:"og:title",content:"Logrotate入门了解及生产实践"},{property:"og:description",content:"Logrotate入门了解及生产实践"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/01.linux/07.Logrotate%E5%85%A5%E9%97%A8%E4%BA%86%E8%A7%A3%E5%8F%8A%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-11T15:19:54.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"Logrotate入门了解及生产实践"},{itemprop:"description",content:"Logrotate入门了解及生产实践"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/01.linux/07.Logrotate%E5%85%A5%E9%97%A8%E4%BA%86%E8%A7%A3%E5%8F%8A%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5.html",relativePath:"04.运维/01.linux/07.Logrotate入门了解及生产实践.md",key:"v-78111152",path:"/pages/a8c469/",headers:[{level:2,title:"1，简单了解",slug:"_1-简单了解",normalizedTitle:"1，简单了解",charIndex:2},{level:2,title:"2，配置了解",slug:"_2-配置了解",normalizedTitle:"2，配置了解",charIndex:833},{level:2,title:"3，生产案例",slug:"_3-生产案例",normalizedTitle:"3，生产案例",charIndex:1810},{level:3,title:"",slug:"",normalizedTitle:"",charIndex:0},{level:3,title:"2，catalina 日志",slug:"_2-catalina-日志",normalizedTitle:"2，catalina 日志",charIndex:2497},{level:3,title:"3，日志量大",slug:"_3-日志量大",normalizedTitle:"3，日志量大",charIndex:2934}],headersStr:"1，简单了解 2，配置了解 3，生产案例  2，catalina 日志 3，日志量大",content:"# 1，简单了解\n\nlogrotate 在 CentOS 系统中是默认安装的日志轮替组件，通过如下命令可以看到主配置内容：\n\negrep -v '^$|^#' logrotate.conf\nweekly\nrotate 4\ncreate\ndateext\ninclude /etc/logrotate.d\n/var/log/wtmp {\n    monthly\n    create 0664 root utmp\n    minsize 1M\n    rotate 1\n}\n/var/log/btmp {\n    missingok\n    monthly\n    create 0600 root utmp\n    rotate 1\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n这个配置当中其他内容基本上可以忽略，主要内容是第 6 行的引用，我们新增一些日志管理策略也都可以在这个目录下创建相关策略来维护。\n\nlogrorate 默认在系统的 /etc/cron.daily中存放了一个执行脚本，因此默认情况下脚本将会每天执行一次：\n\n$cat /etc/cron.daily/logrotate\n#!/bin/sh\n/usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf\nEXITVALUE=$?\nif [ $EXITVALUE != 0 ]; then\n    /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\"\nfi\nexit 0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n了解了这些基本信息之后，就可以对这个工具投入使用了，不过还需要了解一下配置文件中的参数意义，以便配制出符合自己需求的轮替脚本。\n\n\n# 2，配置了解\n\n * compress：通过 gzip，压缩转储以后的日志\n * nocompress：不需要压缩时，用这个参数\n * copytruncate：用于还在打开中的日志文件，把当前日志备份并截断\n * nocopytruncate：备份日志文件但是不截断\n * createmodeownergroup：转储文件，使用指定的文件模式创建新的日志文件\n * nocreate：不建立新的日志文件\n * delaycompress和compress：一起使用时，转储的日志文件到下一次转储时才压缩\n * missingok：在日志轮循期间，任何错误将被忽略，例如 “文件无法找到” 之类的错误。\n * nodelaycompress：覆盖 delaycompress 选项，转储同时压缩。\n * errorsaddress：专储时的错误信息发送到指定的 Email 地址\n * ifempty：即使是空文件也转储，这个是 logrotate 的缺省选项。\n * notifempty：如果是空文件的话，不转储\n * mailaddress：把转储的日志文件发送到指定的 E-mail 地址\n * nomail：转储时不发送日志文件\n * dateext：转储后的日志文件以日期命名\n * olddirdirectory：转储后的日志文件放入指定的目录，必须和当前日志文件在同一个文件系统\n * noolddir：转储后的日志文件和当前日志文件放在同一个目录下\n * prerotate/endscript：在转储以前需要执行的命令可以放入这个对，这两个关键字必须单独成行\n * postrotate/endscript：在转储以后需要执行的命令可以放入这个对，这两个关键字必须单独成行\n * daily：指定转储周期为每天\n * weekly：指定转储周期为每周\n * monthly：指定转储周期为每月\n * rotatecount：指定日志文件删除之前转储的次数，0 指没有备份，5 指保留 5 个备份\n * size（或 minsize）：size 当日志文件到达指定的大小时才转储，Size 可以指定 bytes(缺省) 以及 KB(sizek) 或者 MB(sizem)\n\n以上是一些常用的配置，可以按需定义自己的配置文件。\n\n\n# 3，生产案例\n\n接下来介绍几个生产当中常用的例子，可以直接借鉴以使用。\n\n\n#\n\n1，NGINX 日志\n\nNGINX 日志的处理是一种常见的场景，处理的方式用的较多的是结合 logrotate 来进行处理，当然也可以基于 NGINX 自身的配置文件策略对日志进行自动切割，此种方案可以参考另外一篇文章：从 NGINX 自身配置文件中定义访问日志按时间切割，这里则介绍 logrotate 方案的具体配置方式。\n\n在 /etc/logrotate.d下添加如下配置：\n\n$ cat /etc/logrotate.d/nginx\n/data/log/access.log\n/data/log/error.log\n{\n    daily\n    dateext\n    missingok\n    rotate 7\n    notifempty\n    create 755 www\n    sharedscripts\n    postrotate\n        [ -f /var/run/nginx.pid ] && kill -USR1 `cat /var/run/nginx.pid`\n    endscript\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n这个案例是针对这种提供了平滑过渡日志写操作的脚本，其他类似的应用日志也可以套用这个地方的配置。\n\n当我们配置添加之后，等不了系统的执行，想要自己看一下效果，可用如下命令手动运行：\n\nlogrotate -f /etc/logrotate.d/nginx\n\n\n1\n\n\n\n# 2，catalina 日志\n\ncatalina 是一个代指，与上边配置相对的，特指那种始终写到一个文件里，自身没有任何轮替策略，也无法通过调用所有平滑过渡来处理的情况下，可以使用如下配置对日志进行处理。\n\n配置如下：\n\n$ cat /etc/logrotate.d/catalina\n/data/log/catalina.log \n{\n    missingok\n    copytruncate\n    compress\n    dateext\n    notifempty\n    daily\n    rotate 7\n    create 755 root root\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n这个配置文件看起来与上边的区别除了脚本的处理之外，还多了一个重要的配置是 copytruncate，如果你在配置了 rotate 策略之后，发现日志转储了，但是写的对象也转移到了转储之后的文件了，那么应该就是漏了这个配置项。\n\n\n# 3，日志量大\n\n有时候可能业务量非常大，默认的按天切割就有点不大能够满足我们的需求，磁盘可能很快就要满了，这个时候可以将策略自定义为每小时执行一次。\n\n$ cat /etc/logrotate.d/nginx\n/data/log/access.log\n/data/log/error.log\n{\n    hourly\n    dateext\n    missingok\n    rotate 7\n    notifempty\n    create 755 www\n    sharedscripts\n    postrotate\n        [ -f /var/run/nginx.pid ] && kill -USR1 `cat /var/run/nginx.pid`\n    endscript\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n然后记得把执行脚本往小时执行池子里拷贝一份：\n\ncp /etc/cron.daily/logrotate /etc/cron.hourly/\n\n\n1\n\n\n当然也可将策略调整为按大小进行切割，这个方案也是可以的，这里就不做示例了。\n\n原文链接：https://wiki.eryajf.net/pages/4721.html",normalizedContent:"# 1，简单了解\n\nlogrotate 在 centos 系统中是默认安装的日志轮替组件，通过如下命令可以看到主配置内容：\n\negrep -v '^$|^#' logrotate.conf\nweekly\nrotate 4\ncreate\ndateext\ninclude /etc/logrotate.d\n/var/log/wtmp {\n    monthly\n    create 0664 root utmp\n    minsize 1m\n    rotate 1\n}\n/var/log/btmp {\n    missingok\n    monthly\n    create 0600 root utmp\n    rotate 1\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n这个配置当中其他内容基本上可以忽略，主要内容是第 6 行的引用，我们新增一些日志管理策略也都可以在这个目录下创建相关策略来维护。\n\nlogrorate 默认在系统的 /etc/cron.daily中存放了一个执行脚本，因此默认情况下脚本将会每天执行一次：\n\n$cat /etc/cron.daily/logrotate\n#!/bin/sh\n/usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf\nexitvalue=$?\nif [ $exitvalue != 0 ]; then\n    /usr/bin/logger -t logrotate \"alert exited abnormally with [$exitvalue]\"\nfi\nexit 0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n了解了这些基本信息之后，就可以对这个工具投入使用了，不过还需要了解一下配置文件中的参数意义，以便配制出符合自己需求的轮替脚本。\n\n\n# 2，配置了解\n\n * compress：通过 gzip，压缩转储以后的日志\n * nocompress：不需要压缩时，用这个参数\n * copytruncate：用于还在打开中的日志文件，把当前日志备份并截断\n * nocopytruncate：备份日志文件但是不截断\n * createmodeownergroup：转储文件，使用指定的文件模式创建新的日志文件\n * nocreate：不建立新的日志文件\n * delaycompress和compress：一起使用时，转储的日志文件到下一次转储时才压缩\n * missingok：在日志轮循期间，任何错误将被忽略，例如 “文件无法找到” 之类的错误。\n * nodelaycompress：覆盖 delaycompress 选项，转储同时压缩。\n * errorsaddress：专储时的错误信息发送到指定的 email 地址\n * ifempty：即使是空文件也转储，这个是 logrotate 的缺省选项。\n * notifempty：如果是空文件的话，不转储\n * mailaddress：把转储的日志文件发送到指定的 e-mail 地址\n * nomail：转储时不发送日志文件\n * dateext：转储后的日志文件以日期命名\n * olddirdirectory：转储后的日志文件放入指定的目录，必须和当前日志文件在同一个文件系统\n * noolddir：转储后的日志文件和当前日志文件放在同一个目录下\n * prerotate/endscript：在转储以前需要执行的命令可以放入这个对，这两个关键字必须单独成行\n * postrotate/endscript：在转储以后需要执行的命令可以放入这个对，这两个关键字必须单独成行\n * daily：指定转储周期为每天\n * weekly：指定转储周期为每周\n * monthly：指定转储周期为每月\n * rotatecount：指定日志文件删除之前转储的次数，0 指没有备份，5 指保留 5 个备份\n * size（或 minsize）：size 当日志文件到达指定的大小时才转储，size 可以指定 bytes(缺省) 以及 kb(sizek) 或者 mb(sizem)\n\n以上是一些常用的配置，可以按需定义自己的配置文件。\n\n\n# 3，生产案例\n\n接下来介绍几个生产当中常用的例子，可以直接借鉴以使用。\n\n\n#\n\n1，nginx 日志\n\nnginx 日志的处理是一种常见的场景，处理的方式用的较多的是结合 logrotate 来进行处理，当然也可以基于 nginx 自身的配置文件策略对日志进行自动切割，此种方案可以参考另外一篇文章：从 nginx 自身配置文件中定义访问日志按时间切割，这里则介绍 logrotate 方案的具体配置方式。\n\n在 /etc/logrotate.d下添加如下配置：\n\n$ cat /etc/logrotate.d/nginx\n/data/log/access.log\n/data/log/error.log\n{\n    daily\n    dateext\n    missingok\n    rotate 7\n    notifempty\n    create 755 www\n    sharedscripts\n    postrotate\n        [ -f /var/run/nginx.pid ] && kill -usr1 `cat /var/run/nginx.pid`\n    endscript\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n这个案例是针对这种提供了平滑过渡日志写操作的脚本，其他类似的应用日志也可以套用这个地方的配置。\n\n当我们配置添加之后，等不了系统的执行，想要自己看一下效果，可用如下命令手动运行：\n\nlogrotate -f /etc/logrotate.d/nginx\n\n\n1\n\n\n\n# 2，catalina 日志\n\ncatalina 是一个代指，与上边配置相对的，特指那种始终写到一个文件里，自身没有任何轮替策略，也无法通过调用所有平滑过渡来处理的情况下，可以使用如下配置对日志进行处理。\n\n配置如下：\n\n$ cat /etc/logrotate.d/catalina\n/data/log/catalina.log \n{\n    missingok\n    copytruncate\n    compress\n    dateext\n    notifempty\n    daily\n    rotate 7\n    create 755 root root\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n这个配置文件看起来与上边的区别除了脚本的处理之外，还多了一个重要的配置是 copytruncate，如果你在配置了 rotate 策略之后，发现日志转储了，但是写的对象也转移到了转储之后的文件了，那么应该就是漏了这个配置项。\n\n\n# 3，日志量大\n\n有时候可能业务量非常大，默认的按天切割就有点不大能够满足我们的需求，磁盘可能很快就要满了，这个时候可以将策略自定义为每小时执行一次。\n\n$ cat /etc/logrotate.d/nginx\n/data/log/access.log\n/data/log/error.log\n{\n    hourly\n    dateext\n    missingok\n    rotate 7\n    notifempty\n    create 755 www\n    sharedscripts\n    postrotate\n        [ -f /var/run/nginx.pid ] && kill -usr1 `cat /var/run/nginx.pid`\n    endscript\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n然后记得把执行脚本往小时执行池子里拷贝一份：\n\ncp /etc/cron.daily/logrotate /etc/cron.hourly/\n\n\n1\n\n\n当然也可将策略调整为按大小进行切割，这个方案也是可以的，这里就不做示例了。\n\n原文链接：https://wiki.eryajf.net/pages/4721.html",charsets:{cjk:!0}},{title:"windows支持多用户远程登录",frontmatter:{title:"windows支持多用户远程登录",date:"2022-12-15T18:56:36.000Z",permalink:"/pages/18b06c/",categories:["运维","windows系统"],tags:[null],readingShow:"top",description:"目前测试的windows server2016可以实现",meta:[{name:"twitter:title",content:"windows支持多用户远程登录"},{name:"twitter:description",content:"目前测试的windows server2016可以实现"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/02.windows/01.windows%E6%94%AF%E6%8C%81%E5%A4%9A%E7%94%A8%E6%88%B7%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95.html"},{property:"og:type",content:"article"},{property:"og:title",content:"windows支持多用户远程登录"},{property:"og:description",content:"目前测试的windows server2016可以实现"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/02.windows/01.windows%E6%94%AF%E6%8C%81%E5%A4%9A%E7%94%A8%E6%88%B7%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T18:56:36.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"windows支持多用户远程登录"},{itemprop:"description",content:"目前测试的windows server2016可以实现"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/02.windows/01.windows%E6%94%AF%E6%8C%81%E5%A4%9A%E7%94%A8%E6%88%B7%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95.html",relativePath:"04.运维/02.windows/01.windows支持多用户远程登录.md",key:"v-c1c1c24e",path:"/pages/18b06c/",headersStr:null,content:"目前测试的windows server2016可以实现\n\n配置如下\n\n微软Server版操作系统默认是支持多用户登陆的，例如Windows Server 2012，而Win10操作系统正常情况下是不允许用户同时远程的，即一个用户远程进来会把另一个用户踢掉，因此需要破解才能使得多个用户同时登陆远程桌面。Win10系统实现多用户远程需要以下两个步骤：\n\n一、主机修改远程登录相关配置\n\n1、新建用户\n\n右键[此电脑] --\x3e [管理] --\x3e [计算机管理] --\x3e [本地用户和组]，右键单击[用户] --\x3e [新用户]，添加用户名和设置密码，勾选密码永不过期，然后点击[创建]；\n\n2、添加远程桌面用户\n\n右键[此电脑] --\x3e [属性] --\x3e [远程]，勾选“允许远程协助连接这台计算机”和“允许远程连接到此电脑”，然后点击[选择用户]，添加刚刚创建的用户，点击确定；\n\n3、配置本地组策略\n\n运行gpedit.msc，打开本地组策略编辑器，依次选择[计算机配置] --\x3e [管理模板] --\x3e [Windows组件] --\x3e [远程桌面服务] --\x3e [连接]展开，然后分别进行以下3项配置：\n\n①配置“允许用户通过使用远程桌面服务进行远程连接”，选择：已启用；\n\n②配置“限制连接的数量”，点击“已启用”，其中“允许的RD最大连接数”可以自己视情况而定；\n\n③配置“将远程桌面服务用户限制到单独的远程桌面服务会话”，选择：已启用；\n\n【温馨提示】：“将远程桌面服务用户限制到单独的远程桌面服务会话”这个配置比较重要，如果没启用，会导致断开一个远程登录连接后，再重新连接，会重新打开一个新的桌面。但是打开新的应用程序时，系统提示系统后台正在运行，之前断开前的应用程序却一个也找不到。因为同一个用户先后登陆远程系统，系统会分配不同的会话，从而导致你在一个远程桌面的操作都不见了，虽然你运行的一些程序并没有被系统关闭，但是你无法对他们进行管理。\n\n二、破解远程登录用户限制\n\n1、下载解除远程桌面多用户连接限制补丁，下载地址见最后附件；\n\n2、解压下载好的RDPWrap压缩包后，进行以下操作：\n\n①以管理员身份运行install.bat安装；\n\n②拷贝rdpwrap.ini到C:\\Program Files\\RDP Wrapper目录下，并覆盖；\n\n③重启电脑；\n\n④运行RDPConf.exe，查看各个组件运行状态，状态全部为绿色就可以使用多用户远程桌面了。如果端口监听为Nothing listening, 或者出现Listening [not supported], 则需要运行一下update.bat；\n\n⑤运行RDPCheck.exe，测试远程是否正常；\n\n三、使用说明\n\n1、以上方法对于Win10系统版本号是1803以下的大部分计算机是完全可行的，若操作系统版是1809及以上版本号，则行不通；\n\n2、如果操作系统版本号是1809，该怎么操作呢？根据RDPWrap补丁的原理是：修改termsrv.dll和termsrv.dll.mui这两个远程服务文件，达到多用户远程登陆的目的。可以把其它操作系统上的termsrv.dll和termsrv.dll.mui这两个文件提取出来，然后替换本机上的文件，再运行RDPConf.exe查看各个组件运行状态。\n\n● termsrv.dll文件路径：C:\\Windows\\System32\n\n● termsrv.dll.mui文件路径：C:\\Windows\\System32\\zh-CN\n\n原文章链接：http://www.zhulincat.com/post/265.html\n\nwindows server 2019多用户\n\nWindows Server 2019远程桌面服务配置和授权激活_51CTO博客_windows server 2019 远程桌面",normalizedContent:"目前测试的windows server2016可以实现\n\n配置如下\n\n微软server版操作系统默认是支持多用户登陆的，例如windows server 2012，而win10操作系统正常情况下是不允许用户同时远程的，即一个用户远程进来会把另一个用户踢掉，因此需要破解才能使得多个用户同时登陆远程桌面。win10系统实现多用户远程需要以下两个步骤：\n\n一、主机修改远程登录相关配置\n\n1、新建用户\n\n右键[此电脑] --\x3e [管理] --\x3e [计算机管理] --\x3e [本地用户和组]，右键单击[用户] --\x3e [新用户]，添加用户名和设置密码，勾选密码永不过期，然后点击[创建]；\n\n2、添加远程桌面用户\n\n右键[此电脑] --\x3e [属性] --\x3e [远程]，勾选“允许远程协助连接这台计算机”和“允许远程连接到此电脑”，然后点击[选择用户]，添加刚刚创建的用户，点击确定；\n\n3、配置本地组策略\n\n运行gpedit.msc，打开本地组策略编辑器，依次选择[计算机配置] --\x3e [管理模板] --\x3e [windows组件] --\x3e [远程桌面服务] --\x3e [连接]展开，然后分别进行以下3项配置：\n\n①配置“允许用户通过使用远程桌面服务进行远程连接”，选择：已启用；\n\n②配置“限制连接的数量”，点击“已启用”，其中“允许的rd最大连接数”可以自己视情况而定；\n\n③配置“将远程桌面服务用户限制到单独的远程桌面服务会话”，选择：已启用；\n\n【温馨提示】：“将远程桌面服务用户限制到单独的远程桌面服务会话”这个配置比较重要，如果没启用，会导致断开一个远程登录连接后，再重新连接，会重新打开一个新的桌面。但是打开新的应用程序时，系统提示系统后台正在运行，之前断开前的应用程序却一个也找不到。因为同一个用户先后登陆远程系统，系统会分配不同的会话，从而导致你在一个远程桌面的操作都不见了，虽然你运行的一些程序并没有被系统关闭，但是你无法对他们进行管理。\n\n二、破解远程登录用户限制\n\n1、下载解除远程桌面多用户连接限制补丁，下载地址见最后附件；\n\n2、解压下载好的rdpwrap压缩包后，进行以下操作：\n\n①以管理员身份运行install.bat安装；\n\n②拷贝rdpwrap.ini到c:\\program files\\rdp wrapper目录下，并覆盖；\n\n③重启电脑；\n\n④运行rdpconf.exe，查看各个组件运行状态，状态全部为绿色就可以使用多用户远程桌面了。如果端口监听为nothing listening, 或者出现listening [not supported], 则需要运行一下update.bat；\n\n⑤运行rdpcheck.exe，测试远程是否正常；\n\n三、使用说明\n\n1、以上方法对于win10系统版本号是1803以下的大部分计算机是完全可行的，若操作系统版是1809及以上版本号，则行不通；\n\n2、如果操作系统版本号是1809，该怎么操作呢？根据rdpwrap补丁的原理是：修改termsrv.dll和termsrv.dll.mui这两个远程服务文件，达到多用户远程登陆的目的。可以把其它操作系统上的termsrv.dll和termsrv.dll.mui这两个文件提取出来，然后替换本机上的文件，再运行rdpconf.exe查看各个组件运行状态。\n\n● termsrv.dll文件路径：c:\\windows\\system32\n\n● termsrv.dll.mui文件路径：c:\\windows\\system32\\zh-cn\n\n原文章链接：http://www.zhulincat.com/post/265.html\n\nwindows server 2019多用户\n\nwindows server 2019远程桌面服务配置和授权激活_51cto博客_windows server 2019 远程桌面",charsets:{cjk:!0}},{title:"windows应用服务部署脚本",frontmatter:{title:"windows应用服务部署脚本",date:"2023-02-28T22:13:50.000Z",permalink:"/pages/adbe78/",categories:["运维","windows"],tags:[null],readingShow:"top",description:"",meta:[{name:"image",content:"http://pic.zzppjj.top/LightPicture/2023/02/46cae67941eaa158.jpg"},{name:"twitter:title",content:"windows应用服务部署脚本"},{name:"twitter:description",content:""},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://pic.zzppjj.top/LightPicture/2023/02/46cae67941eaa158.jpg"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/02.windows/02.windows%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E8%84%9A%E6%9C%AC.html"},{property:"og:type",content:"article"},{property:"og:title",content:"windows应用服务部署脚本"},{property:"og:description",content:""},{property:"og:image",content:"http://pic.zzppjj.top/LightPicture/2023/02/46cae67941eaa158.jpg"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/02.windows/02.windows%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E8%84%9A%E6%9C%AC.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-28T22:13:50.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"windows应用服务部署脚本"},{itemprop:"description",content:""},{itemprop:"image",content:"http://pic.zzppjj.top/LightPicture/2023/02/46cae67941eaa158.jpg"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/02.windows/02.windows%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E8%84%9A%E6%9C%AC.html",relativePath:"04.运维/02.windows/02.windows应用服务部署脚本.md",key:"v-0cc13236",path:"/pages/adbe78/",headers:[{level:3,title:"杀掉服务脚本",slug:"杀掉服务脚本",normalizedTitle:"杀掉服务脚本",charIndex:154},{level:3,title:"服务启动配置",slug:"服务启动配置",normalizedTitle:"服务启动配置",charIndex:396},{level:3,title:"ansible部署脚本",slug:"ansible部署脚本",normalizedTitle:"ansible部署脚本",charIndex:430},{level:4,title:"hosts文件",slug:"hosts文件",normalizedTitle:"hosts文件",charIndex:1896},{level:4,title:"jenkinsfile文件",slug:"jenkinsfile文件",normalizedTitle:"jenkinsfile文件",charIndex:2114}],headersStr:"杀掉服务脚本 服务启动配置 ansible部署脚本 hosts文件 jenkinsfile文件",content:'�����������������������������������������������������������������������������������������������������������������������������������������������������\n\n\n# 杀掉服务脚本\n\n@echo off\nset TempFile=%TEMP%\\sthUnique.tmp\nwmic process where name="md.exe" get processid,commandline | find "gf" >%TempFile%\nset /P _string=<%TempFile%\nset _pid=%_string:~32%\necho %_pid%\ntaskkill /f /pid %_pid%\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 服务启动配置\n\n打开任务计划程序，创建任务\n\n\n\n\n\n\n\n\n\n\n# ansible部署脚本\n\n---\n- hosts: "windows_deploy02"\n  tasks:\n    - name: kill md service\n      win_command: c:\\\\app\\md.bat\n      ignore_errors: True\n      tags:\n        - taskkill_md\n    # - name: kill md service\n    #   win_command: taskkill /F /im md.exe\n    #   ignore_errors: True\n    #   tags:\n    #     - taskkill_md  \n    - name: copy gf file\n      win_copy:\n        src: "{{ WORKSPACE }}/Quote_md/"\n        dest: c:\\\\app\\gf\\Quote_md\\\n      tags:\n        - copy_gf\n    - name: copy gf config file\n      win_copy:\n        src: "/var/jenkins_home/project_config/gf_md/source_md.yaml"\n        dest: c:\\\\app\\gf\\Quote_md\\etc\n      tags:\n        - copy_gf_config\n    - name: copy quote file\n      win_copy:\n        src: "{{ WORKSPACE }}/Quote_md/"\n        dest: c:\\\\app\\quote\\md\\\n      tags:\n        - copy_quote\n    - name: copy quote config file\n      win_copy:\n        src: "/var/jenkins_home/project_config/airm_md/source_md.yaml"\n        dest: c:\\\\app\\quote\\md\\etc\n      tags:\n        - copy_quote_config\n    - name: start trade service\n      win_command: C:\\Windows\\System32\\schtasks.exe /Run /TN start_gf\n      tags:\n        - start_gf\n      ignore_errors: True\n    - name: start quote service\n      win_command: C:\\Windows\\System32\\schtasks.exe /Run /TN start_quote\n      ignore_errors: True\n      tags:\n        - start_quote\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n# hosts文件\n\n[windows_deploy02]\n172.16.30.198 ansible_ssh_port=5985 ansible_ssh_user=admin ansible_ssh_pass=123456 ansible_connection=winrm ansible_winrm_server_cert_validation=ignore ansible_winrm_transport=ssl\n\n\n1\n2\n\n\n# jenkinsfile文件\n\n\n\n\n1\n',normalizedContent:'�����������������������������������������������������������������������������������������������������������������������������������������������������\n\n\n# 杀掉服务脚本\n\n@echo off\nset tempfile=%temp%\\sthunique.tmp\nwmic process where name="md.exe" get processid,commandline | find "gf" >%tempfile%\nset /p _string=<%tempfile%\nset _pid=%_string:~32%\necho %_pid%\ntaskkill /f /pid %_pid%\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 服务启动配置\n\n打开任务计划程序，创建任务\n\n\n\n\n\n\n\n\n\n\n# ansible部署脚本\n\n---\n- hosts: "windows_deploy02"\n  tasks:\n    - name: kill md service\n      win_command: c:\\\\app\\md.bat\n      ignore_errors: true\n      tags:\n        - taskkill_md\n    # - name: kill md service\n    #   win_command: taskkill /f /im md.exe\n    #   ignore_errors: true\n    #   tags:\n    #     - taskkill_md  \n    - name: copy gf file\n      win_copy:\n        src: "{{ workspace }}/quote_md/"\n        dest: c:\\\\app\\gf\\quote_md\\\n      tags:\n        - copy_gf\n    - name: copy gf config file\n      win_copy:\n        src: "/var/jenkins_home/project_config/gf_md/source_md.yaml"\n        dest: c:\\\\app\\gf\\quote_md\\etc\n      tags:\n        - copy_gf_config\n    - name: copy quote file\n      win_copy:\n        src: "{{ workspace }}/quote_md/"\n        dest: c:\\\\app\\quote\\md\\\n      tags:\n        - copy_quote\n    - name: copy quote config file\n      win_copy:\n        src: "/var/jenkins_home/project_config/airm_md/source_md.yaml"\n        dest: c:\\\\app\\quote\\md\\etc\n      tags:\n        - copy_quote_config\n    - name: start trade service\n      win_command: c:\\windows\\system32\\schtasks.exe /run /tn start_gf\n      tags:\n        - start_gf\n      ignore_errors: true\n    - name: start quote service\n      win_command: c:\\windows\\system32\\schtasks.exe /run /tn start_quote\n      ignore_errors: true\n      tags:\n        - start_quote\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n# hosts文件\n\n[windows_deploy02]\n172.16.30.198 ansible_ssh_port=5985 ansible_ssh_user=admin ansible_ssh_pass=123456 ansible_connection=winrm ansible_winrm_server_cert_validation=ignore ansible_winrm_transport=ssl\n\n\n1\n2\n\n\n# jenkinsfile文件\n\n\n\n\n1\n',charsets:{cjk:!0}},{title:"nginx配置教程",frontmatter:{title:"nginx配置教程",categories:"nginx",tags:["nginx"],date:"2022-12-09T20:48:36.000Z",permalink:"/pages/5ed327/",readingShow:"top",description:"](http://nginx.org/)",meta:[{name:"image",content:"https://dictator.oss-cn-hongkong.aliyuncs.com/hexo/title/nginx.svg"},{name:"twitter:title",content:"nginx配置教程"},{name:"twitter:description",content:"](http://nginx.org/)"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://dictator.oss-cn-hongkong.aliyuncs.com/hexo/title/nginx.svg"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.nginx/01.nginx%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html"},{property:"og:type",content:"article"},{property:"og:title",content:"nginx配置教程"},{property:"og:description",content:"](http://nginx.org/)"},{property:"og:image",content:"https://dictator.oss-cn-hongkong.aliyuncs.com/hexo/title/nginx.svg"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.nginx/01.nginx%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:48:36.000Z"},{property:"article:tag",content:"nginx"},{itemprop:"name",content:"nginx配置教程"},{itemprop:"description",content:"](http://nginx.org/)"},{itemprop:"image",content:"https://dictator.oss-cn-hongkong.aliyuncs.com/hexo/title/nginx.svg"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.nginx/01.nginx%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html",relativePath:"04.运维/03.中间件/01.nginx/01.nginx配置教程.md",key:"v-0df2a669",path:"/pages/5ed327/",headers:[{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:260},{level:3,title:"安装依赖",slug:"安装依赖",normalizedTitle:"安装依赖",charIndex:267},{level:3,title:"下载",slug:"下载",normalizedTitle:"下载",charIndex:494},{level:3,title:"编译安装",slug:"编译安装",normalizedTitle:"编译安装",charIndex:722},{level:3,title:"nginx测试",slug:"nginx测试",normalizedTitle:"nginx测试",charIndex:1828},{level:3,title:"设置全局nginx命令",slug:"设置全局nginx命令",normalizedTitle:"设置全局nginx命令",charIndex:2100},{level:2,title:"Mac 安装",slug:"mac-安装",normalizedTitle:"mac 安装",charIndex:2296},{level:3,title:"安装nginx",slug:"安装nginx",normalizedTitle:"安装nginx",charIndex:2357},{level:3,title:"启动服务",slug:"启动服务",normalizedTitle:"启动服务",charIndex:4952},{level:2,title:"开机自启动",slug:"开机自启动",normalizedTitle:"开机自启动",charIndex:5050},{level:2,title:"运维",slug:"运维",normalizedTitle:"运维",charIndex:7065},{level:3,title:"服务管理",slug:"服务管理",normalizedTitle:"服务管理",charIndex:7072},{level:3,title:"重启服务防火墙报错解决",slug:"重启服务防火墙报错解决",normalizedTitle:"重启服务防火墙报错解决",charIndex:7610},{level:2,title:"nginx卸载",slug:"nginx卸载",normalizedTitle:"nginx卸载",charIndex:8083},{level:2,title:"参数说明",slug:"参数说明",normalizedTitle:"参数说明",charIndex:748},{level:2,title:"配置",slug:"配置",normalizedTitle:"配置",charIndex:7},{level:3,title:"常用正则",slug:"常用正则",normalizedTitle:"常用正则",charIndex:15868},{level:3,title:"全局变量",slug:"全局变量",normalizedTitle:"全局变量",charIndex:16132},{level:3,title:"符号参考",slug:"符号参考",normalizedTitle:"符号参考",charIndex:17474},{level:3,title:"配置文件",slug:"配置文件",normalizedTitle:"配置文件",charIndex:6588},{level:3,title:"内置预定义变量",slug:"内置预定义变量",normalizedTitle:"内置预定义变量",charIndex:20544},{level:3,title:"反向代理",slug:"反向代理",normalizedTitle:"反向代理",charIndex:42},{level:3,title:"负载均衡",slug:"负载均衡",normalizedTitle:"负载均衡",charIndex:90},{level:4,title:"RR",slug:"rr",normalizedTitle:"rr",charIndex:25126},{level:4,title:"权重",slug:"权重",normalizedTitle:"权重",charIndex:23275},{level:4,title:"ip_hash",slug:"ip-hash",normalizedTitle:"ip_hash",charIndex:12201},{level:4,title:"fair",slug:"fair",normalizedTitle:"fair",charIndex:26296},{level:4,title:"url_hash",slug:"url-hash",normalizedTitle:"url_hash",charIndex:26444},{level:3,title:"屏蔽ip",slug:"屏蔽ip",normalizedTitle:"屏蔽ip",charIndex:27110},{level:2,title:"第三方模块安装方法",slug:"第三方模块安装方法",normalizedTitle:"第三方模块安装方法",charIndex:27687},{level:2,title:"重定向",slug:"重定向",normalizedTitle:"重定向",charIndex:280},{level:3,title:"重定向整个网站",slug:"重定向整个网站",normalizedTitle:"重定向整个网站",charIndex:27836},{level:3,title:"重定向单页",slug:"重定向单页",normalizedTitle:"重定向单页",charIndex:27953},{level:3,title:"重定向整个子路径",slug:"重定向整个子路径",normalizedTitle:"重定向整个子路径",charIndex:28079},{level:2,title:"性能",slug:"性能",normalizedTitle:"性能",charIndex:26},{level:3,title:"内容缓存",slug:"内容缓存",normalizedTitle:"内容缓存",charIndex:28204},{level:3,title:"Gzip压缩",slug:"gzip压缩",normalizedTitle:"gzip压缩",charIndex:28442},{level:3,title:"打开文件缓存",slug:"打开文件缓存",normalizedTitle:"打开文件缓存",charIndex:29027},{level:3,title:"SSL缓存",slug:"ssl缓存",normalizedTitle:"ssl缓存",charIndex:29173},{level:3,title:"上游Keepalive",slug:"上游keepalive",normalizedTitle:"上游keepalive",charIndex:29252},{level:3,title:"监控",slug:"监控",normalizedTitle:"监控",charIndex:29520},{level:2,title:"常见使用场景",slug:"常见使用场景",normalizedTitle:"常见使用场景",charIndex:30303},{level:3,title:"跨域问题",slug:"跨域问题",normalizedTitle:"跨域问题",charIndex:30314},{level:3,title:"跳转到带www的域上面",slug:"跳转到带www的域上面",normalizedTitle:"跳转到带www的域上面",charIndex:32164},{level:3,title:"代理转发",slug:"代理转发",normalizedTitle:"代理转发",charIndex:32598},{level:3,title:"监控状态信息",slug:"监控状态信息",normalizedTitle:"监控状态信息",charIndex:33561},{level:3,title:"代理转发连接替换",slug:"代理转发连接替换",normalizedTitle:"代理转发连接替换",charIndex:34104},{level:3,title:"ssl配置",slug:"ssl配置",normalizedTitle:"ssl配置",charIndex:34231},{level:3,title:"强制将http重定向到https",slug:"强制将http重定向到https",normalizedTitle:"强制将http重定向到https",charIndex:37156},{level:3,title:"两个虚拟主机",slug:"两个虚拟主机",normalizedTitle:"两个虚拟主机",charIndex:37413},{level:3,title:"虚拟主机标准配置",slug:"虚拟主机标准配置",normalizedTitle:"虚拟主机标准配置",charIndex:38003},{level:3,title:"爬虫过滤",slug:"爬虫过滤",normalizedTitle:"爬虫过滤",charIndex:38263},{level:3,title:"防盗链",slug:"防盗链",normalizedTitle:"防盗链",charIndex:38503},{level:3,title:"虚拟目录配置",slug:"虚拟目录配置",normalizedTitle:"虚拟目录配置",charIndex:38712},{level:3,title:"防盗图配置",slug:"防盗图配置",normalizedTitle:"防盗图配置",charIndex:38995},{level:3,title:"屏蔽.git等文件",slug:"屏蔽-git等文件",normalizedTitle:"屏蔽.git等文件",charIndex:39237},{level:3,title:"域名路径加不加需要都能正常访问",slug:"域名路径加不加需要都能正常访问",normalizedTitle:"域名路径加不加需要都能正常访问",charIndex:39330},{level:2,title:"错误问题",slug:"错误问题",normalizedTitle:"错误问题",charIndex:39814},{level:2,title:"Nginx 模块",slug:"nginx-模块",normalizedTitle:"nginx 模块",charIndex:40271},{level:2,title:"精品文章参考",slug:"精品文章参考",normalizedTitle:"精品文章参考",charIndex:40339}],headersStr:"安装 安装依赖 下载 编译安装 nginx测试 设置全局nginx命令 Mac 安装 安装nginx 启动服务 开机自启动 运维 服务管理 重启服务防火墙报错解决 nginx卸载 参数说明 配置 常用正则 全局变量 符号参考 配置文件 内置预定义变量 反向代理 负载均衡 RR 权重 ip_hash fair url_hash 屏蔽ip 第三方模块安装方法 重定向 重定向整个网站 重定向单页 重定向整个子路径 性能 内容缓存 Gzip压缩 打开文件缓存 SSL缓存 上游Keepalive 监控 常见使用场景 跨域问题 跳转到带www的域上面 代理转发 监控状态信息 代理转发连接替换 ssl配置 强制将http重定向到https 两个虚拟主机 虚拟主机标准配置 爬虫过滤 防盗链 虚拟目录配置 防盗图配置 屏蔽.git等文件 域名路径加不加需要都能正常访问 错误问题 Nginx 模块 精品文章参考",content:'# Nginx配置教程\n\n\n\nNginx 是一款面向性能设计的 HTTP 服务器，能反向代理 HTTP，HTTPS 和邮件相关(SMTP，POP3，IMAP)的协议链接。并且提供了负载均衡以及 HTTP 缓存。它的设计充分使用异步事件模型，削减上下文调度的开销，提高服务器并发能力。采用了模块化设计，提供了丰富模块的第三方模块。\n\n所以关于 Nginx，有这些标签：「异步」「事件」「模块化」「高性能」「高并发」「反向代理」「负载均衡」\n\nLinux系统：Centos 7 x64 Nginx版本：1.11.5\n\n\n# 安装\n\n\n# 安装依赖\n\n> prce(重定向支持)和openssl(https支持，如果不需要https可以不安装。)\n\n复制yum install -y pcre-devel \nyum -y install gcc make gcc-c++ wget\nyum -y install openssl openssl-devel\n\n\n1\n2\n3\n\n\nCentOS 6.5 我安装的时候是选择的“基本服务器”，默认这两个包都没安装全，所以这两个都运行安装即可。\n\n\n# 下载\n\nnginx的所有版本在这里\n\n复制wget http://nginx.org/download/nginx-1.13.3.tar.gz\nwget http://nginx.org/download/nginx-1.13.7.tar.gz\n\n# 如果没有安装wget\n# 下载已编译版本\n$ yum install wget\n\n# 解压压缩包\ntar zxf nginx-1.13.3.tar.gz\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 编译安装\n\n然后进入目录编译安装，configure参数说明\n\n复制cd nginx-1.11.5\n./configure\n\n....\nConfiguration summary\n  + using system PCRE library\n  + OpenSSL library is not used\n  + using system zlib library\n\n  nginx path prefix: "/usr/local/nginx"\n  nginx binary file: "/usr/local/nginx/sbin/nginx"\n  nginx modules path: "/usr/local/nginx/modules"\n  nginx configuration prefix: "/usr/local/nginx/conf"\n  nginx configuration file: "/usr/local/nginx/conf/nginx.conf"\n  nginx pid file: "/usr/local/nginx/logs/nginx.pid"\n  nginx error log file: "/usr/local/nginx/logs/error.log"\n  nginx http access log file: "/usr/local/nginx/logs/access.log"\n  nginx http client request body temporary files: "client_body_temp"\n  nginx http proxy temporary files: "proxy_temp"\n  nginx http fastcgi temporary files: "fastcgi_temp"\n  nginx http uwsgi temporary files: "uwsgi_temp"\n  nginx http scgi temporary files: "scgi_temp"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n安装报错误的话比如：“C compiler cc is not found”，这个就是缺少编译环境，安装一下就可以了 yum -y install gcc make gcc-c++ openssl-devel\n\n如果没有error信息，就可以执行下边的安装了：\n\n复制make\nmake install\n\n\n1\n2\n\n\n\n# nginx测试\n\n运行下面命令会出现两个结果，一般情况nginx会安装在/usr/local/nginx目录中\n\n复制cd /usr/local/nginx/sbin/\n./nginx -t\n\n# nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is ok\n# nginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful\n\n\n1\n2\n3\n4\n5\n\n\n\n# 设置全局nginx命令\n\n复制vi ~/.bash_profile\n\n\n1\n\n\n将下面内容添加到 ~/.bash_profile 文件中\n\n复制PATH=$PATH:$HOME/bin:/usr/local/nginx/sbin/\nexport PATH\n\n\n1\n2\n\n\n运行命令 source ~/.bash_profile 让配置立即生效。你就可以全局运行 nginx 命令了。\n\n\n# Mac 安装\n\nMac OSX 安装特别简单，首先你需要安装 Brew， 通过 brew 快速安装 nginx。\n\n\n# 安装nginx\n\n复制brew install nginx\n# Updating Homebrew...\n# ==> Auto-updated Homebrew!\n# Updated 2 taps (homebrew/core, homebrew/cask).\n# ==> Updated Formulae\n# ==> Installing dependencies for nginx: openssl, pcre\n# ==> Installing nginx dependency: openssl\n# ==> Downloading https://homebrew.bintray.com/bottles/openssl-1.0.2o_1.high_sierra.bottle.tar.gz\n# ######################################################################## 100.0%\n# ==> Pouring openssl-1.0.2o_1.high_sierra.bottle.tar.gz\n# ==> Caveats\n# A CA file has been bootstrapped using certificates from the SystemRoots\n# keychain. To add additional certificates (e.g. the certificates added in\n# the System keychain), place .pem files in\n#   /usr/local/etc/openssl/certs\n# \n# and run\n#   /usr/local/opt/openssl/bin/c_rehash\n# \n# This formula is keg-only, which means it was not symlinked into /usr/local,\n# because Apple has deprecated use of OpenSSL in favor of its own TLS and crypto libraries.\n# \n# If you need to have this software first in your PATH run:\n#   echo \'export PATH="/usr/local/opt/openssl/bin:$PATH"\' >> ~/.zshrc\n# \n# For compilers to find this software you may need to set:\n#     LDFLAGS:  -L/usr/local/opt/openssl/lib\n#     CPPFLAGS: -I/usr/local/opt/openssl/include\n# For pkg-config to find this software you may need to set:\n#     PKG_CONFIG_PATH: /usr/local/opt/openssl/lib/pkgconfig\n# \n# ==> Summary\n# 🍺  /usr/local/Cellar/openssl/1.0.2o_1: 1,791 files, 12.3MB\n# ==> Installing nginx dependency: pcre\n# ==> Downloading https://homebrew.bintray.com/bottles/pcre-8.42.high_sierra.bottle.tar.gz\n# ######################################################################## 100.0%\n# ==> Pouring pcre-8.42.high_sierra.bottle.tar.gz\n# 🍺  /usr/local/Cellar/pcre/8.42: 204 files, 5.3MB\n# ==> Installing nginx\n# ==> Downloading https://homebrew.bintray.com/bottles/nginx-1.13.12.high_sierra.bottle.tar.gz\n# ######################################################################## 100.0%\n# ==> Pouring nginx-1.13.12.high_sierra.bottle.tar.gz\n# ==> Caveats\n# Docroot is: /usr/local/var/www\n# \n# The default port has been set in /usr/local/etc/nginx/nginx.conf to 8080 so that\n# nginx can run without sudo.\n# \n# nginx will load all files in /usr/local/etc/nginx/servers/.\n# \n# To have launchd start nginx now and restart at login:\n#   brew services start nginx\n# Or, if you don\'t wacd /usr/local/Cellar/nginx/1.13.12/n just run:\n# cd /usr/local/Cellar/nginx/1.13.12/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n\n# 启动服务\n\n注意默认端口不是 8080 查看确认端口是否被占用。\n\n复制brew services start nginx\n# http://localhost:8080/\n\n\n1\n2\n\n\n\n# 开机自启动\n\n开机自启动方法一：\n\n编辑 vi /lib/systemd/system/nginx.service 文件，没有创建一个 touch nginx.service 然后将如下内容根据具体情况进行修改后，添加到nginx.service文件中：\n\n复制[Unit]\nDescription=nginx\nAfter=network.target remote-fs.target nss-lookup.target\n\n[Service]\n\nType=forking\nPIDFile=/var/run/nginx.pid\nExecStartPre=/usr/local/nginx/sbin/nginx -t -c /usr/local/nginx/conf/nginx.conf\nExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf\nExecReload=/bin/kill -s HUP $MAINPID\nExecStop=/bin/kill -s QUIT $MAINPID\nPrivateTmp=true\n\n[Install]\nWantedBy=multi-user.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n * [Unit]:服务的说明\n * Description:描述服务\n * After:描述服务类别\n * [Service]服务运行参数的设置\n * Type=forking是后台运行的形式\n * ExecStart为服务的具体运行命令\n * ExecReload为重启命令\n * ExecStop为停止命令\n * PrivateTmp=True表示给服务分配独立的临时空间\n\n注意：[Service]的启动、重启、停止命令全部要求使用绝对路径。\n\n[Install]运行级别下服务安装的相关设置，可设置为多用户，即系统运行级别为3。\n\n保存退出。\n\n设置开机启动，使配置生效：\n\n复制# 启动nginx服务\nsystemctl start nginx.service\n# 停止开机自启动\nsystemctl disable nginx.service\n# 查看服务当前状态\nsystemctl status nginx.service\n# 查看所有已启动的服务\nsystemctl list-units --type=service\n# 重新启动服务\nsystemctl restart nginx.service\n# 设置开机自启动\nsystemctl enable nginx.service\n# 输出下面内容表示成功了\nCreated symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /usr/lib/systemd/system/nginx.service.\n复制systemctl is-enabled servicename.service # 查询服务是否开机启动\nsystemctl enable *.service # 开机运行服务\nsystemctl disable *.service # 取消开机运行\nsystemctl start *.service # 启动服务\nsystemctl stop *.service # 停止服务\nsystemctl restart *.service # 重启服务\nsystemctl reload *.service # 重新加载服务配置文件\nsystemctl status *.service # 查询服务运行状态\nsystemctl --failed # 显示启动失败的服务\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n注：*代表某个服务的名字，如http的服务名为httpd\n\n开机自启动方法二：\n\n复制vi /etc/rc.local\n\n# 在 rc.local 文件中，添加下面这条命令\n/usr/local/nginx/sbin/nginx start\n\n\n1\n2\n3\n4\n\n\n如果开机后发现自启动脚本没有执行，你要去确认一下rc.local这个文件的访问权限是否是可执行的，因为rc.local默认是不可执行的。修改rc.local访问权限，增加可执行权限：\n\n复制# /etc/rc.local是/etc/rc.d/rc.local的软连接，\nchmod +x /etc/rc.d/rc.local\n\n\n1\n2\n\n\n官方脚本 ed Hat NGINX Init Script。\n\n\n# 运维\n\n\n# 服务管理\n\n复制# 启动\n/usr/local/nginx/sbin/nginx\n\n# 重启\n/usr/local/nginx/sbin/nginx -s reload\n\n# 关闭进程\n/usr/local/nginx/sbin/nginx -s stop\n\n# 平滑关闭nginx\n/usr/local/nginx/sbin/nginx -s quit\n\n# 查看nginx的安装状态，\n/usr/local/nginx/sbin/nginx -V\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n关闭防火墙，或者添加防火墙规则就可以测试了\n\n复制service iptables stop\n\n\n1\n\n\n或者编辑配置文件：\n\n复制vi /etc/sysconfig/iptables\n\n\n1\n\n\n添加这样一条开放80端口的规则后保存：\n\n复制-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT\n\n\n1\n\n\n重启服务即可:\n\n复制service iptables restart\n# 命令进行查看目前nat\niptables -t nat -L\n\n\n1\n2\n3\n\n\n\n# 重启服务防火墙报错解决\n\n复制service iptables restart\n# Redirecting to /bin/systemctl restart  iptables.service\n# Failed to restart iptables.service: Unit iptables.service failed to load: No such file or directory.\n\n\n1\n2\n3\n\n\n在CentOS 7或RHEL 7或Fedora中防火墙由 firewalld 来管理，当然你可以还原传统的管理方式。或则使用新的命令进行管理。 假如采用传统请执行一下命令：\n\n复制# 传统命令\nsystemctl stop firewalld\nsystemctl mask firewalld\n复制# 安装命令\nyum install iptables-services\n\nsystemctl enable iptables \nservice iptables restart\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# nginx卸载\n\n如果通过yum安装，使用下面命令安装。\n\n复制yum remove nginx\n\n\n1\n\n\n编译安装，删除/usr/local/nginx目录即可 如果配置了自启动脚本，也需要删除。\n\n\n# 参数说明\n\n参数                                           说明\n–prefix=<path>                               Nginx安装路径。如果没有指定，默认为 /usr/local/nginx。\n–sbin-path=<path>                            Nginx可执行文件安装路径。只能安装时指定，如果没有指定，默认为<prefix>/sbin/nginx。\n–conf-path=<path>                            在没有给定-c选项下默认的nginx.conf的路径。如果没有指定，默认为<prefix>/conf/nginx.conf。\n–pid-path=<path>                             在nginx.conf中没有指定pid指令的情况下，默认的nginx.pid的路径。如果没有指定，默认为\n                                             <prefix>/logs/nginx.pid。\n–lock-path=<path>                            nginx.lock文件的路径。\n–error-log-path=<path>                       在nginx.conf中没有指定error_log指令的情况下，默认的错误日志的路径。如果没有指定，默认为\n                                             <prefix>/- logs/error.log。\n–http-log-path=<path>                        在nginx.conf中没有指定access_log指令的情况下，默认的访问日志的路径。如果没有指定，默认为\n                                             <prefix>/- logs/access.log。\n–user=<user>                                 在nginx.conf中没有指定user指令的情况下，默认的nginx使用的用户。如果没有指定，默认为 nobody。\n–group=<group>                               在nginx.conf中没有指定user指令的情况下，默认的nginx使用的组。如果没有指定，默认为 nobody。\n–builddir=DIR                                指定编译的目录\n–with-rtsig_module                           启用 rtsig 模块\n–with-select_module –without-select_module   允许或不允许开启SELECT模式，如果 configure 没有找到更合适的模式，比如：kqueue(sun\n                                             os),epoll (linux kenel 2.6+), rtsig(-\n                                             实时信号)或者/dev/poll(一种类似select的模式，底层实现与SELECT基本相 同，都是采用轮训方法)\n                                             SELECT模式将是默认安装模式\n–with-poll_module –without-poll_module       Whether or not to enable the poll module. This module is\n                                             enabled by, default if a more suitable method such as\n                                             kqueue, epoll, rtsig or /dev/poll is not discovered by\n                                             configure.\n–with-http_ssl_module                        Enable ngx_http_ssl_module. Enables SSL support and the\n                                             ability to handle HTTPS requests. Requires OpenSSL. On\n                                             Debian, this is libssl-dev. 开启HTTP\n                                             SSL模块，使NGINX可以支持HTTPS请求。这个模块需要已经安装了OPENSSL，在DEBIAN上是libssl\n–with-http_realip_module                     启用 ngx_http_realip_module\n–with-http_addition_module                   启用 ngx_http_addition_module\n–with-http_sub_module                        启用 ngx_http_sub_module\n–with-http_dav_module                        启用 ngx_http_dav_module\n–with-http_flv_module                        启用 ngx_http_flv_module\n–with-http_stub_status_module                启用 “server status” 页\n–without-http_charset_module                 禁用 ngx_http_charset_module\n–without-http_gzip_module                    禁用 ngx_http_gzip_module. 如果启用，需要 zlib 。\n–without-http_ssi_module                     禁用 ngx_http_ssi_module\n–without-http_userid_module                  禁用 ngx_http_userid_module\n–without-http_access_module                  禁用 ngx_http_access_module\n–without-http_auth_basic_module              禁用 ngx_http_auth_basic_module\n–without-http_autoindex_module               禁用 ngx_http_autoindex_module\n–without-http_geo_module                     禁用 ngx_http_geo_module\n–without-http_map_module                     禁用 ngx_http_map_module\n–without-http_referer_module                 禁用 ngx_http_referer_module\n–without-http_rewrite_module                 禁用 ngx_http_rewrite_module. 如果启用需要 PCRE 。\n–without-http_proxy_module                   禁用 ngx_http_proxy_module\n–without-http_fastcgi_module                 禁用 ngx_http_fastcgi_module\n–without-http_memcached_module               禁用 ngx_http_memcached_module\n–without-http_limit_zone_module              禁用 ngx_http_limit_zone_module\n–without-http_empty_gif_module               禁用 ngx_http_empty_gif_module\n–without-http_browser_module                 禁用 ngx_http_browser_module\n–without-http_upstream_ip_hash_module        禁用 ngx_http_upstream_ip_hash_module\n–with-http_perl_module                       启用 ngx_http_perl_module\n–with-perl_modules_path=PATH                 指定 perl 模块的路径\n–with-perl=PATH                              指定 perl 执行文件的路径\n–http-log-path=PATH                          Set path to the http access log\n–http-client-body-temp-path=PATH             Set path to the http client request body temporary files\n–http-proxy-temp-path=PATH                   Set path to the http proxy temporary files\n–http-fastcgi-temp-path=PATH                 Set path to the http fastcgi temporary files\n–without-http                                禁用 HTTP server\n–with-mail                                   启用 IMAP4/POP3/SMTP 代理模块\n–with-mail_ssl_module                        启用 ngx_mail_ssl_module\n–with-cc=PATH                                指定 C 编译器的路径\n–with-cpp=PATH                               指定 C 预处理器的路径\n–with-cc-opt=OPTIONS                         Additional parameters which will be added to the variable\n                                             CFLAGS. With the use of the system library PCRE in FreeBSD,\n                                             it is necessary to indicate –with-cc-opt=”-I\n                                             /usr/local/include”. If we are using select() and it is\n                                             necessary to increase the number of file descriptors, then\n                                             this also can be assigned here: –with-cc-opt=”-D\n                                             FD_SETSIZE=2048”.\n–with-ld-opt=OPTIONS                         Additional parameters passed to the linker. With the use of\n                                             the system library PCRE in - FreeBSD, it is necessary to\n                                             indicate –with-ld-opt=”-L /usr/local/lib”.\n–with-cpu-opt=CPU                            为特定的 CPU 编译，有效的值包括：pentium, pentiumpro, pentium3, pentium4,\n                                             athlon, opteron, amd64, sparc32, sparc64, ppc64\n–without-pcre                                禁止 PCRE 库的使用。同时也会禁止 HTTP rewrite 模块。在 “location”\n                                             配置指令中的正则表达式也需要 PCRE 。\n–with-pcre=DIR                               指定 PCRE 库的源代码的路径。\n–with-pcre-opt=OPTIONS                       Set additional options for PCRE building.\n–with-md5=DIR                                Set path to md5 library sources.\n–with-md5-opt=OPTIONS                        Set additional options for md5 building.\n–with-md5-asm                                Use md5 assembler sources.\n–with-sha1=DIR                               Set path to sha1 library sources.\n–with-sha1-opt=OPTIONS                       Set additional options for sha1 building.\n–with-sha1-asm                               Use sha1 assembler sources.\n–with-zlib=DIR                               Set path to zlib library sources.\n–with-zlib-opt=OPTIONS                       Set additional options for zlib building.\n–with-zlib-asm=CPU                           Use zlib assembler sources optimized for specified CPU,\n                                             valid values are: pentium, pentiumpro\n–with-openssl=DIR                            Set path to OpenSSL library sources\n–with-openssl-opt=OPTIONS                    Set additional options for OpenSSL building\n–with-debug                                  启用调试日志\n–add-module=PATH                             Add in a third-party module found in directory PATH\n\n\n# 配置\n\n在Centos 默认配置文件在 /usr/local/nginx-1.5.1/conf/nginx.conf 我们要在这里配置一些文件。nginx.conf是主配置文件，由若干个部分组成，每个大括号{}表示一个部分。每一行指令都由分号结束;，标志着一行的结束。\n\n\n# 常用正则\n\n正则   说明              正则      说明\n.    匹配除换行符以外的任意字符   $       匹配字符串的结束\n?    重复0次或1次         {n}     重复n次\n+    重复1次或更多次        {n,}    重复n次或更多次\n*    重复0次或更多次        [c]     匹配单个字符c\n\\d   匹配数字            [a-z]   匹配a-z小写字母的任意一个\n^    匹配字符串的开始        -       -\n\n\n# 全局变量\n\n变量                 说明                             变量                  说明\n$args              这个变量等于请求行中的参数，同$query_string   $remote_port        客户端的端口。\n$content_length    请求头中的Content-length字段。         $remote_user        已经经过Auth Basic Module验证的用户名。\n$content_type      请求头中的Content-Type字段。           $request_filename   当前请求的文件路径，由root或alias指令与URI请求生成。\n$document_root     当前请求在root指令中指定的值。              $scheme             HTTP方法（如http，https）。\n$host              请求主机头字段，否则为服务器名称。              $server_protocol    请求使用的协议，通常是HTTP/1.0或HTTP/1.1。\n$http_user_agent   客户端agent信息                     $server_addr        服务器地址，在完成一次系统调用后可以确定这个值。\n$http_cookie       客户端cookie信息                    $server_name        服务器名称。\n$limit_rate        这个变量可以限制连接速率。                  $server_port        请求到达服务器的端口号。\n$request_method    客户端请求的动作，通常为GET或POST。          $request_uri        包含请求参数的原始URI，不包含主机名，如：/foo/bar.php?arg=baz。\n$remote_addr       客户端的IP地址。                      $uri                不带请求参数的当前URI，$uri不包含主机名，如/foo/bar.html。\n$document_uri      与$uri相同。                       -                   -\n\n例如请求：http://localhost:3000/test1/test2/test.php\n\n$host：localhost $server_port：3000 $request_uri：/test1/test2/test.php $document_uri：/test1/test2/test.php $document_root：/var/www/html $request_filename：/var/www/html/test1/test2/test.php\n\n\n# 符号参考\n\n符号    说明    符号    说明    符号   说明\nk,K   千字节   m,M   兆字节   ms   毫秒\ns     秒     m     分钟    h    小时\nd     日     w     周     M    一个月, 30天\n\n例如，”8k”，”1m” 代表字节数计量。 例如，”1h 30m”，”1y 6M”。代表 “1小时 30分”，”1年零6个月”。\n\n\n# 配置文件\n\nnginx 的配置系统由一个主配置文件和其他一些辅助的配置文件构成。这些配置文件均是纯文本文件，全部位于 nginx 安装目录下的 conf 目录下。\n\n指令由 nginx 的各个模块提供，不同的模块会提供不同的指令来实现配置。 指令除了 Key-Value 的形式，还有作用域指令。\n\nnginx.conf 中的配置信息，根据其逻辑上的意义，对它们进行了分类，也就是分成了多个作用域，或者称之为配置指令上下文。不同的作用域含有一个或者多个配置项。\n\n下面的这些上下文指令是用的比较多：\n\nDIRECTIVE        DESCRIPTION                                                      CONTAINS DIRECTIVE\nmain             nginx 在运行时与具体业务功能（比如 http 服务或者 email                             user, worker_processes, error_log, events, http, mail\n                 服务代理）无关的一些参数，比如工作进程数，运行的身份等。\nhttp             与提供 http 服务相关的一些配置参数。例如：是否使用 keepalive 啊，是否使用 gzip 进行压缩等。        server\nserver           http 服务上支持若干虚拟主机。每个虚拟主机一个对应的 server                              listen, server_name, access_log, location, protocol, proxy,\n                 配置项，配置项里面包含该虚拟主机相关的配置。在提供 mail 服务的代理时，也可以建立若干 server. 每个         smtp_auth, xclient\n                 server 通过监听的地址来区分。\nlocation         http 服务中，某些特定的 URL 对应的一系列配置项。                                    index, root\nmail             实现 email 相关的 SMTP/IMAP/POP3                                      server, http, imap_capabilities\n                 代理时，共享的一些配置项（因为可能实现多个代理，工作在多个监听地址上）。\ninclude          以便增强配置文件的可读性，使得部分配置文件可以重新使用。                                     -\nvalid_referers   用来校验Http请求头Referer是否有效。                                          -\ntry_files        用在server部分，不过最常见的还是用在location部分，它会按照给定的参数顺序进行尝试，第一个被匹配到的将会被使用。   -\nif               当在location块中使用if指令，在某些情况下它并不按照预期运行，一般来说避免使用if指令。                 -\n\n例如我们再 nginx.conf 里面引用两个配置 vhost/example.com.conf 和 vhost/gitlab.com.conf 它们都被放在一个我自己新建的目录 vhost 下面。nginx.conf 配置如下：\n\n复制worker_processes  1;\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n\n    #log_format  main  \'$remote_addr - $remote_user [$time_local] "$request" \'\n    #                  \'$status $body_bytes_sent "$http_referer" \'\n    #                  \'"$http_user_agent" "$http_x_forwarded_for"\';\n\n    #access_log  logs/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    #keepalive_timeout  0;\n    keepalive_timeout  65;\n\n    #gzip  on;\n    server {\n        listen       80;\n        server_name  localhost;\n        location / {\n            root   html;\n            index  index.html index.htm;\n        }\n        error_page   500 502 503 504  /50x.html;\n        location = /50x.html {\n            root   html;\n        }\n    }\n    include  vhost/example.com.conf;\n    include  vhost/gitlab.com.conf;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n简单的配置: example.com.conf\n\n复制server {\n    #侦听的80端口\n    listen       80;\n    server_name  baidu.com app.baidu.com; # 这里指定域名\n    index        index.html index.htm;    # 这里指定默认入口页面\n    root /home/www/app.baidu.com;         # 这里指定目录\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 内置预定义变量\n\nNginx提供了许多预定义的变量，也可以通过使用set来设置变量。你可以在if中使用预定义变量，也可以将它们传递给代理服务器。以下是一些常见的预定义变量，更多详见\n\n变量名称              值\n$args_name        在请求中的name参数\n$args             所有请求参数\n$query_string     $args的别名\n$content_length   请求头Content-Length的值\n$content_type     请求头Content-Type的值\n$host             如果当前有Host，则为请求头Host的值；如果没有这个头，那么该值等于匹配该请求的server_name的值\n$remote_addr      客户端的IP地址\n$request          完整的请求，从客户端收到，包括Http请求方法、URI、Http协议、头、请求体\n$request_uri      完整请求的URI，从客户端来的请求，包括参数\n$scheme           当前请求的协议\n$uri              当前请求的标准化URI\n\n\n# 反向代理\n\n反向代理是一个Web服务器，它接受客户端的连接请求，然后将请求转发给上游服务器，并将从服务器得到的结果返回给连接的客户端。下面简单的反向代理的例子：\n\n复制server {  \n  listen       80;                                                        \n  server_name  localhost;                                              \n  client_max_body_size 1024M;  # 允许客户端请求的最大单文件字节数\n\n  location / {\n    proxy_pass                         http://localhost:8080;\n    proxy_set_header Host              $host:$server_port;\n    proxy_set_header X-Forwarded-For   $remote_addr; # HTTP的请求端真实的IP\n    proxy_set_header X-Forwarded-Proto $scheme;      # 为了正确地识别实际用户发出的协议是 http 还是 https\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n复杂的配置: gitlab.com.conf。\n\n复制server {\n    #侦听的80端口\n    listen       80;\n    server_name  git.example.cn;\n    location / {\n        proxy_pass   http://localhost:3000;\n        #以下是一些反向代理的配置可删除\n        proxy_redirect             off;\n        #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP\n        proxy_set_header           Host $host;\n        client_max_body_size       10m; #允许客户端请求的最大单文件字节数\n        client_body_buffer_size    128k; #缓冲区代理缓冲用户端请求的最大字节数\n        proxy_connect_timeout      300; #nginx跟后端服务器连接超时时间(代理连接超时)\n        proxy_send_timeout         300; #后端服务器数据回传时间(代理发送超时)\n        proxy_read_timeout         300; #连接成功后，后端服务器响应时间(代理接收超时)\n        proxy_buffer_size          4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小\n        proxy_buffers              4 32k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置\n        proxy_busy_buffers_size    64k; #高负荷下缓冲大小（proxy_buffers*2）\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n代理到上游服务器的配置中，最重要的是proxy_pass指令。以下是代理模块中的一些常用指令：\n\n指令                       说明\nproxy_connect_timeout    Nginx从接受请求至连接到上游服务器的最长等待时间\nproxy_send_timeout       后端服务器数据回传时间(代理发送超时)\nproxy_read_timeout       连接成功后，后端服务器响应时间(代理接收超时)\nproxy_cookie_domain      替代从上游服务器来的Set-Cookie头的domain属性\nproxy_cookie_path        替代从上游服务器来的Set-Cookie头的path属性\nproxy_buffer_size        设置代理服务器（nginx）保存用户头信息的缓冲区大小\nproxy_buffers            proxy_buffers缓冲区，网页平均在多少k以下\nproxy_set_header         重写发送到上游服务器头的内容，也可以通过将某个头部的值设置为空字符串，而不发送某个头部的方法实现\nproxy_ignore_headers     这个指令禁止处理来自代理服务器的应答。\nproxy_intercept_errors   使nginx阻止HTTP应答代码为400或者更高的应答。\n\n\n# 负载均衡\n\nupstream指令启用一个新的配置区段，在该区段定义一组上游服务器。这些服务器可能被设置不同的权重，也可能出于对服务器进行维护，标记为down。\n\n复制upstream gitlab {\n    ip_hash;\n    # upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。\n    server 192.168.122.11:8081 ;\n    server 127.0.0.1:82 weight=3;\n    server 127.0.0.1:83 weight=3 down;\n    server 127.0.0.1:84 weight=3; max_fails=3  fail_timeout=20s;\n    server 127.0.0.1:85 weight=4;;\n    keepalive 32;\n}\nserver {\n    #侦听的80端口\n    listen       80;\n    server_name  git.example.cn;\n    location / {\n        proxy_pass   http://gitlab;    #在这里设置一个代理，和upstream的名字一样\n        #以下是一些反向代理的配置可删除\n        proxy_redirect             off;\n        #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP\n        proxy_set_header           Host $host;\n        proxy_set_header           X-Real-IP $remote_addr;\n        proxy_set_header           X-Forwarded-For $proxy_add_x_forwarded_for;\n        client_max_body_size       10m;  #允许客户端请求的最大单文件字节数\n        client_body_buffer_size    128k; #缓冲区代理缓冲用户端请求的最大字节数\n        proxy_connect_timeout      300;  #nginx跟后端服务器连接超时时间(代理连接超时)\n        proxy_send_timeout         300;  #后端服务器数据回传时间(代理发送超时)\n        proxy_read_timeout         300;  #连接成功后，后端服务器响应时间(代理接收超时)\n        proxy_buffer_size          4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小\n        proxy_buffers              4 32k;# 缓冲区，网页平均在32k以下的话，这样设置\n        proxy_busy_buffers_size    64k; #高负荷下缓冲大小（proxy_buffers*2）\n        proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。\n\n负载均衡：\n\nupstream模块能够使用3种负载均衡算法：轮询、IP哈希、最少连接数。\n\n轮询： 默认情况下使用轮询算法，不需要配置指令来激活它，它是基于在队列中谁是下一个的原理确保访问均匀地分布到每个上游服务器； IP哈希： 通过ip_hash指令来激活，Nginx通过IPv4地址的前3个字节或者整个IPv6地址作为哈希键来实现，同一个IP地址总是能被映射到同一个上游服务器； 最少连接数： 通过least_conn指令来激活，该算法通过选择一个活跃数最少的上游服务器进行连接。如果上游服务器处理能力不同，可以通过给server配置weight权重来说明，该算法将考虑到不同服务器的加权最少连接数。\n\n# RR\n\n简单配置 ，这里我配置了2台服务器，当然实际上是一台，只是端口不一样而已，而8081的服务器是不存在的，也就是说访问不到，但是我们访问 http://localhost 的时候，也不会有问题，会默认跳转到http://localhost:8080具体是因为Nginx会自动判断服务器的状态，如果服务器处于不能访问（服务器挂了），就不会跳转到这台服务器，所以也避免了一台服务器挂了影响使用的情况，由于Nginx默认是RR策略，所以我们不需要其他更多的设置\n\n复制upstream test {\n    server localhost:8080;\n    server localhost:8081;\n}\nserver {\n    listen       81;\n    server_name  localhost;\n    client_max_body_size 1024M;\n \n    location / {\n        proxy_pass http://test;\n        proxy_set_header Host $host:$server_port;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n负载均衡的核心代码为\n\n复制upstream test {\n    server localhost:8080;\n    server localhost:8081;\n}\n\n\n1\n2\n3\n4\n\n\n# 权重\n\n指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 例如\n\n复制upstream test {\n    server localhost:8080 weight=9;\n    server localhost:8081 weight=1;\n}\n\n\n1\n2\n3\n4\n\n\n那么10次一般只会有1次会访问到8081，而有9次会访问到8080\n\n# ip_hash\n\n上面的2种方式都有一个问题，那就是下一个请求来的时候请求可能分发到另外一个服务器，当我们的程序不是无状态的时候（采用了session保存数据），这时候就有一个很大的很问题了，比如把登录信息保存到了session中，那么跳转到另外一台服务器的时候就需要重新登录了，所以很多时候我们需要一个客户只访问一个服务器，那么就需要用iphash了，iphash的每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。\n\n复制upstream test {\n    ip_hash;\n    server localhost:8080;\n    server localhost:8081;\n}\n\n\n1\n2\n3\n4\n5\n\n\n# fair\n\n这是个第三方模块，按后端服务器的响应时间来分配请求，响应时间短的优先分配。\n\n复制upstream backend {\n    fair;\n    server localhost:8080;\n    server localhost:8081;\n}\n\n\n1\n2\n3\n4\n5\n\n\n# url_hash\n\n这是个第三方模块，按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法\n\n复制upstream backend {\n    hash $request_uri;\n    hash_method crc32;\n    server localhost:8080;\n    server localhost:8081;\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n以上5种负载均衡各自适用不同情况下使用，所以可以根据实际情况选择使用哪种策略模式，不过fair和url_hash需要安装第三方模块才能使用\n\nserver指令可选参数：\n\n 1. weight：设置一个服务器的访问权重，数值越高，收到的请求也越多；\n 2. fail_timeout：在这个指定的时间内服务器必须提供响应，如果在这个时间内没有收到响应，那么服务器将会被标记为down状态；\n 3. max_fails：设置在fail_timeout时间之内尝试对一个服务器连接的最大次数，如果超过这个次数，那么服务器将会被标记为down;\n 4. down：标记一个服务器不再接受任何请求；\n 5. backup：一旦其他服务器宕机，那么有该标记的机器将会接收请求。\n\nkeepalive指令：\n\nNginx服务器将会为每一个worker进行保持同上游服务器的连接。\n\n\n# 屏蔽ip\n\n在nginx的配置文件nginx.conf中加入如下配置，可以放到http, server, location, limit_except语句块，需要注意相对路径，本例当中nginx.conf，blocksip.conf在同一个目录中。\n\n复制include blockip.conf;\n\n\n1\n\n\n在blockip.conf里面输入内容，如：\n\n复制deny 165.91.122.67;\n\ndeny IP;   # 屏蔽单个ip访问\nallow IP;  # 允许单个ip访问\ndeny all;  # 屏蔽所有ip访问\nallow all; # 允许所有ip访问\ndeny 123.0.0.0/8   # 屏蔽整个段即从123.0.0.1到123.255.255.254访问的命令\ndeny 124.45.0.0/16 # 屏蔽IP段即从123.45.0.1到123.45.255.254访问的命令\ndeny 123.45.6.0/24 # 屏蔽IP段即从123.45.6.1到123.45.6.254访问的命令\n\n# 如果你想实现这样的应用，除了几个IP外，其他全部拒绝\nallow 1.1.1.1; \nallow 1.1.1.2;\ndeny all;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 第三方模块安装方法\n\n复制./configure --prefix=/你的安装目录  --add-module=/第三方模块目录\n\n\n1\n\n\n\n# 重定向\n\n * permanent 永久性重定向。请求日志中的状态码为301\n * redirect 临时重定向。请求日志中的状态码为302\n\n\n# 重定向整个网站\n\n复制server {\n    server_name old-site.com\n    return 301 $scheme://new-site.com$request_uri;\n}\n\n\n1\n2\n3\n4\n\n\n\n# 重定向单页\n\n复制server {\n    location = /oldpage.html {\n        return 301 http://example.org/newpage.html;\n    }\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# 重定向整个子路径\n\n复制location /old-site {\n    rewrite ^/old-site/(.*) http://example.org/new-site/$1 permanent;\n}\n\n\n1\n2\n3\n\n\n\n# 性能\n\n\n# 内容缓存\n\n允许浏览器基本上永久地缓存静态内容。 Nginx将为您设置Expires和Cache-Control头信息。\n\n复制location /static {\n    root /data;\n    expires max;\n}\n\n\n1\n2\n3\n4\n\n\n如果要求浏览器永远不会缓存响应（例如用于跟踪请求），请使用-1。\n\n复制location = /empty.gif {\n    empty_gif;\n    expires -1;\n}\n\n\n1\n2\n3\n4\n\n\n\n# Gzip压缩\n\n复制gzip  on;\ngzip_buffers 16 8k;\ngzip_comp_level 6;\ngzip_http_version 1.1;\ngzip_min_length 256;\ngzip_proxied any;\ngzip_vary on;\ngzip_types\n    text/xml application/xml application/atom+xml application/rss+xml application/xhtml+xml image/svg+xml\n    text/javascript application/javascript application/x-javascript\n    text/x-json application/json application/x-web-app-manifest+json\n    text/css text/plain text/x-component\n    font/opentype application/x-font-ttf application/vnd.ms-fontobject\n    image/x-icon;\ngzip_disable  "msie6";\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 打开文件缓存\n\n复制open_file_cache max=1000 inactive=20s;\nopen_file_cache_valid 30s;\nopen_file_cache_min_uses 2;\nopen_file_cache_errors on;\n\n\n1\n2\n3\n4\n\n\n\n# SSL缓存\n\n复制ssl_session_cache shared:SSL:10m;\nssl_session_timeout 10m;\n\n\n1\n2\n\n\n\n# 上游Keepalive\n\n复制upstream backend {\n    server 127.0.0.1:8080;\n    keepalive 32;\n}\nserver {\n    ...\n    location /api/ {\n        proxy_pass http://backend;\n        proxy_http_version 1.1;\n        proxy_set_header Connection "";\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 监控\n\n使用ngxtop实时解析nginx访问日志，并且将处理结果输出到终端，功能类似于系统命令top。所有示例都读取nginx配置文件的访问日志位置和格式。如果要指定访问日志文件和/或日志格式，请使用-f和-a选项。\n\n注意：在nginx配置中/usr/local/nginx/conf/nginx.conf日志文件必须是绝对路径。\n\n复制# 安装 ngxtop\npip install ngxtop\n\n# 实时状态\nngxtop\n# 状态为404的前10个请求的路径：\nngxtop top request_path --filter \'status == 404\'\n\n# 发送总字节数最多的前10个请求\nngxtop --order-by \'avg(bytes_sent) * count\'\n\n# 排名前十位的IP，例如，谁攻击你最多\nngxtop --group-by remote_addr\n\n# 打印具有4xx或5xx状态的请求，以及status和http referer\nngxtop -i \'status >= 400\' print request status http_referer\n\n# 由200个请求路径响应发送的平均正文字节以\'foo\'开始：\nngxtop avg bytes_sent --filter \'status == 200 and request_path.startswith("foo")\'\n\n# 使用“common”日志格式从远程机器分析apache访问日志\nssh remote tail -f /var/log/apache2/access.log | ngxtop -f common\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 常见使用场景\n\n\n# 跨域问题\n\n在工作中，有时候会遇到一些接口不支持跨域，这时候可以简单的添加add_headers来支持cors跨域。配置如下：\n\n复制server {\n  listen 80;\n  server_name api.xxx.com;\n    \n  add_header \'Access-Control-Allow-Origin\' \'*\';\n  add_header \'Access-Control-Allow-Credentials\' \'true\';\n  add_header \'Access-Control-Allow-Methods\' \'GET,POST,HEAD\';\n\n  location / {\n    proxy_pass http://127.0.0.1:3000;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header Host  $http_host;    \n  } \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n上面更改头信息，还有一种，使用 rewrite 指令重定向URI来解决跨域问题。\n\n复制upstream test {\n  server 127.0.0.1:8080;\n  server localhost:8081;\n}\nserver {\n  listen 80;\n  server_name api.xxx.com;\n  location / { \n    root  html;                   #去请求../html文件夹里的文件\n    index  index.html index.htm;  #首页响应地址\n  }\n  # 用于拦截请求，匹配任何以 /api/开头的地址，\n  # 匹配符合以后，停止往下搜索正则。\n  location ^~/api/{ \n    # 代表重写拦截进来的请求，并且只能对域名后边的除去传递的参数外的字符串起作用，\n    # 例如www.a.com/proxy/api/msg?meth=1&par=2重写，只对/proxy/api/msg重写。\n    # rewrite后面的参数是一个简单的正则 ^/api/(.*)$，\n    # $1代表正则中的第一个()，$2代表第二个()的值，以此类推。\n    rewrite ^/api/(.*)$ /$1 break;\n    \n    # 把请求代理到其他主机 \n    # 其中 http://www.b.com/ 写法和 http://www.b.com写法的区别如下\n    # 如果你的请求地址是他 http://server/html/test.jsp\n    # 配置一： http://www.b.com/ 后面有“/” \n    #         将反向代理成 http://www.b.com/html/test.jsp 访问\n    # 配置一： http://www.b.com 后面没有有“/” \n    #         将反向代理成 http://www.b.com/test.jsp 访问\n    proxy_pass http://test;\n\n    # 如果 proxy_pass  URL 是 http://a.xx.com/platform/ 这种情况\n    # proxy_cookie_path应该设置成 /platform/ / (注意两个斜杠之间有空格)。\n    proxy_cookie_path /platfrom/ /;\n\n    # http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass_header\n    # 设置 Cookie 头通过\n    proxy_pass_header Set-Cookie;\n  } \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n\n# 跳转到带www的域上面\n\n复制server {\n    listen 80;\n    # 配置正常的带www的域名\n    server_name www.wangchujiang.com;\n    root /home/www/wabg/download;\n    location / {\n        try_files $uri $uri/ /index.html =404;\n    }\n}\nserver {\n    # 这个要放到下面，\n    # 将不带www的 wangchujiang.com 永久性重定向到  https://www.wangchujiang.com\n    server_name wangchujiang.com;\n    rewrite ^(.*) https://www.wangchujiang.com$1 permanent;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 代理转发\n\n复制upstream server-api{\n    # api 代理服务地址\n    server 127.0.0.1:3110;    \n}\nupstream server-resource{\n    # 静态资源 代理服务地址\n    server 127.0.0.1:3120;\n}\nserver {\n    listen       3111;\n    server_name  localhost;      # 这里指定域名\n    root /home/www/server-statics;\n    # 匹配 api 路由的反向代理到API服务\n    location ^~/api/ {\n        rewrite ^/(.*)$ /$1 break;\n        proxy_pass http://server-api;\n    }\n    # 假设这里验证码也在API服务中\n    location ^~/captcha {\n        rewrite ^/(.*)$ /$1 break;\n        proxy_pass http://server-api;\n    }\n    # 假设你的图片资源全部在另外一个服务上面\n    location ^~/img/ {\n        rewrite ^/(.*)$ /$1 break;\n        proxy_pass http://server-resource;\n    }\n    # 路由在前端，后端没有真实路由，在路由不存在的 404状态的页面返回 /index.html\n    # 这个方式使用场景，你在写React或者Vue项目的时候，没有真实路由\n    location / {\n        try_files $uri $uri/ /index.html =404;\n        #                               ^ 空格很重要\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# 监控状态信息\n\n通过 nginx -V 来查看是否有 with-http_stub_status_module 该模块。\n\n> nginx -V 这里 V 是大写的，如果是小写的 v 即 nginx -v，则不会出现有哪些模块，只会出现 nginx 的版本\n\n复制location /nginx_status {\n    stub_status on;\n    access_log off;\n}\n\n\n1\n2\n3\n4\n\n\n通过 http://127.0.0.1/nginx_status 访问出现下面结果。\n\n复制Active connections: 3\nserver accepts handled requests\n 7 7 5 \nReading: 0 Writing: 1 Waiting: 2\n\n\n1\n2\n3\n4\n\n 1. 主动连接(第 1 行)\n\n当前与http建立的连接数，包括等待的客户端连接：3\n\n 1. 服务器接受处理的请求(第 2~3 行)\n\n接受的客户端连接总数目：7 处理的客户端连接总数目：7 客户端总的请求数目：5\n\n 1. 读取其它信(第 4 行)\n\n当前，nginx读请求连接 当前，nginx写响应返回给客户端 目前有多少空闲客户端请求连接\n\n\n# 代理转发连接替换\n\n复制location ^~/api/upload {\n    rewrite ^/(.*)$ /wfs/v1/upload break;\n    proxy_pass http://wfs-api;\n}\n\n\n1\n2\n3\n4\n\n\n\n# ssl配置\n\n超文本传输安全协议（缩写：HTTPS，英语：Hypertext Transfer Protocol Secure）是超文本传输协议和SSL/TLS的组合，用以提供加密通讯及对网络服务器身份的鉴定。HTTPS连接经常被用于万维网上的交易支付和企业信息系统中敏感信息的传输。HTTPS不应与在RFC 2660中定义的安全超文本传输协议（S-HTTP）相混。HTTPS 目前已经是所有注重隐私和安全的网站的首选，随着技术的不断发展，HTTPS 网站已不再是大型网站的专利，所有普通的个人站长和博客均可以自己动手搭建一个安全的加密的网站。\n\n创建SSL证书，如果你购买的证书，就可以直接下载\n\n复制sudo mkdir /etc/nginx/ssl\n# 创建了有效期100年，加密强度为RSA2048的SSL密钥key和X509证书文件。\nsudo openssl req -x509 -nodes -days 36500 -newkey rsa:2048 -keyout /etc/nginx/ssl/nginx.key -out /etc/nginx/ssl/nginx.crt\n# 上面命令，会有下面需要填写内容\nCountry Name (2 letter code) [AU]:US\nState or Province Name (full name) [Some-State]:New York\nLocality Name (eg, city) []:New York City\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Bouncy Castles, Inc.\nOrganizational Unit Name (eg, section) []:Ministry of Water Slides\nCommon Name (e.g. server FQDN or YOUR name) []:your_domain.com\nEmail Address []:admin@your_domain.com\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n创建自签证书\n\n复制首先，创建证书和私钥的目录\n# mkdir -p /etc/nginx/cert\n# cd /etc/nginx/cert\n创建服务器私钥，命令会让你输入一个口令：\n# openssl genrsa -des3 -out nginx.key 2048\n创建签名请求的证书（CSR）：\n# openssl req -new -key nginx.key -out nginx.csr\n在加载SSL支持的Nginx并使用上述私钥时除去必须的口令：\n# cp nginx.key nginx.key.org\n# openssl rsa -in nginx.key.org -out nginx.key\n最后标记证书使用上述私钥和CSR：\n# openssl x509 -req -days 365 -in nginx.csr -signkey nginx.key -out nginx.crt\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查看目前nginx编译选项\n\n复制sbin/nginx -V\n\n\n1\n\n\n输出下面内容\n\n复制nginx version: nginx/1.7.8\nbuilt by gcc 4.4.7 20120313 (Red Hat 4.4.7-4) (GCC)\nTLS SNI support enabled\nconfigure arguments: --prefix=/usr/local/nginx-1.7.8 --with-http_ssl_module --with-http_spdy_module --with-http_stub_status_module --with-pcre\n\n\n1\n2\n3\n4\n\n\n如果依赖的模块不存在，可以进入安装目录，输入下面命令重新编译安装。\n\n复制./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module\n\n\n1\n\n\n运行完成之后还需要make (不用make install)\n\n复制# 备份nginx的二进制文件\ncp -rf /usr/local/nginx/sbin/nginx　 /usr/local/nginx/sbin/nginx.bak\n# 覆盖nginx的二进制文件\ncp -rf objs/nginx   /usr/local/nginx/sbin/\n\n\n1\n2\n3\n4\n\n\nHTTPS server\n\n复制server {\n    listen       443 ssl;\n    server_name  localhost;\n\n    ssl_certificate /etc/nginx/ssl/nginx.crt;\n    ssl_certificate_key /etc/nginx/ssl/nginx.key;\n    # 禁止在header中出现服务器版本，防止黑客利用版本漏洞攻击\n    server_tokens off;\n    # 设置ssl/tls会话缓存的类型和大小。如果设置了这个参数一般是shared，buildin可能会参数内存碎片，默认是none，和off差不多，停用缓存。如shared:SSL:10m表示我所有的nginx工作进程共享ssl会话缓存，官网介绍说1M可以存放约4000个sessions。 \n    ssl_session_cache    shared:SSL:1m; \n\n    # 客户端可以重用会话缓存中ssl参数的过期时间，内网系统默认5分钟太短了，可以设成30m即30分钟甚至4h。\n    ssl_session_timeout  5m; \n\n    # 选择加密套件，不同的浏览器所支持的套件（和顺序）可能会不同。\n    # 这里指定的是OpenSSL库能够识别的写法，你可以通过 openssl -v cipher \'RC4:HIGH:!aNULL:!MD5\'（后面是你所指定的套件加密算法） 来看所支持算法。\n    ssl_ciphers  HIGH:!aNULL:!MD5;\n\n    # 设置协商加密算法时，优先使用我们服务端的加密套件，而不是客户端浏览器的加密套件。\n    ssl_prefer_server_ciphers  on;\n\n    location / {\n        root   html;\n        index  index.html index.htm;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# 强制将http重定向到https\n\n复制server {\n    listen       80;\n    server_name  example.com;\n    rewrite ^ https://$http_host$request_uri? permanent;    # 强制将http重定向到https\n    # 在错误页面和“服务器”响应头字段中启用或禁用发射nginx版本。 防止黑客利用版本漏洞攻击\n    server_tokens off;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 两个虚拟主机\n\n纯静态-html 支持\n\n复制http {\n    server {\n        listen          80;\n        server_name     www.domain1.com;\n        access_log      logs/domain1.access.log main;\n        location / {\n            index index.html;\n            root  /var/www/domain1.com/htdocs;\n        }\n    }\n    server {\n        listen          80;\n        server_name     www.domain2.com;\n        access_log      logs/domain2.access.log main;\n        location / {\n            index index.html;\n            root  /var/www/domain2.com/htdocs;\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 虚拟主机标准配置\n\n复制http {\n  server {\n    listen          80 default;\n    server_name     _ *;\n    access_log      logs/default.access.log main;\n    location / {\n       index index.html;\n       root  /var/www/default/htdocs;\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 爬虫过滤\n\n根据 User-Agent 过滤请求，通过一个简单的正则表达式，就可以过滤不符合要求的爬虫请求(初级爬虫)。\n\n> ~* 表示不区分大小写的正则匹配\n\n复制location / {\n    if ($http_user_agent ~* "python|curl|java|wget|httpclient|okhttp") {\n        return 503;\n    }\n    # 正常处理\n    # ...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 防盗链\n\n复制location ~* \\.(gif|jpg|png|swf|flv)$ {\n   root html\n   valid_referers none blocked *.nginxcn.com;\n   if ($invalid_referer) {\n     rewrite ^/ www.nginx.cn\n     #return 404;\n   }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 虚拟目录配置\n\nalias指定的目录是准确的，root是指定目录的上级目录，并且该上级目录要含有location指定名称的同名目录。\n\n复制location /img/ {\n    alias /var/www/image/;\n}\n# 访问/img/目录里面的文件时，ningx会自动去/var/www/image/目录找文件\nlocation /img/ {\n    root /var/www/image;\n}\n# 访问/img/目录下的文件时，nginx会去/var/www/image/img/目录下找文件。]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 防盗图配置\n\n复制location ~ \\/public\\/(css|js|img)\\/.*\\.(js|css|gif|jpg|jpeg|png|bmp|swf) {\n    valid_referers none blocked *.jslite.io;\n    if ($invalid_referer) {\n        rewrite ^/  http://wangchujiang.com/piratesp.png;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 屏蔽.git等文件\n\n复制location ~ (.git|.gitattributes|.gitignore|.svn) {\n    deny all;\n}\n\n\n1\n2\n3\n\n\n\n# 域名路径加不加需要都能正常访问\n\n复制http://wangchujiang.com/api/index.php?a=1&name=wcj\n                                  ^ 有后缀\n\nhttp://wangchujiang.com/api/index?a=1&name=wcj\n                                 ^ 没有后缀\n\n\n1\n2\n3\n4\n5\n\n\nnginx rewrite规则如下：\n\n复制rewrite ^/(.*)/$ /index.php?/$1 permanent;\nif (!-d $request_filename){\n        set $rule_1 1$rule_1;\n}\nif (!-f $request_filename){\n        set $rule_1 2$rule_1;\n}\nif ($rule_1 = "21"){\n        rewrite ^/ /index.php last;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 错误问题\n\n复制The plain HTTP request was sent to HTTPS port\n\n\n1\n\n\n解决办法，fastcgi_param HTTPS $https if_not_empty 添加这条规则，\n\n复制server {\n    listen 443 ssl; # 注意这条规则\n    server_name  my.domain.com;\n    \n    fastcgi_param HTTPS $https if_not_empty;\n    fastcgi_param HTTPS on;\n\n    ssl_certificate /etc/ssl/certs/your.pem;\n    ssl_certificate_key /etc/ssl/private/your.key;\n\n    location / {\n        # Your config here...\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# Nginx 模块\n\n * Nginx Office Hours 一个 nginx 模块，允许您仅在办公时间内提供访问访问网站。\n\n\n# 精品文章参考\n\n * 负载均衡原理的解析\n\n * Nginx泛域名解析，实现多个二级域名\n\n * 深入 NGINX: 我们如何设计性能和扩展\n\n * Inside NGINX: How We Designed for Performance & Scale\n\n * Nginx开发从入门到精通\n\n * Nginx的优化与防盗链\n\n * 实战开发一个Nginx扩展 (Nginx Module)\n\n * Nginx+Keepalived(双机热备)搭建高可用负载均衡环境(HA)\n\n * Nginx 平滑升级\n\n * Nginx最新模块—ngx_http_mirror_module分析可以做版本发布前的预先验证，进行流量放大后的压测等等\n   \n   原文链接https://www.viayc.com/2019/03/13/Nginx%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/',normalizedContent:'# nginx配置教程\n\n\n\nnginx 是一款面向性能设计的 http 服务器，能反向代理 http，https 和邮件相关(smtp，pop3，imap)的协议链接。并且提供了负载均衡以及 http 缓存。它的设计充分使用异步事件模型，削减上下文调度的开销，提高服务器并发能力。采用了模块化设计，提供了丰富模块的第三方模块。\n\n所以关于 nginx，有这些标签：「异步」「事件」「模块化」「高性能」「高并发」「反向代理」「负载均衡」\n\nlinux系统：centos 7 x64 nginx版本：1.11.5\n\n\n# 安装\n\n\n# 安装依赖\n\n> prce(重定向支持)和openssl(https支持，如果不需要https可以不安装。)\n\n复制yum install -y pcre-devel \nyum -y install gcc make gcc-c++ wget\nyum -y install openssl openssl-devel\n\n\n1\n2\n3\n\n\ncentos 6.5 我安装的时候是选择的“基本服务器”，默认这两个包都没安装全，所以这两个都运行安装即可。\n\n\n# 下载\n\nnginx的所有版本在这里\n\n复制wget http://nginx.org/download/nginx-1.13.3.tar.gz\nwget http://nginx.org/download/nginx-1.13.7.tar.gz\n\n# 如果没有安装wget\n# 下载已编译版本\n$ yum install wget\n\n# 解压压缩包\ntar zxf nginx-1.13.3.tar.gz\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 编译安装\n\n然后进入目录编译安装，configure参数说明\n\n复制cd nginx-1.11.5\n./configure\n\n....\nconfiguration summary\n  + using system pcre library\n  + openssl library is not used\n  + using system zlib library\n\n  nginx path prefix: "/usr/local/nginx"\n  nginx binary file: "/usr/local/nginx/sbin/nginx"\n  nginx modules path: "/usr/local/nginx/modules"\n  nginx configuration prefix: "/usr/local/nginx/conf"\n  nginx configuration file: "/usr/local/nginx/conf/nginx.conf"\n  nginx pid file: "/usr/local/nginx/logs/nginx.pid"\n  nginx error log file: "/usr/local/nginx/logs/error.log"\n  nginx http access log file: "/usr/local/nginx/logs/access.log"\n  nginx http client request body temporary files: "client_body_temp"\n  nginx http proxy temporary files: "proxy_temp"\n  nginx http fastcgi temporary files: "fastcgi_temp"\n  nginx http uwsgi temporary files: "uwsgi_temp"\n  nginx http scgi temporary files: "scgi_temp"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n安装报错误的话比如：“c compiler cc is not found”，这个就是缺少编译环境，安装一下就可以了 yum -y install gcc make gcc-c++ openssl-devel\n\n如果没有error信息，就可以执行下边的安装了：\n\n复制make\nmake install\n\n\n1\n2\n\n\n\n# nginx测试\n\n运行下面命令会出现两个结果，一般情况nginx会安装在/usr/local/nginx目录中\n\n复制cd /usr/local/nginx/sbin/\n./nginx -t\n\n# nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is ok\n# nginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful\n\n\n1\n2\n3\n4\n5\n\n\n\n# 设置全局nginx命令\n\n复制vi ~/.bash_profile\n\n\n1\n\n\n将下面内容添加到 ~/.bash_profile 文件中\n\n复制path=$path:$home/bin:/usr/local/nginx/sbin/\nexport path\n\n\n1\n2\n\n\n运行命令 source ~/.bash_profile 让配置立即生效。你就可以全局运行 nginx 命令了。\n\n\n# mac 安装\n\nmac osx 安装特别简单，首先你需要安装 brew， 通过 brew 快速安装 nginx。\n\n\n# 安装nginx\n\n复制brew install nginx\n# updating homebrew...\n# ==> auto-updated homebrew!\n# updated 2 taps (homebrew/core, homebrew/cask).\n# ==> updated formulae\n# ==> installing dependencies for nginx: openssl, pcre\n# ==> installing nginx dependency: openssl\n# ==> downloading https://homebrew.bintray.com/bottles/openssl-1.0.2o_1.high_sierra.bottle.tar.gz\n# ######################################################################## 100.0%\n# ==> pouring openssl-1.0.2o_1.high_sierra.bottle.tar.gz\n# ==> caveats\n# a ca file has been bootstrapped using certificates from the systemroots\n# keychain. to add additional certificates (e.g. the certificates added in\n# the system keychain), place .pem files in\n#   /usr/local/etc/openssl/certs\n# \n# and run\n#   /usr/local/opt/openssl/bin/c_rehash\n# \n# this formula is keg-only, which means it was not symlinked into /usr/local,\n# because apple has deprecated use of openssl in favor of its own tls and crypto libraries.\n# \n# if you need to have this software first in your path run:\n#   echo \'export path="/usr/local/opt/openssl/bin:$path"\' >> ~/.zshrc\n# \n# for compilers to find this software you may need to set:\n#     ldflags:  -l/usr/local/opt/openssl/lib\n#     cppflags: -i/usr/local/opt/openssl/include\n# for pkg-config to find this software you may need to set:\n#     pkg_config_path: /usr/local/opt/openssl/lib/pkgconfig\n# \n# ==> summary\n# 🍺  /usr/local/cellar/openssl/1.0.2o_1: 1,791 files, 12.3mb\n# ==> installing nginx dependency: pcre\n# ==> downloading https://homebrew.bintray.com/bottles/pcre-8.42.high_sierra.bottle.tar.gz\n# ######################################################################## 100.0%\n# ==> pouring pcre-8.42.high_sierra.bottle.tar.gz\n# 🍺  /usr/local/cellar/pcre/8.42: 204 files, 5.3mb\n# ==> installing nginx\n# ==> downloading https://homebrew.bintray.com/bottles/nginx-1.13.12.high_sierra.bottle.tar.gz\n# ######################################################################## 100.0%\n# ==> pouring nginx-1.13.12.high_sierra.bottle.tar.gz\n# ==> caveats\n# docroot is: /usr/local/var/www\n# \n# the default port has been set in /usr/local/etc/nginx/nginx.conf to 8080 so that\n# nginx can run without sudo.\n# \n# nginx will load all files in /usr/local/etc/nginx/servers/.\n# \n# to have launchd start nginx now and restart at login:\n#   brew services start nginx\n# or, if you don\'t wacd /usr/local/cellar/nginx/1.13.12/n just run:\n# cd /usr/local/cellar/nginx/1.13.12/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n\n\n\n# 启动服务\n\n注意默认端口不是 8080 查看确认端口是否被占用。\n\n复制brew services start nginx\n# http://localhost:8080/\n\n\n1\n2\n\n\n\n# 开机自启动\n\n开机自启动方法一：\n\n编辑 vi /lib/systemd/system/nginx.service 文件，没有创建一个 touch nginx.service 然后将如下内容根据具体情况进行修改后，添加到nginx.service文件中：\n\n复制[unit]\ndescription=nginx\nafter=network.target remote-fs.target nss-lookup.target\n\n[service]\n\ntype=forking\npidfile=/var/run/nginx.pid\nexecstartpre=/usr/local/nginx/sbin/nginx -t -c /usr/local/nginx/conf/nginx.conf\nexecstart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf\nexecreload=/bin/kill -s hup $mainpid\nexecstop=/bin/kill -s quit $mainpid\nprivatetmp=true\n\n[install]\nwantedby=multi-user.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n * [unit]:服务的说明\n * description:描述服务\n * after:描述服务类别\n * [service]服务运行参数的设置\n * type=forking是后台运行的形式\n * execstart为服务的具体运行命令\n * execreload为重启命令\n * execstop为停止命令\n * privatetmp=true表示给服务分配独立的临时空间\n\n注意：[service]的启动、重启、停止命令全部要求使用绝对路径。\n\n[install]运行级别下服务安装的相关设置，可设置为多用户，即系统运行级别为3。\n\n保存退出。\n\n设置开机启动，使配置生效：\n\n复制# 启动nginx服务\nsystemctl start nginx.service\n# 停止开机自启动\nsystemctl disable nginx.service\n# 查看服务当前状态\nsystemctl status nginx.service\n# 查看所有已启动的服务\nsystemctl list-units --type=service\n# 重新启动服务\nsystemctl restart nginx.service\n# 设置开机自启动\nsystemctl enable nginx.service\n# 输出下面内容表示成功了\ncreated symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /usr/lib/systemd/system/nginx.service.\n复制systemctl is-enabled servicename.service # 查询服务是否开机启动\nsystemctl enable *.service # 开机运行服务\nsystemctl disable *.service # 取消开机运行\nsystemctl start *.service # 启动服务\nsystemctl stop *.service # 停止服务\nsystemctl restart *.service # 重启服务\nsystemctl reload *.service # 重新加载服务配置文件\nsystemctl status *.service # 查询服务运行状态\nsystemctl --failed # 显示启动失败的服务\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n注：*代表某个服务的名字，如http的服务名为httpd\n\n开机自启动方法二：\n\n复制vi /etc/rc.local\n\n# 在 rc.local 文件中，添加下面这条命令\n/usr/local/nginx/sbin/nginx start\n\n\n1\n2\n3\n4\n\n\n如果开机后发现自启动脚本没有执行，你要去确认一下rc.local这个文件的访问权限是否是可执行的，因为rc.local默认是不可执行的。修改rc.local访问权限，增加可执行权限：\n\n复制# /etc/rc.local是/etc/rc.d/rc.local的软连接，\nchmod +x /etc/rc.d/rc.local\n\n\n1\n2\n\n\n官方脚本 ed hat nginx init script。\n\n\n# 运维\n\n\n# 服务管理\n\n复制# 启动\n/usr/local/nginx/sbin/nginx\n\n# 重启\n/usr/local/nginx/sbin/nginx -s reload\n\n# 关闭进程\n/usr/local/nginx/sbin/nginx -s stop\n\n# 平滑关闭nginx\n/usr/local/nginx/sbin/nginx -s quit\n\n# 查看nginx的安装状态，\n/usr/local/nginx/sbin/nginx -v\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n关闭防火墙，或者添加防火墙规则就可以测试了\n\n复制service iptables stop\n\n\n1\n\n\n或者编辑配置文件：\n\n复制vi /etc/sysconfig/iptables\n\n\n1\n\n\n添加这样一条开放80端口的规则后保存：\n\n复制-a input -m state --state new -m tcp -p tcp --dport 80 -j accept\n\n\n1\n\n\n重启服务即可:\n\n复制service iptables restart\n# 命令进行查看目前nat\niptables -t nat -l\n\n\n1\n2\n3\n\n\n\n# 重启服务防火墙报错解决\n\n复制service iptables restart\n# redirecting to /bin/systemctl restart  iptables.service\n# failed to restart iptables.service: unit iptables.service failed to load: no such file or directory.\n\n\n1\n2\n3\n\n\n在centos 7或rhel 7或fedora中防火墙由 firewalld 来管理，当然你可以还原传统的管理方式。或则使用新的命令进行管理。 假如采用传统请执行一下命令：\n\n复制# 传统命令\nsystemctl stop firewalld\nsystemctl mask firewalld\n复制# 安装命令\nyum install iptables-services\n\nsystemctl enable iptables \nservice iptables restart\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# nginx卸载\n\n如果通过yum安装，使用下面命令安装。\n\n复制yum remove nginx\n\n\n1\n\n\n编译安装，删除/usr/local/nginx目录即可 如果配置了自启动脚本，也需要删除。\n\n\n# 参数说明\n\n参数                                           说明\n–prefix=<path>                               nginx安装路径。如果没有指定，默认为 /usr/local/nginx。\n–sbin-path=<path>                            nginx可执行文件安装路径。只能安装时指定，如果没有指定，默认为<prefix>/sbin/nginx。\n–conf-path=<path>                            在没有给定-c选项下默认的nginx.conf的路径。如果没有指定，默认为<prefix>/conf/nginx.conf。\n–pid-path=<path>                             在nginx.conf中没有指定pid指令的情况下，默认的nginx.pid的路径。如果没有指定，默认为\n                                             <prefix>/logs/nginx.pid。\n–lock-path=<path>                            nginx.lock文件的路径。\n–error-log-path=<path>                       在nginx.conf中没有指定error_log指令的情况下，默认的错误日志的路径。如果没有指定，默认为\n                                             <prefix>/- logs/error.log。\n–http-log-path=<path>                        在nginx.conf中没有指定access_log指令的情况下，默认的访问日志的路径。如果没有指定，默认为\n                                             <prefix>/- logs/access.log。\n–user=<user>                                 在nginx.conf中没有指定user指令的情况下，默认的nginx使用的用户。如果没有指定，默认为 nobody。\n–group=<group>                               在nginx.conf中没有指定user指令的情况下，默认的nginx使用的组。如果没有指定，默认为 nobody。\n–builddir=dir                                指定编译的目录\n–with-rtsig_module                           启用 rtsig 模块\n–with-select_module –without-select_module   允许或不允许开启select模式，如果 configure 没有找到更合适的模式，比如：kqueue(sun\n                                             os),epoll (linux kenel 2.6+), rtsig(-\n                                             实时信号)或者/dev/poll(一种类似select的模式，底层实现与select基本相 同，都是采用轮训方法)\n                                             select模式将是默认安装模式\n–with-poll_module –without-poll_module       whether or not to enable the poll module. this module is\n                                             enabled by, default if a more suitable method such as\n                                             kqueue, epoll, rtsig or /dev/poll is not discovered by\n                                             configure.\n–with-http_ssl_module                        enable ngx_http_ssl_module. enables ssl support and the\n                                             ability to handle https requests. requires openssl. on\n                                             debian, this is libssl-dev. 开启http\n                                             ssl模块，使nginx可以支持https请求。这个模块需要已经安装了openssl，在debian上是libssl\n–with-http_realip_module                     启用 ngx_http_realip_module\n–with-http_addition_module                   启用 ngx_http_addition_module\n–with-http_sub_module                        启用 ngx_http_sub_module\n–with-http_dav_module                        启用 ngx_http_dav_module\n–with-http_flv_module                        启用 ngx_http_flv_module\n–with-http_stub_status_module                启用 “server status” 页\n–without-http_charset_module                 禁用 ngx_http_charset_module\n–without-http_gzip_module                    禁用 ngx_http_gzip_module. 如果启用，需要 zlib 。\n–without-http_ssi_module                     禁用 ngx_http_ssi_module\n–without-http_userid_module                  禁用 ngx_http_userid_module\n–without-http_access_module                  禁用 ngx_http_access_module\n–without-http_auth_basic_module              禁用 ngx_http_auth_basic_module\n–without-http_autoindex_module               禁用 ngx_http_autoindex_module\n–without-http_geo_module                     禁用 ngx_http_geo_module\n–without-http_map_module                     禁用 ngx_http_map_module\n–without-http_referer_module                 禁用 ngx_http_referer_module\n–without-http_rewrite_module                 禁用 ngx_http_rewrite_module. 如果启用需要 pcre 。\n–without-http_proxy_module                   禁用 ngx_http_proxy_module\n–without-http_fastcgi_module                 禁用 ngx_http_fastcgi_module\n–without-http_memcached_module               禁用 ngx_http_memcached_module\n–without-http_limit_zone_module              禁用 ngx_http_limit_zone_module\n–without-http_empty_gif_module               禁用 ngx_http_empty_gif_module\n–without-http_browser_module                 禁用 ngx_http_browser_module\n–without-http_upstream_ip_hash_module        禁用 ngx_http_upstream_ip_hash_module\n–with-http_perl_module                       启用 ngx_http_perl_module\n–with-perl_modules_path=path                 指定 perl 模块的路径\n–with-perl=path                              指定 perl 执行文件的路径\n–http-log-path=path                          set path to the http access log\n–http-client-body-temp-path=path             set path to the http client request body temporary files\n–http-proxy-temp-path=path                   set path to the http proxy temporary files\n–http-fastcgi-temp-path=path                 set path to the http fastcgi temporary files\n–without-http                                禁用 http server\n–with-mail                                   启用 imap4/pop3/smtp 代理模块\n–with-mail_ssl_module                        启用 ngx_mail_ssl_module\n–with-cc=path                                指定 c 编译器的路径\n–with-cpp=path                               指定 c 预处理器的路径\n–with-cc-opt=options                         additional parameters which will be added to the variable\n                                             cflags. with the use of the system library pcre in freebsd,\n                                             it is necessary to indicate –with-cc-opt=”-i\n                                             /usr/local/include”. if we are using select() and it is\n                                             necessary to increase the number of file descriptors, then\n                                             this also can be assigned here: –with-cc-opt=”-d\n                                             fd_setsize=2048”.\n–with-ld-opt=options                         additional parameters passed to the linker. with the use of\n                                             the system library pcre in - freebsd, it is necessary to\n                                             indicate –with-ld-opt=”-l /usr/local/lib”.\n–with-cpu-opt=cpu                            为特定的 cpu 编译，有效的值包括：pentium, pentiumpro, pentium3, pentium4,\n                                             athlon, opteron, amd64, sparc32, sparc64, ppc64\n–without-pcre                                禁止 pcre 库的使用。同时也会禁止 http rewrite 模块。在 “location”\n                                             配置指令中的正则表达式也需要 pcre 。\n–with-pcre=dir                               指定 pcre 库的源代码的路径。\n–with-pcre-opt=options                       set additional options for pcre building.\n–with-md5=dir                                set path to md5 library sources.\n–with-md5-opt=options                        set additional options for md5 building.\n–with-md5-asm                                use md5 assembler sources.\n–with-sha1=dir                               set path to sha1 library sources.\n–with-sha1-opt=options                       set additional options for sha1 building.\n–with-sha1-asm                               use sha1 assembler sources.\n–with-zlib=dir                               set path to zlib library sources.\n–with-zlib-opt=options                       set additional options for zlib building.\n–with-zlib-asm=cpu                           use zlib assembler sources optimized for specified cpu,\n                                             valid values are: pentium, pentiumpro\n–with-openssl=dir                            set path to openssl library sources\n–with-openssl-opt=options                    set additional options for openssl building\n–with-debug                                  启用调试日志\n–add-module=path                             add in a third-party module found in directory path\n\n\n# 配置\n\n在centos 默认配置文件在 /usr/local/nginx-1.5.1/conf/nginx.conf 我们要在这里配置一些文件。nginx.conf是主配置文件，由若干个部分组成，每个大括号{}表示一个部分。每一行指令都由分号结束;，标志着一行的结束。\n\n\n# 常用正则\n\n正则   说明              正则      说明\n.    匹配除换行符以外的任意字符   $       匹配字符串的结束\n?    重复0次或1次         {n}     重复n次\n+    重复1次或更多次        {n,}    重复n次或更多次\n*    重复0次或更多次        [c]     匹配单个字符c\n\\d   匹配数字            [a-z]   匹配a-z小写字母的任意一个\n^    匹配字符串的开始        -       -\n\n\n# 全局变量\n\n变量                 说明                             变量                  说明\n$args              这个变量等于请求行中的参数，同$query_string   $remote_port        客户端的端口。\n$content_length    请求头中的content-length字段。         $remote_user        已经经过auth basic module验证的用户名。\n$content_type      请求头中的content-type字段。           $request_filename   当前请求的文件路径，由root或alias指令与uri请求生成。\n$document_root     当前请求在root指令中指定的值。              $scheme             http方法（如http，https）。\n$host              请求主机头字段，否则为服务器名称。              $server_protocol    请求使用的协议，通常是http/1.0或http/1.1。\n$http_user_agent   客户端agent信息                     $server_addr        服务器地址，在完成一次系统调用后可以确定这个值。\n$http_cookie       客户端cookie信息                    $server_name        服务器名称。\n$limit_rate        这个变量可以限制连接速率。                  $server_port        请求到达服务器的端口号。\n$request_method    客户端请求的动作，通常为get或post。          $request_uri        包含请求参数的原始uri，不包含主机名，如：/foo/bar.php?arg=baz。\n$remote_addr       客户端的ip地址。                      $uri                不带请求参数的当前uri，$uri不包含主机名，如/foo/bar.html。\n$document_uri      与$uri相同。                       -                   -\n\n例如请求：http://localhost:3000/test1/test2/test.php\n\n$host：localhost $server_port：3000 $request_uri：/test1/test2/test.php $document_uri：/test1/test2/test.php $document_root：/var/www/html $request_filename：/var/www/html/test1/test2/test.php\n\n\n# 符号参考\n\n符号    说明    符号    说明    符号   说明\nk,k   千字节   m,m   兆字节   ms   毫秒\ns     秒     m     分钟    h    小时\nd     日     w     周     m    一个月, 30天\n\n例如，”8k”，”1m” 代表字节数计量。 例如，”1h 30m”，”1y 6m”。代表 “1小时 30分”，”1年零6个月”。\n\n\n# 配置文件\n\nnginx 的配置系统由一个主配置文件和其他一些辅助的配置文件构成。这些配置文件均是纯文本文件，全部位于 nginx 安装目录下的 conf 目录下。\n\n指令由 nginx 的各个模块提供，不同的模块会提供不同的指令来实现配置。 指令除了 key-value 的形式，还有作用域指令。\n\nnginx.conf 中的配置信息，根据其逻辑上的意义，对它们进行了分类，也就是分成了多个作用域，或者称之为配置指令上下文。不同的作用域含有一个或者多个配置项。\n\n下面的这些上下文指令是用的比较多：\n\ndirective        description                                                      contains directive\nmain             nginx 在运行时与具体业务功能（比如 http 服务或者 email                             user, worker_processes, error_log, events, http, mail\n                 服务代理）无关的一些参数，比如工作进程数，运行的身份等。\nhttp             与提供 http 服务相关的一些配置参数。例如：是否使用 keepalive 啊，是否使用 gzip 进行压缩等。        server\nserver           http 服务上支持若干虚拟主机。每个虚拟主机一个对应的 server                              listen, server_name, access_log, location, protocol, proxy,\n                 配置项，配置项里面包含该虚拟主机相关的配置。在提供 mail 服务的代理时，也可以建立若干 server. 每个         smtp_auth, xclient\n                 server 通过监听的地址来区分。\nlocation         http 服务中，某些特定的 url 对应的一系列配置项。                                    index, root\nmail             实现 email 相关的 smtp/imap/pop3                                      server, http, imap_capabilities\n                 代理时，共享的一些配置项（因为可能实现多个代理，工作在多个监听地址上）。\ninclude          以便增强配置文件的可读性，使得部分配置文件可以重新使用。                                     -\nvalid_referers   用来校验http请求头referer是否有效。                                          -\ntry_files        用在server部分，不过最常见的还是用在location部分，它会按照给定的参数顺序进行尝试，第一个被匹配到的将会被使用。   -\nif               当在location块中使用if指令，在某些情况下它并不按照预期运行，一般来说避免使用if指令。                 -\n\n例如我们再 nginx.conf 里面引用两个配置 vhost/example.com.conf 和 vhost/gitlab.com.conf 它们都被放在一个我自己新建的目录 vhost 下面。nginx.conf 配置如下：\n\n复制worker_processes  1;\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n\n    #log_format  main  \'$remote_addr - $remote_user [$time_local] "$request" \'\n    #                  \'$status $body_bytes_sent "$http_referer" \'\n    #                  \'"$http_user_agent" "$http_x_forwarded_for"\';\n\n    #access_log  logs/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    #keepalive_timeout  0;\n    keepalive_timeout  65;\n\n    #gzip  on;\n    server {\n        listen       80;\n        server_name  localhost;\n        location / {\n            root   html;\n            index  index.html index.htm;\n        }\n        error_page   500 502 503 504  /50x.html;\n        location = /50x.html {\n            root   html;\n        }\n    }\n    include  vhost/example.com.conf;\n    include  vhost/gitlab.com.conf;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n简单的配置: example.com.conf\n\n复制server {\n    #侦听的80端口\n    listen       80;\n    server_name  baidu.com app.baidu.com; # 这里指定域名\n    index        index.html index.htm;    # 这里指定默认入口页面\n    root /home/www/app.baidu.com;         # 这里指定目录\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 内置预定义变量\n\nnginx提供了许多预定义的变量，也可以通过使用set来设置变量。你可以在if中使用预定义变量，也可以将它们传递给代理服务器。以下是一些常见的预定义变量，更多详见\n\n变量名称              值\n$args_name        在请求中的name参数\n$args             所有请求参数\n$query_string     $args的别名\n$content_length   请求头content-length的值\n$content_type     请求头content-type的值\n$host             如果当前有host，则为请求头host的值；如果没有这个头，那么该值等于匹配该请求的server_name的值\n$remote_addr      客户端的ip地址\n$request          完整的请求，从客户端收到，包括http请求方法、uri、http协议、头、请求体\n$request_uri      完整请求的uri，从客户端来的请求，包括参数\n$scheme           当前请求的协议\n$uri              当前请求的标准化uri\n\n\n# 反向代理\n\n反向代理是一个web服务器，它接受客户端的连接请求，然后将请求转发给上游服务器，并将从服务器得到的结果返回给连接的客户端。下面简单的反向代理的例子：\n\n复制server {  \n  listen       80;                                                        \n  server_name  localhost;                                              \n  client_max_body_size 1024m;  # 允许客户端请求的最大单文件字节数\n\n  location / {\n    proxy_pass                         http://localhost:8080;\n    proxy_set_header host              $host:$server_port;\n    proxy_set_header x-forwarded-for   $remote_addr; # http的请求端真实的ip\n    proxy_set_header x-forwarded-proto $scheme;      # 为了正确地识别实际用户发出的协议是 http 还是 https\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n复杂的配置: gitlab.com.conf。\n\n复制server {\n    #侦听的80端口\n    listen       80;\n    server_name  git.example.cn;\n    location / {\n        proxy_pass   http://localhost:3000;\n        #以下是一些反向代理的配置可删除\n        proxy_redirect             off;\n        #后端的web服务器可以通过x-forwarded-for获取用户真实ip\n        proxy_set_header           host $host;\n        client_max_body_size       10m; #允许客户端请求的最大单文件字节数\n        client_body_buffer_size    128k; #缓冲区代理缓冲用户端请求的最大字节数\n        proxy_connect_timeout      300; #nginx跟后端服务器连接超时时间(代理连接超时)\n        proxy_send_timeout         300; #后端服务器数据回传时间(代理发送超时)\n        proxy_read_timeout         300; #连接成功后，后端服务器响应时间(代理接收超时)\n        proxy_buffer_size          4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小\n        proxy_buffers              4 32k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置\n        proxy_busy_buffers_size    64k; #高负荷下缓冲大小（proxy_buffers*2）\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n代理到上游服务器的配置中，最重要的是proxy_pass指令。以下是代理模块中的一些常用指令：\n\n指令                       说明\nproxy_connect_timeout    nginx从接受请求至连接到上游服务器的最长等待时间\nproxy_send_timeout       后端服务器数据回传时间(代理发送超时)\nproxy_read_timeout       连接成功后，后端服务器响应时间(代理接收超时)\nproxy_cookie_domain      替代从上游服务器来的set-cookie头的domain属性\nproxy_cookie_path        替代从上游服务器来的set-cookie头的path属性\nproxy_buffer_size        设置代理服务器（nginx）保存用户头信息的缓冲区大小\nproxy_buffers            proxy_buffers缓冲区，网页平均在多少k以下\nproxy_set_header         重写发送到上游服务器头的内容，也可以通过将某个头部的值设置为空字符串，而不发送某个头部的方法实现\nproxy_ignore_headers     这个指令禁止处理来自代理服务器的应答。\nproxy_intercept_errors   使nginx阻止http应答代码为400或者更高的应答。\n\n\n# 负载均衡\n\nupstream指令启用一个新的配置区段，在该区段定义一组上游服务器。这些服务器可能被设置不同的权重，也可能出于对服务器进行维护，标记为down。\n\n复制upstream gitlab {\n    ip_hash;\n    # upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。\n    server 192.168.122.11:8081 ;\n    server 127.0.0.1:82 weight=3;\n    server 127.0.0.1:83 weight=3 down;\n    server 127.0.0.1:84 weight=3; max_fails=3  fail_timeout=20s;\n    server 127.0.0.1:85 weight=4;;\n    keepalive 32;\n}\nserver {\n    #侦听的80端口\n    listen       80;\n    server_name  git.example.cn;\n    location / {\n        proxy_pass   http://gitlab;    #在这里设置一个代理，和upstream的名字一样\n        #以下是一些反向代理的配置可删除\n        proxy_redirect             off;\n        #后端的web服务器可以通过x-forwarded-for获取用户真实ip\n        proxy_set_header           host $host;\n        proxy_set_header           x-real-ip $remote_addr;\n        proxy_set_header           x-forwarded-for $proxy_add_x_forwarded_for;\n        client_max_body_size       10m;  #允许客户端请求的最大单文件字节数\n        client_body_buffer_size    128k; #缓冲区代理缓冲用户端请求的最大字节数\n        proxy_connect_timeout      300;  #nginx跟后端服务器连接超时时间(代理连接超时)\n        proxy_send_timeout         300;  #后端服务器数据回传时间(代理发送超时)\n        proxy_read_timeout         300;  #连接成功后，后端服务器响应时间(代理接收超时)\n        proxy_buffer_size          4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小\n        proxy_buffers              4 32k;# 缓冲区，网页平均在32k以下的话，这样设置\n        proxy_busy_buffers_size    64k; #高负荷下缓冲大小（proxy_buffers*2）\n        proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。\n\n负载均衡：\n\nupstream模块能够使用3种负载均衡算法：轮询、ip哈希、最少连接数。\n\n轮询： 默认情况下使用轮询算法，不需要配置指令来激活它，它是基于在队列中谁是下一个的原理确保访问均匀地分布到每个上游服务器； ip哈希： 通过ip_hash指令来激活，nginx通过ipv4地址的前3个字节或者整个ipv6地址作为哈希键来实现，同一个ip地址总是能被映射到同一个上游服务器； 最少连接数： 通过least_conn指令来激活，该算法通过选择一个活跃数最少的上游服务器进行连接。如果上游服务器处理能力不同，可以通过给server配置weight权重来说明，该算法将考虑到不同服务器的加权最少连接数。\n\n# rr\n\n简单配置 ，这里我配置了2台服务器，当然实际上是一台，只是端口不一样而已，而8081的服务器是不存在的，也就是说访问不到，但是我们访问 http://localhost 的时候，也不会有问题，会默认跳转到http://localhost:8080具体是因为nginx会自动判断服务器的状态，如果服务器处于不能访问（服务器挂了），就不会跳转到这台服务器，所以也避免了一台服务器挂了影响使用的情况，由于nginx默认是rr策略，所以我们不需要其他更多的设置\n\n复制upstream test {\n    server localhost:8080;\n    server localhost:8081;\n}\nserver {\n    listen       81;\n    server_name  localhost;\n    client_max_body_size 1024m;\n \n    location / {\n        proxy_pass http://test;\n        proxy_set_header host $host:$server_port;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n负载均衡的核心代码为\n\n复制upstream test {\n    server localhost:8080;\n    server localhost:8081;\n}\n\n\n1\n2\n3\n4\n\n\n# 权重\n\n指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 例如\n\n复制upstream test {\n    server localhost:8080 weight=9;\n    server localhost:8081 weight=1;\n}\n\n\n1\n2\n3\n4\n\n\n那么10次一般只会有1次会访问到8081，而有9次会访问到8080\n\n# ip_hash\n\n上面的2种方式都有一个问题，那就是下一个请求来的时候请求可能分发到另外一个服务器，当我们的程序不是无状态的时候（采用了session保存数据），这时候就有一个很大的很问题了，比如把登录信息保存到了session中，那么跳转到另外一台服务器的时候就需要重新登录了，所以很多时候我们需要一个客户只访问一个服务器，那么就需要用iphash了，iphash的每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。\n\n复制upstream test {\n    ip_hash;\n    server localhost:8080;\n    server localhost:8081;\n}\n\n\n1\n2\n3\n4\n5\n\n\n# fair\n\n这是个第三方模块，按后端服务器的响应时间来分配请求，响应时间短的优先分配。\n\n复制upstream backend {\n    fair;\n    server localhost:8080;\n    server localhost:8081;\n}\n\n\n1\n2\n3\n4\n5\n\n\n# url_hash\n\n这是个第三方模块，按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法\n\n复制upstream backend {\n    hash $request_uri;\n    hash_method crc32;\n    server localhost:8080;\n    server localhost:8081;\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n以上5种负载均衡各自适用不同情况下使用，所以可以根据实际情况选择使用哪种策略模式，不过fair和url_hash需要安装第三方模块才能使用\n\nserver指令可选参数：\n\n 1. weight：设置一个服务器的访问权重，数值越高，收到的请求也越多；\n 2. fail_timeout：在这个指定的时间内服务器必须提供响应，如果在这个时间内没有收到响应，那么服务器将会被标记为down状态；\n 3. max_fails：设置在fail_timeout时间之内尝试对一个服务器连接的最大次数，如果超过这个次数，那么服务器将会被标记为down;\n 4. down：标记一个服务器不再接受任何请求；\n 5. backup：一旦其他服务器宕机，那么有该标记的机器将会接收请求。\n\nkeepalive指令：\n\nnginx服务器将会为每一个worker进行保持同上游服务器的连接。\n\n\n# 屏蔽ip\n\n在nginx的配置文件nginx.conf中加入如下配置，可以放到http, server, location, limit_except语句块，需要注意相对路径，本例当中nginx.conf，blocksip.conf在同一个目录中。\n\n复制include blockip.conf;\n\n\n1\n\n\n在blockip.conf里面输入内容，如：\n\n复制deny 165.91.122.67;\n\ndeny ip;   # 屏蔽单个ip访问\nallow ip;  # 允许单个ip访问\ndeny all;  # 屏蔽所有ip访问\nallow all; # 允许所有ip访问\ndeny 123.0.0.0/8   # 屏蔽整个段即从123.0.0.1到123.255.255.254访问的命令\ndeny 124.45.0.0/16 # 屏蔽ip段即从123.45.0.1到123.45.255.254访问的命令\ndeny 123.45.6.0/24 # 屏蔽ip段即从123.45.6.1到123.45.6.254访问的命令\n\n# 如果你想实现这样的应用，除了几个ip外，其他全部拒绝\nallow 1.1.1.1; \nallow 1.1.1.2;\ndeny all;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 第三方模块安装方法\n\n复制./configure --prefix=/你的安装目录  --add-module=/第三方模块目录\n\n\n1\n\n\n\n# 重定向\n\n * permanent 永久性重定向。请求日志中的状态码为301\n * redirect 临时重定向。请求日志中的状态码为302\n\n\n# 重定向整个网站\n\n复制server {\n    server_name old-site.com\n    return 301 $scheme://new-site.com$request_uri;\n}\n\n\n1\n2\n3\n4\n\n\n\n# 重定向单页\n\n复制server {\n    location = /oldpage.html {\n        return 301 http://example.org/newpage.html;\n    }\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# 重定向整个子路径\n\n复制location /old-site {\n    rewrite ^/old-site/(.*) http://example.org/new-site/$1 permanent;\n}\n\n\n1\n2\n3\n\n\n\n# 性能\n\n\n# 内容缓存\n\n允许浏览器基本上永久地缓存静态内容。 nginx将为您设置expires和cache-control头信息。\n\n复制location /static {\n    root /data;\n    expires max;\n}\n\n\n1\n2\n3\n4\n\n\n如果要求浏览器永远不会缓存响应（例如用于跟踪请求），请使用-1。\n\n复制location = /empty.gif {\n    empty_gif;\n    expires -1;\n}\n\n\n1\n2\n3\n4\n\n\n\n# gzip压缩\n\n复制gzip  on;\ngzip_buffers 16 8k;\ngzip_comp_level 6;\ngzip_http_version 1.1;\ngzip_min_length 256;\ngzip_proxied any;\ngzip_vary on;\ngzip_types\n    text/xml application/xml application/atom+xml application/rss+xml application/xhtml+xml image/svg+xml\n    text/javascript application/javascript application/x-javascript\n    text/x-json application/json application/x-web-app-manifest+json\n    text/css text/plain text/x-component\n    font/opentype application/x-font-ttf application/vnd.ms-fontobject\n    image/x-icon;\ngzip_disable  "msie6";\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 打开文件缓存\n\n复制open_file_cache max=1000 inactive=20s;\nopen_file_cache_valid 30s;\nopen_file_cache_min_uses 2;\nopen_file_cache_errors on;\n\n\n1\n2\n3\n4\n\n\n\n# ssl缓存\n\n复制ssl_session_cache shared:ssl:10m;\nssl_session_timeout 10m;\n\n\n1\n2\n\n\n\n# 上游keepalive\n\n复制upstream backend {\n    server 127.0.0.1:8080;\n    keepalive 32;\n}\nserver {\n    ...\n    location /api/ {\n        proxy_pass http://backend;\n        proxy_http_version 1.1;\n        proxy_set_header connection "";\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 监控\n\n使用ngxtop实时解析nginx访问日志，并且将处理结果输出到终端，功能类似于系统命令top。所有示例都读取nginx配置文件的访问日志位置和格式。如果要指定访问日志文件和/或日志格式，请使用-f和-a选项。\n\n注意：在nginx配置中/usr/local/nginx/conf/nginx.conf日志文件必须是绝对路径。\n\n复制# 安装 ngxtop\npip install ngxtop\n\n# 实时状态\nngxtop\n# 状态为404的前10个请求的路径：\nngxtop top request_path --filter \'status == 404\'\n\n# 发送总字节数最多的前10个请求\nngxtop --order-by \'avg(bytes_sent) * count\'\n\n# 排名前十位的ip，例如，谁攻击你最多\nngxtop --group-by remote_addr\n\n# 打印具有4xx或5xx状态的请求，以及status和http referer\nngxtop -i \'status >= 400\' print request status http_referer\n\n# 由200个请求路径响应发送的平均正文字节以\'foo\'开始：\nngxtop avg bytes_sent --filter \'status == 200 and request_path.startswith("foo")\'\n\n# 使用“common”日志格式从远程机器分析apache访问日志\nssh remote tail -f /var/log/apache2/access.log | ngxtop -f common\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 常见使用场景\n\n\n# 跨域问题\n\n在工作中，有时候会遇到一些接口不支持跨域，这时候可以简单的添加add_headers来支持cors跨域。配置如下：\n\n复制server {\n  listen 80;\n  server_name api.xxx.com;\n    \n  add_header \'access-control-allow-origin\' \'*\';\n  add_header \'access-control-allow-credentials\' \'true\';\n  add_header \'access-control-allow-methods\' \'get,post,head\';\n\n  location / {\n    proxy_pass http://127.0.0.1:3000;\n    proxy_set_header x-real-ip $remote_addr;\n    proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n    proxy_set_header host  $http_host;    \n  } \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n上面更改头信息，还有一种，使用 rewrite 指令重定向uri来解决跨域问题。\n\n复制upstream test {\n  server 127.0.0.1:8080;\n  server localhost:8081;\n}\nserver {\n  listen 80;\n  server_name api.xxx.com;\n  location / { \n    root  html;                   #去请求../html文件夹里的文件\n    index  index.html index.htm;  #首页响应地址\n  }\n  # 用于拦截请求，匹配任何以 /api/开头的地址，\n  # 匹配符合以后，停止往下搜索正则。\n  location ^~/api/{ \n    # 代表重写拦截进来的请求，并且只能对域名后边的除去传递的参数外的字符串起作用，\n    # 例如www.a.com/proxy/api/msg?meth=1&par=2重写，只对/proxy/api/msg重写。\n    # rewrite后面的参数是一个简单的正则 ^/api/(.*)$，\n    # $1代表正则中的第一个()，$2代表第二个()的值，以此类推。\n    rewrite ^/api/(.*)$ /$1 break;\n    \n    # 把请求代理到其他主机 \n    # 其中 http://www.b.com/ 写法和 http://www.b.com写法的区别如下\n    # 如果你的请求地址是他 http://server/html/test.jsp\n    # 配置一： http://www.b.com/ 后面有“/” \n    #         将反向代理成 http://www.b.com/html/test.jsp 访问\n    # 配置一： http://www.b.com 后面没有有“/” \n    #         将反向代理成 http://www.b.com/test.jsp 访问\n    proxy_pass http://test;\n\n    # 如果 proxy_pass  url 是 http://a.xx.com/platform/ 这种情况\n    # proxy_cookie_path应该设置成 /platform/ / (注意两个斜杠之间有空格)。\n    proxy_cookie_path /platfrom/ /;\n\n    # http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass_header\n    # 设置 cookie 头通过\n    proxy_pass_header set-cookie;\n  } \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n\n# 跳转到带www的域上面\n\n复制server {\n    listen 80;\n    # 配置正常的带www的域名\n    server_name www.wangchujiang.com;\n    root /home/www/wabg/download;\n    location / {\n        try_files $uri $uri/ /index.html =404;\n    }\n}\nserver {\n    # 这个要放到下面，\n    # 将不带www的 wangchujiang.com 永久性重定向到  https://www.wangchujiang.com\n    server_name wangchujiang.com;\n    rewrite ^(.*) https://www.wangchujiang.com$1 permanent;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 代理转发\n\n复制upstream server-api{\n    # api 代理服务地址\n    server 127.0.0.1:3110;    \n}\nupstream server-resource{\n    # 静态资源 代理服务地址\n    server 127.0.0.1:3120;\n}\nserver {\n    listen       3111;\n    server_name  localhost;      # 这里指定域名\n    root /home/www/server-statics;\n    # 匹配 api 路由的反向代理到api服务\n    location ^~/api/ {\n        rewrite ^/(.*)$ /$1 break;\n        proxy_pass http://server-api;\n    }\n    # 假设这里验证码也在api服务中\n    location ^~/captcha {\n        rewrite ^/(.*)$ /$1 break;\n        proxy_pass http://server-api;\n    }\n    # 假设你的图片资源全部在另外一个服务上面\n    location ^~/img/ {\n        rewrite ^/(.*)$ /$1 break;\n        proxy_pass http://server-resource;\n    }\n    # 路由在前端，后端没有真实路由，在路由不存在的 404状态的页面返回 /index.html\n    # 这个方式使用场景，你在写react或者vue项目的时候，没有真实路由\n    location / {\n        try_files $uri $uri/ /index.html =404;\n        #                               ^ 空格很重要\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# 监控状态信息\n\n通过 nginx -v 来查看是否有 with-http_stub_status_module 该模块。\n\n> nginx -v 这里 v 是大写的，如果是小写的 v 即 nginx -v，则不会出现有哪些模块，只会出现 nginx 的版本\n\n复制location /nginx_status {\n    stub_status on;\n    access_log off;\n}\n\n\n1\n2\n3\n4\n\n\n通过 http://127.0.0.1/nginx_status 访问出现下面结果。\n\n复制active connections: 3\nserver accepts handled requests\n 7 7 5 \nreading: 0 writing: 1 waiting: 2\n\n\n1\n2\n3\n4\n\n 1. 主动连接(第 1 行)\n\n当前与http建立的连接数，包括等待的客户端连接：3\n\n 1. 服务器接受处理的请求(第 2~3 行)\n\n接受的客户端连接总数目：7 处理的客户端连接总数目：7 客户端总的请求数目：5\n\n 1. 读取其它信(第 4 行)\n\n当前，nginx读请求连接 当前，nginx写响应返回给客户端 目前有多少空闲客户端请求连接\n\n\n# 代理转发连接替换\n\n复制location ^~/api/upload {\n    rewrite ^/(.*)$ /wfs/v1/upload break;\n    proxy_pass http://wfs-api;\n}\n\n\n1\n2\n3\n4\n\n\n\n# ssl配置\n\n超文本传输安全协议（缩写：https，英语：hypertext transfer protocol secure）是超文本传输协议和ssl/tls的组合，用以提供加密通讯及对网络服务器身份的鉴定。https连接经常被用于万维网上的交易支付和企业信息系统中敏感信息的传输。https不应与在rfc 2660中定义的安全超文本传输协议（s-http）相混。https 目前已经是所有注重隐私和安全的网站的首选，随着技术的不断发展，https 网站已不再是大型网站的专利，所有普通的个人站长和博客均可以自己动手搭建一个安全的加密的网站。\n\n创建ssl证书，如果你购买的证书，就可以直接下载\n\n复制sudo mkdir /etc/nginx/ssl\n# 创建了有效期100年，加密强度为rsa2048的ssl密钥key和x509证书文件。\nsudo openssl req -x509 -nodes -days 36500 -newkey rsa:2048 -keyout /etc/nginx/ssl/nginx.key -out /etc/nginx/ssl/nginx.crt\n# 上面命令，会有下面需要填写内容\ncountry name (2 letter code) [au]:us\nstate or province name (full name) [some-state]:new york\nlocality name (eg, city) []:new york city\norganization name (eg, company) [internet widgits pty ltd]:bouncy castles, inc.\norganizational unit name (eg, section) []:ministry of water slides\ncommon name (e.g. server fqdn or your name) []:your_domain.com\nemail address []:admin@your_domain.com\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n创建自签证书\n\n复制首先，创建证书和私钥的目录\n# mkdir -p /etc/nginx/cert\n# cd /etc/nginx/cert\n创建服务器私钥，命令会让你输入一个口令：\n# openssl genrsa -des3 -out nginx.key 2048\n创建签名请求的证书（csr）：\n# openssl req -new -key nginx.key -out nginx.csr\n在加载ssl支持的nginx并使用上述私钥时除去必须的口令：\n# cp nginx.key nginx.key.org\n# openssl rsa -in nginx.key.org -out nginx.key\n最后标记证书使用上述私钥和csr：\n# openssl x509 -req -days 365 -in nginx.csr -signkey nginx.key -out nginx.crt\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查看目前nginx编译选项\n\n复制sbin/nginx -v\n\n\n1\n\n\n输出下面内容\n\n复制nginx version: nginx/1.7.8\nbuilt by gcc 4.4.7 20120313 (red hat 4.4.7-4) (gcc)\ntls sni support enabled\nconfigure arguments: --prefix=/usr/local/nginx-1.7.8 --with-http_ssl_module --with-http_spdy_module --with-http_stub_status_module --with-pcre\n\n\n1\n2\n3\n4\n\n\n如果依赖的模块不存在，可以进入安装目录，输入下面命令重新编译安装。\n\n复制./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module\n\n\n1\n\n\n运行完成之后还需要make (不用make install)\n\n复制# 备份nginx的二进制文件\ncp -rf /usr/local/nginx/sbin/nginx　 /usr/local/nginx/sbin/nginx.bak\n# 覆盖nginx的二进制文件\ncp -rf objs/nginx   /usr/local/nginx/sbin/\n\n\n1\n2\n3\n4\n\n\nhttps server\n\n复制server {\n    listen       443 ssl;\n    server_name  localhost;\n\n    ssl_certificate /etc/nginx/ssl/nginx.crt;\n    ssl_certificate_key /etc/nginx/ssl/nginx.key;\n    # 禁止在header中出现服务器版本，防止黑客利用版本漏洞攻击\n    server_tokens off;\n    # 设置ssl/tls会话缓存的类型和大小。如果设置了这个参数一般是shared，buildin可能会参数内存碎片，默认是none，和off差不多，停用缓存。如shared:ssl:10m表示我所有的nginx工作进程共享ssl会话缓存，官网介绍说1m可以存放约4000个sessions。 \n    ssl_session_cache    shared:ssl:1m; \n\n    # 客户端可以重用会话缓存中ssl参数的过期时间，内网系统默认5分钟太短了，可以设成30m即30分钟甚至4h。\n    ssl_session_timeout  5m; \n\n    # 选择加密套件，不同的浏览器所支持的套件（和顺序）可能会不同。\n    # 这里指定的是openssl库能够识别的写法，你可以通过 openssl -v cipher \'rc4:high:!anull:!md5\'（后面是你所指定的套件加密算法） 来看所支持算法。\n    ssl_ciphers  high:!anull:!md5;\n\n    # 设置协商加密算法时，优先使用我们服务端的加密套件，而不是客户端浏览器的加密套件。\n    ssl_prefer_server_ciphers  on;\n\n    location / {\n        root   html;\n        index  index.html index.htm;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# 强制将http重定向到https\n\n复制server {\n    listen       80;\n    server_name  example.com;\n    rewrite ^ https://$http_host$request_uri? permanent;    # 强制将http重定向到https\n    # 在错误页面和“服务器”响应头字段中启用或禁用发射nginx版本。 防止黑客利用版本漏洞攻击\n    server_tokens off;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 两个虚拟主机\n\n纯静态-html 支持\n\n复制http {\n    server {\n        listen          80;\n        server_name     www.domain1.com;\n        access_log      logs/domain1.access.log main;\n        location / {\n            index index.html;\n            root  /var/www/domain1.com/htdocs;\n        }\n    }\n    server {\n        listen          80;\n        server_name     www.domain2.com;\n        access_log      logs/domain2.access.log main;\n        location / {\n            index index.html;\n            root  /var/www/domain2.com/htdocs;\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 虚拟主机标准配置\n\n复制http {\n  server {\n    listen          80 default;\n    server_name     _ *;\n    access_log      logs/default.access.log main;\n    location / {\n       index index.html;\n       root  /var/www/default/htdocs;\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 爬虫过滤\n\n根据 user-agent 过滤请求，通过一个简单的正则表达式，就可以过滤不符合要求的爬虫请求(初级爬虫)。\n\n> ~* 表示不区分大小写的正则匹配\n\n复制location / {\n    if ($http_user_agent ~* "python|curl|java|wget|httpclient|okhttp") {\n        return 503;\n    }\n    # 正常处理\n    # ...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 防盗链\n\n复制location ~* \\.(gif|jpg|png|swf|flv)$ {\n   root html\n   valid_referers none blocked *.nginxcn.com;\n   if ($invalid_referer) {\n     rewrite ^/ www.nginx.cn\n     #return 404;\n   }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 虚拟目录配置\n\nalias指定的目录是准确的，root是指定目录的上级目录，并且该上级目录要含有location指定名称的同名目录。\n\n复制location /img/ {\n    alias /var/www/image/;\n}\n# 访问/img/目录里面的文件时，ningx会自动去/var/www/image/目录找文件\nlocation /img/ {\n    root /var/www/image;\n}\n# 访问/img/目录下的文件时，nginx会去/var/www/image/img/目录下找文件。]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 防盗图配置\n\n复制location ~ \\/public\\/(css|js|img)\\/.*\\.(js|css|gif|jpg|jpeg|png|bmp|swf) {\n    valid_referers none blocked *.jslite.io;\n    if ($invalid_referer) {\n        rewrite ^/  http://wangchujiang.com/piratesp.png;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 屏蔽.git等文件\n\n复制location ~ (.git|.gitattributes|.gitignore|.svn) {\n    deny all;\n}\n\n\n1\n2\n3\n\n\n\n# 域名路径加不加需要都能正常访问\n\n复制http://wangchujiang.com/api/index.php?a=1&name=wcj\n                                  ^ 有后缀\n\nhttp://wangchujiang.com/api/index?a=1&name=wcj\n                                 ^ 没有后缀\n\n\n1\n2\n3\n4\n5\n\n\nnginx rewrite规则如下：\n\n复制rewrite ^/(.*)/$ /index.php?/$1 permanent;\nif (!-d $request_filename){\n        set $rule_1 1$rule_1;\n}\nif (!-f $request_filename){\n        set $rule_1 2$rule_1;\n}\nif ($rule_1 = "21"){\n        rewrite ^/ /index.php last;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 错误问题\n\n复制the plain http request was sent to https port\n\n\n1\n\n\n解决办法，fastcgi_param https $https if_not_empty 添加这条规则，\n\n复制server {\n    listen 443 ssl; # 注意这条规则\n    server_name  my.domain.com;\n    \n    fastcgi_param https $https if_not_empty;\n    fastcgi_param https on;\n\n    ssl_certificate /etc/ssl/certs/your.pem;\n    ssl_certificate_key /etc/ssl/private/your.key;\n\n    location / {\n        # your config here...\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# nginx 模块\n\n * nginx office hours 一个 nginx 模块，允许您仅在办公时间内提供访问访问网站。\n\n\n# 精品文章参考\n\n * 负载均衡原理的解析\n\n * nginx泛域名解析，实现多个二级域名\n\n * 深入 nginx: 我们如何设计性能和扩展\n\n * inside nginx: how we designed for performance & scale\n\n * nginx开发从入门到精通\n\n * nginx的优化与防盗链\n\n * 实战开发一个nginx扩展 (nginx module)\n\n * nginx+keepalived(双机热备)搭建高可用负载均衡环境(ha)\n\n * nginx 平滑升级\n\n * nginx最新模块—ngx_http_mirror_module分析可以做版本发布前的预先验证，进行流量放大后的压测等等\n   \n   原文链接https://www.viayc.com/2019/03/13/nginx%e9%85%8d%e7%bd%ae%e6%95%99%e7%a8%8b/',charsets:{cjk:!0}},{title:"nginx常用配置",frontmatter:{title:"nginx常用配置",date:"2023-02-09T20:26:18.000Z",permalink:"/pages/df9ea8/",categories:["运维","中间件","nginx"],tags:[null],readingShow:"top",description:"虚拟主机的三种方式：基于端口、IP地址、域名",meta:[{name:"twitter:title",content:"nginx常用配置"},{name:"twitter:description",content:"虚拟主机的三种方式：基于端口、IP地址、域名"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.nginx/02.nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE.html"},{property:"og:type",content:"article"},{property:"og:title",content:"nginx常用配置"},{property:"og:description",content:"虚拟主机的三种方式：基于端口、IP地址、域名"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.nginx/02.nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-09T20:26:18.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"nginx常用配置"},{itemprop:"description",content:"虚拟主机的三种方式：基于端口、IP地址、域名"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.nginx/02.nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE.html",relativePath:"04.运维/03.中间件/01.nginx/02.nginx常用配置.md",key:"v-74be9f5b",path:"/pages/df9ea8/",headers:[{level:2,title:"虚拟主机",slug:"虚拟主机",normalizedTitle:"虚拟主机",charIndex:2},{level:2,title:"nginx负载均衡",slug:"nginx负载均衡",normalizedTitle:"nginx负载均衡",charIndex:295},{level:2,title:"反向代理",slug:"反向代理",normalizedTitle:"反向代理",charIndex:640},{level:2,title:"location规则",slug:"location规则",normalizedTitle:"location规则",charIndex:2128},{level:2,title:"证书",slug:"证书",normalizedTitle:"证书",charIndex:3734},{level:2,title:"日志",slug:"日志",normalizedTitle:"日志",charIndex:5144},{level:2,title:"php配置",slug:"php配置",normalizedTitle:"php配置",charIndex:5445},{level:2,title:"模块",slug:"模块",normalizedTitle:"模块",charIndex:5725},{level:2,title:"跳转、重定向",slug:"跳转、重定向",normalizedTitle:"跳转、重定向",charIndex:6004},{level:2,title:"虚拟目录",slug:"虚拟目录",normalizedTitle:"虚拟目录",charIndex:7841},{level:2,title:"nginx跨域",slug:"nginx跨域",normalizedTitle:"nginx跨域",charIndex:7906},{level:3,title:"防止域名恶意解析",slug:"防止域名恶意解析",normalizedTitle:"防止域名恶意解析",charIndex:14121},{level:3,title:"网站置灰",slug:"网站置灰",normalizedTitle:"网站置灰",charIndex:14473}],headersStr:"虚拟主机 nginx负载均衡 反向代理 location规则 证书 日志 php配置 模块 跳转、重定向 虚拟目录 nginx跨域 防止域名恶意解析 网站置灰",content:'# 虚拟主机\n\n虚拟主机的三种方式：基于端口、IP地址、域名\n\nserver {\n\nlisten 80;\n\nserver_name www.accp.com;\n\ncharset utf-8;\n\naccess_log logs/www.accp.com.access.log;\n\nlocation / {\n\nroot /var/www/html/accp;\n\nindex index.html index.htm;\n\n}\n\nerror_page 500 502 503 504 /50x.html;\n\nlocation = /50x.html {\n\nroot html;\n\n}\n\n}\n\n\n# nginx负载均衡\n\nupstream cluster {\n\nip_hash; #如果你的系统中没有使用第三方缓存管理工具 ,建议使用此方式\n\nserver 192.168.1.210:80 weight=5;\n\nserver 192.168.1.211:80 weight=3;\n\nserver 192.168.1.212:80 weight=1;\n\n}\n\nupstream dxTreasureIsland_http { server dal05iis39.sl.dx weight=1 max_fails=1 fail_timeout=45s; server iis01.zl.dx:8093 weight=1 max_fails=1 fail_timeout=45s; }\n\n\n# 反向代理\n\n​ location / {\n​ proxy_pass http://localhost:8000/;\n​ proxy_redirect off;\n​ proxy_set_header Host $http_host;\n​ proxy_set_header X-Real-IP $remote_addr;\n​ proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n​ location / {\n​ proxy_pass http://172.18.184.130:10085;\n​ proxy_set_header Host $host;\n​ proxy_set_header X-Real-IP $remote_addr;\n​ proxy_redirect http:// $scheme://; #以上指令会将后端响应header location内容中的http://替换成用户端协议https://\n​ proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n​ }\n\nlocation / {\n\nproxy_pass http://baidunode;\n\nproxy_set_header X-Forwarded-Proto $scheme;\nproxy_set_header Host $host; #HTTP header 中的 Host 含义为所请求的目的主机名。当 nginx 作为反向代理使用，而后端真实 web 服务器设置有类似 防盗链功能\n\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #HTTP header 中的 X_Forward_For 表示该条 http 请求是由谁发起的\n\nproxy_set_header X-Real-IP $remote_addr;\n proxy_redirect off;\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n常用的代理\n\n    location / {\n        proxy_pass         http://localhost:8000/;\n        proxy_set_header   Host             $http_host;\n        proxy_set_header   X-Real-IP        $remote_addr;\n        proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nproxy_redirect的作用\n\nproxy_redirect 该指令用来修改被代理服务器返回的响应头中的Location头域和“refresh”头域。\n\n>  1. proxy_redirect 旧地址 新地址;\n> \n>  2. proxy_redirect default; #默认配置\n> \n>  3. proxy_redirect off; #关闭重定向\n\n\n# location规则\n\n配置location /wandou可以匹配/wandoudouduo请求，也可以匹配/wandou*/duoduo等等，只要以wandou开头的目录都可以匹配到。而location /wandou/必须精确匹配/wandou/这个目录的请求,不能匹配/wandouduoduo/或/wandou*/duoduo等请求。\n\n第一种：加"/" location /wddd/ { proxy_pass http://127.0.0.1:8080/; } 测试结果，请求被代理跳转到：http://127.0.0.1:8080/index.html 第二种: 不加"/" location /wddd/ {\nproxy_pass http://127.0.0.1:8080; } 测试结果，请求被代理跳转到：http://127.0.0.1:8080/wddd/index.html 第三种: 增加目录加"/" location /wddd/ {\nproxy_pass http://127.0.0.1:8080/sun/; } 测试结果，请求被代理跳转到：http://127.0.0.1:8080/sun/index.html 第四种：增加目录不加"/" location /wddd/ { proxy_pass http://127.0.0.1:8080/sun; } 测试结果，请求被代理跳转到：http://127.0.0.1:8080/sun/index.html 总结 location目录后加"/",只能匹配目录，不加“/”不仅可以匹配目录还对目录进行模糊匹配。而proxy_pass无论加不加“/”,代理跳转地址都直接拼接\n\n符号   说明\n~    正则匹配，区分大小写\n~*   正则匹配，不区分大小写\n^~   普通字符匹配，如果该选项匹配，则，只匹配改选项，不再向下匹配其他选项\n=    普通字符匹配，精确匹配\n@    定义一个命名的 location，用于内部定向，例如 error_page，try_files\n\n符号@使用例子\n\nserver {\n    listen 80;\n    client_max_body_size 100M;\n    keepalive_timeout 5;\n    root /app/static;\n    location /backend {\n    rewrite ^/backend/(.*) /$1 break;\n    try_files $uri @proxy_to_app;\n    }\n    location / {\n    try_files $uri @proxy_to_app;\n    }\n    location @proxy_to_spp {\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $http_host;\n        proxy_set_header   X-Real-IP        $remote_addr;\n        proxy_redirect off;\n        proxy_pass http://app_server;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nnginx禁止、允许访问某些后缀的文件 location ~* .(ini|cfg|dwt|lbi)$ {\ndeny all;\n}\n\n\n# 证书\n\nssl_certificate /etc/ssl/private/.crt; ssl_certificate_key /etc/ssl/private/.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!ADH:!aNULL:!eNULL:!MD5:!DSS:!DH:!RC4; ssl_prefer_server_ciphers on;\n\nserver {\n    listen 443;\n    listen 80;\n    server_name m.goldmanfutures.com;\n    ssl on;\n    ssl_certificate      /etc/nginx/cert/m.goldmanfutures.com/m.goldmanfutures.com_chain.crt;\n    ssl_certificate_key  /etc/nginx/cert/m.goldmanfutures.com/m.goldmanfutures.com_key.key;\n    proxy_ssl_session_reuse   off;\n    ssl_session_cache         shared:SSL:1m;\n    ssl_session_timeout       5m;\n    ssl_ciphers               ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;\n    ssl_protocols             TLSv1 TLSv1.1 TLSv1.2;\n    ssl_prefer_server_ciphers on;\n    client_max_body_size 1g;\n    client_body_buffer_size 1m;\n        if ($scheme = http) {\n                rewrite ^(.*)$ https://m.goldmanfutures.com$1 permanent;\n        }\n\n\n    index   index.html index.htm index.php;\n    location / {\n        proxy_pass   http://172.31.186.20:10584/;\n        proxy_set_header Host      $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 日志\n\n​ access_log /var/log/nginx/jenkins.dx.com.access.log;\n​ error_log /var/log/nginx/jenkins.dx.com.errors.log;\n​\n\nlog_format main \'$remote_addr - $remote_user [$time_local] "$request" \'\n \'$status $body_bytes_sent "$http_referer" \'\n \'"$http_user_agent" "$http_x_forwarded_for"\';\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# php配置\n\n​ location ~ \\.php$ {\n​ root /data/erp5/website/erp5.pdvee.com/public;\n​ index index.php;\n​ fastcgi_pass 192.168.14.20:9005;\n​ fastcgi_index index.php;\n​ fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n​ include fastcgi_params;\n​ }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 模块\n\nconcat模块：Nginx concat通过合并静态文件来减少http请求数来达到优化前端性能，可以在一定程度上能减少web服务器的压力。\n\nlocation / {\n concat on;\n concat_max_files 100;\n concat_unique off;\n concat_ignore_file_error on;\n if (!-e $request_filename) {\n rewrite ^(.*)$ /index.php?s=/$1 last;\n break;\n }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 跳转、重定向\n\n​ if ($host ~ ^www\\.dealextreme.com\\.com$) {\n​ rewrite ^(.*)$ $scheme://www.dx.com permanent;\n​ }\nif ($scheme = http) {\n            rewrite ^(.*)$ https://www.dx.com$1 permanent;\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\nserver { listen 80; server_name www.zzppjj.top zzppjj.top;\n\n    location ~* / {\n            rewrite ^(.*)$ https://www.zzppjj.top$1 permanent;\n    }\n\n\n1\n2\n3\n\n\n}\n\nserver {\n    listen 80;\n    server_name getpro-h5-business.goingf.hk getpro-php-business.goingf.hk;\n\n\n\n    location / {\n        rewrite ^ https://$http_host$request_uri? permanent;    # force redirect http to https\n        return 301 https://$http_host$request_uri;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n重定向到另外一台服务器示例：（查找目录下是否存在文件，如果不存在转到@new_uploads下，new_uploads对应代理到172.17.0.101上）\n\nlocation ^~ /uploads/ { root /data/weiwend/weiwang; try_files $uri @new_uploads; }\n\nlocation @new_uploads {\n    proxy_redirect off;\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_pass http://172.17.0.101:80;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * 将 /api/v1/game/data?id= 新网站路径地址映射到 http://gameid.escape.com/api/v1/new_game/ 老网站的 API 上面，给开发提供获取游戏用户信息的接口\n\nserver { listen 80; server_name gameid.escape.com;\n\naccess_log /var/log/nginx/gameid.nginx.access.log  netdata;\nerror_log  /var/log/nginx/gameid.nginx.error.log   error;\n\nlocation / {\n    return 404;\n}\n\nlocation /api/v1/ {\n    proxy_pass_header Server;\n    proxy_set_header Host $http_host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Scheme $scheme;\n    proxy_http_version 1.1;\n    if ($request_uri ~* "/api/v1/game/data\\?id=(.*)") {\n        set $id $1;\n        rewrite .* /api/v1/new_game/$id break;\n        proxy_pass http://gameid.escape.com;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n}\n\n\n# 虚拟目录\n\nlocation /media { alias /usr/share/nginx/html/media;\n}\n\n\n# nginx跨域\n\n     #是否允许请求带有验证信息\n     add_header Access-Control-Allow-Credentials true;\n     #允许跨域访问的域名,可以是一个域的列表，也可以是通配符*\n     add_header Access-Control-Allow-Origin  $allow_url;\n     #允许脚本访问的返回头\n     add_header Access-Control-Allow-Headers \'x-requested-with,content-type,Cache-Control,Pragma,Date,x-timestamp\';\n     #允许使用的请求方法，以逗号隔开\n     add_header Access-Control-Allow-Methods \'POST,GET,OPTIONS,PUT,DELETE\';\n     #允许自定义的头部，以逗号隔开,大小写不敏感\n     add_header Access-Control-Expose-Headers \'WWW-Authenticate,Server-Authorization\';\n     #P3P支持跨域cookie操作\n     add_header P3P \'policyref="/w3c/p3p.xml", CP="NOI DSP PSAa OUR BUS IND ONL UNI COM NAV INT LOC"\';\n     add_header test  1;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n需求为浏览器访问B的时候要直接显示A的主页，这样对A做一个代理，浏览器的URL还是B并不是重定向到A。\n\n在A的nginx中加入一下内容\n\nlocation /api/ {\n\nadd_header Access-Control-Allow-Origin *;\n\nadd_header Access-Control-Allow-Credentials \'true\';\n\nadd_header Access-Control-Allow-Methods \'GET, POST, OPTIONS\';\n\nadd_header Access-Control-Allow-Headers \'Origin, X-Requested-With, Content-Type, Accept\';\n\nproxy_pass http://gateway_proxy/;\n\n\n# php配置\n\nserver {\n        listen 80;\n        listen 8888;\n\n        server_name  api.erp.pdvee.com;\n\n        access_log /var/log/nginx/api.erp.pdvee.com.access.log site_log2;\n        error_log  /var/log/nginx/api.erp.pdvee.com.errors.log;\n\n        proxy_set_header Host $host;\n        proxy_set_header Real-IP $remote_addr;\n        proxy_set_header Region $geoip_country_code;\n        proxy_set_header X-Forwarded-For $remote_addr;\n\n        root /data/erp/website/erp.pdvee.com;\n        index index.php index.html index.htm;\n\n\n      keepalive_timeout 18000s;\n        client_body_timeout 18000s;\n        client_header_timeout 18000s;\n        proxy_connect_timeout 18000s;\n        proxy_send_timeout 18000s;\n        proxy_read_timeout 18000s;\n\n\n    fastcgi_connect_timeout 18000;\n    fastcgi_send_timeout 18000;\n    fastcgi_read_timeout 18000;\n    fastcgi_buffer_size 256k;\n    fastcgi_buffers 8 256k;\n    fastcgi_busy_buffers_size 256k;\n    fastcgi_temp_file_write_size 256k;\n    ##\n    client_max_body_size 2048m;        \n    client_body_buffer_size 256k;\n\n\n\n\n        location / {\n#        proxy_cache my_cache;\n#        proxy_cache_revalidate on;\n                concat on;\n                concat_max_files 100;\n                concat_unique off;\n#                concat_ignore_file_error on;\n            if (!-e $request_filename) {\n            rewrite ^(.*)$ /index.php?s=/$1 last;\n            break;\n           }\n\n        }\n\n        location ~ \\.php$ {\n            root /data/website/erp.pdvee.com;\n            index index.php;\n            fastcgi_pass   192.168.14.13:9001;\n            fastcgi_index  index.php;\n            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;\n            include        fastcgi_params;\n        }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n全局变量\n\n$bytes_sent             发送给客户端的字节数\n$connection             连接序列号\n$connection_requests    当前通过连接发出的请求数量\n$content_length         “Content-Length” 请求头字段\n$content_type           “Content-Type” 请求头字段\n$cookie_name            cookie名称\n$document_root          当前请求的文档根目录或别名\n$uri                    请求中的当前URI(不带请求参数，参数位于$args)，可以不同于浏览器传递的$request_uri的值，它可以通过内部重定向，或者使用index指令进行修改，$uri不包含主机名，如”/foo/bar.html”。\n$document_uri           同 $uri\n$host                   优先级如下：HTTP请求行的主机名>”HOST”请求头字段>符合请求的服务器名\n$hostname               主机名\n$http_name              匹配任意请求头字段； 变量名中的后半部分“name”可以替换成任意请求头字段，如在配置文件中需要获取http请求头：“Accept-Language”，那么将“－”替换为下划线，大写字母替换为小写，形如：$http_accept_language即可。\n$https                  如果开启了SSL安全模式，值为“on”，否则为空字符串。\n$is_args                如果请求中有参数，值为“?”，否则为空字符串。\n$limit_rate             用于设置响应的速度限制\n$nginx_version          nginx版本\n$pid                    工作进程的PID\n$pipe                   如果请求来自管道通信，值为“p”，否则为“.” (1.3.12, 1.2.7)\n$proxy_protocol_addr    获取代理访问服务器的客户端地址，如果是直接访问，该值为空字符串。\n$realpath_root          当前请求的文档根目录或别名的真实路径，会将所有符号连接转换为真实路径。\n$msec                   以秒为单位的时间，日志写入时的毫秒分辨率\n$request_length         请求长度（包括请求行，标题和请求主体）\n$request_method         HTTP请求方法，通常为“GET”或“POST”\n$request_time           处理客户端请求使用的时间;从读取客户端的第一个字节开始计时。\n$request_uri            这个变量等于包含一些客户端请求参数的原始URI，它无法修改，请查看$uri更改或重写URI，不包含主机名，例如：”/cnphp/test.php?arg=freemouse”。\n$request_time           以毫秒分辨率请求处理时间，以秒为单位; 从客户端读取第一个字节之间的时间并在最后一个字节发送到客户端后进行日志写入\n$status                 响应状态码\n$time_local             本地时间采用通用日志格式\n$arg_name               请求中的的参数名，即“?”后面的arg_name=arg_value形式的arg_name\n$args                   请求中的参数值\n$binary_remote_addr     客户端地址的二进制形式, 固定长度为4个字节\n$body_bytes_sent        传输给客户端的字节数，响应头不计算在内；这个变量和Apache的mod_log_config模块中的“%B”参数保持兼容\n$remote_addr            客户端地址\n$remote_port            客户端端口\n$remote_user            用于HTTP基础认证服务的用户名\n$request                客户端请求地址\n$request_body           客户端的请求主体，此变量可在location中使用，将请求主体通过proxy_pass, fastcgi_pass, uwsgi_pass, 和 scgi_pass传递给下一级的代理服务器。\n$request_body_file      将客户端请求主体保存在临时文件中。文件处理结束后，此文件需删除。如果需要之一开启此功能，需要设置client_body_in_file_only。如果将次文件传递给后端的代理服务器，需要禁用request body，即设置proxy_pass_request_body off，fastcgi_pass_request_body off, uwsgi_pass_request_body off, or scgi_pass_request_body off 。\n$request_completion     如果请求成功，值为”OK”，如果请求未完成或者请求不是一个范围请求的最后一部分，则为空。\n$request_filename       当前连接请求的文件路径，由root或alias指令与URI请求生成。\n$scheme                 请求使用的Web协议, “http” 或 “https”\n$sent_http_name         可以设置任意http响应头字段； 变量名中的后半部分“name”可以替换成任意响应头字段，如需要设置响应头Content-length，那么将“－”替换为下划线，大写字母替换为小写，形如：$sent_http_content_length 4096即可。\n$server_addr            服务器端地址，需要注意的是：为了避免访问linux系统内核，应将ip地址提前设置在配置文件中。\n$server_name            服务器名，域名\n$server_port            服务器端口\n$server_protocol        服务器的HTTP版本, 通常为 “HTTP/1.0” 或 “HTTP/1.1”\n$tcpinfo_rtt, $tcpinfo_rttvar, $tcpinfo_snd_cwnd, $tcpinfo_rcv_space 客户端TCP连接的具体信息\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n############\n\nnginx中$host、$http_host和$proxy_host区别\n$host    不显示端口       浏览器请求的ip，不显示端口\n$http_host 端口存在则显示        浏览器请求的ip和端口号\n$proxy_host    默认80端口不显示，其它显示    被代理服务的ip和端口号\n\n\n1\n2\n3\n4\n\n\n\n# 防止域名恶意解析\n\nserver {\n\nlisten 80 default_server;\n\nserver_name _;\n\nreturn 404;\n\n}\n\nserver { listen 443 default_server; server_name _; ssl on; return 444; }\n\n大概解释如下:\n\ndefault_server：默认域名配置，如果找不到，会自动匹配\n\nserver_name _: 无效域名匹配\n\nreturn 444: 非标准状态码，是Nginx服务器扩展的Http错误状态码，服务器不向客户端返回任何信息，并关闭连接, 断开客户端和服务器的连接，防止恶意软件攻击威胁\n\n三、配置上面到主配置文件后。\n\n重启动nginx ，就会为我们屏蔽恶意访问了。\n\n\n# 网站置灰\n\nsub_filter  \'</head>\'  \'<style type="text/css">html {-webkit-filter: grayscale(.95);}</style></head>\';\n    sub_filter_once on; \n\n\n1\n2\n',normalizedContent:'# 虚拟主机\n\n虚拟主机的三种方式：基于端口、ip地址、域名\n\nserver {\n\nlisten 80;\n\nserver_name www.accp.com;\n\ncharset utf-8;\n\naccess_log logs/www.accp.com.access.log;\n\nlocation / {\n\nroot /var/www/html/accp;\n\nindex index.html index.htm;\n\n}\n\nerror_page 500 502 503 504 /50x.html;\n\nlocation = /50x.html {\n\nroot html;\n\n}\n\n}\n\n\n# nginx负载均衡\n\nupstream cluster {\n\nip_hash; #如果你的系统中没有使用第三方缓存管理工具 ,建议使用此方式\n\nserver 192.168.1.210:80 weight=5;\n\nserver 192.168.1.211:80 weight=3;\n\nserver 192.168.1.212:80 weight=1;\n\n}\n\nupstream dxtreasureisland_http { server dal05iis39.sl.dx weight=1 max_fails=1 fail_timeout=45s; server iis01.zl.dx:8093 weight=1 max_fails=1 fail_timeout=45s; }\n\n\n# 反向代理\n\n​ location / {\n​ proxy_pass http://localhost:8000/;\n​ proxy_redirect off;\n​ proxy_set_header host $http_host;\n​ proxy_set_header x-real-ip $remote_addr;\n​ proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n​ location / {\n​ proxy_pass http://172.18.184.130:10085;\n​ proxy_set_header host $host;\n​ proxy_set_header x-real-ip $remote_addr;\n​ proxy_redirect http:// $scheme://; #以上指令会将后端响应header location内容中的http://替换成用户端协议https://\n​ proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n\n​ }\n\nlocation / {\n\nproxy_pass http://baidunode;\n\nproxy_set_header x-forwarded-proto $scheme;\nproxy_set_header host $host; #http header 中的 host 含义为所请求的目的主机名。当 nginx 作为反向代理使用，而后端真实 web 服务器设置有类似 防盗链功能\n\nproxy_set_header x-forwarded-for $proxy_add_x_forwarded_for; #http header 中的 x_forward_for 表示该条 http 请求是由谁发起的\n\nproxy_set_header x-real-ip $remote_addr;\n proxy_redirect off;\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n常用的代理\n\n    location / {\n        proxy_pass         http://localhost:8000/;\n        proxy_set_header   host             $http_host;\n        proxy_set_header   x-real-ip        $remote_addr;\n        proxy_set_header   x-forwarded-for  $proxy_add_x_forwarded_for;\n        proxy_set_header x-forwarded-proto $scheme;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nproxy_redirect的作用\n\nproxy_redirect 该指令用来修改被代理服务器返回的响应头中的location头域和“refresh”头域。\n\n>  1. proxy_redirect 旧地址 新地址;\n> \n>  2. proxy_redirect default; #默认配置\n> \n>  3. proxy_redirect off; #关闭重定向\n\n\n# location规则\n\n配置location /wandou可以匹配/wandoudouduo请求，也可以匹配/wandou*/duoduo等等，只要以wandou开头的目录都可以匹配到。而location /wandou/必须精确匹配/wandou/这个目录的请求,不能匹配/wandouduoduo/或/wandou*/duoduo等请求。\n\n第一种：加"/" location /wddd/ { proxy_pass http://127.0.0.1:8080/; } 测试结果，请求被代理跳转到：http://127.0.0.1:8080/index.html 第二种: 不加"/" location /wddd/ {\nproxy_pass http://127.0.0.1:8080; } 测试结果，请求被代理跳转到：http://127.0.0.1:8080/wddd/index.html 第三种: 增加目录加"/" location /wddd/ {\nproxy_pass http://127.0.0.1:8080/sun/; } 测试结果，请求被代理跳转到：http://127.0.0.1:8080/sun/index.html 第四种：增加目录不加"/" location /wddd/ { proxy_pass http://127.0.0.1:8080/sun; } 测试结果，请求被代理跳转到：http://127.0.0.1:8080/sun/index.html 总结 location目录后加"/",只能匹配目录，不加“/”不仅可以匹配目录还对目录进行模糊匹配。而proxy_pass无论加不加“/”,代理跳转地址都直接拼接\n\n符号   说明\n~    正则匹配，区分大小写\n~*   正则匹配，不区分大小写\n^~   普通字符匹配，如果该选项匹配，则，只匹配改选项，不再向下匹配其他选项\n=    普通字符匹配，精确匹配\n@    定义一个命名的 location，用于内部定向，例如 error_page，try_files\n\n符号@使用例子\n\nserver {\n    listen 80;\n    client_max_body_size 100m;\n    keepalive_timeout 5;\n    root /app/static;\n    location /backend {\n    rewrite ^/backend/(.*) /$1 break;\n    try_files $uri @proxy_to_app;\n    }\n    location / {\n    try_files $uri @proxy_to_app;\n    }\n    location @proxy_to_spp {\n        proxy_set_header x-forwarded-proto $scheme;\n        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n        proxy_set_header host $http_host;\n        proxy_set_header   x-real-ip        $remote_addr;\n        proxy_redirect off;\n        proxy_pass http://app_server;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nnginx禁止、允许访问某些后缀的文件 location ~* .(ini|cfg|dwt|lbi)$ {\ndeny all;\n}\n\n\n# 证书\n\nssl_certificate /etc/ssl/private/.crt; ssl_certificate_key /etc/ssl/private/.key; ssl_session_timeout 5m; ssl_protocols tlsv1 tlsv1.1 tlsv1.2; ssl_ciphers high:!adh:!anull:!enull:!md5:!dss:!dh:!rc4; ssl_prefer_server_ciphers on;\n\nserver {\n    listen 443;\n    listen 80;\n    server_name m.goldmanfutures.com;\n    ssl on;\n    ssl_certificate      /etc/nginx/cert/m.goldmanfutures.com/m.goldmanfutures.com_chain.crt;\n    ssl_certificate_key  /etc/nginx/cert/m.goldmanfutures.com/m.goldmanfutures.com_key.key;\n    proxy_ssl_session_reuse   off;\n    ssl_session_cache         shared:ssl:1m;\n    ssl_session_timeout       5m;\n    ssl_ciphers               ecdhe-rsa-aes128-gcm-sha256:ecdhe:ecdh:aes:high:!null:!anull:!md5:!adh:!rc4;\n    ssl_protocols             tlsv1 tlsv1.1 tlsv1.2;\n    ssl_prefer_server_ciphers on;\n    client_max_body_size 1g;\n    client_body_buffer_size 1m;\n        if ($scheme = http) {\n                rewrite ^(.*)$ https://m.goldmanfutures.com$1 permanent;\n        }\n\n\n    index   index.html index.htm index.php;\n    location / {\n        proxy_pass   http://172.31.186.20:10584/;\n        proxy_set_header host      $host;\n        proxy_set_header x-real-ip $remote_addr;\n        proxy_set_header x-forwarded-proto $scheme;\n        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 日志\n\n​ access_log /var/log/nginx/jenkins.dx.com.access.log;\n​ error_log /var/log/nginx/jenkins.dx.com.errors.log;\n​\n\nlog_format main \'$remote_addr - $remote_user [$time_local] "$request" \'\n \'$status $body_bytes_sent "$http_referer" \'\n \'"$http_user_agent" "$http_x_forwarded_for"\';\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# php配置\n\n​ location ~ \\.php$ {\n​ root /data/erp5/website/erp5.pdvee.com/public;\n​ index index.php;\n​ fastcgi_pass 192.168.14.20:9005;\n​ fastcgi_index index.php;\n​ fastcgi_param script_filename $document_root$fastcgi_script_name;\n​ include fastcgi_params;\n​ }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 模块\n\nconcat模块：nginx concat通过合并静态文件来减少http请求数来达到优化前端性能，可以在一定程度上能减少web服务器的压力。\n\nlocation / {\n concat on;\n concat_max_files 100;\n concat_unique off;\n concat_ignore_file_error on;\n if (!-e $request_filename) {\n rewrite ^(.*)$ /index.php?s=/$1 last;\n break;\n }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 跳转、重定向\n\n​ if ($host ~ ^www\\.dealextreme.com\\.com$) {\n​ rewrite ^(.*)$ $scheme://www.dx.com permanent;\n​ }\nif ($scheme = http) {\n            rewrite ^(.*)$ https://www.dx.com$1 permanent;\n    }\n\n\n1\n2\n3\n4\n5\n6\n\n\nserver { listen 80; server_name www.zzppjj.top zzppjj.top;\n\n    location ~* / {\n            rewrite ^(.*)$ https://www.zzppjj.top$1 permanent;\n    }\n\n\n1\n2\n3\n\n\n}\n\nserver {\n    listen 80;\n    server_name getpro-h5-business.goingf.hk getpro-php-business.goingf.hk;\n\n\n\n    location / {\n        rewrite ^ https://$http_host$request_uri? permanent;    # force redirect http to https\n        return 301 https://$http_host$request_uri;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n重定向到另外一台服务器示例：（查找目录下是否存在文件，如果不存在转到@new_uploads下，new_uploads对应代理到172.17.0.101上）\n\nlocation ^~ /uploads/ { root /data/weiwend/weiwang; try_files $uri @new_uploads; }\n\nlocation @new_uploads {\n    proxy_redirect off;\n    proxy_set_header host $host;\n    proxy_set_header x-real-ip $remote_addr;\n    proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n    proxy_pass http://172.17.0.101:80;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * 将 /api/v1/game/data?id= 新网站路径地址映射到 http://gameid.escape.com/api/v1/new_game/ 老网站的 api 上面，给开发提供获取游戏用户信息的接口\n\nserver { listen 80; server_name gameid.escape.com;\n\naccess_log /var/log/nginx/gameid.nginx.access.log  netdata;\nerror_log  /var/log/nginx/gameid.nginx.error.log   error;\n\nlocation / {\n    return 404;\n}\n\nlocation /api/v1/ {\n    proxy_pass_header server;\n    proxy_set_header host $http_host;\n    proxy_set_header x-real-ip $remote_addr;\n    proxy_set_header x-scheme $scheme;\n    proxy_http_version 1.1;\n    if ($request_uri ~* "/api/v1/game/data\\?id=(.*)") {\n        set $id $1;\n        rewrite .* /api/v1/new_game/$id break;\n        proxy_pass http://gameid.escape.com;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n}\n\n\n# 虚拟目录\n\nlocation /media { alias /usr/share/nginx/html/media;\n}\n\n\n# nginx跨域\n\n     #是否允许请求带有验证信息\n     add_header access-control-allow-credentials true;\n     #允许跨域访问的域名,可以是一个域的列表，也可以是通配符*\n     add_header access-control-allow-origin  $allow_url;\n     #允许脚本访问的返回头\n     add_header access-control-allow-headers \'x-requested-with,content-type,cache-control,pragma,date,x-timestamp\';\n     #允许使用的请求方法，以逗号隔开\n     add_header access-control-allow-methods \'post,get,options,put,delete\';\n     #允许自定义的头部，以逗号隔开,大小写不敏感\n     add_header access-control-expose-headers \'www-authenticate,server-authorization\';\n     #p3p支持跨域cookie操作\n     add_header p3p \'policyref="/w3c/p3p.xml", cp="noi dsp psaa our bus ind onl uni com nav int loc"\';\n     add_header test  1;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n需求为浏览器访问b的时候要直接显示a的主页，这样对a做一个代理，浏览器的url还是b并不是重定向到a。\n\n在a的nginx中加入一下内容\n\nlocation /api/ {\n\nadd_header access-control-allow-origin *;\n\nadd_header access-control-allow-credentials \'true\';\n\nadd_header access-control-allow-methods \'get, post, options\';\n\nadd_header access-control-allow-headers \'origin, x-requested-with, content-type, accept\';\n\nproxy_pass http://gateway_proxy/;\n\n\n# php配置\n\nserver {\n        listen 80;\n        listen 8888;\n\n        server_name  api.erp.pdvee.com;\n\n        access_log /var/log/nginx/api.erp.pdvee.com.access.log site_log2;\n        error_log  /var/log/nginx/api.erp.pdvee.com.errors.log;\n\n        proxy_set_header host $host;\n        proxy_set_header real-ip $remote_addr;\n        proxy_set_header region $geoip_country_code;\n        proxy_set_header x-forwarded-for $remote_addr;\n\n        root /data/erp/website/erp.pdvee.com;\n        index index.php index.html index.htm;\n\n\n      keepalive_timeout 18000s;\n        client_body_timeout 18000s;\n        client_header_timeout 18000s;\n        proxy_connect_timeout 18000s;\n        proxy_send_timeout 18000s;\n        proxy_read_timeout 18000s;\n\n\n    fastcgi_connect_timeout 18000;\n    fastcgi_send_timeout 18000;\n    fastcgi_read_timeout 18000;\n    fastcgi_buffer_size 256k;\n    fastcgi_buffers 8 256k;\n    fastcgi_busy_buffers_size 256k;\n    fastcgi_temp_file_write_size 256k;\n    ##\n    client_max_body_size 2048m;        \n    client_body_buffer_size 256k;\n\n\n\n\n        location / {\n#        proxy_cache my_cache;\n#        proxy_cache_revalidate on;\n                concat on;\n                concat_max_files 100;\n                concat_unique off;\n#                concat_ignore_file_error on;\n            if (!-e $request_filename) {\n            rewrite ^(.*)$ /index.php?s=/$1 last;\n            break;\n           }\n\n        }\n\n        location ~ \\.php$ {\n            root /data/website/erp.pdvee.com;\n            index index.php;\n            fastcgi_pass   192.168.14.13:9001;\n            fastcgi_index  index.php;\n            fastcgi_param  script_filename  $document_root$fastcgi_script_name;\n            include        fastcgi_params;\n        }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n全局变量\n\n$bytes_sent             发送给客户端的字节数\n$connection             连接序列号\n$connection_requests    当前通过连接发出的请求数量\n$content_length         “content-length” 请求头字段\n$content_type           “content-type” 请求头字段\n$cookie_name            cookie名称\n$document_root          当前请求的文档根目录或别名\n$uri                    请求中的当前uri(不带请求参数，参数位于$args)，可以不同于浏览器传递的$request_uri的值，它可以通过内部重定向，或者使用index指令进行修改，$uri不包含主机名，如”/foo/bar.html”。\n$document_uri           同 $uri\n$host                   优先级如下：http请求行的主机名>”host”请求头字段>符合请求的服务器名\n$hostname               主机名\n$http_name              匹配任意请求头字段； 变量名中的后半部分“name”可以替换成任意请求头字段，如在配置文件中需要获取http请求头：“accept-language”，那么将“－”替换为下划线，大写字母替换为小写，形如：$http_accept_language即可。\n$https                  如果开启了ssl安全模式，值为“on”，否则为空字符串。\n$is_args                如果请求中有参数，值为“?”，否则为空字符串。\n$limit_rate             用于设置响应的速度限制\n$nginx_version          nginx版本\n$pid                    工作进程的pid\n$pipe                   如果请求来自管道通信，值为“p”，否则为“.” (1.3.12, 1.2.7)\n$proxy_protocol_addr    获取代理访问服务器的客户端地址，如果是直接访问，该值为空字符串。\n$realpath_root          当前请求的文档根目录或别名的真实路径，会将所有符号连接转换为真实路径。\n$msec                   以秒为单位的时间，日志写入时的毫秒分辨率\n$request_length         请求长度（包括请求行，标题和请求主体）\n$request_method         http请求方法，通常为“get”或“post”\n$request_time           处理客户端请求使用的时间;从读取客户端的第一个字节开始计时。\n$request_uri            这个变量等于包含一些客户端请求参数的原始uri，它无法修改，请查看$uri更改或重写uri，不包含主机名，例如：”/cnphp/test.php?arg=freemouse”。\n$request_time           以毫秒分辨率请求处理时间，以秒为单位; 从客户端读取第一个字节之间的时间并在最后一个字节发送到客户端后进行日志写入\n$status                 响应状态码\n$time_local             本地时间采用通用日志格式\n$arg_name               请求中的的参数名，即“?”后面的arg_name=arg_value形式的arg_name\n$args                   请求中的参数值\n$binary_remote_addr     客户端地址的二进制形式, 固定长度为4个字节\n$body_bytes_sent        传输给客户端的字节数，响应头不计算在内；这个变量和apache的mod_log_config模块中的“%b”参数保持兼容\n$remote_addr            客户端地址\n$remote_port            客户端端口\n$remote_user            用于http基础认证服务的用户名\n$request                客户端请求地址\n$request_body           客户端的请求主体，此变量可在location中使用，将请求主体通过proxy_pass, fastcgi_pass, uwsgi_pass, 和 scgi_pass传递给下一级的代理服务器。\n$request_body_file      将客户端请求主体保存在临时文件中。文件处理结束后，此文件需删除。如果需要之一开启此功能，需要设置client_body_in_file_only。如果将次文件传递给后端的代理服务器，需要禁用request body，即设置proxy_pass_request_body off，fastcgi_pass_request_body off, uwsgi_pass_request_body off, or scgi_pass_request_body off 。\n$request_completion     如果请求成功，值为”ok”，如果请求未完成或者请求不是一个范围请求的最后一部分，则为空。\n$request_filename       当前连接请求的文件路径，由root或alias指令与uri请求生成。\n$scheme                 请求使用的web协议, “http” 或 “https”\n$sent_http_name         可以设置任意http响应头字段； 变量名中的后半部分“name”可以替换成任意响应头字段，如需要设置响应头content-length，那么将“－”替换为下划线，大写字母替换为小写，形如：$sent_http_content_length 4096即可。\n$server_addr            服务器端地址，需要注意的是：为了避免访问linux系统内核，应将ip地址提前设置在配置文件中。\n$server_name            服务器名，域名\n$server_port            服务器端口\n$server_protocol        服务器的http版本, 通常为 “http/1.0” 或 “http/1.1”\n$tcpinfo_rtt, $tcpinfo_rttvar, $tcpinfo_snd_cwnd, $tcpinfo_rcv_space 客户端tcp连接的具体信息\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n############\n\nnginx中$host、$http_host和$proxy_host区别\n$host    不显示端口       浏览器请求的ip，不显示端口\n$http_host 端口存在则显示        浏览器请求的ip和端口号\n$proxy_host    默认80端口不显示，其它显示    被代理服务的ip和端口号\n\n\n1\n2\n3\n4\n\n\n\n# 防止域名恶意解析\n\nserver {\n\nlisten 80 default_server;\n\nserver_name _;\n\nreturn 404;\n\n}\n\nserver { listen 443 default_server; server_name _; ssl on; return 444; }\n\n大概解释如下:\n\ndefault_server：默认域名配置，如果找不到，会自动匹配\n\nserver_name _: 无效域名匹配\n\nreturn 444: 非标准状态码，是nginx服务器扩展的http错误状态码，服务器不向客户端返回任何信息，并关闭连接, 断开客户端和服务器的连接，防止恶意软件攻击威胁\n\n三、配置上面到主配置文件后。\n\n重启动nginx ，就会为我们屏蔽恶意访问了。\n\n\n# 网站置灰\n\nsub_filter  \'</head>\'  \'<style type="text/css">html {-webkit-filter: grayscale(.95);}</style></head>\';\n    sub_filter_once on; \n\n\n1\n2\n',charsets:{cjk:!0}},{title:"kafka介绍和常见操作",frontmatter:{title:"kafka介绍和常见操作",date:"2023-02-23T09:16:02.000Z",permalink:"/pages/98b071/",categories:["运维","中间件","kafka"],tags:[null],readingShow:"top",description:"RabbitMQ：用于实时的，对可靠性要求较高的消息传递上。",meta:[{name:"twitter:title",content:"kafka介绍和常见操作"},{name:"twitter:description",content:"RabbitMQ：用于实时的，对可靠性要求较高的消息传递上。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/02.kafka/01.kafka%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C.html"},{property:"og:type",content:"article"},{property:"og:title",content:"kafka介绍和常见操作"},{property:"og:description",content:"RabbitMQ：用于实时的，对可靠性要求较高的消息传递上。"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/02.kafka/01.kafka%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-23T09:16:02.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"kafka介绍和常见操作"},{itemprop:"description",content:"RabbitMQ：用于实时的，对可靠性要求较高的消息传递上。"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/02.kafka/01.kafka%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C.html",relativePath:"04.运维/03.中间件/02.kafka/01.kafka介绍和常见操作.md",key:"v-38030407",path:"/pages/98b071/",headers:[{level:2,title:"1.应用场景方面",slug:"_1-应用场景方面",normalizedTitle:"1.应用场景方面",charIndex:2},{level:2,title:"2.架构模型方面",slug:"_2-架构模型方面",normalizedTitle:"2.架构模型方面",charIndex:78},{level:2,title:"3.吞吐量方面",slug:"_3-吞吐量方面",normalizedTitle:"3.吞吐量方面",charIndex:176},{level:2,title:"4.集群负载均衡方面",slug:"_4-集群负载均衡方面",normalizedTitle:"4.集群负载均衡方面",charIndex:305},{level:2,title:"5.kafka常见操作",slug:"_5-kafka常见操作",normalizedTitle:"5.kafka常见操作",charIndex:2143}],headersStr:"1.应用场景方面 2.架构模型方面 3.吞吐量方面 4.集群负载均衡方面 5.kafka常见操作",content:"# 1.应用场景方面\n\nRabbitMQ：用于实时的，对可靠性要求较高的消息传递上。\n\nkafka：用于处于活跃的流式数据，大数据量的数据处理上。\n\n\n# 2.架构模型方面\n\nproducer，broker，consumer\n\nRabbitMQ：以broker为中心，有消息的确认机制\n\nkafka：以consumer为中心，无消息的确认机制\n\n\n# 3.吞吐量方面\n\nRabbitMQ：支持消息的可靠的传递，支持事务，不支持批量操作，基于存储的可靠性的要求存储可以采用内存或硬盘，吞吐量小。\n\nkafka：内部采用消息的批量处理，数据的存储和获取是本地磁盘顺序批量操作，消息处理的效率高，吞吐量高。\n\n\n# 4.集群负载均衡方面\n\nRabbitMQ：本身不支持负载均衡，需要loadbalancer的支持\n\nkafka：采用zookeeper对集群中的broker，consumer进行管理，可以注册topic到zookeeper上，通过zookeeper的协调机制，producer保存对应的topic的broker信息，可以随机或者轮询发送到broker上，producer可以基于语义指定分片，消息发送到broker的某个分片上。\n\nkafka单机版搭建\n\n下载kafka_2.13-2.6.0.tgz安装包\n\ntar -zxf kafka_2.13-2.6.0.tgz\n\nmv kafka_2.13-2.6.0/ kafka\n\n自带的Zookeeper程序脚本与配置文件名与原生Zookeeper稍有不同。kafka自带的Zookeeper程序在bin目录下的zookeeper-server-start.sh脚本进行启动，zookeeper-server-stop.sh脚本进行停止。另外Zookeeper的配制文件在路径config/zookeeper.properties，如果有需要可以修改其中的参数。\n\n首先强调一点，kafka的日志目录和zookeeper数据目录，这两项默认放在tmp目录，而tmp目录中内容会随重启而丢失,所以我们遇到的时候最好自定义一个路径。 zookeeper.properties 配置为\n\ndataDir=/data/kafka/data/kfkzookeeper clientPort=2181 admin.enableServer=false tickTime=2000 initLimit=10 syncLimit=5 server.1=172.168.199.223:2888:3888\n\nzookeeper配置myid文件 三台服务器都要在其zookeeper数据目录dataDir下创建一个myid文件，文件内只需填入上述配置文件中broker id的值，作为集群识别标识。以其中一台192.168.11.21服务器为例：\n\n[root@host-192-168-11-21 ~]# cd /data/kafka/data/kfkzookeeper [root@host-192-168-11-21 kfkzookeeper]# echo 1 > myid 创建好后，查看一下，没问题，myid里面只有一个数值\n\nkafka配置文件\n\n进入kafka/config目录下，参考如下修改server.properties文件\n\nbroker.id=1 delete.topic.enable=true num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=/data/kafka/data/kafka num.partitions=3 num.recovery.threads.per.data.dir=1 offsets.topic.replication.factor=3 transaction.state.log.replication.factor=3 transaction.state.log.min.isr=3 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=172.168.199.223:2181 zookeeper.connection.timeout.ms=60000 group.initial.rebalance.delay.ms=0\n\n启动zookeeper服务\n\nbin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n\n启动kafka\n\nbin/kafka-server-start.sh -daemon config/server.properties\n\n查看java项目进程\n\njps\n\n\n# 5.kafka常见操作\n\n启动Zookeeper\n\nbin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n\n关闭 Zookeeper\n\nbin/zookeeper-server-stop.sh -daemon config/zookeeper.properties\n\n启动kafka\n\nbin/kafka-server-start.sh -daemon config/server.properties\n\n关闭kafka\n\nbin/kafka-server-stop.sh config/server.properties\n\n创建topic\n\n--partitions指定分区数 --replication-factor 指定分区数的副本\n\nbin/kafka-topics.sh --create --zookeeper 192.168.11.21:2181 --replication-factor 3 --partitions 3 --topic test\n\n./kafka-topics.sh --create --zookeeper 172.168.199.223:2181 --replication-factor 1 --partitions 1 --topic fgbp-log-pro\n\n查看topic列表\n\nbin/kafka-topics.sh --list --zookeeper 192.168.11.21:2181\n\n查看topic详情\n\nbin/kafka-topics.sh --zookeeper 192.168.11.21:2181 --describe --topic test\n\n创建生产者，在一台服务器\n\nbin/kafka-console-producer.sh --broker-list 192.168.11.21:9092 --topic test\n\n创建消费者，在另一台服务器\n\nbin/kafka-console-consumer.sh --bootstrap-server 192.168.11.22:9092 --topic test\n\n删除topic\n\nbin/kafka-topics.sh --zookeeper 192.168.11.22:2181 --delete --topic test\n\n查看kafka topic数据内容\n\nkafka-console-consumer.sh --bootstrap-server kafka-node-0:9093 kafka-node-1:9094 kafka-node-2:9095 --from-beginning --topic gaoyingfutures_bars_live",normalizedContent:"# 1.应用场景方面\n\nrabbitmq：用于实时的，对可靠性要求较高的消息传递上。\n\nkafka：用于处于活跃的流式数据，大数据量的数据处理上。\n\n\n# 2.架构模型方面\n\nproducer，broker，consumer\n\nrabbitmq：以broker为中心，有消息的确认机制\n\nkafka：以consumer为中心，无消息的确认机制\n\n\n# 3.吞吐量方面\n\nrabbitmq：支持消息的可靠的传递，支持事务，不支持批量操作，基于存储的可靠性的要求存储可以采用内存或硬盘，吞吐量小。\n\nkafka：内部采用消息的批量处理，数据的存储和获取是本地磁盘顺序批量操作，消息处理的效率高，吞吐量高。\n\n\n# 4.集群负载均衡方面\n\nrabbitmq：本身不支持负载均衡，需要loadbalancer的支持\n\nkafka：采用zookeeper对集群中的broker，consumer进行管理，可以注册topic到zookeeper上，通过zookeeper的协调机制，producer保存对应的topic的broker信息，可以随机或者轮询发送到broker上，producer可以基于语义指定分片，消息发送到broker的某个分片上。\n\nkafka单机版搭建\n\n下载kafka_2.13-2.6.0.tgz安装包\n\ntar -zxf kafka_2.13-2.6.0.tgz\n\nmv kafka_2.13-2.6.0/ kafka\n\n自带的zookeeper程序脚本与配置文件名与原生zookeeper稍有不同。kafka自带的zookeeper程序在bin目录下的zookeeper-server-start.sh脚本进行启动，zookeeper-server-stop.sh脚本进行停止。另外zookeeper的配制文件在路径config/zookeeper.properties，如果有需要可以修改其中的参数。\n\n首先强调一点，kafka的日志目录和zookeeper数据目录，这两项默认放在tmp目录，而tmp目录中内容会随重启而丢失,所以我们遇到的时候最好自定义一个路径。 zookeeper.properties 配置为\n\ndatadir=/data/kafka/data/kfkzookeeper clientport=2181 admin.enableserver=false ticktime=2000 initlimit=10 synclimit=5 server.1=172.168.199.223:2888:3888\n\nzookeeper配置myid文件 三台服务器都要在其zookeeper数据目录datadir下创建一个myid文件，文件内只需填入上述配置文件中broker id的值，作为集群识别标识。以其中一台192.168.11.21服务器为例：\n\n[root@host-192-168-11-21 ~]# cd /data/kafka/data/kfkzookeeper [root@host-192-168-11-21 kfkzookeeper]# echo 1 > myid 创建好后，查看一下，没问题，myid里面只有一个数值\n\nkafka配置文件\n\n进入kafka/config目录下，参考如下修改server.properties文件\n\nbroker.id=1 delete.topic.enable=true num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=/data/kafka/data/kafka num.partitions=3 num.recovery.threads.per.data.dir=1 offsets.topic.replication.factor=3 transaction.state.log.replication.factor=3 transaction.state.log.min.isr=3 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=172.168.199.223:2181 zookeeper.connection.timeout.ms=60000 group.initial.rebalance.delay.ms=0\n\n启动zookeeper服务\n\nbin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n\n启动kafka\n\nbin/kafka-server-start.sh -daemon config/server.properties\n\n查看java项目进程\n\njps\n\n\n# 5.kafka常见操作\n\n启动zookeeper\n\nbin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n\n关闭 zookeeper\n\nbin/zookeeper-server-stop.sh -daemon config/zookeeper.properties\n\n启动kafka\n\nbin/kafka-server-start.sh -daemon config/server.properties\n\n关闭kafka\n\nbin/kafka-server-stop.sh config/server.properties\n\n创建topic\n\n--partitions指定分区数 --replication-factor 指定分区数的副本\n\nbin/kafka-topics.sh --create --zookeeper 192.168.11.21:2181 --replication-factor 3 --partitions 3 --topic test\n\n./kafka-topics.sh --create --zookeeper 172.168.199.223:2181 --replication-factor 1 --partitions 1 --topic fgbp-log-pro\n\n查看topic列表\n\nbin/kafka-topics.sh --list --zookeeper 192.168.11.21:2181\n\n查看topic详情\n\nbin/kafka-topics.sh --zookeeper 192.168.11.21:2181 --describe --topic test\n\n创建生产者，在一台服务器\n\nbin/kafka-console-producer.sh --broker-list 192.168.11.21:9092 --topic test\n\n创建消费者，在另一台服务器\n\nbin/kafka-console-consumer.sh --bootstrap-server 192.168.11.22:9092 --topic test\n\n删除topic\n\nbin/kafka-topics.sh --zookeeper 192.168.11.22:2181 --delete --topic test\n\n查看kafka topic数据内容\n\nkafka-console-consumer.sh --bootstrap-server kafka-node-0:9093 kafka-node-1:9094 kafka-node-2:9095 --from-beginning --topic gaoyingfutures_bars_live",charsets:{cjk:!0}},{title:"docker-compose安装kafka集群",frontmatter:{title:"docker-compose安装kafka集群",date:"2023-01-13T16:48:12.000Z",permalink:"/pages/43f361/",categories:["运维","系统"],tags:[null],readingShow:"top",description:'version: "3"',meta:[{name:"twitter:title",content:"docker-compose安装kafka集群"},{name:"twitter:description",content:'version: "3"'},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/02.kafka/02.docker-compose%E5%AE%89%E8%A3%85kafka%E9%9B%86%E7%BE%A4.html"},{property:"og:type",content:"article"},{property:"og:title",content:"docker-compose安装kafka集群"},{property:"og:description",content:'version: "3"'},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/02.kafka/02.docker-compose%E5%AE%89%E8%A3%85kafka%E9%9B%86%E7%BE%A4.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-13T16:48:12.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"docker-compose安装kafka集群"},{itemprop:"description",content:'version: "3"'}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/02.kafka/02.docker-compose%E5%AE%89%E8%A3%85kafka%E9%9B%86%E7%BE%A4.html",relativePath:"04.运维/03.中间件/02.kafka/02.docker-compose安装kafka集群.md",key:"v-0576e07d",path:"/pages/43f361/",headers:[{level:3,title:"安装文件",slug:"安装文件",normalizedTitle:"安装文件",charIndex:2}],headersStr:"安装文件",content:'# 安装文件\n\nversion: "3"\n\nservices:\n  zookeeper:\n    image: zookeeper\n    container_name: zookeeper\n    ports:\n      - 2181:2181\n    volumes:\n      - /data/zookeeper/data:/data\n      - /data/zookeeper/datalog:/datalog\n      - /data/zookeeper/logs:/logs\n    restart: always\n\n  kafka_node_0:\n    depends_on:\n      - zookeeper\n    container_name: kafka-node-0\n    image: wurstmeister/kafka\n    environment:\n      KAFKA_BROKER_ID: 0\n      KAFKA_ZOOKEEPER_CONNECT: 172.16.30.247:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://xxx:9093\n      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9093\n      KAFKA_NUM_PARTITIONS: 3\n      KAFKA_DEFAULT_REPLICATION_FACTOR: 2\n    ports:\n      - 9093:9093\n    volumes:\n      - /data/kafka/node_0:/kafka\n    restart: unless-stopped\n\n  kafka_node_1:\n    depends_on:\n      - kafka_node_0\n    container_name: kafka-node-1\n    image: wurstmeister/kafka\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: 172.16.30.247:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://xxx:9094\n      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094\n      KAFKA_NUM_PARTITIONS: 3\n      KAFKA_DEFAULT_REPLICATION_FACTOR: 2\n    ports:\n      - 9094:9094\n    volumes:\n      - /data/kafka/node_1:/kafka\n    restart: unless-stopped\n\n  kafka_node_2:\n    depends_on:\n      - kafka_node_1\n    container_name: kafka-node-2\n    image: wurstmeister/kafka\n    environment:\n      KAFKA_BROKER_ID: 2\n      KAFKA_ZOOKEEPER_CONNECT: 172.16.30.247:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://xxx:9095\n      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9095\n      KAFKA_NUM_PARTITIONS: 3\n      KAFKA_DEFAULT_REPLICATION_FACTOR: 2\n    ports:\n      - 9095:9095\n    volumes:\n      - /data/kafka/node_2:/kafka\n    restart: unless-stopped\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n',normalizedContent:'# 安装文件\n\nversion: "3"\n\nservices:\n  zookeeper:\n    image: zookeeper\n    container_name: zookeeper\n    ports:\n      - 2181:2181\n    volumes:\n      - /data/zookeeper/data:/data\n      - /data/zookeeper/datalog:/datalog\n      - /data/zookeeper/logs:/logs\n    restart: always\n\n  kafka_node_0:\n    depends_on:\n      - zookeeper\n    container_name: kafka-node-0\n    image: wurstmeister/kafka\n    environment:\n      kafka_broker_id: 0\n      kafka_zookeeper_connect: 172.16.30.247:2181\n      kafka_advertised_listeners: plaintext://xxx:9093\n      kafka_listeners: plaintext://0.0.0.0:9093\n      kafka_num_partitions: 3\n      kafka_default_replication_factor: 2\n    ports:\n      - 9093:9093\n    volumes:\n      - /data/kafka/node_0:/kafka\n    restart: unless-stopped\n\n  kafka_node_1:\n    depends_on:\n      - kafka_node_0\n    container_name: kafka-node-1\n    image: wurstmeister/kafka\n    environment:\n      kafka_broker_id: 1\n      kafka_zookeeper_connect: 172.16.30.247:2181\n      kafka_advertised_listeners: plaintext://xxx:9094\n      kafka_listeners: plaintext://0.0.0.0:9094\n      kafka_num_partitions: 3\n      kafka_default_replication_factor: 2\n    ports:\n      - 9094:9094\n    volumes:\n      - /data/kafka/node_1:/kafka\n    restart: unless-stopped\n\n  kafka_node_2:\n    depends_on:\n      - kafka_node_1\n    container_name: kafka-node-2\n    image: wurstmeister/kafka\n    environment:\n      kafka_broker_id: 2\n      kafka_zookeeper_connect: 172.16.30.247:2181\n      kafka_advertised_listeners: plaintext://xxx:9095\n      kafka_listeners: plaintext://0.0.0.0:9095\n      kafka_num_partitions: 3\n      kafka_default_replication_factor: 2\n    ports:\n      - 9095:9095\n    volumes:\n      - /data/kafka/node_2:/kafka\n    restart: unless-stopped\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n',charsets:{cjk:!0}},{title:"kafka工作原理",frontmatter:{title:"kafka工作原理",date:"2023-03-01T14:14:49.000Z",permalink:"/pages/b73d79/",categories:["运维","中间件","kafka"],tags:[null],readingShow:"top",description:"目录",meta:[{name:"image",content:"http://pic.zzppjj.top/LightPicture/2023/03/24f8307f8bf0c110.png"},{name:"twitter:title",content:"kafka工作原理"},{name:"twitter:description",content:"目录"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://pic.zzppjj.top/LightPicture/2023/03/24f8307f8bf0c110.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/02.kafka/03.kafka%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.html"},{property:"og:type",content:"article"},{property:"og:title",content:"kafka工作原理"},{property:"og:description",content:"目录"},{property:"og:image",content:"http://pic.zzppjj.top/LightPicture/2023/03/24f8307f8bf0c110.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/02.kafka/03.kafka%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-03-01T14:14:49.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"kafka工作原理"},{itemprop:"description",content:"目录"},{itemprop:"image",content:"http://pic.zzppjj.top/LightPicture/2023/03/24f8307f8bf0c110.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/02.kafka/03.kafka%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.html",relativePath:"04.运维/03.中间件/02.kafka/03.kafka工作原理.md",key:"v-8488e3a8",path:"/pages/b73d79/",headers:[{level:2,title:"Kafka系统的角色",slug:"kafka系统的角色",normalizedTitle:"kafka系统的角色",charIndex:530},{level:2,title:"Topic、Partition和Replica的关系",slug:"topic、partition和replica的关系",normalizedTitle:"topic、partition和replica的关系",charIndex:1972},{level:2,title:"问题",slug:"问题",normalizedTitle:"问题",charIndex:2674},{level:2,title:"zookeeper存储结果",slug:"zookeeper存储结果",normalizedTitle:"zookeeper存储结果",charIndex:2865}],headersStr:"Kafka系统的角色 Topic、Partition和Replica的关系 问题 zookeeper存储结果",content:"# kafka工作原理\n\n目录\n\n * http://www.aboutyun.com/thread-11895-1-1.html\n * kafka入门：简介、使用场景、设计原理、主要配置及集群搭建（转）:http://www.cnblogs.com/likehua/p/3999538.html\n * apache kafka系列之在zookeeper中存储结构：http://blog.csdn.net/strawbingo/article/details/45484139\n * Kafka文件存储机制那些事:http://www.open-open.com/lib/view/open1421150566328.html\n * apache kafka系列之server.properties配置文件参数说明:http://blog.csdn.net/lizhitao/article/details/25667831\n * kafka入门:http://bit1129.iteye.com/blog/2174791\n * Kafka 设计与原理详解:Kafka 设计与原理详解_kafka设计原理_Heaven-Wang的博客-CSDN博客\n\n\n# Kafka系统的角色\n\n * Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic\n * topic： 可以理解为一个MQ消息队列的名字\n * Partition：\n   * 为了实现扩展性，一个非常大的topic可以分布到多个 broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。\n   * partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体 （多个partition间）的顺序。\n   * 也就是说，一个topic在集群中可以有多个partition，那么分区的策略是什么？(消息发送到哪个分区上，有两种基本的策略，一是采用Key Hash算法，一是采用Round Robin算法)\n\n\n\n * Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka\n * Producer ：消息生产者，就是向kafka broker发消息的客户端。\n * Consumer ：消息消费者，向kafka broker取消息的客户端\n\n\n\n * Consumer Group （CG）：\n   \n   * 消息系统有两类，一是广播，二是订阅发布。广播是把消息发送给所有的消费者；发布订阅是把消息只发送给订阅者。Kafka通过Consumer Group组合实现了这两种机制： 实现一个topic消息广播（发给所有的consumer）和单播（发给任意一个consumer）。一个topic可以有多个CG。\n   * topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个CG只会把消息发给该CG中的一个 consumer（这是实现一个Topic多Consumer的关键点：为一个Topic定义一个CG，CG下定义多个Consumer）。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。\n   * 典型的应用场景是，多个Consumer来读取一个Topic(理想情况下是一个Consumer读取Topic的一个Partition）,那么可以让这些Consumer属于同一个Consumer Group即可实现消息的多Consumer并行处理，原理是Kafka将一个消息发布出去后，ConsumerGroup中的Consumers可以通过Round Robin的方式进行消费(Consumers之间的负载均衡使用Zookeeper来实现)\n   \n   \n\nA two server Kafka cluster hosting four partitions (P0-P3) with two consumer groups. Consumer group A has two consumer instances and group B has four.\n\n\n# Topic、Partition和Replica的关系\n\n\n\n如上图，一个Topic有四个Partition，每个Partition两个replication。\n\nZookeeper在Kakfa中扮演的角色Kafka将元数据信息保存在Zookeeper中，但是发送给Topic本身的数据是不会发到Zk上的，否则Zk就疯了。\n\n * kafka使用zookeeper来实现动态的集群扩展，不需要更改客户端（producer和consumer）的配置。broker会在zookeeper注册并保持相关的元数据（topic，partition信息等）更新。\n * 而客户端会在zookeeper上注册相关的watcher。一旦zookeeper发生变化，客户端能及时感知并作出相应调整。这样就保证了添加或去除broker时，各broker间仍能自动实现负载均衡。这里的客户端指的是Kafka的消息生产端(Producer)和消息消费端(Consumer)\n * Broker端使用zookeeper来注册broker信息，以及监测partition leader存活性。\n * Consumer端使用zookeeper用来注册consumer信息，其中包括consumer消费的partition列表等，同时也用来发现broker列表，并和partition leader建立socket连接，并获取消息。\n * Zookeeper和Producer没有建立关系，只和Brokers、Consumers建立关系以实现负载均衡，即同一个Consumer Group中的Consumers可以实现负载均衡。\n\n\n# 问题\n\n * Topic有多个Partition，那么消息分配到某个Partition的依据是什么？\n   * Key Hash或者Round Robin\n * 如何查看一个Topic有多少个Partition？\n   * 使用kakfa-topic.sh --list topic topicName --zookeeper zookeeper.servers.list\n\n\n# zookeeper存储结果\n\n * 原文：apache kafka系列之在zookeeper中存储结构_strawbingo的博客-CSDN博客\n   \n   [zk: localhost:2181(CONNECTED) 0] ls / [admin, consumers, config, brokers]\n\n",normalizedContent:"# kafka工作原理\n\n目录\n\n * http://www.aboutyun.com/thread-11895-1-1.html\n * kafka入门：简介、使用场景、设计原理、主要配置及集群搭建（转）:http://www.cnblogs.com/likehua/p/3999538.html\n * apache kafka系列之在zookeeper中存储结构：http://blog.csdn.net/strawbingo/article/details/45484139\n * kafka文件存储机制那些事:http://www.open-open.com/lib/view/open1421150566328.html\n * apache kafka系列之server.properties配置文件参数说明:http://blog.csdn.net/lizhitao/article/details/25667831\n * kafka入门:http://bit1129.iteye.com/blog/2174791\n * kafka 设计与原理详解:kafka 设计与原理详解_kafka设计原理_heaven-wang的博客-csdn博客\n\n\n# kafka系统的角色\n\n * broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic\n * topic： 可以理解为一个mq消息队列的名字\n * partition：\n   * 为了实现扩展性，一个非常大的topic可以分布到多个 broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。\n   * partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体 （多个partition间）的顺序。\n   * 也就是说，一个topic在集群中可以有多个partition，那么分区的策略是什么？(消息发送到哪个分区上，有两种基本的策略，一是采用key hash算法，一是采用round robin算法)\n\n\n\n * offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka\n * producer ：消息生产者，就是向kafka broker发消息的客户端。\n * consumer ：消息消费者，向kafka broker取消息的客户端\n\n\n\n * consumer group （cg）：\n   \n   * 消息系统有两类，一是广播，二是订阅发布。广播是把消息发送给所有的消费者；发布订阅是把消息只发送给订阅者。kafka通过consumer group组合实现了这两种机制： 实现一个topic消息广播（发给所有的consumer）和单播（发给任意一个consumer）。一个topic可以有多个cg。\n   * topic的消息会复制（不是真的复制，是概念上的）到所有的cg，但每个cg只会把消息发给该cg中的一个 consumer（这是实现一个topic多consumer的关键点：为一个topic定义一个cg，cg下定义多个consumer）。如果需要实现广播，只要每个consumer有一个独立的cg就可以了。要实现单播只要所有的consumer在同一个cg。用cg还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。\n   * 典型的应用场景是，多个consumer来读取一个topic(理想情况下是一个consumer读取topic的一个partition）,那么可以让这些consumer属于同一个consumer group即可实现消息的多consumer并行处理，原理是kafka将一个消息发布出去后，consumergroup中的consumers可以通过round robin的方式进行消费(consumers之间的负载均衡使用zookeeper来实现)\n   \n   \n\na two server kafka cluster hosting four partitions (p0-p3) with two consumer groups. consumer group a has two consumer instances and group b has four.\n\n\n# topic、partition和replica的关系\n\n\n\n如上图，一个topic有四个partition，每个partition两个replication。\n\nzookeeper在kakfa中扮演的角色kafka将元数据信息保存在zookeeper中，但是发送给topic本身的数据是不会发到zk上的，否则zk就疯了。\n\n * kafka使用zookeeper来实现动态的集群扩展，不需要更改客户端（producer和consumer）的配置。broker会在zookeeper注册并保持相关的元数据（topic，partition信息等）更新。\n * 而客户端会在zookeeper上注册相关的watcher。一旦zookeeper发生变化，客户端能及时感知并作出相应调整。这样就保证了添加或去除broker时，各broker间仍能自动实现负载均衡。这里的客户端指的是kafka的消息生产端(producer)和消息消费端(consumer)\n * broker端使用zookeeper来注册broker信息，以及监测partition leader存活性。\n * consumer端使用zookeeper用来注册consumer信息，其中包括consumer消费的partition列表等，同时也用来发现broker列表，并和partition leader建立socket连接，并获取消息。\n * zookeeper和producer没有建立关系，只和brokers、consumers建立关系以实现负载均衡，即同一个consumer group中的consumers可以实现负载均衡。\n\n\n# 问题\n\n * topic有多个partition，那么消息分配到某个partition的依据是什么？\n   * key hash或者round robin\n * 如何查看一个topic有多少个partition？\n   * 使用kakfa-topic.sh --list topic topicname --zookeeper zookeeper.servers.list\n\n\n# zookeeper存储结果\n\n * 原文：apache kafka系列之在zookeeper中存储结构_strawbingo的博客-csdn博客\n   \n   [zk: localhost:2181(connected) 0] ls / [admin, consumers, config, brokers]\n\n",charsets:{cjk:!0}},{title:"apollo部署",frontmatter:{title:"apollo部署",date:"2022-12-15T19:20:45.000Z",permalink:"/pages/fd6cf8/",categories:["运维","程序"],tags:[null],readingShow:"top",description:"本文档介绍了如何按照分布式部署的方式编译、打包、部署Apollo配置中心，从而可以在开发、测试、生产等环境分别部署运行。",meta:[{name:"twitter:title",content:"apollo部署"},{name:"twitter:description",content:"本文档介绍了如何按照分布式部署的方式编译、打包、部署Apollo配置中心，从而可以在开发、测试、生产等环境分别部署运行。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.apollo%E9%83%A8%E7%BD%B2.html"},{property:"og:type",content:"article"},{property:"og:title",content:"apollo部署"},{property:"og:description",content:"本文档介绍了如何按照分布式部署的方式编译、打包、部署Apollo配置中心，从而可以在开发、测试、生产等环境分别部署运行。"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.apollo%E9%83%A8%E7%BD%B2.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T19:20:45.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"apollo部署"},{itemprop:"description",content:"本文档介绍了如何按照分布式部署的方式编译、打包、部署Apollo配置中心，从而可以在开发、测试、生产等环境分别部署运行。"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/03.%E4%B8%AD%E9%97%B4%E4%BB%B6/03.apollo%E9%83%A8%E7%BD%B2.html",relativePath:"04.运维/03.中间件/03.apollo部署.md",key:"v-515e9f9a",path:"/pages/fd6cf8/",headersStr:null,content:'本文档介绍了如何按照分布式部署的方式编译、打包、部署Apollo配置中心，从而可以在开发、测试、生产等环境分别部署运行。\n\n一、准备工作\n\n1.1 运行时环境\n\n1.1.1 OS\n\n服务端基于Spring Boot，启动脚本理论上支持所有Linux发行版，建议CentOS 7\n\n1.1.2 Java\n\n * Apollo服务端：1.8+\n * Apollo客户端：1.7+\n\n由于需要同时运行服务端和客户端，所以建议安装Java 1.8+。\n\n对于Apollo客户端，运行时环境只需要1.7+即可。\n\n注：对于Apollo客户端，如果有需要的话，可以做少量代码修改来降级到Java 1.6\n\n在配置好后，可以通过如下命令检查：\n\njava -version\n\n样例输出：\n\njava version "1.8.0_74"\n\nJava(TM) SE Runtime Environment (build 1.8.0_74-b02)\n\nJava HotSpot(TM) 64-Bit Server VM (build 25.74-b02, mixed mode)\n\n1.2 Mysql\n\n * 版本要求：5.6.5+\n\nApollo的表结构对timestamp使用了多个default声明，所以需要5.6.5以上版本。\n\n连接上MySQL后，可以通过如下命令检查：\n\nSHOW VARIABLES WHERE Variable_name = \'version\';点击复制错误复制成功\n\n                \nVariable_name   Value\nversion         5.7.11\n\n1.3 环境\n\n分布式部署需要事先确定部署的环境以及部署方式。\n\nApollo目前支持以下环境：\n\n * DEV\n\n * 开发环境\n\n * FAT\n\n * 测试环境，相当于alpha环境(功能测试)\n\n * UAT\n\n * 集成环境，相当于beta环境（回归测试）\n\n * PRO\n\n * 生产环境\n\n如果希望添加自定义的环境名称，具体步骤可以参考Portal如何增加环境\n\n1.4 网络策略\n\n分布式部署的时候，apollo-configservice和apollo-adminservice需要把自己的IP和端口注册到Meta Server（apollo-configservice本身）。\n\nApollo客户端和Portal会从Meta Server获取服务的地址（IP+端口），然后通过服务地址直接访问。\n\n需要注意的是，apollo-configservice和apollo-adminservice是基于内网可信网络设计的，所以出于安全考虑，请不要将apollo-configservice和apollo-adminservice直接暴露在公网。\n\n所以如果实际部署的机器有多块网卡（如docker），或者存在某些网卡的IP是Apollo客户端和Portal无法访问的（如网络安全限制），那么我们就需要在apollo-configservice和apollo-adminservice中做相关配置来解决连通性问题。\n\n1.4.1 忽略某些网卡\n\n可以分别修改apollo-configservice和apollo-adminservice的startup.sh，通过JVM System Property传入-D参数，也可以通过OS Environment Variable传入，下面的例子会把docker0和veth开头的网卡在注册到Eureka时忽略掉。\n\nJVM System Property示例：\n\n-Dspring.cloud.inetutils.ignoredInterfaces[0]=docker0\n\n-Dspring.cloud.inetutils.ignoredInterfaces[1]=veth.*\n\nOS Environment Variable示例：\n\nSPRING_CLOUD_INETUTILS_IGNORED_INTERFACES[0]=docker0\n\nSPRING_CLOUD_INETUTILS_IGNORED_INTERFACES[1]=veth.*\n\n1.4.2 指定要注册的ip\n\n可以分别修改apollo-configservice和apollo-adminservice的startup.sh，通过JVM System Property传入-D参数，也可以通过OS Environment Variable传入，下面的例子会指定注册的IP为1.2.3.4。\n\nJVM System Property示例：\n\n-Deureka.instance.ip-address=1.2.3.4\n\nOS Environment Variable示例：\n\nEUREKA_INSTANCE_IP_ADDRESS=1.2.3.4\n\n1.4.3 指定要注册的url\n\n可以分别修改apollo-configservice和apollo-adminservice的startup.sh，通过JVM System Property传入-D参数，也可以通过OS Environment Variable传入，下面的例子会指定注册的URL为http://1.2.3.4:8080。\n\nJVM System Property示例：\n\n-Deureka.instance.homePageUrl=http://1.2.3.4:8080\n\n-Deureka.instance.preferIpAddress=false\n\nOS Environment Variable示例：\n\nEUREKA_INSTANCE_HOME_PAGE_URL=http://1.2.3.4:8080\n\nEUREKA_INSTANCE_PREFER_IP_ADDRESS=false\n\n1.4.4 直接指定apollo-configservice地址\n\n如果Apollo部署在公有云上，本地开发环境无法连接，但又需要做开发测试的话，客户端可以升级到0.11.0版本及以上，然后配置跳过Apollo Meta Server服务发现\n\n二 部署步骤\n\n部署步骤总体还是比较简单的，Apollo的唯一依赖是数据库，所以需要首先把数据库准备好，然后根据实际情况，选择不同的部署方式：\n\n2.1 创建数据库\n\nApollo服务端共需要两个数据库：ApolloPortalDB和ApolloConfigDB，我们把数据库、表的创建和样例数据都分别准备了sql文件，只需要导入数据库即可。\n\n需要注意的是ApolloPortalDB只需要在生产环境部署一个即可，而ApolloConfigDB需要在每个环境部署一套，如fat、uat和pro分别部署3套ApolloConfigDB。\n\n注意：如果你本地已经创建过Apollo数据库，请注意备份数据。我们准备的sql文件会清空Apollo相关的表。\n\n2.1.1 创建ApolloPortalDB\n\n可以根据实际情况选择通过手动导入SQL\n\n2.1.1.1 手动导入sql创建\n\n通过各种MySQL客户端导入apolloportaldb.sql即可。\n\n以MySQL原生客户端为例：\n\nsource /your_local_path/scripts/sql/apolloportaldb.sql\n\n2.1.1.2 验证\n\n导入成功后，可以通过执行以下sql语句来验证：\n\nselect Id, Key, Value, Comment from ApolloPortalDB.ServerConfig limit 1;点击复制错误复制成功\n\n                                  \nId   Key                  Value   Comment\n1    apollo.portal.envs   dev     可支持的环境列表\n\n注：ApolloPortalDB只需要在生产环境部署一个即可\n\n2.1.2 创建apolloconfigdb\n\n可以根据实际情况选择通过手动导入SQL\n\n2.1.2.1 手动导入sql\n\n通过各种MySQL客户端导入apolloconfigdb.sql即可。\n\n以MySQL原生客户端为例：\n\nsource /your_local_path/scripts/sql/apolloconfigdb.sql\n\n2.1.2.2 验证\n\n导入成功后，可以通过执行以下sql语句来验证：\n\nselect Id, Key, Value, Comment from ApolloConfigDB.ServerConfig limit 1;点击复制错误复制成功\n\n                                                          \nId   Key                  Value                           Comment\n1    eureka.service.url   http://127.0.0.1:8080/eureka/   Eureka服务Url\n\n注：ApolloConfigDB需要在每个环境部署一套，如fat、uat和pro分别部署3套ApolloConfigDB\n\n2.1.2.3 从别的环境导入apolloconfigdb数据\n\n如果是全新部署的Apollo配置中心，请忽略此步。\n\n如果不是全新部署的Apollo配置中心，比如已经使用了一段时间，这时在Apollo配置中心已经创建了不少项目以及namespace等，那么在新环境中的ApolloConfigDB中需要从其它正常运行的环境中导入必要的项目数据。\n\n主要涉及ApolloConfigDB的下面4张表，下面同时附上需要导入的数据查询语句：\n\n 1. App\n\n * 导入全部的App\n * 如：insert into 新环境的ApolloConfigDB.App select * from 其它环境的ApolloConfigDB.App where IsDeleted = 0;\n\n 2. AppNamespace\n\n * 导入全部的AppNamespace\n * 如：insert into 新环境的ApolloConfigDB.AppNamespace select * from 其它环境的ApolloConfigDB.AppNamespace where IsDeleted = 0;\n\n 3. Cluster\n\n * 导入默认的default集群\n * 如：insert into 新环境的ApolloConfigDB.Cluster select * from 其它环境的ApolloConfigDB.Cluster where Name = \'default\' and IsDeleted = 0;\n\n 4. Namespace\n\n * 导入默认的default集群中的namespace\n * 如：insert into 新环境的ApolloConfigDB.Namespace select * from 其它环境的ApolloConfigDB.Namespace where ClusterName = \'default\' and IsDeleted = 0;\n\n同时也别忘了通知用户在新的环境给自己的项目设置正确的配置信息，尤其是一些影响面比较大的公共namespace配置。\n\n如果是为正在运行的环境迁移数据，建议迁移完重启一下config service，因为config service中有appnamespace的缓存数据\n\n2.1.3 调整服务端配置\n\nApollo自身的一些配置是放在数据库里面的，所以需要针对实际情况做一些调整，具体参数说明请参考三、服务端配置说明。\n\n大部分配置可以先使用默认值，不过 apollo.portal.envs 和 eureka.service.url 请务必配置正确后再进行下面的部署步骤。\n\n2.2 虚拟机/物理机部署\n\n2.2.1 获取安装包\n\n可以通过两种方式获取安装包：\n\n 1. 直接下载安装包\n\n * 从GitHub Release页面下载预先打好的安装包\n * 如果对Apollo的代码没有定制需求，建议使用这种方式，可以省去本地打包的过程\n\n 2. 通过源码构建\n\n * 从GitHub Release页面下载Source code包或直接clone源码后在本地构建\n * 如果需要对Apollo的做定制开发，需要使用这种方式\n\n2.2.1.1 直接下载安装包\n\n2.2.1.1.1 获取apollo-configservice、apollo-adminservice、apollo-portal安装包\n\n从GitHub Release页面下载最新版本的apollo-configservice-x.x.x-github.zip、apollo-adminservice-x.x.x-github.zip和apollo-portal-x.x.x-github.zip即可。\n\n2.2.1.1.2 配置数据库连接信息\n\nApollo服务端需要知道如何连接到你前面创建的数据库，数据库连接串信息位于上一步下载的压缩包中的config/application-github.properties中。\n\n2.2.1.1.2.1 配置apollo-configservice的数据库连接信息\n\n 1. 解压apollo-configservice-x.x.x-github.zip\n 2. 用程序员专用编辑器（如vim，notepad++，sublime等）打开config目录下的application-github.properties文件\n 3. 填写正确的ApolloConfigDB数据库连接串信息，注意用户名和密码后面不要有空格!\n 4. 修改完的效果如下：\n\n\n# DataSource\n\nspring.datasource.url = jdbc:mysql://localhost:3306/ApolloConfigDB?useSSL=false&characterEncoding=utf8\n\nspring.datasource.username = someuser\n\nspring.datasource.password = somepwd点击复制错误复制成功\n\n注：由于ApolloConfigDB在每个环境都有部署，所以对不同的环境config-service需要配置对应环境的数据库参数\n\n2.2.1.1.2.2 配置apollo-adminservice的数据库连接信息\n\n 1. 解压apollo-adminservice-x.x.x-github.zip\n 2. 用程序员专用编辑器（如vim，notepad++，sublime等）打开config目录下的application-github.properties文件\n 3. 填写正确的ApolloConfigDB数据库连接串信息，注意用户名和密码后面不要有空格!\n 4. 修改完的效果如下：\n\n\n# DataSource\n\nspring.datasource.url = jdbc:mysql://localhost:3306/ApolloConfigDB?useSSL=false&characterEncoding=utf8\n\nspring.datasource.username = someuser\n\nspring.datasource.password = somepwd点击复制错误复制成功\n\n注：由于ApolloConfigDB在每个环境都有部署，所以对不同的环境admin-service需要配置对应环境的数据库参数\n\n2.2.1.1.2.3 配置apollo-portal的数据库连接信息\n\n 1. 解压apollo-portal-x.x.x-github.zip\n 2. 用程序员专用编辑器（如vim，notepad++，sublime等）打开config目录下的application-github.properties文件\n 3. 填写正确的ApolloPortalDB数据库连接串信息，注意用户名和密码后面不要有空格!\n 4. 修改完的效果如下：\n\n\n# DataSource\n\nspring.datasource.url = jdbc:mysql://localhost:3306/ApolloPortalDB?useSSL=false&characterEncoding=utf8\n\nspring.datasource.username = someuser\n\nspring.datasource.password = somepwd点击复制错误复制成功\n\n2.2.1.1.2.4 配置apollo-portal的meta service信息\n\nApollo Portal需要在不同的环境访问不同的meta service(apollo-configservice)地址，所以我们需要在配置中提供这些信息。默认情况下，meta service和config service是部署在同一个JVM进程，所以meta service的地址就是config service的地址。\n\n对于1.6.0及以上版本，可以通过ApolloPortalDB.ServerConfig中的配置项来配置Meta Service地址，详见apollo.portal.meta.servers - 各环境Meta Service列表\n\n使用程序员专用编辑器（如vim，notepad++，sublime等）打开apollo-portal-x.x.x-github.zip中config目录下的apollo-env.properties文件。\n\n假设DEV的apollo-configservice未绑定域名，地址是1.1.1.1:8080，FAT的apollo-configservice绑定了域名apollo.fat.xxx.com，UAT的apollo-configservice绑定了域名apollo.uat.xxx.com，PRO的apollo-configservice绑定了域名apollo.xxx.com，那么可以如下修改各环境meta service服务地址，格式为${env}.meta=http://${config-service-url:port}，如果某个环境不需要，也可以直接删除对应的配置项（如lpt.meta）：\n\ndev.meta=http://1.1.1.1:8080\n\nfat.meta=http://apollo.fat.xxx.com\n\nuat.meta=http://apollo.uat.xxx.com\n\npro.meta=http://apollo.xxx.com点击复制错误复制成功\n\n除了通过apollo-env.properties方式配置meta service以外，apollo也支持在运行时指定meta service（优先级比apollo-env.properties高）：\n\n 1. 通过Java System Property\n\n${env}_meta\n\n * 可以通过Java的System Property ${env}_meta来指定\n\n * 如java -Ddev_meta=http://config-service-url -jar xxx.jar\n\n * 也可以通过程序指定，如System.setProperty("dev_meta", "http://config-service-url");\n\n 2. 通过操作系统的System Environment\n\n${ENV}_META\n\n * 如DEV_META=http://config-service-url\n * 注意key为全大写，且中间是_分隔\n\n注1: 为了实现meta service的高可用，推荐通过SLB（Software Load Balancer）做动态负载均衡\n\n注2: meta service地址也可以填入IP，0.11.0版本之前只支持填入一个IP。从0.11.0版本开始支持填入以逗号分隔的多个地址（PR #1214），如http://1.1.1.1:8080,http://2.2.2.2:8080，不过生产环境还是建议使用域名（走slb），因为机器扩容、缩容等都可能导致IP列表的变化。\n\n2.2.1.2 通过源码构建\n\n2.2.1.2.1 配置数据库连接信息\n\nApollo服务端需要知道如何连接到你前面创建的数据库，所以需要编辑scripts/build.sh，修改ApolloPortalDB和ApolloConfigDB相关的数据库连接串信息。\n\n注意：填入的用户需要具备对ApolloPortalDB和ApolloConfigDB数据的读写权限。\n\n#apollo config db info\n\napollo_config_db_url=jdbc:mysql://localhost:3306/ApolloConfigDB?useSSL=false&characterEncoding=utf8\n\napollo_config_db_username=用户名\n\napollo_config_db_password=密码（如果没有密码，留空即可）\n\n\n# apollo portal db info\n\napollo_portal_db_url=jdbc:mysql://localhost:3306/ApolloPortalDB?useSSL=false&characterEncoding=utf8\n\napollo_portal_db_username=用户名\n\napollo_portal_db_password=密码（如果没有密码，留空即可）点击复制错误复制成功\n\n注1：由于ApolloConfigDB在每个环境都有部署，所以对不同的环境config-service和admin-service需要使用不同的数据库参数打不同的包，portal只需要打一次包即可\n\n注2：如果不想config-service和admin-service每个环境打一个包的话，也可以通过运行时传入数据库连接串信息实现，具体可以参考 Issue 869\n\n注3：每个环境都需要独立部署一套config-service、admin-service和ApolloConfigDB\n\n2.2.1.2.2 配置各环境meta service地址\n\nApollo Portal需要在不同的环境访问不同的meta service(apollo-configservice)地址，所以需要在打包时提供这些信息。\n\n假设DEV的apollo-configservice未绑定域名，地址是1.1.1.1:8080，FAT的apollo-configservice绑定了域名apollo.fat.xxx.com，UAT的apollo-configservice绑定了域名apollo.uat.xxx.com，PRO的apollo-configservice绑定了域名apollo.xxx.com，那么编辑scripts/build.sh，如下修改各环境meta service服务地址，格式为${env}_meta=http://${config-service-url:port}，如果某个环境不需要，也可以直接删除对应的配置项：\n\ndev_meta=http://1.1.1.1:8080\n\nfat_meta=http://apollo.fat.xxx.com\n\nuat_meta=http://apollo.uat.xxx.com\n\npro_meta=http://apollo.xxx.com\n\nMETA_SERVERS_OPTS="-Ddev_meta=$dev_meta -Dfat_meta=$fat_meta -Duat_meta=$uat_meta -Dpro_meta=$pro_meta"\n\n除了在打包时配置meta service以外，apollo也支持在运行时指定meta service：\n\n 1. 通过Java System Property\n\n${env}_meta\n\n * 可以通过Java的System Property ${env}_meta来指定\n\n * 如java -Ddev_meta=http://config-service-url -jar xxx.jar\n\n * 也可以通过程序指定，如System.setProperty("dev_meta", "http://config-service-url");\n\n 2. 通过操作系统的System Environment\n\n${ENV}_META\n\n * 如DEV_META=http://config-service-url\n * 注意key为全大写，且中间是_分隔\n\n注1: 为了实现meta service的高可用，推荐通过SLB（Software Load Balancer）做动态负载均衡\n\n注2: meta service地址也可以填入IP，0.11.0版本之前只支持填入一个IP。从0.11.0版本开始支持填入以逗号分隔的多个地址（PR #1214），如http://1.1.1.1:8080,http://2.2.2.2:8080，不过生产环境还是建议使用域名（走slb），因为机器扩容、缩容等都可能导致IP列表的变化。\n\n2.2.1.2.3 执行编译、打包\n\n做完上述配置后，就可以执行编译和打包了。\n\n注：初次编译会从Maven中央仓库下载不少依赖，如果网络情况不佳时很容易出错，建议使用国内的Maven仓库源，比如阿里云Maven镜像\n\n./build.sh\n\n该脚本会依次打包apollo-configservice, apollo-adminservice, apollo-portal。\n\n注：由于ApolloConfigDB在每个环境都有部署，所以对不同环境的config-service和admin-service需要使用不同的数据库连接信息打不同的包，portal只需要打一次包即可\n\n2.2.1.2.4 获取apollo-configservice安装包\n\n位于apollo-configservice/target/目录下的apollo-configservice-x.x.x-github.zip\n\n需要注意的是由于ApolloConfigDB在每个环境都有部署，所以对不同环境的config-service需要使用不同的数据库参数打不同的包后分别部署\n\n2.2.1.2.5 获取apollo-adminservice安装包\n\n位于apollo-adminservice/target/目录下的apollo-adminservice-x.x.x-github.zip\n\n需要注意的是由于ApolloConfigDB在每个环境都有部署，所以对不同环境的admin-service需要使用不同的数据库参数打不同的包后分别部署\n\n2.2.1.2.6 获取apollo-portal安装包\n\n位于apollo-portal/target/目录下的apollo-portal-x.x.x-github.zip\n\n2.2.1.2.7 启用外部nacos服务注册中心替换内置eureka\n\n 1. 修改build.sh/build.bat，将config-service和admin-service的maven编译命令更改为\n\nmvn clean package -Pgithub,nacos-discovery -DskipTests -pl apollo-configservice,apollo-adminservice -am -Dapollo_profile=github,nacos-discovery -Dspring_datasource_url=$apollo_config_db_url -Dspring_datasource_username=$apollo_config_db_username -Dspring_datasource_password=$apollo_config_db_password点击复制错误复制成功\n\n 2. 分别修改apollo-configservice和apollo-adminservice安装包中config目录下的application-github.properties，配置nacos服务器地址\n\nnacos.discovery.server-addr=127.0.0.1:8848\n\n\n# 更多 nacos 配置\n\nnacos.discovery.access-key=\n\nnacos.discovery.username=\n\nnacos.discovery.password=\n\nnacos.discovery.secret-key=\n\nnacos.discovery.namespace=\n\nnacos.discovery.context-path=\n\n2.2.1.2.8 启用外部Consul服务注册中心替换内置eureka\n\n 1. 修改build.sh/build.bat，将config-service和admin-service的maven编译命令更改为\n\nmvn clean package -Pgithub -DskipTests -pl apollo-configservice,apollo-adminservice -am -Dapollo_profile=github,consul-discovery -Dspring_datasource_url=$apollo_config_db_url -Dspring_datasource_username=$apollo_config_db_username -Dspring_datasource_password=$apollo_config_db_password\n\n 2. 分别修改apollo-configservice和apollo-adminservice安装包中config目录下的application-github.properties，配置consul服务器地址\n\nspring.cloud.consul.host=127.0.0.1\n\nspring.cloud.consul.port=8500点\n\n2.2.2 部署Apollo服务端\n\n2.2.2.1 部署apollo-configservice\n\n将对应环境的apollo-configservice-x.x.x-github.zip上传到服务器上，解压后执行scripts/startup.sh即可。如需停止服务，执行scripts/shutdown.sh.\n\n记得在scripts/startup.sh中按照实际的环境设置一个JVM内存，以下是我们的默认设置，供参考：\n\nexport JAVA_OPTS="-server -Xms6144m -Xmx6144m -Xss256k -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=384m -XX:NewSize=4096m -XX:MaxNewSize=4096m -XX:SurvivorRatio=18"点击复制错误复制成功\n\n注1：如果需要修改JVM参数，可以修改scripts/startup.sh的JAVA_OPTS部分。\n\n注2：如要调整服务的日志输出路径，可以修改scripts/startup.sh和apollo-configservice.conf中的LOG_DIR。\n\n注3：如要调整服务的监听端口，可以修改scripts/startup.sh中的SERVER_PORT。另外apollo-configservice同时承担meta server职责，如果要修改端口，注意要同时ApolloConfigDB.ServerConfig表中的eureka.service.url配置项以及apollo-portal和apollo-client中的使用到的meta server信息，详见：2.2.1.1.2.4 配置apollo-portal的meta service信息和1.2.2 Apollo Meta Server。\n\n注4：如果ApolloConfigDB.ServerConfig的eureka.service.url只配了当前正在启动的机器的话，在启动apollo-configservice的过程中会在日志中输出eureka注册失败的信息，如com.sun.jersey.api.client.ClientHandlerException: java.net.ConnectException: Connection refused。需要注意的是，这个是预期的情况，因为apollo-configservice需要向Meta Server（它自己）注册服务，但是因为在启动过程中，自己还没起来，所以会报这个错。后面会进行重试的动作，所以等自己服务起来后就会注册正常了。\n\n注5：如果你看到了这里，相信你一定是一个细心阅读文档的人，而且离成功就差一点点了，继续加油，应该很快就能完成Apollo的分布式部署了！不过你是否有感觉Apollo的分布式部署步骤有点繁琐？是否有啥建议想要和作者说？如果答案是肯定的话，请移步 #1424，期待你的建议！\n\n2.2.2.2 部署apollo-adminservice\n\n将对应环境的apollo-adminservice-x.x.x-github.zip上传到服务器上，解压后执行scripts/startup.sh即可。如需停止服务，执行scripts/shutdown.sh.\n\n记得在scripts/startup.sh中按照实际的环境设置一个JVM内存，以下是我们的默认设置，供参考：\n\nexport JAVA_OPTS="-server -Xms2560m -Xmx2560m -Xss256k -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=384m -XX:NewSize=1024m -XX:MaxNewSize=1024m -XX:SurvivorRatio=22"\n\n注1：如果需要修改JVM参数，可以修改scripts/startup.sh的JAVA_OPTS部分。\n\n注2：如要调整服务的日志输出路径，可以修改scripts/startup.sh和apollo-adminservice.conf中的LOG_DIR。\n\n注3：如要调整服务的监听端口，可以修改scripts/startup.sh中的SERVER_PORT。\n\n2.2.2.3 部署apollo-portal\n\n将apollo-portal-x.x.x-github.zip上传到服务器上，解压后执行scripts/startup.sh即可。如需停止服务，执行scripts/shutdown.sh.\n\n记得在startup.sh中按照实际的环境设置一个JVM内存，以下是我们的默认设置，供参考：\n\nexport JAVA_OPTS="-server -Xms4096m -Xmx4096m -Xss256k -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=384m -XX:NewSize=1536m -XX:MaxNewSize=1536m -XX:SurvivorRatio=22"\n\n注1：如果需要修改JVM参数，可以修改scripts/startup.sh的JAVA_OPTS部分。\n\n注2：如要调整服务的日志输出路径，可以修改scripts/startup.sh和apollo-portal.conf中的LOG_DIR。\n\n注3：如要调整服务的监听端口，可以修改scripts/startup.sh中的SERVER_PORT。',normalizedContent:'本文档介绍了如何按照分布式部署的方式编译、打包、部署apollo配置中心，从而可以在开发、测试、生产等环境分别部署运行。\n\n一、准备工作\n\n1.1 运行时环境\n\n1.1.1 os\n\n服务端基于spring boot，启动脚本理论上支持所有linux发行版，建议centos 7\n\n1.1.2 java\n\n * apollo服务端：1.8+\n * apollo客户端：1.7+\n\n由于需要同时运行服务端和客户端，所以建议安装java 1.8+。\n\n对于apollo客户端，运行时环境只需要1.7+即可。\n\n注：对于apollo客户端，如果有需要的话，可以做少量代码修改来降级到java 1.6\n\n在配置好后，可以通过如下命令检查：\n\njava -version\n\n样例输出：\n\njava version "1.8.0_74"\n\njava(tm) se runtime environment (build 1.8.0_74-b02)\n\njava hotspot(tm) 64-bit server vm (build 25.74-b02, mixed mode)\n\n1.2 mysql\n\n * 版本要求：5.6.5+\n\napollo的表结构对timestamp使用了多个default声明，所以需要5.6.5以上版本。\n\n连接上mysql后，可以通过如下命令检查：\n\nshow variables where variable_name = \'version\';点击复制错误复制成功\n\n                \nvariable_name   value\nversion         5.7.11\n\n1.3 环境\n\n分布式部署需要事先确定部署的环境以及部署方式。\n\napollo目前支持以下环境：\n\n * dev\n\n * 开发环境\n\n * fat\n\n * 测试环境，相当于alpha环境(功能测试)\n\n * uat\n\n * 集成环境，相当于beta环境（回归测试）\n\n * pro\n\n * 生产环境\n\n如果希望添加自定义的环境名称，具体步骤可以参考portal如何增加环境\n\n1.4 网络策略\n\n分布式部署的时候，apollo-configservice和apollo-adminservice需要把自己的ip和端口注册到meta server（apollo-configservice本身）。\n\napollo客户端和portal会从meta server获取服务的地址（ip+端口），然后通过服务地址直接访问。\n\n需要注意的是，apollo-configservice和apollo-adminservice是基于内网可信网络设计的，所以出于安全考虑，请不要将apollo-configservice和apollo-adminservice直接暴露在公网。\n\n所以如果实际部署的机器有多块网卡（如docker），或者存在某些网卡的ip是apollo客户端和portal无法访问的（如网络安全限制），那么我们就需要在apollo-configservice和apollo-adminservice中做相关配置来解决连通性问题。\n\n1.4.1 忽略某些网卡\n\n可以分别修改apollo-configservice和apollo-adminservice的startup.sh，通过jvm system property传入-d参数，也可以通过os environment variable传入，下面的例子会把docker0和veth开头的网卡在注册到eureka时忽略掉。\n\njvm system property示例：\n\n-dspring.cloud.inetutils.ignoredinterfaces[0]=docker0\n\n-dspring.cloud.inetutils.ignoredinterfaces[1]=veth.*\n\nos environment variable示例：\n\nspring_cloud_inetutils_ignored_interfaces[0]=docker0\n\nspring_cloud_inetutils_ignored_interfaces[1]=veth.*\n\n1.4.2 指定要注册的ip\n\n可以分别修改apollo-configservice和apollo-adminservice的startup.sh，通过jvm system property传入-d参数，也可以通过os environment variable传入，下面的例子会指定注册的ip为1.2.3.4。\n\njvm system property示例：\n\n-deureka.instance.ip-address=1.2.3.4\n\nos environment variable示例：\n\neureka_instance_ip_address=1.2.3.4\n\n1.4.3 指定要注册的url\n\n可以分别修改apollo-configservice和apollo-adminservice的startup.sh，通过jvm system property传入-d参数，也可以通过os environment variable传入，下面的例子会指定注册的url为http://1.2.3.4:8080。\n\njvm system property示例：\n\n-deureka.instance.homepageurl=http://1.2.3.4:8080\n\n-deureka.instance.preferipaddress=false\n\nos environment variable示例：\n\neureka_instance_home_page_url=http://1.2.3.4:8080\n\neureka_instance_prefer_ip_address=false\n\n1.4.4 直接指定apollo-configservice地址\n\n如果apollo部署在公有云上，本地开发环境无法连接，但又需要做开发测试的话，客户端可以升级到0.11.0版本及以上，然后配置跳过apollo meta server服务发现\n\n二 部署步骤\n\n部署步骤总体还是比较简单的，apollo的唯一依赖是数据库，所以需要首先把数据库准备好，然后根据实际情况，选择不同的部署方式：\n\n2.1 创建数据库\n\napollo服务端共需要两个数据库：apolloportaldb和apolloconfigdb，我们把数据库、表的创建和样例数据都分别准备了sql文件，只需要导入数据库即可。\n\n需要注意的是apolloportaldb只需要在生产环境部署一个即可，而apolloconfigdb需要在每个环境部署一套，如fat、uat和pro分别部署3套apolloconfigdb。\n\n注意：如果你本地已经创建过apollo数据库，请注意备份数据。我们准备的sql文件会清空apollo相关的表。\n\n2.1.1 创建apolloportaldb\n\n可以根据实际情况选择通过手动导入sql\n\n2.1.1.1 手动导入sql创建\n\n通过各种mysql客户端导入apolloportaldb.sql即可。\n\n以mysql原生客户端为例：\n\nsource /your_local_path/scripts/sql/apolloportaldb.sql\n\n2.1.1.2 验证\n\n导入成功后，可以通过执行以下sql语句来验证：\n\nselect id, key, value, comment from apolloportaldb.serverconfig limit 1;点击复制错误复制成功\n\n                                  \nid   key                  value   comment\n1    apollo.portal.envs   dev     可支持的环境列表\n\n注：apolloportaldb只需要在生产环境部署一个即可\n\n2.1.2 创建apolloconfigdb\n\n可以根据实际情况选择通过手动导入sql\n\n2.1.2.1 手动导入sql\n\n通过各种mysql客户端导入apolloconfigdb.sql即可。\n\n以mysql原生客户端为例：\n\nsource /your_local_path/scripts/sql/apolloconfigdb.sql\n\n2.1.2.2 验证\n\n导入成功后，可以通过执行以下sql语句来验证：\n\nselect id, key, value, comment from apolloconfigdb.serverconfig limit 1;点击复制错误复制成功\n\n                                                          \nid   key                  value                           comment\n1    eureka.service.url   http://127.0.0.1:8080/eureka/   eureka服务url\n\n注：apolloconfigdb需要在每个环境部署一套，如fat、uat和pro分别部署3套apolloconfigdb\n\n2.1.2.3 从别的环境导入apolloconfigdb数据\n\n如果是全新部署的apollo配置中心，请忽略此步。\n\n如果不是全新部署的apollo配置中心，比如已经使用了一段时间，这时在apollo配置中心已经创建了不少项目以及namespace等，那么在新环境中的apolloconfigdb中需要从其它正常运行的环境中导入必要的项目数据。\n\n主要涉及apolloconfigdb的下面4张表，下面同时附上需要导入的数据查询语句：\n\n 1. app\n\n * 导入全部的app\n * 如：insert into 新环境的apolloconfigdb.app select * from 其它环境的apolloconfigdb.app where isdeleted = 0;\n\n 2. appnamespace\n\n * 导入全部的appnamespace\n * 如：insert into 新环境的apolloconfigdb.appnamespace select * from 其它环境的apolloconfigdb.appnamespace where isdeleted = 0;\n\n 3. cluster\n\n * 导入默认的default集群\n * 如：insert into 新环境的apolloconfigdb.cluster select * from 其它环境的apolloconfigdb.cluster where name = \'default\' and isdeleted = 0;\n\n 4. namespace\n\n * 导入默认的default集群中的namespace\n * 如：insert into 新环境的apolloconfigdb.namespace select * from 其它环境的apolloconfigdb.namespace where clustername = \'default\' and isdeleted = 0;\n\n同时也别忘了通知用户在新的环境给自己的项目设置正确的配置信息，尤其是一些影响面比较大的公共namespace配置。\n\n如果是为正在运行的环境迁移数据，建议迁移完重启一下config service，因为config service中有appnamespace的缓存数据\n\n2.1.3 调整服务端配置\n\napollo自身的一些配置是放在数据库里面的，所以需要针对实际情况做一些调整，具体参数说明请参考三、服务端配置说明。\n\n大部分配置可以先使用默认值，不过 apollo.portal.envs 和 eureka.service.url 请务必配置正确后再进行下面的部署步骤。\n\n2.2 虚拟机/物理机部署\n\n2.2.1 获取安装包\n\n可以通过两种方式获取安装包：\n\n 1. 直接下载安装包\n\n * 从github release页面下载预先打好的安装包\n * 如果对apollo的代码没有定制需求，建议使用这种方式，可以省去本地打包的过程\n\n 2. 通过源码构建\n\n * 从github release页面下载source code包或直接clone源码后在本地构建\n * 如果需要对apollo的做定制开发，需要使用这种方式\n\n2.2.1.1 直接下载安装包\n\n2.2.1.1.1 获取apollo-configservice、apollo-adminservice、apollo-portal安装包\n\n从github release页面下载最新版本的apollo-configservice-x.x.x-github.zip、apollo-adminservice-x.x.x-github.zip和apollo-portal-x.x.x-github.zip即可。\n\n2.2.1.1.2 配置数据库连接信息\n\napollo服务端需要知道如何连接到你前面创建的数据库，数据库连接串信息位于上一步下载的压缩包中的config/application-github.properties中。\n\n2.2.1.1.2.1 配置apollo-configservice的数据库连接信息\n\n 1. 解压apollo-configservice-x.x.x-github.zip\n 2. 用程序员专用编辑器（如vim，notepad++，sublime等）打开config目录下的application-github.properties文件\n 3. 填写正确的apolloconfigdb数据库连接串信息，注意用户名和密码后面不要有空格!\n 4. 修改完的效果如下：\n\n\n# datasource\n\nspring.datasource.url = jdbc:mysql://localhost:3306/apolloconfigdb?usessl=false&characterencoding=utf8\n\nspring.datasource.username = someuser\n\nspring.datasource.password = somepwd点击复制错误复制成功\n\n注：由于apolloconfigdb在每个环境都有部署，所以对不同的环境config-service需要配置对应环境的数据库参数\n\n2.2.1.1.2.2 配置apollo-adminservice的数据库连接信息\n\n 1. 解压apollo-adminservice-x.x.x-github.zip\n 2. 用程序员专用编辑器（如vim，notepad++，sublime等）打开config目录下的application-github.properties文件\n 3. 填写正确的apolloconfigdb数据库连接串信息，注意用户名和密码后面不要有空格!\n 4. 修改完的效果如下：\n\n\n# datasource\n\nspring.datasource.url = jdbc:mysql://localhost:3306/apolloconfigdb?usessl=false&characterencoding=utf8\n\nspring.datasource.username = someuser\n\nspring.datasource.password = somepwd点击复制错误复制成功\n\n注：由于apolloconfigdb在每个环境都有部署，所以对不同的环境admin-service需要配置对应环境的数据库参数\n\n2.2.1.1.2.3 配置apollo-portal的数据库连接信息\n\n 1. 解压apollo-portal-x.x.x-github.zip\n 2. 用程序员专用编辑器（如vim，notepad++，sublime等）打开config目录下的application-github.properties文件\n 3. 填写正确的apolloportaldb数据库连接串信息，注意用户名和密码后面不要有空格!\n 4. 修改完的效果如下：\n\n\n# datasource\n\nspring.datasource.url = jdbc:mysql://localhost:3306/apolloportaldb?usessl=false&characterencoding=utf8\n\nspring.datasource.username = someuser\n\nspring.datasource.password = somepwd点击复制错误复制成功\n\n2.2.1.1.2.4 配置apollo-portal的meta service信息\n\napollo portal需要在不同的环境访问不同的meta service(apollo-configservice)地址，所以我们需要在配置中提供这些信息。默认情况下，meta service和config service是部署在同一个jvm进程，所以meta service的地址就是config service的地址。\n\n对于1.6.0及以上版本，可以通过apolloportaldb.serverconfig中的配置项来配置meta service地址，详见apollo.portal.meta.servers - 各环境meta service列表\n\n使用程序员专用编辑器（如vim，notepad++，sublime等）打开apollo-portal-x.x.x-github.zip中config目录下的apollo-env.properties文件。\n\n假设dev的apollo-configservice未绑定域名，地址是1.1.1.1:8080，fat的apollo-configservice绑定了域名apollo.fat.xxx.com，uat的apollo-configservice绑定了域名apollo.uat.xxx.com，pro的apollo-configservice绑定了域名apollo.xxx.com，那么可以如下修改各环境meta service服务地址，格式为${env}.meta=http://${config-service-url:port}，如果某个环境不需要，也可以直接删除对应的配置项（如lpt.meta）：\n\ndev.meta=http://1.1.1.1:8080\n\nfat.meta=http://apollo.fat.xxx.com\n\nuat.meta=http://apollo.uat.xxx.com\n\npro.meta=http://apollo.xxx.com点击复制错误复制成功\n\n除了通过apollo-env.properties方式配置meta service以外，apollo也支持在运行时指定meta service（优先级比apollo-env.properties高）：\n\n 1. 通过java system property\n\n${env}_meta\n\n * 可以通过java的system property ${env}_meta来指定\n\n * 如java -ddev_meta=http://config-service-url -jar xxx.jar\n\n * 也可以通过程序指定，如system.setproperty("dev_meta", "http://config-service-url");\n\n 2. 通过操作系统的system environment\n\n${env}_meta\n\n * 如dev_meta=http://config-service-url\n * 注意key为全大写，且中间是_分隔\n\n注1: 为了实现meta service的高可用，推荐通过slb（software load balancer）做动态负载均衡\n\n注2: meta service地址也可以填入ip，0.11.0版本之前只支持填入一个ip。从0.11.0版本开始支持填入以逗号分隔的多个地址（pr #1214），如http://1.1.1.1:8080,http://2.2.2.2:8080，不过生产环境还是建议使用域名（走slb），因为机器扩容、缩容等都可能导致ip列表的变化。\n\n2.2.1.2 通过源码构建\n\n2.2.1.2.1 配置数据库连接信息\n\napollo服务端需要知道如何连接到你前面创建的数据库，所以需要编辑scripts/build.sh，修改apolloportaldb和apolloconfigdb相关的数据库连接串信息。\n\n注意：填入的用户需要具备对apolloportaldb和apolloconfigdb数据的读写权限。\n\n#apollo config db info\n\napollo_config_db_url=jdbc:mysql://localhost:3306/apolloconfigdb?usessl=false&characterencoding=utf8\n\napollo_config_db_username=用户名\n\napollo_config_db_password=密码（如果没有密码，留空即可）\n\n\n# apollo portal db info\n\napollo_portal_db_url=jdbc:mysql://localhost:3306/apolloportaldb?usessl=false&characterencoding=utf8\n\napollo_portal_db_username=用户名\n\napollo_portal_db_password=密码（如果没有密码，留空即可）点击复制错误复制成功\n\n注1：由于apolloconfigdb在每个环境都有部署，所以对不同的环境config-service和admin-service需要使用不同的数据库参数打不同的包，portal只需要打一次包即可\n\n注2：如果不想config-service和admin-service每个环境打一个包的话，也可以通过运行时传入数据库连接串信息实现，具体可以参考 issue 869\n\n注3：每个环境都需要独立部署一套config-service、admin-service和apolloconfigdb\n\n2.2.1.2.2 配置各环境meta service地址\n\napollo portal需要在不同的环境访问不同的meta service(apollo-configservice)地址，所以需要在打包时提供这些信息。\n\n假设dev的apollo-configservice未绑定域名，地址是1.1.1.1:8080，fat的apollo-configservice绑定了域名apollo.fat.xxx.com，uat的apollo-configservice绑定了域名apollo.uat.xxx.com，pro的apollo-configservice绑定了域名apollo.xxx.com，那么编辑scripts/build.sh，如下修改各环境meta service服务地址，格式为${env}_meta=http://${config-service-url:port}，如果某个环境不需要，也可以直接删除对应的配置项：\n\ndev_meta=http://1.1.1.1:8080\n\nfat_meta=http://apollo.fat.xxx.com\n\nuat_meta=http://apollo.uat.xxx.com\n\npro_meta=http://apollo.xxx.com\n\nmeta_servers_opts="-ddev_meta=$dev_meta -dfat_meta=$fat_meta -duat_meta=$uat_meta -dpro_meta=$pro_meta"\n\n除了在打包时配置meta service以外，apollo也支持在运行时指定meta service：\n\n 1. 通过java system property\n\n${env}_meta\n\n * 可以通过java的system property ${env}_meta来指定\n\n * 如java -ddev_meta=http://config-service-url -jar xxx.jar\n\n * 也可以通过程序指定，如system.setproperty("dev_meta", "http://config-service-url");\n\n 2. 通过操作系统的system environment\n\n${env}_meta\n\n * 如dev_meta=http://config-service-url\n * 注意key为全大写，且中间是_分隔\n\n注1: 为了实现meta service的高可用，推荐通过slb（software load balancer）做动态负载均衡\n\n注2: meta service地址也可以填入ip，0.11.0版本之前只支持填入一个ip。从0.11.0版本开始支持填入以逗号分隔的多个地址（pr #1214），如http://1.1.1.1:8080,http://2.2.2.2:8080，不过生产环境还是建议使用域名（走slb），因为机器扩容、缩容等都可能导致ip列表的变化。\n\n2.2.1.2.3 执行编译、打包\n\n做完上述配置后，就可以执行编译和打包了。\n\n注：初次编译会从maven中央仓库下载不少依赖，如果网络情况不佳时很容易出错，建议使用国内的maven仓库源，比如阿里云maven镜像\n\n./build.sh\n\n该脚本会依次打包apollo-configservice, apollo-adminservice, apollo-portal。\n\n注：由于apolloconfigdb在每个环境都有部署，所以对不同环境的config-service和admin-service需要使用不同的数据库连接信息打不同的包，portal只需要打一次包即可\n\n2.2.1.2.4 获取apollo-configservice安装包\n\n位于apollo-configservice/target/目录下的apollo-configservice-x.x.x-github.zip\n\n需要注意的是由于apolloconfigdb在每个环境都有部署，所以对不同环境的config-service需要使用不同的数据库参数打不同的包后分别部署\n\n2.2.1.2.5 获取apollo-adminservice安装包\n\n位于apollo-adminservice/target/目录下的apollo-adminservice-x.x.x-github.zip\n\n需要注意的是由于apolloconfigdb在每个环境都有部署，所以对不同环境的admin-service需要使用不同的数据库参数打不同的包后分别部署\n\n2.2.1.2.6 获取apollo-portal安装包\n\n位于apollo-portal/target/目录下的apollo-portal-x.x.x-github.zip\n\n2.2.1.2.7 启用外部nacos服务注册中心替换内置eureka\n\n 1. 修改build.sh/build.bat，将config-service和admin-service的maven编译命令更改为\n\nmvn clean package -pgithub,nacos-discovery -dskiptests -pl apollo-configservice,apollo-adminservice -am -dapollo_profile=github,nacos-discovery -dspring_datasource_url=$apollo_config_db_url -dspring_datasource_username=$apollo_config_db_username -dspring_datasource_password=$apollo_config_db_password点击复制错误复制成功\n\n 2. 分别修改apollo-configservice和apollo-adminservice安装包中config目录下的application-github.properties，配置nacos服务器地址\n\nnacos.discovery.server-addr=127.0.0.1:8848\n\n\n# 更多 nacos 配置\n\nnacos.discovery.access-key=\n\nnacos.discovery.username=\n\nnacos.discovery.password=\n\nnacos.discovery.secret-key=\n\nnacos.discovery.namespace=\n\nnacos.discovery.context-path=\n\n2.2.1.2.8 启用外部consul服务注册中心替换内置eureka\n\n 1. 修改build.sh/build.bat，将config-service和admin-service的maven编译命令更改为\n\nmvn clean package -pgithub -dskiptests -pl apollo-configservice,apollo-adminservice -am -dapollo_profile=github,consul-discovery -dspring_datasource_url=$apollo_config_db_url -dspring_datasource_username=$apollo_config_db_username -dspring_datasource_password=$apollo_config_db_password\n\n 2. 分别修改apollo-configservice和apollo-adminservice安装包中config目录下的application-github.properties，配置consul服务器地址\n\nspring.cloud.consul.host=127.0.0.1\n\nspring.cloud.consul.port=8500点\n\n2.2.2 部署apollo服务端\n\n2.2.2.1 部署apollo-configservice\n\n将对应环境的apollo-configservice-x.x.x-github.zip上传到服务器上，解压后执行scripts/startup.sh即可。如需停止服务，执行scripts/shutdown.sh.\n\n记得在scripts/startup.sh中按照实际的环境设置一个jvm内存，以下是我们的默认设置，供参考：\n\nexport java_opts="-server -xms6144m -xmx6144m -xss256k -xx:metaspacesize=128m -xx:maxmetaspacesize=384m -xx:newsize=4096m -xx:maxnewsize=4096m -xx:survivorratio=18"点击复制错误复制成功\n\n注1：如果需要修改jvm参数，可以修改scripts/startup.sh的java_opts部分。\n\n注2：如要调整服务的日志输出路径，可以修改scripts/startup.sh和apollo-configservice.conf中的log_dir。\n\n注3：如要调整服务的监听端口，可以修改scripts/startup.sh中的server_port。另外apollo-configservice同时承担meta server职责，如果要修改端口，注意要同时apolloconfigdb.serverconfig表中的eureka.service.url配置项以及apollo-portal和apollo-client中的使用到的meta server信息，详见：2.2.1.1.2.4 配置apollo-portal的meta service信息和1.2.2 apollo meta server。\n\n注4：如果apolloconfigdb.serverconfig的eureka.service.url只配了当前正在启动的机器的话，在启动apollo-configservice的过程中会在日志中输出eureka注册失败的信息，如com.sun.jersey.api.client.clienthandlerexception: java.net.connectexception: connection refused。需要注意的是，这个是预期的情况，因为apollo-configservice需要向meta server（它自己）注册服务，但是因为在启动过程中，自己还没起来，所以会报这个错。后面会进行重试的动作，所以等自己服务起来后就会注册正常了。\n\n注5：如果你看到了这里，相信你一定是一个细心阅读文档的人，而且离成功就差一点点了，继续加油，应该很快就能完成apollo的分布式部署了！不过你是否有感觉apollo的分布式部署步骤有点繁琐？是否有啥建议想要和作者说？如果答案是肯定的话，请移步 #1424，期待你的建议！\n\n2.2.2.2 部署apollo-adminservice\n\n将对应环境的apollo-adminservice-x.x.x-github.zip上传到服务器上，解压后执行scripts/startup.sh即可。如需停止服务，执行scripts/shutdown.sh.\n\n记得在scripts/startup.sh中按照实际的环境设置一个jvm内存，以下是我们的默认设置，供参考：\n\nexport java_opts="-server -xms2560m -xmx2560m -xss256k -xx:metaspacesize=128m -xx:maxmetaspacesize=384m -xx:newsize=1024m -xx:maxnewsize=1024m -xx:survivorratio=22"\n\n注1：如果需要修改jvm参数，可以修改scripts/startup.sh的java_opts部分。\n\n注2：如要调整服务的日志输出路径，可以修改scripts/startup.sh和apollo-adminservice.conf中的log_dir。\n\n注3：如要调整服务的监听端口，可以修改scripts/startup.sh中的server_port。\n\n2.2.2.3 部署apollo-portal\n\n将apollo-portal-x.x.x-github.zip上传到服务器上，解压后执行scripts/startup.sh即可。如需停止服务，执行scripts/shutdown.sh.\n\n记得在startup.sh中按照实际的环境设置一个jvm内存，以下是我们的默认设置，供参考：\n\nexport java_opts="-server -xms4096m -xmx4096m -xss256k -xx:metaspacesize=128m -xx:maxmetaspacesize=384m -xx:newsize=1536m -xx:maxnewsize=1536m -xx:survivorratio=22"\n\n注1：如果需要修改jvm参数，可以修改scripts/startup.sh的java_opts部分。\n\n注2：如要调整服务的日志输出路径，可以修改scripts/startup.sh和apollo-portal.conf中的log_dir。\n\n注3：如要调整服务的监听端口，可以修改scripts/startup.sh中的server_port。',charsets:{cjk:!0}},{title:"网络工具",frontmatter:{title:"网络工具",date:"2023-02-20T09:33:21.000Z",permalink:"/pages/38ce88/",categories:["运维","网络"],tags:[null],readingShow:"top",description:"在线 DNS 检测 ---online nslookup",meta:[{name:"twitter:title",content:"网络工具"},{name:"twitter:description",content:"在线 DNS 检测 ---online nslookup"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/04.%E7%BD%91%E7%BB%9C/03.%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7.html"},{property:"og:type",content:"article"},{property:"og:title",content:"网络工具"},{property:"og:description",content:"在线 DNS 检测 ---online nslookup"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/04.%E7%BD%91%E7%BB%9C/03.%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-20T09:33:21.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"网络工具"},{itemprop:"description",content:"在线 DNS 检测 ---online nslookup"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/04.%E7%BD%91%E7%BB%9C/03.%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7.html",relativePath:"04.运维/04.网络/03.网络工具.md",key:"v-66a7ae24",path:"/pages/38ce88/",headers:[{level:3,title:"网络工具",slug:"网络工具",normalizedTitle:"网络工具",charIndex:2}],headersStr:"网络工具",content:"# 网络工具\n\n在线 DNS 检测 ---online nslookup\n\nhttps://www.nslookup.io/\n\n全球ping检测 ---ping.pe\n\nhttps://ping.pe/\n\n内地 wegt 检测 ---网站测速工具boce.com\n\nhttps://www.boce.com/\n\n内地 + 部分常见海外地区 wegt 检测 ---网站测速 17ce.com\n\nhttps://www.17ce.com/\n\n亚信提供的 SSL/TLS 证书检测 ---证书检测myssl.com\n\nhttps://myssl.com/\n\n其它工具：GitHub - saveweb/tools: Save The Web Project 所使用的工具列表",normalizedContent:"# 网络工具\n\n在线 dns 检测 ---online nslookup\n\nhttps://www.nslookup.io/\n\n全球ping检测 ---ping.pe\n\nhttps://ping.pe/\n\n内地 wegt 检测 ---网站测速工具boce.com\n\nhttps://www.boce.com/\n\n内地 + 部分常见海外地区 wegt 检测 ---网站测速 17ce.com\n\nhttps://www.17ce.com/\n\n亚信提供的 ssl/tls 证书检测 ---证书检测myssl.com\n\nhttps://myssl.com/\n\n其它工具：github - saveweb/tools: save the web project 所使用的工具列表",charsets:{cjk:!0}},{title:"网络代理",frontmatter:{title:"网络代理",date:"2023-02-20T09:19:56.000Z",permalink:"/pages/5933ee/",categories:["运维","网络"],tags:[null],readingShow:"top",description:"：Proxy Server ，用于代替一个主机与其它主机进行通信。",meta:[{name:"twitter:title",content:"网络代理"},{name:"twitter:description",content:"：Proxy Server ，用于代替一个主机与其它主机进行通信。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/04.%E7%BD%91%E7%BB%9C/02.%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html"},{property:"og:type",content:"article"},{property:"og:title",content:"网络代理"},{property:"og:description",content:"：Proxy Server ，用于代替一个主机与其它主机进行通信。"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/04.%E7%BD%91%E7%BB%9C/02.%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-20T09:19:56.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"网络代理"},{itemprop:"description",content:"：Proxy Server ，用于代替一个主机与其它主机进行通信。"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/04.%E7%BD%91%E7%BB%9C/02.%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html",relativePath:"04.运维/04.网络/02.网络代理.md",key:"v-084ed8e0",path:"/pages/5933ee/",headers:[{level:2,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8)代理服务器",slug:"代理服务器",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#%e4%bb%a3%e7%90%86%e6%9c%8d%e5%8a%a1%e5%99%a8" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>代理服务器',charIndex:null},{level:3,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#squid)Squid",slug:"squid",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#squid" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>squid',charIndex:null},{level:4,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E9%83%A8%E7%BD%B2)部署",slug:"部署",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#%e9%83%a8%e7%bd%b2" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>部署',charIndex:null},{level:4,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E9%85%8D%E7%BD%AE)配置",slug:"配置",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#%e9%85%8d%e7%bd%ae" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>配置',charIndex:null},{level:3,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#socks5)SOCKS5",slug:"socks5",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#socks5" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>socks5',charIndex:null},{level:3,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#shadowsocks)Shadowsocks",slug:"shadowsocks",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#shadowsocks" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>shadowsocks',charIndex:null},{level:4,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#shadowsocks-libev)Shadowsocks-libev",slug:"shadowsocks-libev",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#shadowsocks-libev" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>shadowsocks-libev',charIndex:null},{level:4,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#shadowsocks-rust)Shadowsocks-rust",slug:"shadowsocks-rust",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#shadowsocks-rust" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>shadowsocks-rust',charIndex:null},{level:4,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E5%AE%A2%E6%88%B7%E7%AB%AF)客户端",slug:"客户端",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#%e5%ae%a2%e6%88%b7%e7%ab%af" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>客户端',charIndex:null},{level:3,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#v2ray)V2Ray",slug:"v2ray",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#v2ray" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>v2ray',charIndex:null},{level:3,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#trojan)Trojan",slug:"trojan",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#trojan" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>trojan',charIndex:null},{level:2,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E4%BB%A3%E7%90%86%E5%AE%A2%E6%88%B7%E7%AB%AF)代理客户端",slug:"代理客户端",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#%e4%bb%a3%e7%90%86%e5%ae%a2%e6%88%b7%e7%ab%af" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>代理客户端',charIndex:null},{level:3,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#proxifier)Proxifier",slug:"proxifier",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#proxifier" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>proxifier',charIndex:null},{level:3,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#proxychains)ProxyChains",slug:"proxychains",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#proxychains" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>proxychains',charIndex:null},{level:3,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#redsocks)redsocks",slug:"redsocks",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#redsocks" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>redsocks',charIndex:null},{level:3,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#clash)Clash",slug:"clash",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#clash" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>clash',charIndex:null},{level:2,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86)反向代理",slug:"反向代理",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#%e5%8f%8d%e5%90%91%e4%bb%a3%e7%90%86" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>反向代理',charIndex:null},{level:3,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#lvs)LVS",slug:"lvs",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#lvs" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>lvs',charIndex:null},{level:3,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#frp)frp",slug:"frp",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#frp" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>frp',charIndex:null},{level:2,title:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#vpn)VPN",slug:"vpn",normalizedTitle:'<a href="https://leohsiao.com/linux/%e7%bd%91%e7%bb%9c/%e7%bd%91%e7%bb%9c%e4%bb%a3%e7%90%86.html#vpn" target="_blank" rel="noopener noreferrer">#<outboundlink/></a>vpn',charIndex:null}],headersStr:"[#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8)代理服务器 [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#squid)Squid [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E9%83%A8%E7%BD%B2)部署 [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E9%85%8D%E7%BD%AE)配置 [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#socks5)SOCKS5 [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#shadowsocks)Shadowsocks [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#shadowsocks-libev)Shadowsocks-libev [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#shadowsocks-rust)Shadowsocks-rust [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E5%AE%A2%E6%88%B7%E7%AB%AF)客户端 [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#v2ray)V2Ray [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#trojan)Trojan [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E4%BB%A3%E7%90%86%E5%AE%A2%E6%88%B7%E7%AB%AF)代理客户端 [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#proxifier)Proxifier [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#proxychains)ProxyChains [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#redsocks)redsocks [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#clash)Clash [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86)反向代理 [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#lvs)LVS [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#frp)frp [#](https://leohsiao.com/Linux/%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86.html#vpn)VPN",content:'# #代理服务器\n\n：Proxy Server ，用于代替一个主机与其它主机进行通信。\n\n * 工作在会话层，代理应用层的消息。\n * 按协议分类：\n   * SSH 代理\n   * FTP 代理\n   * HTTP 代理\n   * HTTPS 代理\n   * SOCKS 代理：全称为 SOCKetS ，工作在应用层与传输层之间，比 HTTP 代理更底层、更快。\n     * SOCKS4 只支持 TCP 连接，而 SOCKS5 还支持 UDP 连接、密码认证。\n     * FTP、HTTP、SOCKS 代理都是明文通信，而 SSH、HTTPS、shadowsocks 代理是加密通信。\n * 按代理反向分类：\n   * 正向代理 ：侧重于代替客户端，向服务器发出访问请求。\n   * 反向代理 ：侧重于代替服务器，接收客户端的访问请求。\n * 用途：\n   * 可以使客户端访问到某些代理服务器才能访问的网络，像 VPN 的功能。\n   * 可以担任防火墙，过滤客户端发送、接收的数据。\n   * 可以动态更改将客户端的流量转发到哪个服务器，比如实现负载均衡。\n   * 可以隔离服务器与客户端，使得服务器不知道客户端的真实 IP 、客户端不知道服务器的真实 IP 。\n * 缺点：\n   * 客户端的通信数据都要经过代理服务器，可能被监听、篡改。\n\n\n# #Squid\n\n：一个代理服务器，采用 C++ 开发。\n\n * 官网(opens new window)\n * 支持 FTP、HTTP、HTTPS 代理协议。\n * 常用作简单的 HTTP 正向代理服务器。\n * 也可用于反向代理，缓存 HTTP 服务器的响应报文，但比 Nginx 的功能少。\n\n# #部署\n\n * 用 yum 安装：\n   \n   yum install squid\n   systemctl start squid\n   systemctl enable squid\n   \n   \n   1\n   2\n   3\n   \n\n * 或者用 docker-compose 部署：\n   \n   version: \'3\'\n   \n   services:\n     squid:\n       container_name: squid\n       image: sameersbn/squid:3.5.27-2\n       restart: unless-stopped\n       ports:\n         - 3128:3128\n       volumes:\n         - ./conf:/etc/squid/\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   \n\n# #配置\n\n用 yum 安装 Squid 时，开启正向代理的步骤：\n\n 1. 编辑配置文件 /etc/squid/squid.conf ：\n    \n    http_port 3128\n    \n    # 定义一个 acl 组，名为 local_ip ，指向源 IP 为 10.0.0.1/24 的流量\n    # acl local_ip src 10.0.0.1/24\n    \n    # 定义一个 acl 组，用密码文件进行身份认证\n    auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwd\n    acl auth_user proxy_auth REQUIRED\n    \n    # 允许指定 acl 组的流量\n    # http_access allow local_ip\n    http_access allow auth_user\n    # 禁止剩下的所有流量\n    http_access deny all\n    \n    # 不缓存所有响应报文\n    cache deny all\n    \n    # 默认会在 HTTP 请求 Headers 中加入 X-Forwarded-For ，声明客户端的真实 IP 。这里删除该 Header\n    forwarded_for delete\n    \n    # 记录访问日志\n    access_log /var/log/squid/access.log squid\n    # 每隔 30 天翻转一次日志\n    logfile_rotate 30\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    \n\n 2. 生成密码文件：\n    \n    yum install httpd-tools\n    htpasswd -cb /etc/squid/passwd leo ******\n    \n    \n    1\n    2\n    \n\n 3. 测试使用该代理：\n    \n    systemctl restart squid\n    curl -x http://leo:******@127.0.0.1:3128 cip.cc\n    \n    \n    1\n    2\n    \n\n\n# #SOCKS5\n\n * 用 docker-compose 部署一个 SOCKS5 服务器：\n   \n   version: \'3\'\n   \n   services:\n     socks5:\n       container_name: socks5\n       image: serjs/go-socks5-proxy\n       restart: unless-stopped\n       environment:\n         PROXY_USER: root\n         PROXY_PASSWORD: ******\n       ports:\n         - 1080:1080\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   \n   * 该镜像采用 Golang 开发，详见官方文档 (opens new window)。\n\n\n# #Shadowsocks\n\n：一种基于 SOCKS5 协议的代理协议，简称为 ss ，常用于实现 VPN 。\n\n * 官方文档(opens new window)\n * 支持设置密码、加密传输数据。\n * 原理：\n   1. 在一台主机上运行 ss 服务器。\n   2. 在本机运行 ss 客户端，将本机的流量发送到 ss 服务器，被它代理转发。\n * ShadowsocksR ：简称为 SSR ，是 shadowsocks 的分叉项目。\n\n# #Shadowsocks-libev\n\n：一个 ss 代理服务器软件。\n\n * 于 2015 年发布，采用 C 语言开发，于 2020 年停止更新。\n\n * 支持 AEAD Cipher 加密算法。\n\n * 支持 Obfs 混淆，可以将 ss 流量伪装成 http 流量。\n\n * 用 Docker 部署服务器：\n   \n   docker run -d --name shadowsocks \\\n           --restart unless-stopped \\\n           -p 8388:8388 \\\n           shadowsocks/shadowsocks-libev:v3.3.5 \\\n           ss-server -s 0.0.0.0 -k ****** -m aes-256-gcm\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n   \n   服务器的命令：\n   \n   ss-server               # 启动服务器\n           -c config.json  # 使用指定的配置文件\n           -s 0.0.0.0      # 设置服务器监听的 IP\n           -p 8388         # 设置服务器监听的端口\n           -k ******       # 设置服务器的认证密码\n           -m aes-256-gcm  # 设置服务器的加密方式\n           -v              # 显示详细日志\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   \n\n# #Shadowsocks-rust\n\n：一个 ss 代理服务器软件。\n\n * 用 Docker 部署服务器：\n   \n   docker run -d --name ssserver-rust \\\n           --restart unless-stopped \\\n           -p 8388:8388 \\\n           -v /root/shadowsocks-rust/config.json:/etc/shadowsocks-rust/config.json \\\n           ghcr.io/shadowsocks/ssserver-rust:v1.14.3\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n   \n   配置文件示例：\n   \n   {\n       "server": "0.0.0.0",\n       "server_port": 8388,\n       "password": "******",\n       "method": "chacha20-ietf-poly1305",\n       "fast_open": false\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   \n\n# #客户端\n\nWindows 版 ss 客户端 (opens new window)：\n\n * 运行 ss 客户端之后，它会连接到 ss 服务器，同时在本机监听一个代理端口。\n * 本机的进程可以通过 HTTP、Socket 代理协议，将数据发送到该代理端口，然后 ss 客户端会将这些数据转发到 ss 服务器，实现正向代理。\n * 也可以开启 ss 客户端的全局模式，代理本机的所有流量。\n\nLinux 版 ss 客户端 ：\n\n * 它是一个命令行工具，功能较少，不能连接多个 ss 服务器，在本机提供的代理端口只支持 Socket 协议。\n\n * 部署示例：\n   \n   # 安装\n   pip3 install shadowsocks\n   \n   # 编辑配置文件\n   cat > ss.json <<EOF{    "server": "10.0.0.1",    "server_port": 8388,    "local_address": "127.0.0.1",    "local_port": 8388,    "password": "******",    "timeout": 5,    "method": "aes-256-cfb"}EOF\n   \n   # 启动客户端\n   sslocal -c ss.json -d start\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   \n   \n   一个 sslocal 客户端只能连接一个 ss 服务器。\n\n * 可以再运行代理服务器 privoxy ，监听一个 HTTP 代理端口，将该端口的流量转发到 Socket 代理端口。\n   \n   # 安装\n   yum install -y privoxy\n   systemctl start privoxy\n   systemctl enable privoxy\n   \n   # 编辑配置文件\n   sed \'/listen-address  127.0.0.1:8118/d\' /etc/privoxy/config -i\n   cat >> /etc/privoxy/config <<EOFlisten-address  0.0.0.0:8118# 将 HTTP 请求转发到该代理forward-socks5  /   127.0.0.1:8388 .# 第一个字段为 url_pattern ，取值为 / 则匹配所有 HTTP 请求# 第二、三个字段为代理、父级代理，取值为 . 则表示忽略# 不代理这些 HTTP 请求forward         10.*.*.*/      .forward         127.*.*.*/     .forward         172.16.*.*/    .forward         192.168.*.*/   .EOF\n   \n   # 重启\n   systemctl restart privoxy\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   \n   \n   试用该代理：\n   \n   curl -x 127.0.0.1:8118 google.com\n   \n   \n   1\n   \n\n\n# #V2Ray\n\n：一个代理服务器，比 ss 的功能更多。\n\n * GitHub(opens new window)\n\n * 于 2019 年发布，采用 Goalng 语言开发。是 Project V 社区研发的主要软件，后来改名为 v2fly 。\n\n * V2Ray 服务器的流量分为两个方向：\n   \n   * inbounds ：入站流量，即客户端发送到 V2Ray 的流量。\n     * 客户端可使用 V2Ray 原创的 VMess 代理协议，也兼容 HTTP、Shadowsocks 等代理协议。\n   * outbounds ：出站流量，即 V2Ray 将客户端的流量转发到某个网站。\n\n * 用 Docker 部署服务器：\n   \n   docker run -d --name v2fly \\\n           --restart unless-stopped \\\n           -v /root/v2fly/config.json:/etc/v2fly/config.json \\\n           -p 80:80 \\\n           v2fly/v2fly-core:v5.1.0 \\\n           run -c /etc/v2fly/config.json\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   \n   \n   配置文件示例：\n   \n   {\n     "inbounds": [{\n       "port": 80,\n       "protocol": "vmess",\n       "settings": {\n         // 只允许这些 id 的客户端连接\n         "clients": [{ "id": "******" }]\n       }\n     }],\n     "outbounds": [{\n       "protocol": "freedom",\n       "settings": {}\n     }]\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   \n\n * 在 Windows 上常用的客户端软件是 v2rayN (opens new window)。\n\n\n# #Trojan\n\n：一个代理服务器。能加密通信，并伪装成互联网常见的 HTTPS 流量。\n\n * 于 2019 年发布，采用 C++ 语言开发。\n\n\n# #代理客户端\n\n * 有的程序不支持使用代理，有的程序只支持使用 HTTP 等简单的代理协议。因此建议安装一个专用的代理客户端，通过两种方式代理其它程序的流量：\n   * 透明代理（Transparent proxy）：\n     * 拦截其它程序发出的流量，转发到代理服务器。其它程序不会感知到代理，不需要主动使用代理。\n   * 转换代理协议：\n     * 通过某种代理协议连接到代理服务器，再在本机监听一个采用 HTTP 等简单代理协议的端口，将该端口收到的流量转发到代理服务器。\n     * 此时，具有代理功能的程序，一般都能通过该端口使用代理。但没有代理功能的程序，依然不能使用代理。\n\n\n# #Proxifier\n\n：一个 GUI 工具，用作代理客户端，收费。\n\n * 特点：\n   * 支持 Windows、MacOS 系统，不支持 Linux 。\n   * 支持 HTTP、HTTPS、SOCKS4、SOCKS5 代理协议。\n   * 支持透明代理。原理如下：\n     * Windows 系统上，一般程序需要调用 Winsock API 进行 Socket 通信。\n     * Winsock 提供了 LSP（Layered Service Provider） 功能：允许程序自定义一个 LSP DLL 库，在一般程序调用 Winsock API 时被加载，从而可以劫持其流量。\n   * 支持灵活的代理规则：将某个程序发向某 IP:Port 的 TCP 包，转发到某个代理服务器。支持通配符。\n   * 支持用多个代理服务器组成代理链。\n * 用法：\n   1. 安装 Proxifier 并启动。\n   2. 添加代理服务器。\n   3. 添加代理规则。\n   4. 保持 Proxifier 运行，即可代理本机程序的流量。\n\n\n# #ProxyChains\n\n：一个 Unix 系统的命令行工具，采用 C 语言开发，用作代理客户端。\n\n * GitHub(opens new window)\n * 支持透明代理。原理如下：\n   1. 声明环境变量 LD_PRELOAD ，让程序在导入 DLL 库时，优先导入 libproxychains4.so 库。\n   2. 在 libproxychains4.so 库中，重写关于 Socket 通信的函数，将程序发出的 TCP 流量转发到代理服务器。\n * 缺点：\n   * 只能代理 TCP 流量，不支持 UDP、ICMP 。\n   * 只能代理指定的程序，不能代理系统所有进程。\n   * 只能代理采用动态链接库的程序。\n     * Chrome 浏览器在沙盒中运行网页，也不会被代理。\n\n用法：\n\n 1. 安装：\n    \n    wget https://github.com/rofl0r/proxychains-ng/archive/refs/tags/v4.14.tar.gz\n    tar -zxvf v4.14.tar.gz\n    cd proxychains-ng-4.14\n    ./configure --prefix=/usr --sysconfdir=/etc\n    make\n    make install\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 2. 编辑配置文件 /etc/proxychains.conf ，示例：\n    \n    quiet_mode          # 安静模式，运行时不输出过程信息\n    dynamic_chain       # 自动选用 ProxyList 中可用的代理，且按顺序\n    proxy_dns           # 将 DNS 请求也代理\n    \n    # 取消代理发向指定 IP 的数据包\n    localnet 127.0.0.0/255.0.0.0\n    localnet 10.0.0.0/255.0.0.0\n    localnet 172.16.0.0/255.240.0.0\n    localnet 192.168.0.0/255.255.0.0\n    \n    # 发送数据包时，修改目标 IP:Port\n    dnat 1.1.1.1:80  1.1.1.2:443\n    dnat 1.1.1.1:443 1.1.1.2\n    dnat 1.1.1.1     1.1.1.2\n    \n    # 声明一组代理服务器，支持 HTTP、SOCKS4、SOCKS5 代理协议，支持设置账号密码\n    [ProxyList]\n    http    127.0.0.1   3128\n    socks5  127.0.0.1   1080  root  ******\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    \n\n 3. 执行命令：\n    \n    proxychains4 curl cip.cc  # 在代理下执行一条命令\n    proxychains4 bash         # 创建一个 shell ，在其中执行的命令都采用代理\n    \n    \n    1\n    2\n    \n\n\n# #redsocks\n\n：一个 Linux 系统的命令行工具，采用 C 语言开发，用作代理客户端。\n\n * GitHub(opens new window)\n * 支持透明代理。原理如下：\n   1. redsocks 连接到代理服务器，并在本机监听一个端口，将该端口收到的流量转发到代理服务器。\n   2. 用户手动配置 iptables 防火墙规则，将本机发出的 TCP 流量重定向到 redsocks 端口。\n * 缺点：\n   * 需要手动配置 iptables 规则，比较麻烦。\n   * 有些程序可能无视 iptables 规则。\n\n用法：\n\n 1. 用 apt 安装：\n    \n    apt install redsocks\n    \n    \n    1\n    \n    \n    或者手动编译后安装：\n    \n    git clone https://github.com/darkk/redsocks\n    cd redsocks\n    make\n    cp redsocks /usr/bin/redsocks\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 编辑配置文件 /etc/redsocks.conf ，示例：\n    \n    base {\n       log_debug  = off;\n       log_info   = on;\n       log        = stderr;    # 日志的保存位置，可改为 "file:/var/log/redsocks.log" 文件\n       daemon     = on;\n       redirector = iptables;\n    }\n    \n    redsocks {\n       // 在本机监听的端口\n       local_ip   = 127.0.0.1;\n       local_port = 12345;\n    \n       // 要连接的代理服务器\n       ip       = 10.0.0.1;\n       port     = 1080;\n       type     = socks5;\n       login    = <username>;\n       password = <password>;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    \n    * 可以定义多个 redsocks{} 代理配置。\n\n 3. 用 systemctl 保持运行：\n    \n    systemctl restart redsocks\n    systemctl enable  redsocks\n    \n    \n    1\n    2\n    \n    \n    或者手动启动：\n    \n    redsocks -c /etc/redsocks.conf\n    \n    \n    1\n    \n\n 4. 添加 iptables 规则：\n    \n    iptables -t nat -A OUTPUT -p tcp -d 192.168.1.0/24 -j REDIRECT --to-ports 12345   # 编辑 nat 表，将本机发出的某些流量重定向 REDSOCKS 端口\n    iptables -t nat -L OUTPUT   # 查看 iptables 规则\n    \n    \n    1\n    2\n    \n    \n    系统重启时会重置 iptables 规则。因此建议将配置 iptables 的命令保存为一个 shell 脚本，每次系统重启时执行它。\n\n\n# #Clash\n\n：一个通用的代理客户端，采用 Golang 开发。\n\n * GitHub(opens new window)\n * 支持 HTTP、SOCKS、Shadowsocks、Vmess 等多种代理协议。\n * 支持 Linux、MacOS、Windows、Android 系统。\n * 在 Linux 系统上支持透明代理。\n   * 原理与 redsocks 类似，需要手动配置 iptables 规则。\n\n\n# #反向代理\n\n实现反向代理的常见工具：\n\n * LVS\n * F5\n   * ：一个通过硬件实现负载均衡的服务器，基于 BIG-IP 协议。\n * HAProxy\n   * ：一个反向代理服务器，可以实现第四层的 TCP 代理、第七层的 HTTP 代理。\n * Nginx\n   * ：一个反向代理服务器，可以实现第四层的 TCP 代理、第七层的 HTTP 代理。\n * frp\n\n\n# #LVS\n\n：Linux 虚拟服务器（Linux Virtual Server），是 Linux 的一个内核模块，通过转发 TCP/UDP 数据包，实现第四层的反向代理。\n\n * LVS 工作在内核态，比 Nginx 等用户态代理的性能更高。\n\n * 原理：\n   \n   * 创建一些 IPVS（IP Virtual Server）作为负载均衡器。\n     * 每个 IPVS 绑定一个 Virtual IP ，称为 VIP 。\n     * 每个 IPVS 负责将将访问 VIP 的 TCP、UDP 流量反向代理到某些后端的 Real Server IP ，称为 RIP 。\n   * 配置 iptables 规则，将访问 VIP 的流量交给 IPVS 处理。\n\n * 有多种反向代理模式：\n   \n   * NAT 模式\n     * ：转发请求报文给后端时进行 DNAT ，转发响应报文给客户端时进行 SNAT ，实现透明代理。\n   * TUN 模式\n     * ：将客户端的请求报文通过 IP 隧道（Tunnel）转发给后端，而后端直接返回响应报文给客户端。\n     * 比 NAT 模式的速度更快。\n   * DR 模式\n     * ：通过改写请求报文的目标 MAC 地址，将它转发给后端。而后端直接返回响应报文给客户端。\n     * 比 TUN 模式的速度更快，但需要后端有一个独立的网卡。\n\n * 支持多种负载均衡算法：\n   \n   rr    # round robin（轮询），将请求报文轮流转发给各个后端，使得每个后端收到的请求数差不多相等。这是默认策略\n   wrr   # weight round robin（加权轮询），给每个后端设置权重，权重更大的后端，更大概率收到请求\n   \n   sh    # source hashing ，计算请求报文的源 IP 的哈希值，将哈希值相同的请求转发给同一个后端\n   dh    # destination hashing ，计算报文的目标 IP 的哈希值\n   \n   lc    # least connection（最少连接），监控每个后端的当前 TCP 连接数，将请求转发给连接数最少的后端\n   wlc   # weight least connection（加权的 lc ）\n   \n   lblc  # locality based least connections（基于局部的最少连接），记录每个请求的目标 IP 上一次使用的后端，作为转发规则，如果目标后端负载过大，则采用 lc 算法\n   lblcr # locality based least connections with replication（带复制的 lblc ），记录每个请求的目标 IP 使用的一组后端，采用 lc 从组中选出一个后端\n   \n   sed   # shortest expected delay ，监控每个后端的平均响应耗时，将请求转发给耗时最短的后端\n   nq    # never queue ，优先将请求转发给当前连接数为 0 的后端，如果不存在这样的后端，则采用 sed 算法\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   \n\n\n# #frp\n\n：一个命令行工具，提供了反向代理、内网穿透的功能。\n\n * GitHub(opens new window)\n * 采用 Golang 开发。\n * 支持 TCP、UDP、HTTP、HTTPS 等多种代理协议。\n * 采用 C/S 架构工作。\n   * frps ：服务器，部署在任一主机上，不需访问外部，只需被 frpc、用户访问。\n     * frps 所在主机通常拥有一个固定的外网 IP ，供外网用户访问。\n     * 用户向 frps 发送网络包时，会先被 frps 转发到 frpc ，再被 frpc 转发到内网服务。\n   * frpc ：客户端，部署在任一主机上，不需被外部访问，只需访问到 frps、内网服务。\n\n例：\n\n 1. 登录一台主机，下载 frp 的发行版，编辑配置文件 frps.ini ：\n    \n    [common]\n    bind_port = 7000        # 让 frps 监听一个端口，供 frpc 访问，用于它们内部通信\n    token = xxxxxx          # 用于 frpc 与 frps 之间的认证\n    \n    \n    1\n    2\n    3\n    \n    \n    启动服务器：\n    \n    ./frps -c frps.ini\n    \n    \n    1\n    \n    \n    或者用 docker-compose 部署：\n    \n    version: \'3\'\n    \n    services:\n     frps:\n       container_name: frps\n       image: snowdreamtech/frps:0.37.0\n       restart: unless-stopped\n       ports:\n         - 7000:7000\n       volumes:\n         - ./frps.ini:/etc/frp/frps.ini\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n\n 2. 登录另一个主机，编辑配置文件 frpc.ini ：\n    \n    [common]\n    server_addr = 1.1.1.1   # frps 的地址\n    server_port = 7000\n    token = xxxxxx\n    \n    # 定义一个反向代理，名称自定义\n    [proxy1]\n    type = tcp                  # 代理的类型\n    local_ip = 10.0.0.1\n    local_port = 80-90,443\n    remote_port = 80-90,443     # 让 frps 监听 remote_port 端口，供用户访问，并将该端口收到的网络包转发到 local_ip:local_port\n    # bandwidth_limit = 1MB     # 限制带宽，默认无限制\n    # use_compression = false   # frpc 与 frps 之间通信时，是否压缩\n    # use_encryption = false    # frpc 与 frps 之间通信时，是否加密\n    \n    # 可以定义多个反向代理\n    [proxy2]\n    ...\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    \n    \n    启动客户端：\n    \n    ./frpc -c frpc.ini\n    \n    \n    1\n    \n    \n    或者用 docker-compose 部署：\n    \n    version: \'3\'\n    \n    services:\n     frpc:\n       container_name: frpc\n       image: snowdreamtech/frpc:0.37.0\n       restart: unless-stopped\n       volumes:\n         - ./frpc.ini:/etc/frp/frpc.ini\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    \n\n 3. 让用户访问 frps 的反向代理端口：\n    \n    curl 1.1.1.1:10080\n    \n    \n    1\n    \n\n\n# #VPN\n\n：虚拟私有网络（Virtual Private Network），指由公网上的几台主机组成一个虚拟的私有网络。\n\n * 常用于实现正向代理。比如某个机房的各个服务器不暴露到公网，但机房的网关暴露到公网，用户可通过公网连接该网关，从而访问该机房的内网服务器。\n * VPN 软件与代理软件类似，不过通常会转发客户端整个主机的流量、加密通信内容，而代理软件不一定提供这些功能。\n\n原文链接：网络代理 | LeoHsiao\'s Notes',normalizedContent:'# #代理服务器\n\n：proxy server ，用于代替一个主机与其它主机进行通信。\n\n * 工作在会话层，代理应用层的消息。\n * 按协议分类：\n   * ssh 代理\n   * ftp 代理\n   * http 代理\n   * https 代理\n   * socks 代理：全称为 sockets ，工作在应用层与传输层之间，比 http 代理更底层、更快。\n     * socks4 只支持 tcp 连接，而 socks5 还支持 udp 连接、密码认证。\n     * ftp、http、socks 代理都是明文通信，而 ssh、https、shadowsocks 代理是加密通信。\n * 按代理反向分类：\n   * 正向代理 ：侧重于代替客户端，向服务器发出访问请求。\n   * 反向代理 ：侧重于代替服务器，接收客户端的访问请求。\n * 用途：\n   * 可以使客户端访问到某些代理服务器才能访问的网络，像 vpn 的功能。\n   * 可以担任防火墙，过滤客户端发送、接收的数据。\n   * 可以动态更改将客户端的流量转发到哪个服务器，比如实现负载均衡。\n   * 可以隔离服务器与客户端，使得服务器不知道客户端的真实 ip 、客户端不知道服务器的真实 ip 。\n * 缺点：\n   * 客户端的通信数据都要经过代理服务器，可能被监听、篡改。\n\n\n# #squid\n\n：一个代理服务器，采用 c++ 开发。\n\n * 官网(opens new window)\n * 支持 ftp、http、https 代理协议。\n * 常用作简单的 http 正向代理服务器。\n * 也可用于反向代理，缓存 http 服务器的响应报文，但比 nginx 的功能少。\n\n# #部署\n\n * 用 yum 安装：\n   \n   yum install squid\n   systemctl start squid\n   systemctl enable squid\n   \n   \n   1\n   2\n   3\n   \n\n * 或者用 docker-compose 部署：\n   \n   version: \'3\'\n   \n   services:\n     squid:\n       container_name: squid\n       image: sameersbn/squid:3.5.27-2\n       restart: unless-stopped\n       ports:\n         - 3128:3128\n       volumes:\n         - ./conf:/etc/squid/\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   \n\n# #配置\n\n用 yum 安装 squid 时，开启正向代理的步骤：\n\n 1. 编辑配置文件 /etc/squid/squid.conf ：\n    \n    http_port 3128\n    \n    # 定义一个 acl 组，名为 local_ip ，指向源 ip 为 10.0.0.1/24 的流量\n    # acl local_ip src 10.0.0.1/24\n    \n    # 定义一个 acl 组，用密码文件进行身份认证\n    auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwd\n    acl auth_user proxy_auth required\n    \n    # 允许指定 acl 组的流量\n    # http_access allow local_ip\n    http_access allow auth_user\n    # 禁止剩下的所有流量\n    http_access deny all\n    \n    # 不缓存所有响应报文\n    cache deny all\n    \n    # 默认会在 http 请求 headers 中加入 x-forwarded-for ，声明客户端的真实 ip 。这里删除该 header\n    forwarded_for delete\n    \n    # 记录访问日志\n    access_log /var/log/squid/access.log squid\n    # 每隔 30 天翻转一次日志\n    logfile_rotate 30\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    \n\n 2. 生成密码文件：\n    \n    yum install httpd-tools\n    htpasswd -cb /etc/squid/passwd leo ******\n    \n    \n    1\n    2\n    \n\n 3. 测试使用该代理：\n    \n    systemctl restart squid\n    curl -x http://leo:******@127.0.0.1:3128 cip.cc\n    \n    \n    1\n    2\n    \n\n\n# #socks5\n\n * 用 docker-compose 部署一个 socks5 服务器：\n   \n   version: \'3\'\n   \n   services:\n     socks5:\n       container_name: socks5\n       image: serjs/go-socks5-proxy\n       restart: unless-stopped\n       environment:\n         proxy_user: root\n         proxy_password: ******\n       ports:\n         - 1080:1080\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   \n   * 该镜像采用 golang 开发，详见官方文档 (opens new window)。\n\n\n# #shadowsocks\n\n：一种基于 socks5 协议的代理协议，简称为 ss ，常用于实现 vpn 。\n\n * 官方文档(opens new window)\n * 支持设置密码、加密传输数据。\n * 原理：\n   1. 在一台主机上运行 ss 服务器。\n   2. 在本机运行 ss 客户端，将本机的流量发送到 ss 服务器，被它代理转发。\n * shadowsocksr ：简称为 ssr ，是 shadowsocks 的分叉项目。\n\n# #shadowsocks-libev\n\n：一个 ss 代理服务器软件。\n\n * 于 2015 年发布，采用 c 语言开发，于 2020 年停止更新。\n\n * 支持 aead cipher 加密算法。\n\n * 支持 obfs 混淆，可以将 ss 流量伪装成 http 流量。\n\n * 用 docker 部署服务器：\n   \n   docker run -d --name shadowsocks \\\n           --restart unless-stopped \\\n           -p 8388:8388 \\\n           shadowsocks/shadowsocks-libev:v3.3.5 \\\n           ss-server -s 0.0.0.0 -k ****** -m aes-256-gcm\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n   \n   服务器的命令：\n   \n   ss-server               # 启动服务器\n           -c config.json  # 使用指定的配置文件\n           -s 0.0.0.0      # 设置服务器监听的 ip\n           -p 8388         # 设置服务器监听的端口\n           -k ******       # 设置服务器的认证密码\n           -m aes-256-gcm  # 设置服务器的加密方式\n           -v              # 显示详细日志\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   \n\n# #shadowsocks-rust\n\n：一个 ss 代理服务器软件。\n\n * 用 docker 部署服务器：\n   \n   docker run -d --name ssserver-rust \\\n           --restart unless-stopped \\\n           -p 8388:8388 \\\n           -v /root/shadowsocks-rust/config.json:/etc/shadowsocks-rust/config.json \\\n           ghcr.io/shadowsocks/ssserver-rust:v1.14.3\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n   \n   配置文件示例：\n   \n   {\n       "server": "0.0.0.0",\n       "server_port": 8388,\n       "password": "******",\n       "method": "chacha20-ietf-poly1305",\n       "fast_open": false\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   \n\n# #客户端\n\nwindows 版 ss 客户端 (opens new window)：\n\n * 运行 ss 客户端之后，它会连接到 ss 服务器，同时在本机监听一个代理端口。\n * 本机的进程可以通过 http、socket 代理协议，将数据发送到该代理端口，然后 ss 客户端会将这些数据转发到 ss 服务器，实现正向代理。\n * 也可以开启 ss 客户端的全局模式，代理本机的所有流量。\n\nlinux 版 ss 客户端 ：\n\n * 它是一个命令行工具，功能较少，不能连接多个 ss 服务器，在本机提供的代理端口只支持 socket 协议。\n\n * 部署示例：\n   \n   # 安装\n   pip3 install shadowsocks\n   \n   # 编辑配置文件\n   cat > ss.json <<eof{    "server": "10.0.0.1",    "server_port": 8388,    "local_address": "127.0.0.1",    "local_port": 8388,    "password": "******",    "timeout": 5,    "method": "aes-256-cfb"}eof\n   \n   # 启动客户端\n   sslocal -c ss.json -d start\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   \n   \n   一个 sslocal 客户端只能连接一个 ss 服务器。\n\n * 可以再运行代理服务器 privoxy ，监听一个 http 代理端口，将该端口的流量转发到 socket 代理端口。\n   \n   # 安装\n   yum install -y privoxy\n   systemctl start privoxy\n   systemctl enable privoxy\n   \n   # 编辑配置文件\n   sed \'/listen-address  127.0.0.1:8118/d\' /etc/privoxy/config -i\n   cat >> /etc/privoxy/config <<eoflisten-address  0.0.0.0:8118# 将 http 请求转发到该代理forward-socks5  /   127.0.0.1:8388 .# 第一个字段为 url_pattern ，取值为 / 则匹配所有 http 请求# 第二、三个字段为代理、父级代理，取值为 . 则表示忽略# 不代理这些 http 请求forward         10.*.*.*/      .forward         127.*.*.*/     .forward         172.16.*.*/    .forward         192.168.*.*/   .eof\n   \n   # 重启\n   systemctl restart privoxy\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   \n   \n   试用该代理：\n   \n   curl -x 127.0.0.1:8118 google.com\n   \n   \n   1\n   \n\n\n# #v2ray\n\n：一个代理服务器，比 ss 的功能更多。\n\n * github(opens new window)\n\n * 于 2019 年发布，采用 goalng 语言开发。是 project v 社区研发的主要软件，后来改名为 v2fly 。\n\n * v2ray 服务器的流量分为两个方向：\n   \n   * inbounds ：入站流量，即客户端发送到 v2ray 的流量。\n     * 客户端可使用 v2ray 原创的 vmess 代理协议，也兼容 http、shadowsocks 等代理协议。\n   * outbounds ：出站流量，即 v2ray 将客户端的流量转发到某个网站。\n\n * 用 docker 部署服务器：\n   \n   docker run -d --name v2fly \\\n           --restart unless-stopped \\\n           -v /root/v2fly/config.json:/etc/v2fly/config.json \\\n           -p 80:80 \\\n           v2fly/v2fly-core:v5.1.0 \\\n           run -c /etc/v2fly/config.json\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   \n   \n   配置文件示例：\n   \n   {\n     "inbounds": [{\n       "port": 80,\n       "protocol": "vmess",\n       "settings": {\n         // 只允许这些 id 的客户端连接\n         "clients": [{ "id": "******" }]\n       }\n     }],\n     "outbounds": [{\n       "protocol": "freedom",\n       "settings": {}\n     }]\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   \n\n * 在 windows 上常用的客户端软件是 v2rayn (opens new window)。\n\n\n# #trojan\n\n：一个代理服务器。能加密通信，并伪装成互联网常见的 https 流量。\n\n * 于 2019 年发布，采用 c++ 语言开发。\n\n\n# #代理客户端\n\n * 有的程序不支持使用代理，有的程序只支持使用 http 等简单的代理协议。因此建议安装一个专用的代理客户端，通过两种方式代理其它程序的流量：\n   * 透明代理（transparent proxy）：\n     * 拦截其它程序发出的流量，转发到代理服务器。其它程序不会感知到代理，不需要主动使用代理。\n   * 转换代理协议：\n     * 通过某种代理协议连接到代理服务器，再在本机监听一个采用 http 等简单代理协议的端口，将该端口收到的流量转发到代理服务器。\n     * 此时，具有代理功能的程序，一般都能通过该端口使用代理。但没有代理功能的程序，依然不能使用代理。\n\n\n# #proxifier\n\n：一个 gui 工具，用作代理客户端，收费。\n\n * 特点：\n   * 支持 windows、macos 系统，不支持 linux 。\n   * 支持 http、https、socks4、socks5 代理协议。\n   * 支持透明代理。原理如下：\n     * windows 系统上，一般程序需要调用 winsock api 进行 socket 通信。\n     * winsock 提供了 lsp（layered service provider） 功能：允许程序自定义一个 lsp dll 库，在一般程序调用 winsock api 时被加载，从而可以劫持其流量。\n   * 支持灵活的代理规则：将某个程序发向某 ip:port 的 tcp 包，转发到某个代理服务器。支持通配符。\n   * 支持用多个代理服务器组成代理链。\n * 用法：\n   1. 安装 proxifier 并启动。\n   2. 添加代理服务器。\n   3. 添加代理规则。\n   4. 保持 proxifier 运行，即可代理本机程序的流量。\n\n\n# #proxychains\n\n：一个 unix 系统的命令行工具，采用 c 语言开发，用作代理客户端。\n\n * github(opens new window)\n * 支持透明代理。原理如下：\n   1. 声明环境变量 ld_preload ，让程序在导入 dll 库时，优先导入 libproxychains4.so 库。\n   2. 在 libproxychains4.so 库中，重写关于 socket 通信的函数，将程序发出的 tcp 流量转发到代理服务器。\n * 缺点：\n   * 只能代理 tcp 流量，不支持 udp、icmp 。\n   * 只能代理指定的程序，不能代理系统所有进程。\n   * 只能代理采用动态链接库的程序。\n     * chrome 浏览器在沙盒中运行网页，也不会被代理。\n\n用法：\n\n 1. 安装：\n    \n    wget https://github.com/rofl0r/proxychains-ng/archive/refs/tags/v4.14.tar.gz\n    tar -zxvf v4.14.tar.gz\n    cd proxychains-ng-4.14\n    ./configure --prefix=/usr --sysconfdir=/etc\n    make\n    make install\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 2. 编辑配置文件 /etc/proxychains.conf ，示例：\n    \n    quiet_mode          # 安静模式，运行时不输出过程信息\n    dynamic_chain       # 自动选用 proxylist 中可用的代理，且按顺序\n    proxy_dns           # 将 dns 请求也代理\n    \n    # 取消代理发向指定 ip 的数据包\n    localnet 127.0.0.0/255.0.0.0\n    localnet 10.0.0.0/255.0.0.0\n    localnet 172.16.0.0/255.240.0.0\n    localnet 192.168.0.0/255.255.0.0\n    \n    # 发送数据包时，修改目标 ip:port\n    dnat 1.1.1.1:80  1.1.1.2:443\n    dnat 1.1.1.1:443 1.1.1.2\n    dnat 1.1.1.1     1.1.1.2\n    \n    # 声明一组代理服务器，支持 http、socks4、socks5 代理协议，支持设置账号密码\n    [proxylist]\n    http    127.0.0.1   3128\n    socks5  127.0.0.1   1080  root  ******\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    \n\n 3. 执行命令：\n    \n    proxychains4 curl cip.cc  # 在代理下执行一条命令\n    proxychains4 bash         # 创建一个 shell ，在其中执行的命令都采用代理\n    \n    \n    1\n    2\n    \n\n\n# #redsocks\n\n：一个 linux 系统的命令行工具，采用 c 语言开发，用作代理客户端。\n\n * github(opens new window)\n * 支持透明代理。原理如下：\n   1. redsocks 连接到代理服务器，并在本机监听一个端口，将该端口收到的流量转发到代理服务器。\n   2. 用户手动配置 iptables 防火墙规则，将本机发出的 tcp 流量重定向到 redsocks 端口。\n * 缺点：\n   * 需要手动配置 iptables 规则，比较麻烦。\n   * 有些程序可能无视 iptables 规则。\n\n用法：\n\n 1. 用 apt 安装：\n    \n    apt install redsocks\n    \n    \n    1\n    \n    \n    或者手动编译后安装：\n    \n    git clone https://github.com/darkk/redsocks\n    cd redsocks\n    make\n    cp redsocks /usr/bin/redsocks\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 编辑配置文件 /etc/redsocks.conf ，示例：\n    \n    base {\n       log_debug  = off;\n       log_info   = on;\n       log        = stderr;    # 日志的保存位置，可改为 "file:/var/log/redsocks.log" 文件\n       daemon     = on;\n       redirector = iptables;\n    }\n    \n    redsocks {\n       // 在本机监听的端口\n       local_ip   = 127.0.0.1;\n       local_port = 12345;\n    \n       // 要连接的代理服务器\n       ip       = 10.0.0.1;\n       port     = 1080;\n       type     = socks5;\n       login    = <username>;\n       password = <password>;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    \n    * 可以定义多个 redsocks{} 代理配置。\n\n 3. 用 systemctl 保持运行：\n    \n    systemctl restart redsocks\n    systemctl enable  redsocks\n    \n    \n    1\n    2\n    \n    \n    或者手动启动：\n    \n    redsocks -c /etc/redsocks.conf\n    \n    \n    1\n    \n\n 4. 添加 iptables 规则：\n    \n    iptables -t nat -a output -p tcp -d 192.168.1.0/24 -j redirect --to-ports 12345   # 编辑 nat 表，将本机发出的某些流量重定向 redsocks 端口\n    iptables -t nat -l output   # 查看 iptables 规则\n    \n    \n    1\n    2\n    \n    \n    系统重启时会重置 iptables 规则。因此建议将配置 iptables 的命令保存为一个 shell 脚本，每次系统重启时执行它。\n\n\n# #clash\n\n：一个通用的代理客户端，采用 golang 开发。\n\n * github(opens new window)\n * 支持 http、socks、shadowsocks、vmess 等多种代理协议。\n * 支持 linux、macos、windows、android 系统。\n * 在 linux 系统上支持透明代理。\n   * 原理与 redsocks 类似，需要手动配置 iptables 规则。\n\n\n# #反向代理\n\n实现反向代理的常见工具：\n\n * lvs\n * f5\n   * ：一个通过硬件实现负载均衡的服务器，基于 big-ip 协议。\n * haproxy\n   * ：一个反向代理服务器，可以实现第四层的 tcp 代理、第七层的 http 代理。\n * nginx\n   * ：一个反向代理服务器，可以实现第四层的 tcp 代理、第七层的 http 代理。\n * frp\n\n\n# #lvs\n\n：linux 虚拟服务器（linux virtual server），是 linux 的一个内核模块，通过转发 tcp/udp 数据包，实现第四层的反向代理。\n\n * lvs 工作在内核态，比 nginx 等用户态代理的性能更高。\n\n * 原理：\n   \n   * 创建一些 ipvs（ip virtual server）作为负载均衡器。\n     * 每个 ipvs 绑定一个 virtual ip ，称为 vip 。\n     * 每个 ipvs 负责将将访问 vip 的 tcp、udp 流量反向代理到某些后端的 real server ip ，称为 rip 。\n   * 配置 iptables 规则，将访问 vip 的流量交给 ipvs 处理。\n\n * 有多种反向代理模式：\n   \n   * nat 模式\n     * ：转发请求报文给后端时进行 dnat ，转发响应报文给客户端时进行 snat ，实现透明代理。\n   * tun 模式\n     * ：将客户端的请求报文通过 ip 隧道（tunnel）转发给后端，而后端直接返回响应报文给客户端。\n     * 比 nat 模式的速度更快。\n   * dr 模式\n     * ：通过改写请求报文的目标 mac 地址，将它转发给后端。而后端直接返回响应报文给客户端。\n     * 比 tun 模式的速度更快，但需要后端有一个独立的网卡。\n\n * 支持多种负载均衡算法：\n   \n   rr    # round robin（轮询），将请求报文轮流转发给各个后端，使得每个后端收到的请求数差不多相等。这是默认策略\n   wrr   # weight round robin（加权轮询），给每个后端设置权重，权重更大的后端，更大概率收到请求\n   \n   sh    # source hashing ，计算请求报文的源 ip 的哈希值，将哈希值相同的请求转发给同一个后端\n   dh    # destination hashing ，计算报文的目标 ip 的哈希值\n   \n   lc    # least connection（最少连接），监控每个后端的当前 tcp 连接数，将请求转发给连接数最少的后端\n   wlc   # weight least connection（加权的 lc ）\n   \n   lblc  # locality based least connections（基于局部的最少连接），记录每个请求的目标 ip 上一次使用的后端，作为转发规则，如果目标后端负载过大，则采用 lc 算法\n   lblcr # locality based least connections with replication（带复制的 lblc ），记录每个请求的目标 ip 使用的一组后端，采用 lc 从组中选出一个后端\n   \n   sed   # shortest expected delay ，监控每个后端的平均响应耗时，将请求转发给耗时最短的后端\n   nq    # never queue ，优先将请求转发给当前连接数为 0 的后端，如果不存在这样的后端，则采用 sed 算法\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   \n\n\n# #frp\n\n：一个命令行工具，提供了反向代理、内网穿透的功能。\n\n * github(opens new window)\n * 采用 golang 开发。\n * 支持 tcp、udp、http、https 等多种代理协议。\n * 采用 c/s 架构工作。\n   * frps ：服务器，部署在任一主机上，不需访问外部，只需被 frpc、用户访问。\n     * frps 所在主机通常拥有一个固定的外网 ip ，供外网用户访问。\n     * 用户向 frps 发送网络包时，会先被 frps 转发到 frpc ，再被 frpc 转发到内网服务。\n   * frpc ：客户端，部署在任一主机上，不需被外部访问，只需访问到 frps、内网服务。\n\n例：\n\n 1. 登录一台主机，下载 frp 的发行版，编辑配置文件 frps.ini ：\n    \n    [common]\n    bind_port = 7000        # 让 frps 监听一个端口，供 frpc 访问，用于它们内部通信\n    token = xxxxxx          # 用于 frpc 与 frps 之间的认证\n    \n    \n    1\n    2\n    3\n    \n    \n    启动服务器：\n    \n    ./frps -c frps.ini\n    \n    \n    1\n    \n    \n    或者用 docker-compose 部署：\n    \n    version: \'3\'\n    \n    services:\n     frps:\n       container_name: frps\n       image: snowdreamtech/frps:0.37.0\n       restart: unless-stopped\n       ports:\n         - 7000:7000\n       volumes:\n         - ./frps.ini:/etc/frp/frps.ini\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n\n 2. 登录另一个主机，编辑配置文件 frpc.ini ：\n    \n    [common]\n    server_addr = 1.1.1.1   # frps 的地址\n    server_port = 7000\n    token = xxxxxx\n    \n    # 定义一个反向代理，名称自定义\n    [proxy1]\n    type = tcp                  # 代理的类型\n    local_ip = 10.0.0.1\n    local_port = 80-90,443\n    remote_port = 80-90,443     # 让 frps 监听 remote_port 端口，供用户访问，并将该端口收到的网络包转发到 local_ip:local_port\n    # bandwidth_limit = 1mb     # 限制带宽，默认无限制\n    # use_compression = false   # frpc 与 frps 之间通信时，是否压缩\n    # use_encryption = false    # frpc 与 frps 之间通信时，是否加密\n    \n    # 可以定义多个反向代理\n    [proxy2]\n    ...\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    \n    \n    启动客户端：\n    \n    ./frpc -c frpc.ini\n    \n    \n    1\n    \n    \n    或者用 docker-compose 部署：\n    \n    version: \'3\'\n    \n    services:\n     frpc:\n       container_name: frpc\n       image: snowdreamtech/frpc:0.37.0\n       restart: unless-stopped\n       volumes:\n         - ./frpc.ini:/etc/frp/frpc.ini\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    \n\n 3. 让用户访问 frps 的反向代理端口：\n    \n    curl 1.1.1.1:10080\n    \n    \n    1\n    \n\n\n# #vpn\n\n：虚拟私有网络（virtual private network），指由公网上的几台主机组成一个虚拟的私有网络。\n\n * 常用于实现正向代理。比如某个机房的各个服务器不暴露到公网，但机房的网关暴露到公网，用户可通过公网连接该网关，从而访问该机房的内网服务器。\n * vpn 软件与代理软件类似，不过通常会转发客户端整个主机的流量、加密通信内容，而代理软件不一定提供这些功能。\n\n原文链接：网络代理 | leohsiao\'s notes',charsets:{cjk:!0}},{title:"docker部署openvas",frontmatter:{title:"docker部署openvas",date:"2022-12-15T19:21:33.000Z",permalink:"/pages/7f4bdd/",categories:["运维","安全"],tags:[null],readingShow:"top",description:"学安全的人想必都知道openvas，他是一个开源的离线漏洞评估系统，有非常丰富的漏洞库。不过这也是让人烦恼的一个地方，有丰富的漏洞库然后又是离线的就代表安装的时候要下载很多很多数据包。光openvas本身就要下载1G的安装包，更新漏洞库的时候还有下载1G的包 重点是openvas的下载服务器在国外，对我们这种有长城保护的良民来说就是10几K的下载速度\n最近无意间看到github上面有个openvas的开源项目，把openvas打包成一个docker容器，部署只需要启动一个容器，快速，方便还不占空间。github项目地址：https://github.com/mikesplain/openvas-docker",meta:[{name:"image",content:"https://img-blog.csdnimg.cn/20181107134156923.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3doYXRkYXk=,size_16,color_FFFFFF,t_70"},{name:"twitter:title",content:"docker部署openvas"},{name:"twitter:description",content:"学安全的人想必都知道openvas，他是一个开源的离线漏洞评估系统，有非常丰富的漏洞库。不过这也是让人烦恼的一个地方，有丰富的漏洞库然后又是离线的就代表安装的时候要下载很多很多数据包。光openvas本身就要下载1G的安装包，更新漏洞库的时候还有下载1G的包 重点是openvas的下载服务器在国外，对我们这种有长城保护的良民来说就是10几K的下载速度\n最近无意间看到github上面有个openvas的开源项目，把openvas打包成一个docker容器，部署只需要启动一个容器，快速，方便还不占空间。github项目地址：https://github.com/mikesplain/openvas-docker"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://img-blog.csdnimg.cn/20181107134156923.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3doYXRkYXk=,size_16,color_FFFFFF,t_70"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/01.docker%E9%83%A8%E7%BD%B2openvas.html"},{property:"og:type",content:"article"},{property:"og:title",content:"docker部署openvas"},{property:"og:description",content:"学安全的人想必都知道openvas，他是一个开源的离线漏洞评估系统，有非常丰富的漏洞库。不过这也是让人烦恼的一个地方，有丰富的漏洞库然后又是离线的就代表安装的时候要下载很多很多数据包。光openvas本身就要下载1G的安装包，更新漏洞库的时候还有下载1G的包 重点是openvas的下载服务器在国外，对我们这种有长城保护的良民来说就是10几K的下载速度\n最近无意间看到github上面有个openvas的开源项目，把openvas打包成一个docker容器，部署只需要启动一个容器，快速，方便还不占空间。github项目地址：https://github.com/mikesplain/openvas-docker"},{property:"og:image",content:"https://img-blog.csdnimg.cn/20181107134156923.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3doYXRkYXk=,size_16,color_FFFFFF,t_70"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/01.docker%E9%83%A8%E7%BD%B2openvas.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T19:21:33.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"docker部署openvas"},{itemprop:"description",content:"学安全的人想必都知道openvas，他是一个开源的离线漏洞评估系统，有非常丰富的漏洞库。不过这也是让人烦恼的一个地方，有丰富的漏洞库然后又是离线的就代表安装的时候要下载很多很多数据包。光openvas本身就要下载1G的安装包，更新漏洞库的时候还有下载1G的包 重点是openvas的下载服务器在国外，对我们这种有长城保护的良民来说就是10几K的下载速度\n最近无意间看到github上面有个openvas的开源项目，把openvas打包成一个docker容器，部署只需要启动一个容器，快速，方便还不占空间。github项目地址：https://github.com/mikesplain/openvas-docker"},{itemprop:"image",content:"https://img-blog.csdnimg.cn/20181107134156923.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3doYXRkYXk=,size_16,color_FFFFFF,t_70"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/01.docker%E9%83%A8%E7%BD%B2openvas.html",relativePath:"04.运维/05.安全/01.docker部署openvas.md",key:"v-9198a8e6",path:"/pages/7f4bdd/",headersStr:null,content:"学安全的人想必都知道openvas，他是一个开源的离线漏洞评估系统，有非常丰富的漏洞库。不过这也是让人烦恼的一个地方，有丰富的漏洞库然后又是离线的就代表安装的时候要下载很多很多数据包。光openvas本身就要下载1G的安装包，更新漏洞库的时候还有下载1G的包 重点是openvas的下载服务器在国外，对我们这种有长城保护的良民来说就是10几K的下载速度\n最近无意间看到github上面有个openvas的开源项目，把openvas打包成一个docker容器，部署只需要启动一个容器，快速，方便还不占空间。github项目地址：https://github.com/mikesplain/openvas-docker\n\n1，首先你要安装个docker\n源安装很简单，Debian：apt install lxc-docker，Centos：yum install docker-io\n\n2，搜索容器：\nroot@controller-node:~#docker search openvas\n\n\n\n3，下载openvas9版本的容器：\nroot@controller-node:~#docker pull mikesplain/openvas:9\n\n\n\n4，后台运行容器然后映射443端口：\n注意如果宿主机443端口有在使用应映射到其他端口\nroot@controller-node:~#docker run -d -p 443:443 --name openvas mikesplain/openvas:9\n注：启动容器后发现它很容易在扫描的时候把宿主机进程跑满，最好限制下资源使用\nroot@controller-node:~#docker run --cpuset-cpus=4 -m 8192M -d -p 443:443 --name openvas mikesplain/openvas:9\n\n\n\n好了，现在我们的openvas环境已经部署好了，全程不到半小时.\n现在我们访问宿主机的web就可以进入openvas web了\nhttps://127.0.0.1 默认账号:admin 默认密码：admin\n\n如果想在局域网访问宿主机中的docker openvas 需要设置 openvas 的dns hostname 具体如下\n\ndocker run -d -p 443:443 -e PUBLIC_HOSTNAME=192.168.6.141 --name openvas mikesplain/openvas\n\n\n1\n\n\n还要设置宿主机防火墙 centos7 关闭防火墙命令：systemctl stop firewalld.service\n\n其中192.168.6.141是宿主机的IP 这样设置后再局域网就可以访问openvas了\n\ndocker 中的openvas启动需要一点时间 等待一分钟后访问 以下网址\n\nhttps://192.168.6.141 默认账号:admin 默认密码：admin",normalizedContent:"学安全的人想必都知道openvas，他是一个开源的离线漏洞评估系统，有非常丰富的漏洞库。不过这也是让人烦恼的一个地方，有丰富的漏洞库然后又是离线的就代表安装的时候要下载很多很多数据包。光openvas本身就要下载1g的安装包，更新漏洞库的时候还有下载1g的包 重点是openvas的下载服务器在国外，对我们这种有长城保护的良民来说就是10几k的下载速度\n最近无意间看到github上面有个openvas的开源项目，把openvas打包成一个docker容器，部署只需要启动一个容器，快速，方便还不占空间。github项目地址：https://github.com/mikesplain/openvas-docker\n\n1，首先你要安装个docker\n源安装很简单，debian：apt install lxc-docker，centos：yum install docker-io\n\n2，搜索容器：\nroot@controller-node:~#docker search openvas\n\n\n\n3，下载openvas9版本的容器：\nroot@controller-node:~#docker pull mikesplain/openvas:9\n\n\n\n4，后台运行容器然后映射443端口：\n注意如果宿主机443端口有在使用应映射到其他端口\nroot@controller-node:~#docker run -d -p 443:443 --name openvas mikesplain/openvas:9\n注：启动容器后发现它很容易在扫描的时候把宿主机进程跑满，最好限制下资源使用\nroot@controller-node:~#docker run --cpuset-cpus=4 -m 8192m -d -p 443:443 --name openvas mikesplain/openvas:9\n\n\n\n好了，现在我们的openvas环境已经部署好了，全程不到半小时.\n现在我们访问宿主机的web就可以进入openvas web了\nhttps://127.0.0.1 默认账号:admin 默认密码：admin\n\n如果想在局域网访问宿主机中的docker openvas 需要设置 openvas 的dns hostname 具体如下\n\ndocker run -d -p 443:443 -e public_hostname=192.168.6.141 --name openvas mikesplain/openvas\n\n\n1\n\n\n还要设置宿主机防火墙 centos7 关闭防火墙命令：systemctl stop firewalld.service\n\n其中192.168.6.141是宿主机的ip 这样设置后再局域网就可以访问openvas了\n\ndocker 中的openvas启动需要一点时间 等待一分钟后访问 以下网址\n\nhttps://192.168.6.141 默认账号:admin 默认密码：admin",charsets:{cjk:!0}},{title:"抓包工具 tcpdump 用法说明",frontmatter:{title:"抓包工具 tcpdump 用法说明",date:"2022-12-15T19:00:25.000Z",permalink:"/pages/2f613d/",categories:["运维","网络"],tags:[null],readingShow:"top",description:"tcpdump采用命令行方式对接口的数据包进行筛选抓取，其丰富特性表现在灵活的表达式上。",meta:[{name:"twitter:title",content:"抓包工具 tcpdump 用法说明"},{name:"twitter:description",content:"tcpdump采用命令行方式对接口的数据包进行筛选抓取，其丰富特性表现在灵活的表达式上。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/04.%E7%BD%91%E7%BB%9C/01.%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%20tcpdump%20%E7%94%A8%E6%B3%95%E8%AF%B4%E6%98%8E.html"},{property:"og:type",content:"article"},{property:"og:title",content:"抓包工具 tcpdump 用法说明"},{property:"og:description",content:"tcpdump采用命令行方式对接口的数据包进行筛选抓取，其丰富特性表现在灵活的表达式上。"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/04.%E7%BD%91%E7%BB%9C/01.%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%20tcpdump%20%E7%94%A8%E6%B3%95%E8%AF%B4%E6%98%8E.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T19:00:25.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"抓包工具 tcpdump 用法说明"},{itemprop:"description",content:"tcpdump采用命令行方式对接口的数据包进行筛选抓取，其丰富特性表现在灵活的表达式上。"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/04.%E7%BD%91%E7%BB%9C/01.%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%20tcpdump%20%E7%94%A8%E6%B3%95%E8%AF%B4%E6%98%8E.html",relativePath:"04.运维/04.网络/01.抓包工具 tcpdump 用法说明.md",key:"v-28967f21",path:"/pages/2f613d/",headersStr:null,content:'tcpdump采用命令行方式对接口的数据包进行筛选抓取，其丰富特性表现在灵活的表达式上。\n\n不带任何选项的tcpdump，默认会抓取第一个网络接口，且只有将tcpdump进程终止才会停止抓包。\n\n例如：\n\nshell> tcpdump -nn -i eth0 icmp\n\n下面是详细的tcpdump用法。\n\n1.1 tcpdump选项\n\n它的命令格式为：\n\ntcpdump [ -DenNqvX ] [ -c count ] [ -F file ] [ -i interface ] [ -r file ]\n\n[ -s snaplen ] [ -w file ] [ expression ]\n\n抓包选项：\n\n-c：指定要抓取的包数量。注意，是最终要获取这么多个包。例如，指定"-c 10"将获取10个包，但可能已经处理了100个包，只不过只有10个包是满足条件的包。\n\n-i interface：指定tcpdump需要监听的接口。若未指定该选项，将从系统接口列表中搜寻编号最小的已配置好的接口(不包括loopback接口，要抓取loopback接口使用tcpdump -i lo)，\n\n：一旦找到第一个符合条件的接口，搜寻马上结束。可以使用\'any\'关键字表示所有网络接口。\n\n-n：对地址以数字方式显式，否则显式为主机名，也就是说-n选项不做主机名解析。\n\n-nn：除了-n的作用外，还把端口显示为数值，否则显示端口服务名。\n\n-N：不打印出host的域名部分。例如tcpdump将会打印\'nic\'而不是\'nic.ddn.mil\'。\n\n-P：指定要抓取的包是流入还是流出的包。可以给定的值为"in"、"out"和"inout"，默认为"inout"。\n\n-s len：设置tcpdump的数据包抓取长度为len，如果不设置默认将会是65535字节。对于要抓取的数据包较大时，长度设置不够可能会产生包截断，若出现包截断，\n\n：输出行中会出现"[|proto]"的标志(proto实际会显示为协议名)。但是抓取len越长，包的处理时间越长，并且会减少tcpdump可缓存的数据包的数量，\n\n：从而会导致数据包的丢失，所以在能抓取我们想要的包的前提下，抓取长度越小越好。\n\n输出选项：\n\n-e：输出的每行中都将包括数据链路层头部信息，例如源MAC和目标MAC。\n\n-q：快速打印输出。即打印很少的协议相关信息，从而输出行都比较简短。\n\n-X：输出包的头部数据，会以16进制和ASCII两种方式同时输出。\n\n-XX：输出包的头部数据，会以16进制和ASCII两种方式同时输出，更详细。\n\n-v：当分析和打印的时候，产生详细的输出。\n\n-vv：产生比-v更详细的输出。\n\n-vvv：产生比-vv更详细的输出。\n\n其他功能性选项：\n\n-D：列出可用于抓包的接口。将会列出接口的数值编号和接口名，它们都可以用于"-i"后。\n\n-F：从文件中读取抓包的表达式。若使用该选项，则命令行中给定的其他表达式都将失效。\n\n-w：将抓包数据输出到文件中而不是标准输出。可以同时配合"-G time"选项使得输出文件每time秒就自动切换到另一个文件。可通过"-r"选项载入这些文件以进行分析和打印。\n\n-r：从给定的数据包文件中读取数据。使用"-"表示从标准输入中读取。\n\n所以常用的选项也就这几个：\n\n * tcpdump -D\n * tcpdump -c num -i int -nn -XX -vvv\n\n1.2 tcpdump表达式\n\n表达式用于筛选输出哪些类型的数据包，如果没有给定表达式，所有的数据包都将输出，否则只输出表达式为true的包。在表达式中出现的shell元字符建议使用单引号包围。\n\ntcpdump的表达式由一个或多个"单元"组成，每个单元一般包含ID的修饰符和一个ID(数字或名称)。有三种修饰符：\n\n(1).type：指定ID的类型。\n\n可以给定的值有host/net/port/portrange。例如"host foo"，"net 128.3"，"port 20"，"portrange 6000-6008"。默认的type为host。\n\n(2).dir：指定ID的方向。\n\n可以给定的值包括src/dst/src or dst/src and dst，默认为src or dst。例如，"src foo"表示源主机为foo的数据包，"dst net 128.3"表示目标网络为128.3的数据包，"src or dst port 22"表示源或目的端口为22的数据包。\n\n(3).proto：通过给定协议限定匹配的数据包类型。\n\n常用的协议有tcp/udp/arp/ip/ether/icmp等，若未给定协议类型，则匹配所有可能的类型。例如"tcp port 21"，"udp portrange 7000-7009"。\n\n所以，一个基本的表达式单元格式为"proto dir type ID"\n\n除了使用修饰符和ID组成的表达式单元，还有关键字表达式单元：gateway，broadcast，less，greater以及算术表达式。\n\n表达式单元之间可以使用操作符" and / && / or / || / not / ! "进行连接，从而组成复杂的条件表达式。如"host foo and not port ftp and not port ftp-data"，这表示筛选的数据包要满足"主机为foo且端口不是ftp(端口21)和ftp-data(端口20)的包"，常用端口和名字的对应关系可在linux系统中的/etc/service文件中找到。\n\n另外，同样的修饰符可省略，如"tcp dst port ftp or ftp-data or domain"与"tcp dst port ftp or tcp dst port ftp-data or tcp dst port domain"意义相同，都表示包的协议为tcp且目的端口为ftp或ftp-data或domain(端口53)。\n\n使用括号"()"可以改变表达式的优先级，但需要注意的是括号会被shell解释，所以应该使用反斜线""转义为"()"，在需要的时候，还需要包围在引号中。\n\n1.3 tcpdump示例\n\n注意，tcpdump只能抓取流经本机的数据包。\n\n(1).默认启动\n\ntcpdump\n\n默认情况下，直接启动tcpdump将监视第一个网络接口(非lo口)上所有流通的数据包。这样抓取的结果会非常多，滚动非常快。\n\n(2).监视指定网络接口的数据包\n\ntcpdump -i eth1\n\n如果不指定网卡，默认tcpdump只会监视第一个网络接口，如eth0。\n\n(3).监视指定主机的数据包，例如所有进入或离开longshuai的数据包\n\ntcpdump host longshuai\n\n(4).打印helios<--\x3ehot或helios<--\x3eace之间通信的数据包\n\ntcpdump host helios and ( hot or ace )\n\n(5).打印ace与任何其他主机之间通信的IP数据包,但不包括与helios之间的数据包\n\ntcpdump ip host ace and not helios\n\n(6).截获主机hostname发送的所有数据\n\ntcpdump src host hostname\n\n(7).监视所有发送到主机hostname的数据包\n\ntcpdump dst host hostname\n\n(8).监视指定主机和端口的数据包\n\ntcpdump tcp port 22 and host hostname\n\n(9).对本机的udp 123端口进行监视(123为ntp的服务端口)\n\ntcpdump udp port 123\n\n(10).监视指定网络的数据包，如本机与192.168网段通信的数据包，"-c 10"表示只抓取10个包\n\ntcpdump -c 10 net 192.168\n\n(11).打印所有通过网关snup的ftp数据包(注意,表达式被单引号括起来了,这可以防止shell对其中的括号进行错误解析)\n\nshell> tcpdump \'gateway snup and (port ftp or ftp-data)\'\n\n(12).抓取ping包\n\n[root@server2 ~]# tcpdump -c 5 -nn -i eth0 icmp\n\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n\nlistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\n\n12:11:23.273638 IP 192.168.100.70 > 192.168.100.62: ICMP echo request, id 16422, seq 10, length 64\n\n12:11:23.273666 IP 192.168.100.62 > 192.168.100.70: ICMP echo reply, id 16422, seq 10, length 64\n\n12:11:24.356915 IP 192.168.100.70 > 192.168.100.62: ICMP echo request, id 16422, seq 11, length 64\n\n12:11:24.356936 IP 192.168.100.62 > 192.168.100.70: ICMP echo reply, id 16422, seq 11, length 64\n\n12:11:25.440887 IP 192.168.100.70 > 192.168.100.62: ICMP echo request, id 16422, seq 12, length 64\n\npackets captured\n\npackets received by filter\n\npackets dropped by kernel\n\n如果明确要抓取主机为192.168.100.70对本机的ping，则使用and操作符。\n\n[root@server2 ~]# tcpdump -c 5 -nn -i eth0 icmp and src 192.168.100.62\n\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n\nlistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\n\n12:09:29.957132 IP 192.168.100.70 > 192.168.100.62: ICMP echo request, id 16166, seq 1, length 64\n\n12:09:31.041035 IP 192.168.100.70 > 192.168.100.62: ICMP echo request, id 16166, seq 2, length 64\n\n12:09:32.124562 IP 192.168.100.70 > 192.168.100.62: ICMP echo request, id 16166, seq 3, length 64\n\n12:09:33.208514 IP 192.168.100.70 > 192.168.100.62: ICMP echo request, id 16166, seq 4, length 64\n\n12:09:34.292222 IP 192.168.100.70 > 192.168.100.62: ICMP echo request, id 16166, seq 5, length 64\n\npackets captured\n\npackets received by filter\n\npackets dropped by kernel\n\n注意不能直接写icmp src 192.168.100.70，因为icmp协议不支持直接应用host这个type。\n\n(13).抓取到本机22端口包\n\n[root@server2 ~]# tcpdump -c 10 -nn -i eth0 tcp dst port 22\n\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n\nlistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\n\n12:06:57.574293 IP 192.168.100.1.5788 > 192.168.100.62.22: Flags [.], ack 535528834, win 2053, length 0\n\n12:06:57.629125 IP 192.168.100.1.5788 > 192.168.100.62.22: Flags [.], ack 193, win 2052, length 0\n\n12:06:57.684688 IP 192.168.100.1.5788 > 192.168.100.62.22: Flags [.], ack 385, win 2051, length 0\n\n12:06:57.738977 IP 192.168.100.1.5788 > 192.168.100.62.22: Flags [.], ack 577, win 2050, length 0\n\n12:06:57.794305 IP 192.168.100.1.5788 > 192.168.100.62.22: Flags [.], ack 769, win 2050, length 0\n\n12:06:57.848720 IP 192.168.100.1.5788 > 192.168.100.62.22: Flags [.], ack 961, win 2049, length 0\n\n12:06:57.904057 IP 192.168.100.1.5788 > 192.168.100.62.22: Flags [.], ack 1153, win 2048, length 0\n\n12:06:57.958477 IP 192.168.100.1.5788 > 192.168.100.62.22: Flags [.], ack 1345, win 2047, length 0\n\n12:06:58.014338 IP 192.168.100.1.5788 > 192.168.100.62.22: Flags [.], ack 1537, win 2053, length 0\n\n12:06:58.069361 IP 192.168.100.1.5788 > 192.168.100.62.22: Flags [.], ack 1729, win 2052, length 0\n\npackets captured\n\npackets received by filter\n\npackets dropped by kernel\n\n(14).解析包数据\n\n[root@server2 ~]# tcpdump -c 2 -q -XX -vvv -nn -i eth0 tcp dst port 22\n\ntcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\n\n12:15:54.788812 IP (tos 0x0, ttl 64, id 19303, offset 0, flags [DF], proto TCP (6), length 40)\n\n192.168.100.1.5788 > 192.168.100.62.22: tcp 0\n\n0x0000:  000c 2908 9234 0050 56c0 0008 0800 4500  ..)..4.PV.....E.\n\n0x0010:  0028 4b67 4000 4006 a5d8 c0a8 6401 c0a8  .(Kg@.@.....d...\n\n0x0020:  643e 169c 0016 2426 5fd6 1fec 2b62 5010  d>....$&_...+bP.\n\n0x0030:  0803 7844 0000 0000 0000 0000            ..xD........\n\n12:15:54.842641 IP (tos 0x0, ttl 64, id 19304, offset 0, flags [DF], proto TCP (6), length 40)\n\n192.168.100.1.5788 > 192.168.100.62.22: tcp 0\n\n0x0000:  000c 2908 9234 0050 56c0 0008 0800 4500  ..)..4.PV.....E.\n\n0x0010:  0028 4b68 4000 4006 a5d7 c0a8 6401 c0a8  .(Kh@.@.....d...\n\n0x0020:  643e 169c 0016 2426 5fd6 1fec 2d62 5010  d>....$&_...-bP.\n\n0x0030:  0801 7646 0000 0000 0000 0000            ..vF........\n\npackets captured\n\npackets received by filter\n\npackets dropped by kernel\n\n总的来说，tcpdump对基本的数据包抓取方法还是较简单的。只要掌握有限的几个选项(-nn -XX -vvv -i -c -q)，再组合表达式即可。',normalizedContent:'tcpdump采用命令行方式对接口的数据包进行筛选抓取，其丰富特性表现在灵活的表达式上。\n\n不带任何选项的tcpdump，默认会抓取第一个网络接口，且只有将tcpdump进程终止才会停止抓包。\n\n例如：\n\nshell> tcpdump -nn -i eth0 icmp\n\n下面是详细的tcpdump用法。\n\n1.1 tcpdump选项\n\n它的命令格式为：\n\ntcpdump [ -dennqvx ] [ -c count ] [ -f file ] [ -i interface ] [ -r file ]\n\n[ -s snaplen ] [ -w file ] [ expression ]\n\n抓包选项：\n\n-c：指定要抓取的包数量。注意，是最终要获取这么多个包。例如，指定"-c 10"将获取10个包，但可能已经处理了100个包，只不过只有10个包是满足条件的包。\n\n-i interface：指定tcpdump需要监听的接口。若未指定该选项，将从系统接口列表中搜寻编号最小的已配置好的接口(不包括loopback接口，要抓取loopback接口使用tcpdump -i lo)，\n\n：一旦找到第一个符合条件的接口，搜寻马上结束。可以使用\'any\'关键字表示所有网络接口。\n\n-n：对地址以数字方式显式，否则显式为主机名，也就是说-n选项不做主机名解析。\n\n-nn：除了-n的作用外，还把端口显示为数值，否则显示端口服务名。\n\n-n：不打印出host的域名部分。例如tcpdump将会打印\'nic\'而不是\'nic.ddn.mil\'。\n\n-p：指定要抓取的包是流入还是流出的包。可以给定的值为"in"、"out"和"inout"，默认为"inout"。\n\n-s len：设置tcpdump的数据包抓取长度为len，如果不设置默认将会是65535字节。对于要抓取的数据包较大时，长度设置不够可能会产生包截断，若出现包截断，\n\n：输出行中会出现"[|proto]"的标志(proto实际会显示为协议名)。但是抓取len越长，包的处理时间越长，并且会减少tcpdump可缓存的数据包的数量，\n\n：从而会导致数据包的丢失，所以在能抓取我们想要的包的前提下，抓取长度越小越好。\n\n输出选项：\n\n-e：输出的每行中都将包括数据链路层头部信息，例如源mac和目标mac。\n\n-q：快速打印输出。即打印很少的协议相关信息，从而输出行都比较简短。\n\n-x：输出包的头部数据，会以16进制和ascii两种方式同时输出。\n\n-xx：输出包的头部数据，会以16进制和ascii两种方式同时输出，更详细。\n\n-v：当分析和打印的时候，产生详细的输出。\n\n-vv：产生比-v更详细的输出。\n\n-vvv：产生比-vv更详细的输出。\n\n其他功能性选项：\n\n-d：列出可用于抓包的接口。将会列出接口的数值编号和接口名，它们都可以用于"-i"后。\n\n-f：从文件中读取抓包的表达式。若使用该选项，则命令行中给定的其他表达式都将失效。\n\n-w：将抓包数据输出到文件中而不是标准输出。可以同时配合"-g time"选项使得输出文件每time秒就自动切换到另一个文件。可通过"-r"选项载入这些文件以进行分析和打印。\n\n-r：从给定的数据包文件中读取数据。使用"-"表示从标准输入中读取。\n\n所以常用的选项也就这几个：\n\n * tcpdump -d\n * tcpdump -c num -i int -nn -xx -vvv\n\n1.2 tcpdump表达式\n\n表达式用于筛选输出哪些类型的数据包，如果没有给定表达式，所有的数据包都将输出，否则只输出表达式为true的包。在表达式中出现的shell元字符建议使用单引号包围。\n\ntcpdump的表达式由一个或多个"单元"组成，每个单元一般包含id的修饰符和一个id(数字或名称)。有三种修饰符：\n\n(1).type：指定id的类型。\n\n可以给定的值有host/net/port/portrange。例如"host foo"，"net 128.3"，"port 20"，"portrange 6000-6008"。默认的type为host。\n\n(2).dir：指定id的方向。\n\n可以给定的值包括src/dst/src or dst/src and dst，默认为src or dst。例如，"src foo"表示源主机为foo的数据包，"dst net 128.3"表示目标网络为128.3的数据包，"src or dst port 22"表示源或目的端口为22的数据包。\n\n(3).proto：通过给定协议限定匹配的数据包类型。\n\n常用的协议有tcp/udp/arp/ip/ether/icmp等，若未给定协议类型，则匹配所有可能的类型。例如"tcp port 21"，"udp portrange 7000-7009"。\n\n所以，一个基本的表达式单元格式为"proto dir type id"\n\n除了使用修饰符和id组成的表达式单元，还有关键字表达式单元：gateway，broadcast，less，greater以及算术表达式。\n\n表达式单元之间可以使用操作符" and / && / or / || / not / ! "进行连接，从而组成复杂的条件表达式。如"host foo and not port ftp and not port ftp-data"，这表示筛选的数据包要满足"主机为foo且端口不是ftp(端口21)和ftp-data(端口20)的包"，常用端口和名字的对应关系可在linux系统中的/etc/service文件中找到。\n\n另外，同样的修饰符可省略，如"tcp dst port ftp or ftp-data or domain"与"tcp dst port ftp or tcp dst port ftp-data or tcp dst port domain"意义相同，都表示包的协议为tcp且目的端口为ftp或ftp-data或domain(端口53)。\n\n使用括号"()"可以改变表达式的优先级，但需要注意的是括号会被shell解释，所以应该使用反斜线""转义为"()"，在需要的时候，还需要包围在引号中。\n\n1.3 tcpdump示例\n\n注意，tcpdump只能抓取流经本机的数据包。\n\n(1).默认启动\n\ntcpdump\n\n默认情况下，直接启动tcpdump将监视第一个网络接口(非lo口)上所有流通的数据包。这样抓取的结果会非常多，滚动非常快。\n\n(2).监视指定网络接口的数据包\n\ntcpdump -i eth1\n\n如果不指定网卡，默认tcpdump只会监视第一个网络接口，如eth0。\n\n(3).监视指定主机的数据包，例如所有进入或离开longshuai的数据包\n\ntcpdump host longshuai\n\n(4).打印helios<--\x3ehot或helios<--\x3eace之间通信的数据包\n\ntcpdump host helios and ( hot or ace )\n\n(5).打印ace与任何其他主机之间通信的ip数据包,但不包括与helios之间的数据包\n\ntcpdump ip host ace and not helios\n\n(6).截获主机hostname发送的所有数据\n\ntcpdump src host hostname\n\n(7).监视所有发送到主机hostname的数据包\n\ntcpdump dst host hostname\n\n(8).监视指定主机和端口的数据包\n\ntcpdump tcp port 22 and host hostname\n\n(9).对本机的udp 123端口进行监视(123为ntp的服务端口)\n\ntcpdump udp port 123\n\n(10).监视指定网络的数据包，如本机与192.168网段通信的数据包，"-c 10"表示只抓取10个包\n\ntcpdump -c 10 net 192.168\n\n(11).打印所有通过网关snup的ftp数据包(注意,表达式被单引号括起来了,这可以防止shell对其中的括号进行错误解析)\n\nshell> tcpdump \'gateway snup and (port ftp or ftp-data)\'\n\n(12).抓取ping包\n\n[root@server2 ~]# tcpdump -c 5 -nn -i eth0 icmp\n\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n\nlistening on eth0, link-type en10mb (ethernet), capture size 65535 bytes\n\n12:11:23.273638 ip 192.168.100.70 > 192.168.100.62: icmp echo request, id 16422, seq 10, length 64\n\n12:11:23.273666 ip 192.168.100.62 > 192.168.100.70: icmp echo reply, id 16422, seq 10, length 64\n\n12:11:24.356915 ip 192.168.100.70 > 192.168.100.62: icmp echo request, id 16422, seq 11, length 64\n\n12:11:24.356936 ip 192.168.100.62 > 192.168.100.70: icmp echo reply, id 16422, seq 11, length 64\n\n12:11:25.440887 ip 192.168.100.70 > 192.168.100.62: icmp echo request, id 16422, seq 12, length 64\n\npackets captured\n\npackets received by filter\n\npackets dropped by kernel\n\n如果明确要抓取主机为192.168.100.70对本机的ping，则使用and操作符。\n\n[root@server2 ~]# tcpdump -c 5 -nn -i eth0 icmp and src 192.168.100.62\n\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n\nlistening on eth0, link-type en10mb (ethernet), capture size 65535 bytes\n\n12:09:29.957132 ip 192.168.100.70 > 192.168.100.62: icmp echo request, id 16166, seq 1, length 64\n\n12:09:31.041035 ip 192.168.100.70 > 192.168.100.62: icmp echo request, id 16166, seq 2, length 64\n\n12:09:32.124562 ip 192.168.100.70 > 192.168.100.62: icmp echo request, id 16166, seq 3, length 64\n\n12:09:33.208514 ip 192.168.100.70 > 192.168.100.62: icmp echo request, id 16166, seq 4, length 64\n\n12:09:34.292222 ip 192.168.100.70 > 192.168.100.62: icmp echo request, id 16166, seq 5, length 64\n\npackets captured\n\npackets received by filter\n\npackets dropped by kernel\n\n注意不能直接写icmp src 192.168.100.70，因为icmp协议不支持直接应用host这个type。\n\n(13).抓取到本机22端口包\n\n[root@server2 ~]# tcpdump -c 10 -nn -i eth0 tcp dst port 22\n\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n\nlistening on eth0, link-type en10mb (ethernet), capture size 65535 bytes\n\n12:06:57.574293 ip 192.168.100.1.5788 > 192.168.100.62.22: flags [.], ack 535528834, win 2053, length 0\n\n12:06:57.629125 ip 192.168.100.1.5788 > 192.168.100.62.22: flags [.], ack 193, win 2052, length 0\n\n12:06:57.684688 ip 192.168.100.1.5788 > 192.168.100.62.22: flags [.], ack 385, win 2051, length 0\n\n12:06:57.738977 ip 192.168.100.1.5788 > 192.168.100.62.22: flags [.], ack 577, win 2050, length 0\n\n12:06:57.794305 ip 192.168.100.1.5788 > 192.168.100.62.22: flags [.], ack 769, win 2050, length 0\n\n12:06:57.848720 ip 192.168.100.1.5788 > 192.168.100.62.22: flags [.], ack 961, win 2049, length 0\n\n12:06:57.904057 ip 192.168.100.1.5788 > 192.168.100.62.22: flags [.], ack 1153, win 2048, length 0\n\n12:06:57.958477 ip 192.168.100.1.5788 > 192.168.100.62.22: flags [.], ack 1345, win 2047, length 0\n\n12:06:58.014338 ip 192.168.100.1.5788 > 192.168.100.62.22: flags [.], ack 1537, win 2053, length 0\n\n12:06:58.069361 ip 192.168.100.1.5788 > 192.168.100.62.22: flags [.], ack 1729, win 2052, length 0\n\npackets captured\n\npackets received by filter\n\npackets dropped by kernel\n\n(14).解析包数据\n\n[root@server2 ~]# tcpdump -c 2 -q -xx -vvv -nn -i eth0 tcp dst port 22\n\ntcpdump: listening on eth0, link-type en10mb (ethernet), capture size 65535 bytes\n\n12:15:54.788812 ip (tos 0x0, ttl 64, id 19303, offset 0, flags [df], proto tcp (6), length 40)\n\n192.168.100.1.5788 > 192.168.100.62.22: tcp 0\n\n0x0000:  000c 2908 9234 0050 56c0 0008 0800 4500  ..)..4.pv.....e.\n\n0x0010:  0028 4b67 4000 4006 a5d8 c0a8 6401 c0a8  .(kg@.@.....d...\n\n0x0020:  643e 169c 0016 2426 5fd6 1fec 2b62 5010  d>....$&_...+bp.\n\n0x0030:  0803 7844 0000 0000 0000 0000            ..xd........\n\n12:15:54.842641 ip (tos 0x0, ttl 64, id 19304, offset 0, flags [df], proto tcp (6), length 40)\n\n192.168.100.1.5788 > 192.168.100.62.22: tcp 0\n\n0x0000:  000c 2908 9234 0050 56c0 0008 0800 4500  ..)..4.pv.....e.\n\n0x0010:  0028 4b68 4000 4006 a5d7 c0a8 6401 c0a8  .(kh@.@.....d...\n\n0x0020:  643e 169c 0016 2426 5fd6 1fec 2d62 5010  d>....$&_...-bp.\n\n0x0030:  0801 7646 0000 0000 0000 0000            ..vf........\n\npackets captured\n\npackets received by filter\n\npackets dropped by kernel\n\n总的来说，tcpdump对基本的数据包抓取方法还是较简单的。只要掌握有限的几个选项(-nn -xx -vvv -i -c -q)，再组合表达式即可。',charsets:{cjk:!0}},{title:"ssh安全",frontmatter:{title:"ssh安全",date:"2023-02-24T15:44:20.000Z",permalink:"/pages/3913e5/",categories:["运维","安全"],tags:[null],readingShow:"top",description:"sed -i '$a AllowUsers CR@192.168.1.12 root@192.168.1.12' /etc/ssh/sshd_config ;\\\nsystemctl restart sshd\n`",meta:[{name:"twitter:title",content:"ssh安全"},{name:"twitter:description",content:"sed -i '$a AllowUsers CR@192.168.1.12 root@192.168.1.12' /etc/ssh/sshd_config ;\\\nsystemctl restart sshd\n`"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/03.ssh%E5%AE%89%E5%85%A8.html"},{property:"og:type",content:"article"},{property:"og:title",content:"ssh安全"},{property:"og:description",content:"sed -i '$a AllowUsers CR@192.168.1.12 root@192.168.1.12' /etc/ssh/sshd_config ;\\\nsystemctl restart sshd\n`"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/03.ssh%E5%AE%89%E5%85%A8.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-24T15:44:20.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"ssh安全"},{itemprop:"description",content:"sed -i '$a AllowUsers CR@192.168.1.12 root@192.168.1.12' /etc/ssh/sshd_config ;\\\nsystemctl restart sshd\n`"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/03.ssh%E5%AE%89%E5%85%A8.html",relativePath:"04.运维/05.安全/03.ssh安全.md",key:"v-44bb153f",path:"/pages/3913e5/",headers:[{level:2,title:"1、只允许某用户从指定 IP 地址登陆",slug:"_1、只允许某用户从指定-ip-地址登陆",normalizedTitle:"1、只允许某用户从指定 ip 地址登陆",charIndex:2},{level:2,title:"2、设置 SSH 空闲超时退出时间",slug:"_2、设置-ssh-空闲超时退出时间",normalizedTitle:"2、设置 ssh 空闲超时退出时间",charIndex:138},{level:2,title:"3、 限制登陆访问尝试的验证次数",slug:"_3、-限制登陆访问尝试的验证次数",normalizedTitle:"3、 限制登陆访问尝试的验证次数",charIndex:374},{level:2,title:"4、允许 root 用户登录",slug:"_4、允许-root-用户登录",normalizedTitle:"4、允许 root 用户登录",charIndex:417},{level:2,title:"5、设置登录方式",slug:"_5、设置登录方式",normalizedTitle:"5、设置登录方式",charIndex:544},{level:2,title:"6、禁止使用空白密码用户访问",slug:"_6、禁止使用空白密码用户访问",normalizedTitle:"6、禁止使用空白密码用户访问",charIndex:705},{level:2,title:"7、SSH 登录事件通知至 ntfy",slug:"_7、ssh-登录事件通知至-ntfy",normalizedTitle:"7、ssh 登录事件通知至 ntfy",charIndex:754}],headersStr:"1、只允许某用户从指定 IP 地址登陆 2、设置 SSH 空闲超时退出时间 3、 限制登陆访问尝试的验证次数 4、允许 root 用户登录 5、设置登录方式 6、禁止使用空白密码用户访问 7、SSH 登录事件通知至 ntfy",content:"# 1、只允许某用户从指定 IP 地址登陆\n\nsed -i '$a AllowUsers CR@192.168.1.12 root@192.168.1.12' /etc/ssh/sshd_config ;\\\nsystemctl restart sshd\n\n\n1\n2\n\n\n\n# 2、设置 SSH 空闲超时退出时间\n\n/etc/ssh/sshd_config\n\n#ClientAliveInterval 0\n#ClientAliveCountMax 3\n修改成\nClientAliveInterval 30    #（每30秒往客户端发送会话请求，保持连接）\nClientAliveCountMax 3     #（去掉注释即可，3表示重连3次失败后，重启SSH会话）\n\n\n1\n2\n3\n4\n5\n\n\nsystemctl restart sshd\n\n\n# 3、 限制登陆访问尝试的验证次数\n\nMaxAuthTries 20\n\n\n1\n\n\n\n# 4、允许 root 用户登录\n\nsed -i 's/PermitRootLogin no/PermitRootLogin yes/g'  /etc/ssh/sshd_config ;\\\nsystemctl restart sshd\n\n\n1\n2\n\n\n\n# 5、设置登录方式\n\n# AuthorizedKeysFile   .ssh/authorized_keys   //公钥公钥认证文件\n# PubkeyAuthentication yes   //可以使用公钥登录\n# PasswordAuthentication no  //不允许使用密码登录\n\n\n1\n2\n3\n\n\n\n# 6、禁止使用空白密码用户访问\n\nPermitEmptyPasswords no\n\n\n1\n\n\n\n# 7、SSH 登录事件通知至 ntfy\n\n/etc/pam.d/sshd\n\nsession optional pam_exec.so /usr/local/bin/ntfy-ssh-login.sh\n\n\n1\n\n\nntfy-ssh-login.sh\n\n#!/bin/bash\nTOPIC_URL=http://test.curiouser.top:18070/ssh-notify\nif [ \"${PAM_TYPE}\" = \"open_session\" ]; then\n  curl -H tags:warning -H prio:high -d \"SSH login to $(hostname): ${PAM_USER} from ${PAM_RHOST}\" \"${TOPIC_URL}\"\nfi\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n部署私有的 ntfy\n\nntfy_server:\n  image: 'binwiederhier/ntfy'\n  restart: always\n  container_name: ntfy_server\n  command: serve --config /etc/ntfy/server.yml --cache-file /var/cache/ntfy/cache.db --listen-http :18070\n  ports:\n    - '18070:18070'\n  volumes:\n    - '/data/ntfy/data:/var/cache/ntfy'\n    - '/data/ntfy/config:/etc/ntfy'\n    - '/data/ntfy/data:/var/lib/ntfy/'\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n----------------------------------------\n\n原文链接：\n\nssh安全 | Gaoyufu 's blog",normalizedContent:"# 1、只允许某用户从指定 ip 地址登陆\n\nsed -i '$a allowusers cr@192.168.1.12 root@192.168.1.12' /etc/ssh/sshd_config ;\\\nsystemctl restart sshd\n\n\n1\n2\n\n\n\n# 2、设置 ssh 空闲超时退出时间\n\n/etc/ssh/sshd_config\n\n#clientaliveinterval 0\n#clientalivecountmax 3\n修改成\nclientaliveinterval 30    #（每30秒往客户端发送会话请求，保持连接）\nclientalivecountmax 3     #（去掉注释即可，3表示重连3次失败后，重启ssh会话）\n\n\n1\n2\n3\n4\n5\n\n\nsystemctl restart sshd\n\n\n# 3、 限制登陆访问尝试的验证次数\n\nmaxauthtries 20\n\n\n1\n\n\n\n# 4、允许 root 用户登录\n\nsed -i 's/permitrootlogin no/permitrootlogin yes/g'  /etc/ssh/sshd_config ;\\\nsystemctl restart sshd\n\n\n1\n2\n\n\n\n# 5、设置登录方式\n\n# authorizedkeysfile   .ssh/authorized_keys   //公钥公钥认证文件\n# pubkeyauthentication yes   //可以使用公钥登录\n# passwordauthentication no  //不允许使用密码登录\n\n\n1\n2\n3\n\n\n\n# 6、禁止使用空白密码用户访问\n\npermitemptypasswords no\n\n\n1\n\n\n\n# 7、ssh 登录事件通知至 ntfy\n\n/etc/pam.d/sshd\n\nsession optional pam_exec.so /usr/local/bin/ntfy-ssh-login.sh\n\n\n1\n\n\nntfy-ssh-login.sh\n\n#!/bin/bash\ntopic_url=http://test.curiouser.top:18070/ssh-notify\nif [ \"${pam_type}\" = \"open_session\" ]; then\n  curl -h tags:warning -h prio:high -d \"ssh login to $(hostname): ${pam_user} from ${pam_rhost}\" \"${topic_url}\"\nfi\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n部署私有的 ntfy\n\nntfy_server:\n  image: 'binwiederhier/ntfy'\n  restart: always\n  container_name: ntfy_server\n  command: serve --config /etc/ntfy/server.yml --cache-file /var/cache/ntfy/cache.db --listen-http :18070\n  ports:\n    - '18070:18070'\n  volumes:\n    - '/data/ntfy/data:/var/cache/ntfy'\n    - '/data/ntfy/config:/etc/ntfy'\n    - '/data/ntfy/data:/var/lib/ntfy/'\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n----------------------------------------\n\n原文链接：\n\nssh安全 | gaoyufu 's blog",charsets:{cjk:!0}},{title:"白帽子讲web安全",frontmatter:{title:"白帽子讲web安全",date:"2023-02-24T15:38:46.000Z",permalink:"/pages/fcba46/",categories:["运维","安全"],tags:[null],readingShow:"top",description:"信任",meta:[{name:"twitter:title",content:"白帽子讲web安全"},{name:"twitter:description",content:"信任"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/02.%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E5%AE%89%E5%85%A8.html"},{property:"og:type",content:"article"},{property:"og:title",content:"白帽子讲web安全"},{property:"og:description",content:"信任"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/02.%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E5%AE%89%E5%85%A8.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-24T15:38:46.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"白帽子讲web安全"},{itemprop:"description",content:"信任"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/02.%E7%99%BD%E5%B8%BD%E5%AD%90%E8%AE%B2web%E5%AE%89%E5%85%A8.html",relativePath:"04.运维/05.安全/02.白帽子讲web安全.md",key:"v-0cf4e9e4",path:"/pages/fcba46/",headers:[{level:2,title:"安全问题的本质",slug:"安全问题的本质",normalizedTitle:"安全问题的本质",charIndex:2},{level:2,title:"安全三要素",slug:"安全三要素",normalizedTitle:"安全三要素",charIndex:18},{level:2,title:"安全评估的过程",slug:"安全评估的过程",normalizedTitle:"安全评估的过程",charIndex:144},{level:2,title:"互联网安全的核心问题",slug:"互联网安全的核心问题",normalizedTitle:"互联网安全的核心问题",charIndex:182},{level:2,title:"威胁和风险",slug:"威胁和风险",normalizedTitle:"威胁和风险",charIndex:203},{level:2,title:"威胁分析 STRIDE 模型",slug:"威胁分析-stride-模型",normalizedTitle:"威胁分析 stride 模型",charIndex:242},{level:2,title:"风险分析 DREAD 模型",slug:"风险分析-dread-模型",normalizedTitle:"风险分析 dread 模型",charIndex:588},{level:2,title:"Secure By Default 原则",slug:"secure-by-default-原则",normalizedTitle:"secure by default 原则",charIndex:1077},{level:2,title:"纵深防御原则",slug:"纵深防御原则",normalizedTitle:"纵深防御原则",charIndex:1164},{level:2,title:"数据与代码分离原则",slug:"数据与代码分离原则",normalizedTitle:"数据与代码分离原则",charIndex:1280},{level:2,title:"不可预测性原则",slug:"不可预测性原则",normalizedTitle:"不可预测性原则",charIndex:1341},{level:2,title:"浏览器安全",slug:"浏览器安全",normalizedTitle:"浏览器安全",charIndex:1377},{level:3,title:"同源策略",slug:"同源策略",normalizedTitle:"同源策略",charIndex:1387},{level:3,title:"浏览器沙箱",slug:"浏览器沙箱",normalizedTitle:"浏览器沙箱",charIndex:1523},{level:3,title:"恶意网址拦截",slug:"恶意网址拦截",normalizedTitle:"恶意网址拦截",charIndex:1648},{level:2,title:"跨站脚本攻击（XSS）",slug:"跨站脚本攻击-xss",normalizedTitle:"跨站脚本攻击（xss）",charIndex:1742},{level:2,title:"XSS 攻击类型",slug:"xss-攻击类型",normalizedTitle:"xss 攻击类型",charIndex:1846}],headersStr:"安全问题的本质 安全三要素 安全评估的过程 互联网安全的核心问题 威胁和风险 威胁分析 STRIDE 模型 风险分析 DREAD 模型 Secure By Default 原则 纵深防御原则 数据与代码分离原则 不可预测性原则 浏览器安全 同源策略 浏览器沙箱 恶意网址拦截 跨站脚本攻击（XSS） XSS 攻击类型",content:"# 安全问题的本质\n\n信任\n\n\n# 安全三要素\n\n机密性（Confidentiality）、完整性（Integrity）、可用性（Avaliability）\n\n机密性要求保护数据内容不能泄露。\n\n完整性要求数据内容是完整的、没有被篡改的。\n\n可用性要求保护资源是 “随需而得”。\n\n\n# 安全评估的过程\n\n资产等级划分、威胁分析、风险分析、确认解决方案。\n\n\n# 互联网安全的核心问题\n\n数据安全\n\n\n# 威胁和风险\n\n可能造成危害的来源称为威胁，可能会出现的损失称为风险。\n\n\n# 威胁分析 STRIDE 模型\n\n威胁                             定义         对应的安全属性\nSpoofing（伪装）                   冒充他人身份     认证\nTampering（篡改）                  修改数据或代码    完整性\nRepudiation（抵赖）                否份做过的事情    不可抵赖性\nInformationDisclosure（信息泄露）    机密信息泄露     机密性\nDenial of Service（拒绝服务）        拒绝服务       可用性\nElevation of Privilege（提升权限）   未经授权获得许可   授权\n\n\n# 风险分析 DREAD 模型\n\n等级                 高（3）                      中（2）                    低（1）\nDamage Potential   获取完全验证权限；执行管理员操作；非法上传文件   泄露敏感信息                  泄露其他信息\nReproducibility    攻击者可以随意再次攻击               攻击者可以重复攻击，但有时间限制        攻击者很难重复攻击过程\nExploitability     初学者在短期内能掌握攻击方法            熟练的攻击者才能完成这次攻击          漏洞利用条件非常苛刻\nAffected users     所有用户，默认配置，关键用户            部分用户，非默认配置              极少数用户，匿名用户\nDiscoverability    漏洞很显眼，攻击条件很容易获得           在私有区域，部分人能看到，需要深入挖掘漏洞   发现该漏洞极其困难\n\n\n# Secure By Default 原则\n\n白名单、黑名单：如果更多的使用白名单，那么系统就会变得更安全\n\n最小权限原则：要求只授予主体必要的权限，而不要过度授权。\n\n\n# 纵深防御原则\n\n纵深防御包含两层含义：首先，要在各个不同层面、不同方面实施安全方案，避免出现疏漏，不同安全方案之间需要相互配合，构成一个整体；其次，要在正确的地方做正确的事情，即：在解决根本问题的地方实施针对性的安全方案。\n\n\n# 数据与代码分离原则\n\n程序在栈或者堆中，将用户数据当做代码执行，混淆了代码与数据的边界，从而导致安全问题的发生。\n\n\n# 不可预测性原则\n\n通过使数据不可预测达到让攻击方法失效的效果。\n\n\n# 浏览器安全\n\n\n# 同源策略\n\n它用于限制一个 origin 的文档或者它加载的脚本如何能与另一个源的资源进行交互。它能帮助阻隔恶意文档，减少可能被攻击的媒介。\n\n浏览器的同源策略，限制了来自不同源的 “document” 或脚本，对当前 “document” 读取或设置某些属性。\n\n\n# 浏览器沙箱\n\n让不可信任的代码运行在一定的环境中，限制不可信任的代码访问隔离区之外的资源。如果一定要跨越 Sandbox 边界产生数据交换，则只能通过指定的数据通道，比如经过封装的 API 来完成，在这些 API 中会严格检查请求的合法性。\n\n\n# 恶意网址拦截\n\n恶意网址拦截的工作原理很简单，一般都是浏览器周期性地从服务器端获取一份最新的恶意网址黑名单，如果用户上网时访问的网址存在于此黑名单中，浏览器就会弹出一个警告页面。\n\n\n# 跨站脚本攻击（XSS）\n\n跨站脚本攻击，英文全称是 Cross Site Script，通常指黑客通过 “THML 注入” 篡改了网页，插入了恶意的脚本，从而在用户浏览网页时，控制浏览器的一种攻击。\n\n\n# XSS 攻击类型\n\n第一种：反射型攻击。把用户输入的数据 “反射” 给浏览器。\n\n第二种：存储型攻击。把用户输入的数据 “存储” 在服务器端。\n\n第三种：DOM Based XSS。通过修改页面的 DOM 点形成 XSS。\n\n----------------------------------------\n\n原文链接：白帽子讲web安全 | Gaoyufu 's blog",normalizedContent:"# 安全问题的本质\n\n信任\n\n\n# 安全三要素\n\n机密性（confidentiality）、完整性（integrity）、可用性（avaliability）\n\n机密性要求保护数据内容不能泄露。\n\n完整性要求数据内容是完整的、没有被篡改的。\n\n可用性要求保护资源是 “随需而得”。\n\n\n# 安全评估的过程\n\n资产等级划分、威胁分析、风险分析、确认解决方案。\n\n\n# 互联网安全的核心问题\n\n数据安全\n\n\n# 威胁和风险\n\n可能造成危害的来源称为威胁，可能会出现的损失称为风险。\n\n\n# 威胁分析 stride 模型\n\n威胁                             定义         对应的安全属性\nspoofing（伪装）                   冒充他人身份     认证\ntampering（篡改）                  修改数据或代码    完整性\nrepudiation（抵赖）                否份做过的事情    不可抵赖性\ninformationdisclosure（信息泄露）    机密信息泄露     机密性\ndenial of service（拒绝服务）        拒绝服务       可用性\nelevation of privilege（提升权限）   未经授权获得许可   授权\n\n\n# 风险分析 dread 模型\n\n等级                 高（3）                      中（2）                    低（1）\ndamage potential   获取完全验证权限；执行管理员操作；非法上传文件   泄露敏感信息                  泄露其他信息\nreproducibility    攻击者可以随意再次攻击               攻击者可以重复攻击，但有时间限制        攻击者很难重复攻击过程\nexploitability     初学者在短期内能掌握攻击方法            熟练的攻击者才能完成这次攻击          漏洞利用条件非常苛刻\naffected users     所有用户，默认配置，关键用户            部分用户，非默认配置              极少数用户，匿名用户\ndiscoverability    漏洞很显眼，攻击条件很容易获得           在私有区域，部分人能看到，需要深入挖掘漏洞   发现该漏洞极其困难\n\n\n# secure by default 原则\n\n白名单、黑名单：如果更多的使用白名单，那么系统就会变得更安全\n\n最小权限原则：要求只授予主体必要的权限，而不要过度授权。\n\n\n# 纵深防御原则\n\n纵深防御包含两层含义：首先，要在各个不同层面、不同方面实施安全方案，避免出现疏漏，不同安全方案之间需要相互配合，构成一个整体；其次，要在正确的地方做正确的事情，即：在解决根本问题的地方实施针对性的安全方案。\n\n\n# 数据与代码分离原则\n\n程序在栈或者堆中，将用户数据当做代码执行，混淆了代码与数据的边界，从而导致安全问题的发生。\n\n\n# 不可预测性原则\n\n通过使数据不可预测达到让攻击方法失效的效果。\n\n\n# 浏览器安全\n\n\n# 同源策略\n\n它用于限制一个 origin 的文档或者它加载的脚本如何能与另一个源的资源进行交互。它能帮助阻隔恶意文档，减少可能被攻击的媒介。\n\n浏览器的同源策略，限制了来自不同源的 “document” 或脚本，对当前 “document” 读取或设置某些属性。\n\n\n# 浏览器沙箱\n\n让不可信任的代码运行在一定的环境中，限制不可信任的代码访问隔离区之外的资源。如果一定要跨越 sandbox 边界产生数据交换，则只能通过指定的数据通道，比如经过封装的 api 来完成，在这些 api 中会严格检查请求的合法性。\n\n\n# 恶意网址拦截\n\n恶意网址拦截的工作原理很简单，一般都是浏览器周期性地从服务器端获取一份最新的恶意网址黑名单，如果用户上网时访问的网址存在于此黑名单中，浏览器就会弹出一个警告页面。\n\n\n# 跨站脚本攻击（xss）\n\n跨站脚本攻击，英文全称是 cross site script，通常指黑客通过 “thml 注入” 篡改了网页，插入了恶意的脚本，从而在用户浏览网页时，控制浏览器的一种攻击。\n\n\n# xss 攻击类型\n\n第一种：反射型攻击。把用户输入的数据 “反射” 给浏览器。\n\n第二种：存储型攻击。把用户输入的数据 “存储” 在服务器端。\n\n第三种：dom based xss。通过修改页面的 dom 点形成 xss。\n\n----------------------------------------\n\n原文链接：白帽子讲web安全 | gaoyufu 's blog",charsets:{cjk:!0}},{title:"系统安全及应用基础",frontmatter:{title:"系统安全及应用基础",date:"2023-02-24T15:49:26.000Z",permalink:"/pages/8dcd54/",categories:["运维","安全"],tags:[null],readingShow:"top",description:'grep "/sbin/nologin$" /etc/passwd // 查找登录 Shell 是 /sbin/nologin 的用户',meta:[{name:"image",content:"http://pic.zzppjj.top/LightPicture/2023/02/94ab7e82c75e5eff.png"},{name:"twitter:title",content:"系统安全及应用基础"},{name:"twitter:description",content:'grep "/sbin/nologin$" /etc/passwd // 查找登录 Shell 是 /sbin/nologin 的用户'},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://pic.zzppjj.top/LightPicture/2023/02/94ab7e82c75e5eff.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/04.%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9F%BA%E7%A1%80.html"},{property:"og:type",content:"article"},{property:"og:title",content:"系统安全及应用基础"},{property:"og:description",content:'grep "/sbin/nologin$" /etc/passwd // 查找登录 Shell 是 /sbin/nologin 的用户'},{property:"og:image",content:"http://pic.zzppjj.top/LightPicture/2023/02/94ab7e82c75e5eff.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/04.%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9F%BA%E7%A1%80.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-24T15:49:26.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"系统安全及应用基础"},{itemprop:"description",content:'grep "/sbin/nologin$" /etc/passwd // 查找登录 Shell 是 /sbin/nologin 的用户'},{itemprop:"image",content:"http://pic.zzppjj.top/LightPicture/2023/02/94ab7e82c75e5eff.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/05.%E5%AE%89%E5%85%A8/04.%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9F%BA%E7%A1%80.html",relativePath:"04.运维/05.安全/04.系统安全及应用基础.md",key:"v-c30db28e",path:"/pages/8dcd54/",headers:[{level:2,title:"一、账号安全控制",slug:"一、账号安全控制",normalizedTitle:"一、账号安全控制",charIndex:2},{level:3,title:"1、基本安全措施",slug:"_1、基本安全措施",normalizedTitle:"1、基本安全措施",charIndex:15},{level:4,title:"1、系统账号清理",slug:"_1、系统账号清理",normalizedTitle:"1、系统账号清理",charIndex:27},{level:4,title:"2、密码安全控制",slug:"_2、密码安全控制",normalizedTitle:"2、密码安全控制",charIndex:358},{level:4,title:"3、命令历史、自动注销",slug:"_3、命令历史、自动注销",normalizedTitle:"3、命令历史、自动注销",charIndex:961},{level:3,title:"2、用户切换与提权",slug:"_2、用户切换与提权",normalizedTitle:"2、用户切换与提权",charIndex:1355},{level:4,title:"1、su 命令：切换用户",slug:"_1、su-命令-切换用户",normalizedTitle:"1、su 命令：切换用户",charIndex:1368},{level:3,title:"2、 用户帐号限制",slug:"_2、-用户帐号限制",normalizedTitle:"2、 用户帐号限制",charIndex:1955},{level:2,title:"二、系统引导和登录控制",slug:"二、系统引导和登录控制",normalizedTitle:"二、系统引导和登录控制",charIndex:3373},{level:3,title:"1、开关机安全控制",slug:"_1、开关机安全控制",normalizedTitle:"1、开关机安全控制",charIndex:3389},{level:4,title:"1、调整 BIOS 引导设置",slug:"_1、调整-bios-引导设置",normalizedTitle:"1、调整 bios 引导设置",charIndex:3402},{level:4,title:"2、禁止 Ctrl+Alt+Del 快捷键重启",slug:"_2、禁止-ctrl-alt-del-快捷键重启",normalizedTitle:"2、禁止 ctrl+alt+del 快捷键重启",charIndex:3569},{level:4,title:"3、限制更改 GRUB 引导参数",slug:"_3、限制更改-grub-引导参数",normalizedTitle:"3、限制更改 grub 引导参数",charIndex:3921},{level:3,title:"2、终端登录控制",slug:"_2、终端登录控制",normalizedTitle:"2、终端登录控制",charIndex:5472},{level:2,title:"",slug:"",normalizedTitle:"",charIndex:0},{level:3,title:"1、弱口令检测 ——John the Ripper",slug:"_1、弱口令检测-john-the-ripper",normalizedTitle:"1、弱口令检测 ——john the ripper",charIndex:5731},{level:3,title:"2、网络扫描 ——NMAP",slug:"_2、网络扫描-nmap",normalizedTitle:"2、网络扫描 ——nmap",charIndex:5830},{level:4,title:"1、安装 NMAP 软件包",slug:"_1、安装-nmap-软件包",normalizedTitle:"1、安装 nmap 软件包",charIndex:5847}],headersStr:"一、账号安全控制 1、基本安全措施 1、系统账号清理 2、密码安全控制 3、命令历史、自动注销 2、用户切换与提权 1、su 命令：切换用户 2、 用户帐号限制 二、系统引导和登录控制 1、开关机安全控制 1、调整 BIOS 引导设置 2、禁止 Ctrl+Alt+Del 快捷键重启 3、限制更改 GRUB 引导参数 2、终端登录控制  1、弱口令检测 ——John the Ripper 2、网络扫描 ——NMAP 1、安装 NMAP 软件包",content:'# 一、账号安全控制\n\n\n# 1、基本安全措施\n\n# 1、系统账号清理\n\ngrep "/sbin/nologin$" /etc/passwd // 查找登录 Shell 是 /sbin/nologin 的用户\n\nusermod -L 账号名称 // 锁定账号\n\npasswd -S 账号名称 // 查看账号状态\n\nusermod -U 账号名称 // 解锁账号\n\nchattr +i /etc/passwd /etc/shadow // 锁定文件\n\nIsattr /etc/passwd /etc/shadow // 查看为锁定的状态\n\nchattr -i /etc/passwd /etc/shadow // 解锁文件\n\nIsattr /etc/passwd /etc/shadow // 查看为解锁的状态\n\n# 2、密码安全控制\n\n/etc/pam.d/password-auth\n\npassword    requisite     pam_pwquality.so try_first_pass local_users_only retry=3 authtok_type=  difok=1 minlen=8 ucredit=-1 lcredit=-1 dcredit=-1 \n\n\n\n1\n2\n\n * difok= 定义新密码中必须要有几个字符和旧密码不同\n * minlen = 新密码的最小长度\n * ucredit= 新密码中可以包含的大写字母的最大数目。-1 至少一个\n * lcredit = 新密码中可以包含的小写字母的最大数\n * dcredit = 定新密码中可以包含的数字的最大数目\n\n注：这个密码强度的设定只对 "普通用户" 有限制作用，root 用户无论修改自己的密码还是修改普通用户的时候，不符合强度设置依然可以设置成功\n\n1、将密码的有效期设为 30 天，\n\n# vim /etc/login.defs              //适用于新建的用户\nPASS_MAX_DAYS   30\n# chage -M 30 lisi             //适用于已有的用户\n\n\n\n1\n2\n3\n4\n\n\n2、强制要求用户下次登录时重设密码\n\n# chage -d 0 lisi\n\n\n1\n\n\n# 3、命令历史、自动注销\n\n1、设置最多只记录 200 条历史命令\n\n# vim /etc/profile             //适用于新登录用户\nHISTSIZE=200\n# export HISTSIZE=200             //适用于当前用户\n\n\n1\n2\n3\n\n\n2、当用户退出已登录 Bash 环境以后，所记录的历史命令将自动清空\n\n# vim ~/.bash_logout \nhistory -c\nclear\n\n\n1\n2\n3\n\n\n3、当超过指定的时间没有任何输入时即自动注销终端，闲置超时由变量 TMOUT 来控制，默认单位为秒估）\n\n# vim /etc/profile             //适用于新登录用户\nexport TMOUT=600\n# export TMOUT=600             //适用于当前用户\n\n\n1\n2\n3\n\n\n\n# 2、用户切换与提权\n\n# 1、su 命令：切换用户\n\n# su - root\n\n\n1\n\n\n借助于 pam_wheel 认证模块，只允许极个别用户使用 su 命令进行切换\n\n将授权使用 su 命令的用户添加到 wheel 组，修改 /etc/pam. d/su 认证配置以启用 pam_wheel 认证\n\n# gpasswd -a zhangsan wheel\n正在将用户“zhangsan”加入到“wheel”组中\n# grep wheel /etc/group\nwheel:x:10:lisi,zhangsan\n# vim /etc/pam.d/su\nauth            required        pam_wheel.so use_uid             //去掉此行开头的＃号\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n启用 pam_wheel 认证以后，未加入到 wheel 组内的其他用户将无法使用 su 命令\n\n# su - zhangsan\n[zhangsan@localhost ~]$ su - root\n密码：\n上一次登录：四 7月 16 20:31:08 CST 2020pts/0 上\n# su - wangwu\n[wangwu@localhost ~]$ su - root\n密码：\nsu: 拒绝权限\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 2、 用户帐号限制\n\n * /etc/login.defs 配置文件是设置用户帐号限制的文件，可配置密码的最大过期天数，密码的最大长度约束等内容。\n\n * 该文件里的配置对 root 用户无效。此文件中的配置与 /etc/passwd 和 /etc/shadow 文件中的用户信息有冲突时，系统会以 /etc/passwd 和 /etc/shadow 为准。\n\n * /etc/login.defs 文件用于在 Linux 创建用户时，对用户的一些基本属性做默认设置，例如指定用户 UID 和 GID 的范围，用户的过期时间，密码的最大长度，等等。\n\n设置项                        含义\nMAIL_DIR /var/spool/mail   创建用户时，系统会在目录 /var/spool/mail 中创建一个用户邮箱，比如 lamp 用户的邮箱是\n                           /var/spool/mail/lamp。\nPASS_MAX_DAYS 99999        密码有效期，99999 是自 1970 年 1 月 1 日起密码有效的天数，相当于 273 年，可理解为密码始终有效。\nPASS_MIN_DAYS 0            表示自上次修改密码以来，最少隔多少天后用户才能再次修改密码，默认值是 0。\nPASS_MIN_LEN 5             指定密码的最小长度，默认不小于 5 位，但是现在用户登录时验证已经被 PAM 模块取代，所以这个选项并不生效。\nPASS_WARN_AGE 7            指定在密码到期前多少天，系统就开始通过用户密码即将到期，默认为 7 天。\nUID_MIN 500                指定最小 UID 为 500，也就是说，添加用户时，默认 UID 从 500 开始。注意，如果手工指定了一个用户的\n                           UID 是 550，那么下一个创建的用户的 UID 就会从 551 开始，哪怕 500~549 之间的 UID\n                           没有使用。\nUID_MAX 60000              指定用户最大的 UID 为 60000。\nGID_MIN 500                指定最小 GID 为 500，也就是在添加组时，组的 GID 从 500 开始。\nGID_MAX 60000              用户 GID 最大为 60000。\nCREATE_HOME yes            指定在创建用户时，是否同时创建用户主目录，yes 表示创建，no 则不创建，默认是 yes。\nUMASK 077                  用户主目录的权限默认设置为 077。\nUSERGROUPS_ENAB yes        指定删除用户的时候是否同时删除用户组，准备地说，这里指的是删除用户的初始组，此项的默认值为 yes。\nENCRYPT_METHOD SHA512      指定用户密码采用的加密规则，默认采用 SHA512，这是新的密码加密模式，原先的 Linux 只能用 DES 或 MD5\n                           加密。\n\n\n# 二、系统引导和登录控制\n\n\n# 1、开关机安全控制\n\n# 1、调整 BIOS 引导设置\n\n * （1）将第一优先引导设备 (First Boot Device) 设为当前系统所在磁盘。\n * （2）禁止从其他设备 (如光盘、U 盘、网络等) 引导系统，对应的项设为 "Disabled"。\n * （3）将 BOS 的安全级别改为 "setup"，并设置好管理密码，以防止未授权的修改。\n\n# 2、禁止 Ctrl+Alt+Del 快捷键重启\n\n# systemctl mask ctrl-alt-del.target              //禁止Ctrl+Alt+Del快捷键重启\nCreated symlink from /etc/systemd/system/ctrl-alt-del.target to /dev/null.\n# systemctl daemon-reload              //刷新配置\n# systemctl unmask ctrl-alt-del.target             //开启Ctrl+Alt+Del快捷键重启\n# systemctl daemon-reload             //刷新配置\n\n\n1\n2\n3\n4\n5\n\n\n# 3、限制更改 GRUB 引导参数\n\n# grub2-mkpasswd-pbkdf2 \n输入口令：\nReenter password: \nPBKDF2 hash of your password is grub.pbkdf2.sha512.10000.0231854D0AD301240629102B62FCFB09E7347E7E3254E34ACE0186A41BA65A2A749B6A5F59AC37B6431649D5D0F3238CFC2ED92C407468C0CE2B29C2F3FFF9A8.1906AC682652E7EEC736AC3F4E866D2C5038EB4AED72D980331CE80ED4D05A9929F043A5EB7190FD3775294CE50FCFA745C16E783A93E52F77E0D3A9695C246A\n# cp /boot/grub2/grub.cfg /boot/grub2/grub.cfg.bak\n# cp /etc/grub.d/00_header /etc/grub.d/00_header.bak\n# vim /etc/grub.d/00_header\n......\ncat<<  EOF\nset superusers = "root"\npassword_pbkdf2 root grub.pbkdf2.sha512.10000.0231854D0AD301240629102B62FCFB09E7347E7E3254E34ACE0186A41BA65A2A749B6A5F59AC37B6431649D5D0F3238CFC2ED92C407468C0CE2B29C2F3FFF9A8.1906AC682652E7EEC736AC3F4E866D2C5038EB4AED72D980331CE80ED4D05A9929F043A5EB7190FD3775294CE50FCFA745C16E783A93E52F77E0D3A9695C246A\nEOF\n# grub2-mkconfig -o /boot/grub2/grub.cfg\nGenerating grub configuration file ...\nFound linux image: /boot/vmlinuz-3.10.0-1127.8.2.el7.x86_64\nFound initrd image: /boot/initramfs-3.10.0-1127.8.2.el7.x86_64.img\nFound linux image: /boot/vmlinuz-3.10.0-1062.18.1.el7.x86_64\nFound initrd image: /boot/initramfs-3.10.0-1062.18.1.el7.x86_64.img\nFound linux image: /boot/vmlinuz-3.10.0-957.el7.x86_64\nFound initrd image: /boot/initramfs-3.10.0-957.el7.x86_64.img\nFound linux image: /boot/vmlinuz-0-rescue-fc8e001937cb4a35b289459e53090806\nFound initrd image: /boot/initramfs-0-rescue-fc8e001937cb4a35b289459e53090806.img\ndone\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 2、终端登录控制\n\n1、禁止 root 用户登录\n\n# vim /etc/securetty             //禁止root用户从tty5、tty6登录\ntty1\ntty2\ntty3\ntty4\n#tty5\n#tty6\ntty7\ntty8\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n2、禁止普通用户登录（root 用户不受限制），当手动删除 /etc/nologin 文件或者重新启动主机以后，即可恢复正常。\n\n# touch /etc/nologin\n\n\n1\n\n\n\n#\n\n三、弱口令检测、端口扫描\n\n\n# 1、弱口令检测 ——John the Ripper\n\n安装 John the Ripper\n\n\n\n\n\n查看已破解出的账户列表\n\n\n\n使用密码字典文件\n\n清空己破解出的账户列表后重新分析\n\n\n\n\n# 2、网络扫描 ——NMAP\n\n# 1、安装 NMAP 软件包\n\n# yum -y install nmap\n\n\n1\n\n\n1\n\n参数    说明                                  参数    说明\n-p    指定扫描的端口                             -n    禁用反向 DNS 解析（以加快扫描速度）\n-sS   查看目标端口是否正在监听，并立即断开连接；否则认为目标端口并未开放   -sT   查看目标端口正在监听服务，否则认为目标端口并未开放\n-sF   间接检测防火墙的健壮性                         -sU   探测目标主机提供哪些 UDP 服务\n-sP   快速判断目标主机是否存活                        -P0   跳过 ping 检测\n\n扫描常用的 TCP 端口\n\n\n\n扫描常用的 UDP 端口\n\n\n\n例如：\n\n检查 192.168.4.0/24 网段中有哪些主机提供 FTP 服务：nmap -p 21 192.168.4.0/24\n\n检测 192.168.4.0/24 网段中有哪些存活主机（能 ping 通)：nmap -n -sP 192.168.4.0/24\n\n检测 IP 地址位于 192.168.4.100-200 的主机是否开启文件共享服务：nmap -p 139，445 192.168.4.100-200',normalizedContent:'# 一、账号安全控制\n\n\n# 1、基本安全措施\n\n# 1、系统账号清理\n\ngrep "/sbin/nologin$" /etc/passwd // 查找登录 shell 是 /sbin/nologin 的用户\n\nusermod -l 账号名称 // 锁定账号\n\npasswd -s 账号名称 // 查看账号状态\n\nusermod -u 账号名称 // 解锁账号\n\nchattr +i /etc/passwd /etc/shadow // 锁定文件\n\nisattr /etc/passwd /etc/shadow // 查看为锁定的状态\n\nchattr -i /etc/passwd /etc/shadow // 解锁文件\n\nisattr /etc/passwd /etc/shadow // 查看为解锁的状态\n\n# 2、密码安全控制\n\n/etc/pam.d/password-auth\n\npassword    requisite     pam_pwquality.so try_first_pass local_users_only retry=3 authtok_type=  difok=1 minlen=8 ucredit=-1 lcredit=-1 dcredit=-1 \n\n\n\n1\n2\n\n * difok= 定义新密码中必须要有几个字符和旧密码不同\n * minlen = 新密码的最小长度\n * ucredit= 新密码中可以包含的大写字母的最大数目。-1 至少一个\n * lcredit = 新密码中可以包含的小写字母的最大数\n * dcredit = 定新密码中可以包含的数字的最大数目\n\n注：这个密码强度的设定只对 "普通用户" 有限制作用，root 用户无论修改自己的密码还是修改普通用户的时候，不符合强度设置依然可以设置成功\n\n1、将密码的有效期设为 30 天，\n\n# vim /etc/login.defs              //适用于新建的用户\npass_max_days   30\n# chage -m 30 lisi             //适用于已有的用户\n\n\n\n1\n2\n3\n4\n\n\n2、强制要求用户下次登录时重设密码\n\n# chage -d 0 lisi\n\n\n1\n\n\n# 3、命令历史、自动注销\n\n1、设置最多只记录 200 条历史命令\n\n# vim /etc/profile             //适用于新登录用户\nhistsize=200\n# export histsize=200             //适用于当前用户\n\n\n1\n2\n3\n\n\n2、当用户退出已登录 bash 环境以后，所记录的历史命令将自动清空\n\n# vim ~/.bash_logout \nhistory -c\nclear\n\n\n1\n2\n3\n\n\n3、当超过指定的时间没有任何输入时即自动注销终端，闲置超时由变量 tmout 来控制，默认单位为秒估）\n\n# vim /etc/profile             //适用于新登录用户\nexport tmout=600\n# export tmout=600             //适用于当前用户\n\n\n1\n2\n3\n\n\n\n# 2、用户切换与提权\n\n# 1、su 命令：切换用户\n\n# su - root\n\n\n1\n\n\n借助于 pam_wheel 认证模块，只允许极个别用户使用 su 命令进行切换\n\n将授权使用 su 命令的用户添加到 wheel 组，修改 /etc/pam. d/su 认证配置以启用 pam_wheel 认证\n\n# gpasswd -a zhangsan wheel\n正在将用户“zhangsan”加入到“wheel”组中\n# grep wheel /etc/group\nwheel:x:10:lisi,zhangsan\n# vim /etc/pam.d/su\nauth            required        pam_wheel.so use_uid             //去掉此行开头的＃号\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n启用 pam_wheel 认证以后，未加入到 wheel 组内的其他用户将无法使用 su 命令\n\n# su - zhangsan\n[zhangsan@localhost ~]$ su - root\n密码：\n上一次登录：四 7月 16 20:31:08 cst 2020pts/0 上\n# su - wangwu\n[wangwu@localhost ~]$ su - root\n密码：\nsu: 拒绝权限\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 2、 用户帐号限制\n\n * /etc/login.defs 配置文件是设置用户帐号限制的文件，可配置密码的最大过期天数，密码的最大长度约束等内容。\n\n * 该文件里的配置对 root 用户无效。此文件中的配置与 /etc/passwd 和 /etc/shadow 文件中的用户信息有冲突时，系统会以 /etc/passwd 和 /etc/shadow 为准。\n\n * /etc/login.defs 文件用于在 linux 创建用户时，对用户的一些基本属性做默认设置，例如指定用户 uid 和 gid 的范围，用户的过期时间，密码的最大长度，等等。\n\n设置项                        含义\nmail_dir /var/spool/mail   创建用户时，系统会在目录 /var/spool/mail 中创建一个用户邮箱，比如 lamp 用户的邮箱是\n                           /var/spool/mail/lamp。\npass_max_days 99999        密码有效期，99999 是自 1970 年 1 月 1 日起密码有效的天数，相当于 273 年，可理解为密码始终有效。\npass_min_days 0            表示自上次修改密码以来，最少隔多少天后用户才能再次修改密码，默认值是 0。\npass_min_len 5             指定密码的最小长度，默认不小于 5 位，但是现在用户登录时验证已经被 pam 模块取代，所以这个选项并不生效。\npass_warn_age 7            指定在密码到期前多少天，系统就开始通过用户密码即将到期，默认为 7 天。\nuid_min 500                指定最小 uid 为 500，也就是说，添加用户时，默认 uid 从 500 开始。注意，如果手工指定了一个用户的\n                           uid 是 550，那么下一个创建的用户的 uid 就会从 551 开始，哪怕 500~549 之间的 uid\n                           没有使用。\nuid_max 60000              指定用户最大的 uid 为 60000。\ngid_min 500                指定最小 gid 为 500，也就是在添加组时，组的 gid 从 500 开始。\ngid_max 60000              用户 gid 最大为 60000。\ncreate_home yes            指定在创建用户时，是否同时创建用户主目录，yes 表示创建，no 则不创建，默认是 yes。\numask 077                  用户主目录的权限默认设置为 077。\nusergroups_enab yes        指定删除用户的时候是否同时删除用户组，准备地说，这里指的是删除用户的初始组，此项的默认值为 yes。\nencrypt_method sha512      指定用户密码采用的加密规则，默认采用 sha512，这是新的密码加密模式，原先的 linux 只能用 des 或 md5\n                           加密。\n\n\n# 二、系统引导和登录控制\n\n\n# 1、开关机安全控制\n\n# 1、调整 bios 引导设置\n\n * （1）将第一优先引导设备 (first boot device) 设为当前系统所在磁盘。\n * （2）禁止从其他设备 (如光盘、u 盘、网络等) 引导系统，对应的项设为 "disabled"。\n * （3）将 bos 的安全级别改为 "setup"，并设置好管理密码，以防止未授权的修改。\n\n# 2、禁止 ctrl+alt+del 快捷键重启\n\n# systemctl mask ctrl-alt-del.target              //禁止ctrl+alt+del快捷键重启\ncreated symlink from /etc/systemd/system/ctrl-alt-del.target to /dev/null.\n# systemctl daemon-reload              //刷新配置\n# systemctl unmask ctrl-alt-del.target             //开启ctrl+alt+del快捷键重启\n# systemctl daemon-reload             //刷新配置\n\n\n1\n2\n3\n4\n5\n\n\n# 3、限制更改 grub 引导参数\n\n# grub2-mkpasswd-pbkdf2 \n输入口令：\nreenter password: \npbkdf2 hash of your password is grub.pbkdf2.sha512.10000.0231854d0ad301240629102b62fcfb09e7347e7e3254e34ace0186a41ba65a2a749b6a5f59ac37b6431649d5d0f3238cfc2ed92c407468c0ce2b29c2f3fff9a8.1906ac682652e7eec736ac3f4e866d2c5038eb4aed72d980331ce80ed4d05a9929f043a5eb7190fd3775294ce50fcfa745c16e783a93e52f77e0d3a9695c246a\n# cp /boot/grub2/grub.cfg /boot/grub2/grub.cfg.bak\n# cp /etc/grub.d/00_header /etc/grub.d/00_header.bak\n# vim /etc/grub.d/00_header\n......\ncat<<  eof\nset superusers = "root"\npassword_pbkdf2 root grub.pbkdf2.sha512.10000.0231854d0ad301240629102b62fcfb09e7347e7e3254e34ace0186a41ba65a2a749b6a5f59ac37b6431649d5d0f3238cfc2ed92c407468c0ce2b29c2f3fff9a8.1906ac682652e7eec736ac3f4e866d2c5038eb4aed72d980331ce80ed4d05a9929f043a5eb7190fd3775294ce50fcfa745c16e783a93e52f77e0d3a9695c246a\neof\n# grub2-mkconfig -o /boot/grub2/grub.cfg\ngenerating grub configuration file ...\nfound linux image: /boot/vmlinuz-3.10.0-1127.8.2.el7.x86_64\nfound initrd image: /boot/initramfs-3.10.0-1127.8.2.el7.x86_64.img\nfound linux image: /boot/vmlinuz-3.10.0-1062.18.1.el7.x86_64\nfound initrd image: /boot/initramfs-3.10.0-1062.18.1.el7.x86_64.img\nfound linux image: /boot/vmlinuz-3.10.0-957.el7.x86_64\nfound initrd image: /boot/initramfs-3.10.0-957.el7.x86_64.img\nfound linux image: /boot/vmlinuz-0-rescue-fc8e001937cb4a35b289459e53090806\nfound initrd image: /boot/initramfs-0-rescue-fc8e001937cb4a35b289459e53090806.img\ndone\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 2、终端登录控制\n\n1、禁止 root 用户登录\n\n# vim /etc/securetty             //禁止root用户从tty5、tty6登录\ntty1\ntty2\ntty3\ntty4\n#tty5\n#tty6\ntty7\ntty8\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n2、禁止普通用户登录（root 用户不受限制），当手动删除 /etc/nologin 文件或者重新启动主机以后，即可恢复正常。\n\n# touch /etc/nologin\n\n\n1\n\n\n\n#\n\n三、弱口令检测、端口扫描\n\n\n# 1、弱口令检测 ——john the ripper\n\n安装 john the ripper\n\n\n\n\n\n查看已破解出的账户列表\n\n\n\n使用密码字典文件\n\n清空己破解出的账户列表后重新分析\n\n\n\n\n# 2、网络扫描 ——nmap\n\n# 1、安装 nmap 软件包\n\n# yum -y install nmap\n\n\n1\n\n\n1\n\n参数    说明                                  参数    说明\n-p    指定扫描的端口                             -n    禁用反向 dns 解析（以加快扫描速度）\n-ss   查看目标端口是否正在监听，并立即断开连接；否则认为目标端口并未开放   -st   查看目标端口正在监听服务，否则认为目标端口并未开放\n-sf   间接检测防火墙的健壮性                         -su   探测目标主机提供哪些 udp 服务\n-sp   快速判断目标主机是否存活                        -p0   跳过 ping 检测\n\n扫描常用的 tcp 端口\n\n\n\n扫描常用的 udp 端口\n\n\n\n例如：\n\n检查 192.168.4.0/24 网段中有哪些主机提供 ftp 服务：nmap -p 21 192.168.4.0/24\n\n检测 192.168.4.0/24 网段中有哪些存活主机（能 ping 通)：nmap -n -sp 192.168.4.0/24\n\n检测 ip 地址位于 192.168.4.100-200 的主机是否开启文件共享服务：nmap -p 139，445 192.168.4.100-200',charsets:{cjk:!0}},{title:"fastdfs",frontmatter:{title:"fastdfs",categories:"fastdfs",tags:["分布式文件系统"],date:"2022-12-09T20:47:55.000Z",permalink:"/pages/988870/",readingShow:"top",description:"目录",meta:[{name:"image",content:"https://images2017.cnblogs.com/blog/856154/201710/856154-20171011144153840-1185141903.png"},{name:"twitter:title",content:"fastdfs"},{name:"twitter:description",content:"目录"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://images2017.cnblogs.com/blog/856154/201710/856154-20171011144153840-1185141903.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/06.%E5%AD%98%E5%82%A8/01.fastdfs.html"},{property:"og:type",content:"article"},{property:"og:title",content:"fastdfs"},{property:"og:description",content:"目录"},{property:"og:image",content:"https://images2017.cnblogs.com/blog/856154/201710/856154-20171011144153840-1185141903.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/06.%E5%AD%98%E5%82%A8/01.fastdfs.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:47:55.000Z"},{property:"article:tag",content:"分布式文件系统"},{itemprop:"name",content:"fastdfs"},{itemprop:"description",content:"目录"},{itemprop:"image",content:"https://images2017.cnblogs.com/blog/856154/201710/856154-20171011144153840-1185141903.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/06.%E5%AD%98%E5%82%A8/01.fastdfs.html",relativePath:"04.运维/06.存储/01.fastdfs.md",key:"v-477589d8",path:"/pages/988870/",headers:[{level:2,title:"一、FastDFS介绍",slug:"一、fastdfs介绍",normalizedTitle:"一、fastdfs介绍",charIndex:30},{level:3,title:"1、简介",slug:"_1、简介",normalizedTitle:"1、简介",charIndex:45},{level:3,title:"2、FastDFS的存储策略",slug:"_2、fastdfs的存储策略",normalizedTitle:"2、fastdfs的存储策略",charIndex:53},{level:3,title:"3、FastDFS的上传过程",slug:"_3、fastdfs的上传过程",normalizedTitle:"3、fastdfs的上传过程",charIndex:71},{level:3,title:"4、FastDFS的文件同步",slug:"_4、fastdfs的文件同步",normalizedTitle:"4、fastdfs的文件同步",charIndex:89},{level:3,title:"5、FastDFS的文件下载",slug:"_5、fastdfs的文件下载",normalizedTitle:"5、fastdfs的文件下载",charIndex:107},{level:2,title:"二、安装FastDFS环境",slug:"二、安装fastdfs环境",normalizedTitle:"二、安装fastdfs环境",charIndex:125},{level:3,title:"0、前言",slug:"_0、前言",normalizedTitle:"0、前言",charIndex:144},{level:3,title:"1、下载安装 libfastcommon",slug:"_1、下载安装-libfastcommon",normalizedTitle:"1、下载安装 libfastcommon",charIndex:154},{level:3,title:"2、下载安装FastDFS",slug:"_2、下载安装fastdfs",normalizedTitle:"2、下载安装fastdfs",charIndex:180},{level:3,title:"3、配置FastDFS跟踪器(Tracker)",slug:"_3、配置fastdfs跟踪器-tracker",normalizedTitle:"3、配置fastdfs跟踪器(tracker)",charIndex:199},{level:3,title:"4、配置 FastDFS 存储 (Storage)",slug:"_4、配置-fastdfs-存储-storage",normalizedTitle:"4、配置 fastdfs 存储 (storage)",charIndex:228},{level:3,title:"5、文件上传测试",slug:"_5、文件上传测试",normalizedTitle:"5、文件上传测试",charIndex:259},{level:2,title:"三、安装Nginx",slug:"三、安装nginx",normalizedTitle:"三、安装nginx",charIndex:271},{level:3,title:"1、安装nginx所需环境",slug:"_1、安装nginx所需环境",normalizedTitle:"1、安装nginx所需环境",charIndex:286},{level:3,title:"2、安装Nginx",slug:"_2、安装nginx",normalizedTitle:"2、安装nginx",charIndex:307},{level:3,title:"3、访问文件",slug:"_3、访问文件",normalizedTitle:"3、访问文件",charIndex:322},{level:2,title:"四、FastDFS 配置 Nginx 模块",slug:"四、fastdfs-配置-nginx-模块",normalizedTitle:"四、fastdfs 配置 nginx 模块",charIndex:332},{level:3,title:"1、安装配置Nginx模块",slug:"_1、安装配置nginx模块",normalizedTitle:"1、安装配置nginx模块",charIndex:359},{level:2,title:"五、Java客户端",slug:"五、java客户端",normalizedTitle:"五、java客户端",charIndex:376},{level:3,title:"1、首先需要搭建 FastDFS 客户端Java开发环境",slug:"_1、首先需要搭建-fastdfs-客户端java开发环境",normalizedTitle:"1、首先需要搭建 fastdfs 客户端java开发环境",charIndex:391},{level:3,title:"2、客户端API",slug:"_2、客户端api",normalizedTitle:"2、客户端api",charIndex:425},{level:3,title:"六、权限控制",slug:"六、权限控制",normalizedTitle:"六、权限控制",charIndex:439}],headersStr:"一、FastDFS介绍 1、简介 2、FastDFS的存储策略 3、FastDFS的上传过程 4、FastDFS的文件同步 5、FastDFS的文件下载 二、安装FastDFS环境 0、前言 1、下载安装 libfastcommon 2、下载安装FastDFS 3、配置FastDFS跟踪器(Tracker) 4、配置 FastDFS 存储 (Storage) 5、文件上传测试 三、安装Nginx 1、安装nginx所需环境 2、安装Nginx 3、访问文件 四、FastDFS 配置 Nginx 模块 1、安装配置Nginx模块 五、Java客户端 1、首先需要搭建 FastDFS 客户端Java开发环境 2、客户端API 六、权限控制",content:'# 用FastDFS一步步搭建文件管理系统\n\n目录\n\n * 一、FastDFS介绍\n * 1、简介\n * 2、FastDFS的存储策略\n * 3、FastDFS的上传过程\n * 4、FastDFS的文件同步\n * 5、FastDFS的文件下载\n * 二、安装FastDFS环境\n   * 0、前言\n   * 1、下载安装 libfastcommon\n   * 2、下载安装FastDFS\n   * 3、配置FastDFS跟踪器(Tracker)\n   * 4、配置 FastDFS 存储 (Storage)\n   * 5、文件上传测试\n * 三、安装Nginx\n   * 1、安装nginx所需环境　　\n   * 2、安装Nginx\n   * 3、访问文件\n * 四、FastDFS 配置 Nginx 模块\n   * 1、安装配置Nginx模块\n * 五、Java客户端\n   * 1、首先需要搭建 FastDFS 客户端Java开发环境\n   * 2、客户端API\n   * 六、权限控制\n\n----------------------------------------\n\n回到顶部\n\n\n# 一、FastDFS介绍\n\nFastDFS开源地址：https://github.com/happyfish100\n\n参考：分布式文件系统FastDFS设计原理\n\n参考：FastDFS分布式文件系统\n\n个人封装的FastDFS Java API：https://github.com/bojiangzhou/lyyzoo-fastdfs-java\n\n\n# 1、简介\n\nFastDFS 是一个开源的高性能分布式文件系统（DFS）。 它的主要功能包括：文件存储，文件同步和文件访问，以及高容量和负载平衡。主要解决了海量数据存储问题，特别适合以中小文件（建议范围：4KB < file_size <500MB）为载体的在线服务。\n\nFastDFS 系统有三个角色：跟踪服务器(Tracker Server)、存储服务器(Storage Server)和客户端(Client)。\n\nTracker Server：跟踪服务器，主要做调度工作，起到均衡的作用；负责管理所有的 storage server和 group，每个 storage 在启动后会连接 Tracker，告知自己所属 group 等信息，并保持周期性心跳。\n\nStorage Server：存储服务器，主要提供容量和备份服务；以 group 为单位，每个 group 内可以有多台 storage server，数据互为备份。\n\nClient：客户端，上传下载数据的服务器，也就是我们自己的项目所部署在的服务器。\n\n\n\n\n# 2、FastDFS的存储策略\n\n为了支持大容量，存储节点（服务器）采用了分卷（或分组）的组织方式。存储系统由一个或多个卷组成，卷与卷之间的文件是相互独立的，所有卷的文件容量累加就是整个存储系统中的文件容量。一个卷可以由一台或多台存储服务器组成，一个卷下的存储服务器中的文件都是相同的，卷中的多台存储服务器起到了冗余备份和负载均衡的作用。\n\n在卷中增加服务器时，同步已有的文件由系统自动完成，同步完成后，系统自动将新增服务器切换到线上提供服务。当存储空间不足或即将耗尽时，可以动态添加卷。只需要增加一台或多台服务器，并将它们配置为一个新的卷，这样就扩大了存储系统的容量。\n\n\n# 3、FastDFS的上传过程\n\nFastDFS向使用者提供基本文件访问接口，比如upload、download、append、delete等，以客户端库的方式提供给用户使用。\n\nStorage Server会定期的向Tracker Server发送自己的存储信息。当Tracker Server Cluster中的Tracker Server不止一个时，各个Tracker之间的关系是对等的，所以客户端上传时可以选择任意一个Tracker。\n\n当Tracker收到客户端上传文件的请求时，会为该文件分配一个可以存储文件的group，当选定了group后就要决定给客户端分配group中的哪一个storage server。当分配好storage server后，客户端向storage发送写文件请求，storage将会为文件分配一个数据存储目录。然后为文件分配一个fileid，最后根据以上的信息生成文件名存储文件。\n\n\n\n\n# 4、FastDFS的文件同步\n\n写文件时，客户端将文件写至group内一个storage server即认为写文件成功，storage server写完文件后，会由后台线程将文件同步至同group内其他的storage server。\n\n每个storage写文件后，同时会写一份binlog，binlog里不包含文件数据，只包含文件名等元信息，这份binlog用于后台同步，storage会记录向group内其他storage同步的进度，以便重启后能接上次的进度继续同步；进度以时间戳的方式进行记录，所以最好能保证集群内所有server的时钟保持同步。\n\nstorage的同步进度会作为元数据的一部分汇报到tracker上，tracke在选择读storage的时候会以同步进度作为参考。\n\n\n# 5、FastDFS的文件下载\n\n客户端uploadfile成功后，会拿到一个storage生成的文件名，接下来客户端根据这个文件名即可访问到该文件。\n\n\n\n跟upload file一样，在downloadfile时客户端可以选择任意tracker server。tracker发送download请求给某个tracker，必须带上文件名信息，tracke从文件名中解析出文件的group、大小、创建时间等信息，然后为该请求选择一个storage用来服务读请求。\n\n回到顶部\n\n\n# 二、安装FastDFS环境\n\n\n# 0、前言\n\n操作环境：CentOS7 X64，以下操作都是单机环境。\n\n我把所有的安装包下载到/softpackages/下，解压到当前目录。\n\n先做一件事，修改hosts，将文件服务器的ip与域名映射(单机TrackerServer环境)，因为后面很多配置里面都需要去配置服务器地址，ip变了，就只需要修改hosts即可。\n\n# vim /etc/hosts\n\n增加如下一行，这是我的IP\n192.168.51.128 file.ljzsg.com\n\n如果要本机访问虚拟机，在C:\\Windows\\System32\\drivers\\etc\\hosts中同样增加一行\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 1、下载安装 libfastcommon\n\nlibfastcommon是从 FastDFS 和 FastDHT 中提取出来的公共 C 函数库，基础环境，安装即可 。\n\n① 下载libfastcommon\n\n# wget https://github.com/happyfish100/libfastcommon/archive/V1.0.7.tar.gz\n\n\n1\n\n\n② 解压\n\n# tar -zxvf V1.0.7.tar.gz\n# cd libfastcommon-1.0.7\n\n\n1\n2\n\n\n③ 编译、安装\n\n# ./make.sh\n# ./make.sh install\n\n\n1\n2\n\n\n④ libfastcommon.so 安装到了/usr/lib64/libfastcommon.so，但是FastDFS主程序设置的lib目录是/usr/local/lib，所以需要创建软链接。\n\n# ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so\n# ln -s /usr/lib64/libfastcommon.so /usr/lib/libfastcommon.so\n# ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so\n# ln -s /usr/lib64/libfdfsclient.so /usr/lib/libfdfsclient.so \n\n\n1\n2\n3\n4\n\n\n\n# 2、下载安装FastDFS\n\n① 下载FastDFS\n\n# wget https://github.com/happyfish100/fastdfs/archive/V5.05.tar.gz\n\n\n1\n\n\n② 解压\n\n# tar -zxvf V5.05.tar.gz\n# cd fastdfs-5.05\n\n\n1\n2\n\n\n③ 编译、安装\n\n# ./make.sh\n# ./make.sh install\n\n\n1\n2\n\n\n④ 默认安装方式安装后的相应文件与目录 　　A、服务脚本：\n\n/etc/init.d/fdfs_storaged\n/etc/init.d/fdfs_tracker\n\n\n1\n2\n\n\nB、配置文件（这三个是作者给的样例配置文件） :\n\n/etc/fdfs/client.conf.sample\n/etc/fdfs/storage.conf.sample\n/etc/fdfs/tracker.conf.sample\n\n\n1\n2\n3\n\n\nC、命令工具在 /usr/bin/ 目录下：\n\n[](javascript:void(0)😉\n\nfdfs_appender_test\nfdfs_appender_test1\nfdfs_append_file\nfdfs_crc32\nfdfs_delete_file\nfdfs_download_file\nfdfs_file_info\nfdfs_monitor\nfdfs_storaged\nfdfs_test\nfdfs_test1\nfdfs_trackerd\nfdfs_upload_appender\nfdfs_upload_file\nstop.sh\nrestart.sh \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n[](javascript:void(0)😉\n\n⑤ FastDFS 服务脚本设置的 bin 目录是 /usr/local/bin， 但实际命令安装在 /usr/bin/ 下。\n\n两种方式：\n\n》 一是修改FastDFS 服务脚本中相应的命令路径，也就是把 /etc/init.d/fdfs_storaged 和 /etc/init.d/fdfs_tracker 两个脚本中的 /usr/local/bin 修改成 /usr/bin。\n\n# vim fdfs_trackerd 　　　　使用查找替换命令进统一修改:%s+/usr/local/bin+/usr/bin 　　　　# vim fdfs_storaged 　　　　使用查找替换命令进统一修改:%s+/usr/local/bin+/usr/bin\n\n\n\n》 二是建立 /usr/bin 到 /usr/local/bin 的软链接，我是用这种方式。\n\n# ln -s /usr/bin/fdfs_trackerd   /usr/local/bin\n# ln -s /usr/bin/fdfs_storaged   /usr/local/bin\n# ln -s /usr/bin/stop.sh         /usr/local/bin\n# ln -s /usr/bin/restart.sh      /usr/local/bin\n\n\n1\n2\n3\n4\n\n\n\n# 3、配置FastDFS跟踪器(Tracker)\n\n配置文件详细说明参考：FastDFS 配置文件详解\n\n① 进入 /etc/fdfs，复制 FastDFS 跟踪器样例配置文件 tracker.conf.sample，并重命名为 tracker.conf。\n\n# cd /etc/fdfs\n# cp tracker.conf.sample tracker.conf\n# vim tracker.conf\n\n\n1\n2\n3\n\n\n② 编辑tracker.conf ，标红的需要修改下，其它的默认即可。\n\n[](javascript:void(0)😉\n\n# 配置文件是否不生效，false 为生效\ndisabled=false\n# 提供服务的端口\nport=22122\n# Tracker 数据和日志目录地址(根目录必须存在,子目录会自动创建)\nbase_path=/ljzsg/fastdfs/tracker\n# HTTP 服务端口\nhttp.server_port=80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n[](javascript:void(0)😉\n\n③ 创建tracker基础数据目录，即base_path对应的目录\n\n# mkdir -p /ljzsg/fastdfs/tracker\n\n\n1\n\n\n④ 防火墙中打开跟踪端口（默认的22122）\n\n[](javascript:void(0)😉\n\n# vim /etc/sysconfig/iptables\n添加如下端口行：\n-A INPUT -m state --state NEW -m tcp -p tcp --dport 22122 -j ACCEPT\n重启防火墙：\n# service iptables restart\n\n\n1\n2\n3\n4\n5\n\n\n[](javascript:void(0)😉\n\n⑤ 启动Tracker\n\n初次成功启动，会在 /ljzsg/fdfsdfs/tracker/ (配置的base_path)下创建 data、logs 两个目录。\n\n可以用这种方式启动\n# /etc/init.d/fdfs_trackerd start\n\n也可以用这种方式启动，前提是上面创建了软链接，后面都用这种方式\n# service fdfs_trackerd start\n\n\n1\n2\n3\n4\n5\n\n\n查看 FastDFS Tracker 是否已成功启动 ，22122端口正在被监听，则算是Tracker服务安装成功。\n\n# netstat -unltp|grep fdfs\n\n\n1\n\n\n\n\n关闭Tracker命令：\n\n# service fdfs_trackerd stop\n\n\n1\n\n\n⑥ 设置Tracker开机启动\n\n# chkconfig fdfs_trackerd on\n\n或者：\n# vim /etc/rc.d/rc.local\n加入配置：\n/etc/init.d/fdfs_trackerd start \n\n\n1\n2\n3\n4\n5\n6\n\n\n⑦ tracker server 目录及文件结构\n\nTracker服务启动成功后，会在base_path下创建data、logs两个目录。目录结构如下：\n\n${base_path}\n  |__data\n  |   |__storage_groups.dat：存储分组信息\n  |   |__storage_servers.dat：存储服务器列表\n  |__logs\n  |   |__trackerd.log： tracker server 日志文件 \n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 4、配置 FastDFS 存储 (Storage)\n\n① 进入 /etc/fdfs 目录，复制 FastDFS 存储器样例配置文件 storage.conf.sample，并重命名为 storage.conf\n\n# cd /etc/fdfs\n# cp storage.conf.sample storage.conf# vim storage.conf\n\n\n1\n2\n\n\n② 编辑storage.conf\n\n标红的需要修改，其它的默认即可。\n\n[](javascript:void(0)😉\n\n# 配置文件是否不生效，false 为生效\ndisabled=false \n\n# 指定此 storage server 所在 组(卷)\ngroup_name=group1\n\n# storage server 服务端口\nport=23000\n\n# 心跳间隔时间，单位为秒 (这里是指主动向 tracker server 发送心跳)\nheart_beat_interval=30\n\n# Storage 数据和日志目录地址(根目录必须存在，子目录会自动生成)\nbase_path=/ljzsg/fastdfs/storage\n\n# 存放文件时 storage server 支持多个路径。这里配置存放文件的基路径数目，通常只配一个目录。\nstore_path_count=1\n\n\n# 逐一配置 store_path_count 个路径，索引号基于 0。\n# 如果不配置 store_path0，那它就和 base_path 对应的路径一样。\nstore_path0=/ljzsg/fastdfs/file\n\n# FastDFS 存储文件时，采用了两级目录。这里配置存放文件的目录个数。 \n# 如果本参数只为 N（如： 256），那么 storage server 在初次运行时，会在 store_path 下自动创建 N * N 个存放文件的子目录。\nsubdir_count_per_path=256\n\n# tracker_server 的列表 ，会主动连接 tracker_server\n# 有多个 tracker server 时，每个 tracker server 写一行\ntracker_server=file.ljzsg.com:22122# 允许系统同步的时间段 (默认是全天) 。一般用于避免高峰同步产生一些问题而设定。sync_start_time=00:00sync_end_time=23:59\n# 访问端口\nhttp.server_port=80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n[](javascript:void(0)😉\n\n③ 创建Storage基础数据目录，对应base_path目录\n\n# mkdir -p /ljzsg/fastdfs/storage\n\n# 这是配置的store_path0路径\n# mkdir -p /ljzsg/fastdfs/file\n\n\n1\n2\n3\n4\n\n\n④ 防火墙中打开存储器端口（默认的 23000）\n\n[](javascript:void(0)😉\n\n# vim /etc/sysconfig/iptables\n\n添加如下端口行：\n-A INPUT -m state --state NEW -m tcp -p tcp --dport 23000 -j ACCEPT\n\n重启防火墙：\n# service iptables restart\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n[](javascript:void(0)😉\n\n\n\n⑤ 启动 Storage\n\n启动Storage前确保Tracker是启动的。初次启动成功，会在 /ljzsg/fastdfs/storage 目录下创建 data、 logs 两个目录。\n\n可以用这种方式启动\n# /etc/init.d/fdfs_storaged start\n\n也可以用这种方式，后面都用这种\n# service fdfs_storaged start\n\n\n1\n2\n3\n4\n5\n\n\n查看 Storage 是否成功启动，23000 端口正在被监听，就算 Storage 启动成功。\n\n# netstat -unltp|grep fdfs\n\n\n1\n\n\n\n\n关闭Storage命令：\n\n# service fdfs_storaged stop\n\n\n1\n\n\n查看Storage和Tracker是否在通信：\n\n/usr/bin/fdfs_monitor /etc/fdfs/storage.conf\n\n\n1\n\n\n\n\n⑥ 设置 Storage 开机启动\n\n# chkconfig fdfs_storaged on\n或者：\n# vim /etc/rc.d/rc.local\n加入配置：\n/etc/init.d/fdfs_storaged start\n\n\n1\n2\n3\n4\n5\n\n\n⑦ Storage 目录\n\n同 Tracker，Storage 启动成功后，在base_path 下创建了data、logs目录，记录着 Storage Server 的信息。\n\n在 store_path0 目录下，创建了N*N个子目录：\n\n\n\n\n# 5、文件上传测试\n\n① 修改 Tracker 服务器中的客户端配置文件\n\n# cd /etc/fdfs\n# cp client.conf.sample client.conf\n# vim client.conf\n\n\n1\n2\n3\n\n\n修改如下配置即可，其它默认。\n\n# Client 的数据和日志目录\nbase_path=/ljzsg/fastdfs/client\n\n# Tracker端口\ntracker_server=file.ljzsg.com:22122\n\n\n1\n2\n3\n4\n5\n\n\n② 上传测试\n\n在linux内部执行如下命令上传 namei.jpeg 图片\n\n# /usr/bin/fdfs_upload_file /etc/fdfs/client.conf namei.jpeg\n\n\n1\n\n\n上传成功后返回文件ID号：group1/M00/00/00/wKgz6lnduTeAMdrcAAEoRmXZPp870.jpeg\n\n\n\n返回的文件ID由group、存储目录、两级子目录、fileid、文件后缀名（由客户端指定，主要用于区分文件类型）拼接而成。\n\n\n\n回到顶部\n\n\n# 三、安装Nginx\n\n上面将文件上传成功了，但我们无法下载。因此安装Nginx作为服务器以支持Http方式访问文件。同时，后面安装FastDFS的Nginx模块也需要Nginx环境。\n\nNginx只需要安装到StorageServer所在的服务器即可，用于访问文件。我这里由于是单机，TrackerServer和StorageServer在一台服务器上。\n\n\n# 1、安装nginx所需环境\n\n① gcc 安装\n\n# yum install gcc-c++\n\n\n1\n\n\n② PCRE pcre-devel 安装\n\n# yum install -y pcre pcre-devel\n\n\n1\n\n\n③ zlib 安装\n\n# yum install -y zlib zlib-devel\n\n\n1\n\n\n④ OpenSSL 安装\n\n# yum install -y openssl openssl-devel\n\n\n1\n\n\n\n# 2、安装Nginx\n\n① 下载nginx\n\n# wget -c https://nginx.org/download/nginx-1.12.1.tar.gz\n\n\n1\n\n\n② 解压\n\n# tar -zxvf nginx-1.12.1.tar.gz\n# cd nginx-1.12.1\n\n\n1\n2\n\n\n③ 使用默认配置\n\n# ./configure\n\n\n1\n\n\n④ 编译、安装\n\n# make\n# make install\n\n\n1\n2\n\n\n⑤ 启动nginx\n\n[](javascript:void(0)😉\n\n# cd /usr/local/nginx/sbin/\n# ./nginx \n\n其它命令\n# ./nginx -s stop\n# ./nginx -s quit\n# ./nginx -s reload\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n[](javascript:void(0)😉\n\n⑥ 设置开机启动\n\n[](javascript:void(0)😉\n\n# vim /etc/rc.local\n\n添加一行：\n/usr/local/nginx/sbin/nginx# 设置执行权限# chmod 755 rc.local\n\n\n1\n2\n3\n4\n\n\n[](javascript:void(0)😉\n\n⑦ 查看nginx的版本及模块\n\n/usr/local/nginx/sbin/nginx -V\n\n\n1\n\n\n\n\n⑧ 防火墙中打开Nginx端口（默认的 80）\n\n添加后就能在本机使用80端口访问了。\n\n[](javascript:void(0)😉\n\n# vim /etc/sysconfig/iptables\n\n添加如下端口行：\n-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT\n\n重启防火墙：\n# service iptables restart\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n[](javascript:void(0)😉\n\n\n\n\n# 3、访问文件\n\n简单的测试访问文件\n\n① 修改nginx.conf\n\n[](javascript:void(0)😉\n\n# vim /usr/local/nginx/conf/nginx.conf\n\n添加如下行，将 /group1/M00 映射到 /ljzsg/fastdfs/file/data\nlocation /group1/M00 {\n    alias /ljzsg/fastdfs/file/data;\n}# 重启nginx# /usr/local/nginx/sbin/nginx -s reload\n\n\n1\n2\n3\n4\n5\n6\n\n\n[](javascript:void(0)😉\n\n\n\n② 在浏览器访问之前上传的图片、成功。\n\nhttp://file.ljzsg.com/group1/M00/00/00/wKgz6lnduTeAMdrcAAEoRmXZPp870.jpeg\n\n回到顶部\n\n\n# 四、FastDFS 配置 Nginx 模块\n\n\n# 1、安装配置Nginx模块\n\n① fastdfs-nginx-module 模块说明\n\nFastDFS 通过 Tracker 服务器，将文件放在 Storage 服务器存储， 但是同组存储服务器之间需要进行文件复制， 有同步延迟的问题。\n\n假设 Tracker 服务器将文件上传到了 192.168.51.128，上传成功后文件 ID已经返回给客户端。\n\n此时 FastDFS 存储集群机制会将这个文件同步到同组存储 192.168.51.129，在文件还没有复制完成的情况下，客户端如果用这个文件 ID 在 192.168.51.129 上取文件,就会出现文件无法访问的错误。\n\n而 fastdfs-nginx-module 可以重定向文件链接到源服务器取文件，避免客户端由于复制延迟导致的文件无法访问错误。\n\n② 下载 fastdfs-nginx-module、解压\n\n[](javascript:void(0)😉\n\n# 这里为啥这么长一串呢，因为最新版的master与当前nginx有些版本问题。\n# wget https://github.com/happyfish100/fastdfs-nginx-module/archive/5e5f3566bbfa57418b5506aaefbe107a42c9fcb1.zip\n\n# 解压\n# unzip 5e5f3566bbfa57418b5506aaefbe107a42c9fcb1.zip\n\n# 重命名\n# mv fastdfs-nginx-module-5e5f3566bbfa57418b5506aaefbe107a42c9fcb1  fastdfs-nginx-module-master\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n[](javascript:void(0)😉\n\n③ 配置Nginx\n\n在nginx中添加模块\n\n[](javascript:void(0)😉\n\n# 先停掉nginx服务# /usr/local/nginx/sbin/nginx -s stop进入解压包目录\n# cd /softpackages/nginx-1.12.1/\n\n# 添加模块\n# ./configure --add-module=../fastdfs-nginx-module-master/src\n\n重新编译、安装\n# make && make install\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n[](javascript:void(0)😉\n\n④ 查看Nginx的模块\n\n# /usr/local/nginx/sbin/nginx -V\n\n\n1\n\n\n有下面这个就说明添加模块成功\n\n\n\n⑤ 复制 fastdfs-nginx-module 源码中的配置文件到/etc/fdfs 目录， 并修改\n\n# cd /softpackages/fastdfs-nginx-module-master/src\n\n# cp mod_fastdfs.conf /etc/fdfs/\n\n\n1\n2\n3\n\n\n修改如下配置，其它默认\n\n[](javascript:void(0)😉\n\n# 连接超时时间connect_timeout=10\n\n# Tracker Server\ntracker_server=file.ljzsg.com:22122\n# StorageServer 默认端口\nstorage_server_port=23000\n\n# 如果文件ID的uri中包含/group**，则要设置为true\nurl_have_group_name = true\n\n# Storage 配置的store_path0路径，必须和storage.conf中的一致\nstore_path0=/ljzsg/fastdfs/file\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n[](javascript:void(0)😉\n\n⑥ 复制 FastDFS 的部分配置文件到/etc/fdfs 目录\n\n# cd /softpackages/fastdfs-5.05/conf/\n\n# cp anti-steal.jpg http.conf mime.types /etc/fdfs/\n\n\n1\n2\n3\n\n\n⑦ 配置nginx，修改nginx.conf\n\n# vim /usr/local/nginx/conf/nginx.conf\n\n\n1\n\n\n修改配置，其它的默认\n\n在80端口下添加fastdfs-nginx模块\n\nlocation ~/group([0-9])/M00 {\n    ngx_fastdfs_module;\n}\n\n\n1\n2\n3\n\n\n\n\n注意：\n\nlisten 80 端口值是要与 /etc/fdfs/storage.conf 中的 http.server_port=80 (前面改成80了)相对应。如果改成其它端口，则需要统一，同时在防火墙中打开该端口。\n\nlocation 的配置，如果有多个group则配置location ~/group([0-9])/M00 ，没有则不用配group。\n\n⑧ 在/ljzsg/fastdfs/file 文件存储目录下创建软连接，将其链接到实际存放数据的目录，这一步可以省略。\n\n# ln -s /ljzsg/fastdfs/file/data/ /ljzsg/fastdfs/file/data/M00 \n\n\n1\n\n\n⑨ 启动nginx\n\n# /usr/local/nginx/sbin/nginx\n\n\n1\n\n\n打印处如下就算配置成功\n\n\n\n⑩ 在地址栏访问。\n\n能下载文件就算安装成功。注意和第三点中直接使用nginx路由访问不同的是，这里配置 fastdfs-nginx-module 模块，可以重定向文件链接到源服务器取文件。\n\nhttp://file.ljzsg.com/group1/M00/00/00/wKgz6lnduTeAMdrcAAEoRmXZPp870.jpeg\n\n最终部署结构图(盗的图)：可以按照下面的结构搭建环境。\n\n\n\n回到顶部\n\n\n# 五、Java客户端\n\n前面文件系统平台搭建好了，现在就要写客户端代码在系统中实现上传下载，这里只是简单的测试代码。\n\n\n# 1、首先需要搭建 FastDFS 客户端Java开发环境\n\n① 项目中使用maven进行依赖管理，可以在pom.xml中引入如下依赖即可：\n\n<dependency>\n   <groupId>net.oschina.zcx7878</groupId>\n   <artifactId>fastdfs-client-java</artifactId>\n   <version>1.27.0.0</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n其它的方式，参考官方文档：https://github.com/happyfish100/fastdfs-client-java\n\n② 引入配置文件\n\n可直接复制包下的 fastdfs-client.properties.sample 或者 fdfs_client.conf.sample，到你的项目中，去掉.sample。\n\n\n\n我这里直接复制 fastdfs-client.properties.sample 中的配置到项目配置文件 config.properties 中，修改tracker_servers。只需要加载这个配置文件即可\n\n\n\n\n# 2、客户端API\n\n个人封装的FastDFS Java API以同步到github：https://github.com/bojiangzhou/lyyzoo-fastdfs-java.git\n\n\n# 六、权限控制\n\n前面使用nginx支持http方式访问文件，但所有人都能直接访问这个文件服务器了，所以做一下权限控制。\n\nFastDFS的权限控制是在服务端开启token验证，客户端根据文件名、当前unix时间戳、秘钥获取token，在地址中带上token参数即可通过http方式访问文件。\n\n① 服务端开启token验证\n\n[](javascript:void(0)😉\n\n修改http.conf\n# vim /etc/fdfs/http.conf\n\n设置为true表示开启token验证\nhttp.anti_steal.check_token=true设置token失效的时间单位为秒(s)http.anti_steal.token_ttl=1800\n密钥，跟客户端配置文件的fastdfs.http_secret_key保持一致\nhttp.anti_steal.secret_key=FASTDFS1234567890\n\n如果token检查失败，返回的页面\nhttp.anti_steal.token_check_fail=/ljzsg/fastdfs/page/403.html\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n[](javascript:void(0)😉\n\n记得重启服务。\n\n② 配置客户端\n\n客户端只需要设置如下两个参数即可，两边的密钥保持一致。\n\n# token 防盗链功能\nfastdfs.http_anti_steal_token=true\n# 密钥\nfastdfs.http_secret_key=FASTDFS1234567890\n\n\n1\n2\n3\n4\n\n\n③ 客户端生成token\n\n访问文件需要带上生成的token以及unix时间戳，所以返回的token是token和时间戳的拼接。\n\n之后，将token拼接在地址后即可访问：file.ljzsg.com/group1/M00/00/00/wKgzgFnkaXqAIfXyAAEoRmXZPp878.jpeg?token=078d370098b03e9020b82c829c205e1f&ts=1508141521\n\n[](javascript:void(0)😉\n\n 1     /**\n 2      * 获取访问服务器的token，拼接到地址后面\n 3      *\n 4      * @param filepath 文件路径 group1/M00/00/00/wKgzgFnkTPyAIAUGAAEoRmXZPp876.jpeg\n 5      * @param httpSecretKey 密钥\n 6      * @return 返回token，如： token=078d370098b03e9020b82c829c205e1f&ts=1508141521\n 7      */\n 8     public static String getToken(String filepath, String httpSecretKey){\n 9         // unix seconds\n10         int ts = (int) Instant.now().getEpochSecond();\n11         // token\n12         String token = "null";\n13         try {\n14             token = ProtoCommon.getToken(getFilename(filepath), ts, httpSecretKey);\n15         } catch (UnsupportedEncodingException e) {\n16             e.printStackTrace();\n17         } catch (NoSuchAlgorithmException e) {\n18             e.printStackTrace();\n19         } catch (MyException e) {\n20             e.printStackTrace();\n21         }\n22 \n23         StringBuilder sb = new StringBuilder();\n24         sb.append("token=").append(token);\n25         sb.append("&ts=").append(ts);\n26 \n27         return sb.toString();\n28     }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n[](javascript:void(0)😉\n\n④ 注意事项\n\n如果生成的token验证无法通过，请进行如下两项检查： 　　A. 确认调用token生成函数(ProtoCommon.getToken)，传递的文件ID中没有包含group name。传递的文件ID格式形如：M00/00/00/wKgzgFnkTPyAIAUGAAEoRmXZPp876.jpeg\n\nB. 确认服务器时间基本是一致的，注意服务器时间不能相差太多，不要相差到分钟级别。\n\n⑤ 对比下发现，如果系统文件隐私性较高，可以直接通过fastdfs-client提供的API去访问即可，不用再配置Nginx走http访问。配置Nginx的主要目的是为了快速访问服务器的文件(如图片)，如果还要加权限验证，则需要客户端生成token，其实已经没有多大意义。\n\n关键是，这里我没找到FastDFS如何对部分资源加token验证，部分开放。有知道的还请留言。\n\n----------------------------------------\n\nOK，以上就是单机中使用FastDFS搭建文件系统并上传下载的过程。\n\n完！！！\n\n原文链接：https://www.cnblogs.com/chiangchou/p/fastdfs.html',normalizedContent:'# 用fastdfs一步步搭建文件管理系统\n\n目录\n\n * 一、fastdfs介绍\n * 1、简介\n * 2、fastdfs的存储策略\n * 3、fastdfs的上传过程\n * 4、fastdfs的文件同步\n * 5、fastdfs的文件下载\n * 二、安装fastdfs环境\n   * 0、前言\n   * 1、下载安装 libfastcommon\n   * 2、下载安装fastdfs\n   * 3、配置fastdfs跟踪器(tracker)\n   * 4、配置 fastdfs 存储 (storage)\n   * 5、文件上传测试\n * 三、安装nginx\n   * 1、安装nginx所需环境　　\n   * 2、安装nginx\n   * 3、访问文件\n * 四、fastdfs 配置 nginx 模块\n   * 1、安装配置nginx模块\n * 五、java客户端\n   * 1、首先需要搭建 fastdfs 客户端java开发环境\n   * 2、客户端api\n   * 六、权限控制\n\n----------------------------------------\n\n回到顶部\n\n\n# 一、fastdfs介绍\n\nfastdfs开源地址：https://github.com/happyfish100\n\n参考：分布式文件系统fastdfs设计原理\n\n参考：fastdfs分布式文件系统\n\n个人封装的fastdfs java api：https://github.com/bojiangzhou/lyyzoo-fastdfs-java\n\n\n# 1、简介\n\nfastdfs 是一个开源的高性能分布式文件系统（dfs）。 它的主要功能包括：文件存储，文件同步和文件访问，以及高容量和负载平衡。主要解决了海量数据存储问题，特别适合以中小文件（建议范围：4kb < file_size <500mb）为载体的在线服务。\n\nfastdfs 系统有三个角色：跟踪服务器(tracker server)、存储服务器(storage server)和客户端(client)。\n\ntracker server：跟踪服务器，主要做调度工作，起到均衡的作用；负责管理所有的 storage server和 group，每个 storage 在启动后会连接 tracker，告知自己所属 group 等信息，并保持周期性心跳。\n\nstorage server：存储服务器，主要提供容量和备份服务；以 group 为单位，每个 group 内可以有多台 storage server，数据互为备份。\n\nclient：客户端，上传下载数据的服务器，也就是我们自己的项目所部署在的服务器。\n\n\n\n\n# 2、fastdfs的存储策略\n\n为了支持大容量，存储节点（服务器）采用了分卷（或分组）的组织方式。存储系统由一个或多个卷组成，卷与卷之间的文件是相互独立的，所有卷的文件容量累加就是整个存储系统中的文件容量。一个卷可以由一台或多台存储服务器组成，一个卷下的存储服务器中的文件都是相同的，卷中的多台存储服务器起到了冗余备份和负载均衡的作用。\n\n在卷中增加服务器时，同步已有的文件由系统自动完成，同步完成后，系统自动将新增服务器切换到线上提供服务。当存储空间不足或即将耗尽时，可以动态添加卷。只需要增加一台或多台服务器，并将它们配置为一个新的卷，这样就扩大了存储系统的容量。\n\n\n# 3、fastdfs的上传过程\n\nfastdfs向使用者提供基本文件访问接口，比如upload、download、append、delete等，以客户端库的方式提供给用户使用。\n\nstorage server会定期的向tracker server发送自己的存储信息。当tracker server cluster中的tracker server不止一个时，各个tracker之间的关系是对等的，所以客户端上传时可以选择任意一个tracker。\n\n当tracker收到客户端上传文件的请求时，会为该文件分配一个可以存储文件的group，当选定了group后就要决定给客户端分配group中的哪一个storage server。当分配好storage server后，客户端向storage发送写文件请求，storage将会为文件分配一个数据存储目录。然后为文件分配一个fileid，最后根据以上的信息生成文件名存储文件。\n\n\n\n\n# 4、fastdfs的文件同步\n\n写文件时，客户端将文件写至group内一个storage server即认为写文件成功，storage server写完文件后，会由后台线程将文件同步至同group内其他的storage server。\n\n每个storage写文件后，同时会写一份binlog，binlog里不包含文件数据，只包含文件名等元信息，这份binlog用于后台同步，storage会记录向group内其他storage同步的进度，以便重启后能接上次的进度继续同步；进度以时间戳的方式进行记录，所以最好能保证集群内所有server的时钟保持同步。\n\nstorage的同步进度会作为元数据的一部分汇报到tracker上，tracke在选择读storage的时候会以同步进度作为参考。\n\n\n# 5、fastdfs的文件下载\n\n客户端uploadfile成功后，会拿到一个storage生成的文件名，接下来客户端根据这个文件名即可访问到该文件。\n\n\n\n跟upload file一样，在downloadfile时客户端可以选择任意tracker server。tracker发送download请求给某个tracker，必须带上文件名信息，tracke从文件名中解析出文件的group、大小、创建时间等信息，然后为该请求选择一个storage用来服务读请求。\n\n回到顶部\n\n\n# 二、安装fastdfs环境\n\n\n# 0、前言\n\n操作环境：centos7 x64，以下操作都是单机环境。\n\n我把所有的安装包下载到/softpackages/下，解压到当前目录。\n\n先做一件事，修改hosts，将文件服务器的ip与域名映射(单机trackerserver环境)，因为后面很多配置里面都需要去配置服务器地址，ip变了，就只需要修改hosts即可。\n\n# vim /etc/hosts\n\n增加如下一行，这是我的ip\n192.168.51.128 file.ljzsg.com\n\n如果要本机访问虚拟机，在c:\\windows\\system32\\drivers\\etc\\hosts中同样增加一行\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 1、下载安装 libfastcommon\n\nlibfastcommon是从 fastdfs 和 fastdht 中提取出来的公共 c 函数库，基础环境，安装即可 。\n\n① 下载libfastcommon\n\n# wget https://github.com/happyfish100/libfastcommon/archive/v1.0.7.tar.gz\n\n\n1\n\n\n② 解压\n\n# tar -zxvf v1.0.7.tar.gz\n# cd libfastcommon-1.0.7\n\n\n1\n2\n\n\n③ 编译、安装\n\n# ./make.sh\n# ./make.sh install\n\n\n1\n2\n\n\n④ libfastcommon.so 安装到了/usr/lib64/libfastcommon.so，但是fastdfs主程序设置的lib目录是/usr/local/lib，所以需要创建软链接。\n\n# ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so\n# ln -s /usr/lib64/libfastcommon.so /usr/lib/libfastcommon.so\n# ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so\n# ln -s /usr/lib64/libfdfsclient.so /usr/lib/libfdfsclient.so \n\n\n1\n2\n3\n4\n\n\n\n# 2、下载安装fastdfs\n\n① 下载fastdfs\n\n# wget https://github.com/happyfish100/fastdfs/archive/v5.05.tar.gz\n\n\n1\n\n\n② 解压\n\n# tar -zxvf v5.05.tar.gz\n# cd fastdfs-5.05\n\n\n1\n2\n\n\n③ 编译、安装\n\n# ./make.sh\n# ./make.sh install\n\n\n1\n2\n\n\n④ 默认安装方式安装后的相应文件与目录 　　a、服务脚本：\n\n/etc/init.d/fdfs_storaged\n/etc/init.d/fdfs_tracker\n\n\n1\n2\n\n\nb、配置文件（这三个是作者给的样例配置文件） :\n\n/etc/fdfs/client.conf.sample\n/etc/fdfs/storage.conf.sample\n/etc/fdfs/tracker.conf.sample\n\n\n1\n2\n3\n\n\nc、命令工具在 /usr/bin/ 目录下：\n\n[](javascript:void(0)😉\n\nfdfs_appender_test\nfdfs_appender_test1\nfdfs_append_file\nfdfs_crc32\nfdfs_delete_file\nfdfs_download_file\nfdfs_file_info\nfdfs_monitor\nfdfs_storaged\nfdfs_test\nfdfs_test1\nfdfs_trackerd\nfdfs_upload_appender\nfdfs_upload_file\nstop.sh\nrestart.sh \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n[](javascript:void(0)😉\n\n⑤ fastdfs 服务脚本设置的 bin 目录是 /usr/local/bin， 但实际命令安装在 /usr/bin/ 下。\n\n两种方式：\n\n》 一是修改fastdfs 服务脚本中相应的命令路径，也就是把 /etc/init.d/fdfs_storaged 和 /etc/init.d/fdfs_tracker 两个脚本中的 /usr/local/bin 修改成 /usr/bin。\n\n# vim fdfs_trackerd 　　　　使用查找替换命令进统一修改:%s+/usr/local/bin+/usr/bin 　　　　# vim fdfs_storaged 　　　　使用查找替换命令进统一修改:%s+/usr/local/bin+/usr/bin\n\n\n\n》 二是建立 /usr/bin 到 /usr/local/bin 的软链接，我是用这种方式。\n\n# ln -s /usr/bin/fdfs_trackerd   /usr/local/bin\n# ln -s /usr/bin/fdfs_storaged   /usr/local/bin\n# ln -s /usr/bin/stop.sh         /usr/local/bin\n# ln -s /usr/bin/restart.sh      /usr/local/bin\n\n\n1\n2\n3\n4\n\n\n\n# 3、配置fastdfs跟踪器(tracker)\n\n配置文件详细说明参考：fastdfs 配置文件详解\n\n① 进入 /etc/fdfs，复制 fastdfs 跟踪器样例配置文件 tracker.conf.sample，并重命名为 tracker.conf。\n\n# cd /etc/fdfs\n# cp tracker.conf.sample tracker.conf\n# vim tracker.conf\n\n\n1\n2\n3\n\n\n② 编辑tracker.conf ，标红的需要修改下，其它的默认即可。\n\n[](javascript:void(0)😉\n\n# 配置文件是否不生效，false 为生效\ndisabled=false\n# 提供服务的端口\nport=22122\n# tracker 数据和日志目录地址(根目录必须存在,子目录会自动创建)\nbase_path=/ljzsg/fastdfs/tracker\n# http 服务端口\nhttp.server_port=80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n[](javascript:void(0)😉\n\n③ 创建tracker基础数据目录，即base_path对应的目录\n\n# mkdir -p /ljzsg/fastdfs/tracker\n\n\n1\n\n\n④ 防火墙中打开跟踪端口（默认的22122）\n\n[](javascript:void(0)😉\n\n# vim /etc/sysconfig/iptables\n添加如下端口行：\n-a input -m state --state new -m tcp -p tcp --dport 22122 -j accept\n重启防火墙：\n# service iptables restart\n\n\n1\n2\n3\n4\n5\n\n\n[](javascript:void(0)😉\n\n⑤ 启动tracker\n\n初次成功启动，会在 /ljzsg/fdfsdfs/tracker/ (配置的base_path)下创建 data、logs 两个目录。\n\n可以用这种方式启动\n# /etc/init.d/fdfs_trackerd start\n\n也可以用这种方式启动，前提是上面创建了软链接，后面都用这种方式\n# service fdfs_trackerd start\n\n\n1\n2\n3\n4\n5\n\n\n查看 fastdfs tracker 是否已成功启动 ，22122端口正在被监听，则算是tracker服务安装成功。\n\n# netstat -unltp|grep fdfs\n\n\n1\n\n\n\n\n关闭tracker命令：\n\n# service fdfs_trackerd stop\n\n\n1\n\n\n⑥ 设置tracker开机启动\n\n# chkconfig fdfs_trackerd on\n\n或者：\n# vim /etc/rc.d/rc.local\n加入配置：\n/etc/init.d/fdfs_trackerd start \n\n\n1\n2\n3\n4\n5\n6\n\n\n⑦ tracker server 目录及文件结构\n\ntracker服务启动成功后，会在base_path下创建data、logs两个目录。目录结构如下：\n\n${base_path}\n  |__data\n  |   |__storage_groups.dat：存储分组信息\n  |   |__storage_servers.dat：存储服务器列表\n  |__logs\n  |   |__trackerd.log： tracker server 日志文件 \n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 4、配置 fastdfs 存储 (storage)\n\n① 进入 /etc/fdfs 目录，复制 fastdfs 存储器样例配置文件 storage.conf.sample，并重命名为 storage.conf\n\n# cd /etc/fdfs\n# cp storage.conf.sample storage.conf# vim storage.conf\n\n\n1\n2\n\n\n② 编辑storage.conf\n\n标红的需要修改，其它的默认即可。\n\n[](javascript:void(0)😉\n\n# 配置文件是否不生效，false 为生效\ndisabled=false \n\n# 指定此 storage server 所在 组(卷)\ngroup_name=group1\n\n# storage server 服务端口\nport=23000\n\n# 心跳间隔时间，单位为秒 (这里是指主动向 tracker server 发送心跳)\nheart_beat_interval=30\n\n# storage 数据和日志目录地址(根目录必须存在，子目录会自动生成)\nbase_path=/ljzsg/fastdfs/storage\n\n# 存放文件时 storage server 支持多个路径。这里配置存放文件的基路径数目，通常只配一个目录。\nstore_path_count=1\n\n\n# 逐一配置 store_path_count 个路径，索引号基于 0。\n# 如果不配置 store_path0，那它就和 base_path 对应的路径一样。\nstore_path0=/ljzsg/fastdfs/file\n\n# fastdfs 存储文件时，采用了两级目录。这里配置存放文件的目录个数。 \n# 如果本参数只为 n（如： 256），那么 storage server 在初次运行时，会在 store_path 下自动创建 n * n 个存放文件的子目录。\nsubdir_count_per_path=256\n\n# tracker_server 的列表 ，会主动连接 tracker_server\n# 有多个 tracker server 时，每个 tracker server 写一行\ntracker_server=file.ljzsg.com:22122# 允许系统同步的时间段 (默认是全天) 。一般用于避免高峰同步产生一些问题而设定。sync_start_time=00:00sync_end_time=23:59\n# 访问端口\nhttp.server_port=80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n[](javascript:void(0)😉\n\n③ 创建storage基础数据目录，对应base_path目录\n\n# mkdir -p /ljzsg/fastdfs/storage\n\n# 这是配置的store_path0路径\n# mkdir -p /ljzsg/fastdfs/file\n\n\n1\n2\n3\n4\n\n\n④ 防火墙中打开存储器端口（默认的 23000）\n\n[](javascript:void(0)😉\n\n# vim /etc/sysconfig/iptables\n\n添加如下端口行：\n-a input -m state --state new -m tcp -p tcp --dport 23000 -j accept\n\n重启防火墙：\n# service iptables restart\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n[](javascript:void(0)😉\n\n\n\n⑤ 启动 storage\n\n启动storage前确保tracker是启动的。初次启动成功，会在 /ljzsg/fastdfs/storage 目录下创建 data、 logs 两个目录。\n\n可以用这种方式启动\n# /etc/init.d/fdfs_storaged start\n\n也可以用这种方式，后面都用这种\n# service fdfs_storaged start\n\n\n1\n2\n3\n4\n5\n\n\n查看 storage 是否成功启动，23000 端口正在被监听，就算 storage 启动成功。\n\n# netstat -unltp|grep fdfs\n\n\n1\n\n\n\n\n关闭storage命令：\n\n# service fdfs_storaged stop\n\n\n1\n\n\n查看storage和tracker是否在通信：\n\n/usr/bin/fdfs_monitor /etc/fdfs/storage.conf\n\n\n1\n\n\n\n\n⑥ 设置 storage 开机启动\n\n# chkconfig fdfs_storaged on\n或者：\n# vim /etc/rc.d/rc.local\n加入配置：\n/etc/init.d/fdfs_storaged start\n\n\n1\n2\n3\n4\n5\n\n\n⑦ storage 目录\n\n同 tracker，storage 启动成功后，在base_path 下创建了data、logs目录，记录着 storage server 的信息。\n\n在 store_path0 目录下，创建了n*n个子目录：\n\n\n\n\n# 5、文件上传测试\n\n① 修改 tracker 服务器中的客户端配置文件\n\n# cd /etc/fdfs\n# cp client.conf.sample client.conf\n# vim client.conf\n\n\n1\n2\n3\n\n\n修改如下配置即可，其它默认。\n\n# client 的数据和日志目录\nbase_path=/ljzsg/fastdfs/client\n\n# tracker端口\ntracker_server=file.ljzsg.com:22122\n\n\n1\n2\n3\n4\n5\n\n\n② 上传测试\n\n在linux内部执行如下命令上传 namei.jpeg 图片\n\n# /usr/bin/fdfs_upload_file /etc/fdfs/client.conf namei.jpeg\n\n\n1\n\n\n上传成功后返回文件id号：group1/m00/00/00/wkgz6lnduteamdrcaaeormxzpp870.jpeg\n\n\n\n返回的文件id由group、存储目录、两级子目录、fileid、文件后缀名（由客户端指定，主要用于区分文件类型）拼接而成。\n\n\n\n回到顶部\n\n\n# 三、安装nginx\n\n上面将文件上传成功了，但我们无法下载。因此安装nginx作为服务器以支持http方式访问文件。同时，后面安装fastdfs的nginx模块也需要nginx环境。\n\nnginx只需要安装到storageserver所在的服务器即可，用于访问文件。我这里由于是单机，trackerserver和storageserver在一台服务器上。\n\n\n# 1、安装nginx所需环境\n\n① gcc 安装\n\n# yum install gcc-c++\n\n\n1\n\n\n② pcre pcre-devel 安装\n\n# yum install -y pcre pcre-devel\n\n\n1\n\n\n③ zlib 安装\n\n# yum install -y zlib zlib-devel\n\n\n1\n\n\n④ openssl 安装\n\n# yum install -y openssl openssl-devel\n\n\n1\n\n\n\n# 2、安装nginx\n\n① 下载nginx\n\n# wget -c https://nginx.org/download/nginx-1.12.1.tar.gz\n\n\n1\n\n\n② 解压\n\n# tar -zxvf nginx-1.12.1.tar.gz\n# cd nginx-1.12.1\n\n\n1\n2\n\n\n③ 使用默认配置\n\n# ./configure\n\n\n1\n\n\n④ 编译、安装\n\n# make\n# make install\n\n\n1\n2\n\n\n⑤ 启动nginx\n\n[](javascript:void(0)😉\n\n# cd /usr/local/nginx/sbin/\n# ./nginx \n\n其它命令\n# ./nginx -s stop\n# ./nginx -s quit\n# ./nginx -s reload\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n[](javascript:void(0)😉\n\n⑥ 设置开机启动\n\n[](javascript:void(0)😉\n\n# vim /etc/rc.local\n\n添加一行：\n/usr/local/nginx/sbin/nginx# 设置执行权限# chmod 755 rc.local\n\n\n1\n2\n3\n4\n\n\n[](javascript:void(0)😉\n\n⑦ 查看nginx的版本及模块\n\n/usr/local/nginx/sbin/nginx -v\n\n\n1\n\n\n\n\n⑧ 防火墙中打开nginx端口（默认的 80）\n\n添加后就能在本机使用80端口访问了。\n\n[](javascript:void(0)😉\n\n# vim /etc/sysconfig/iptables\n\n添加如下端口行：\n-a input -m state --state new -m tcp -p tcp --dport 80 -j accept\n\n重启防火墙：\n# service iptables restart\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n[](javascript:void(0)😉\n\n\n\n\n# 3、访问文件\n\n简单的测试访问文件\n\n① 修改nginx.conf\n\n[](javascript:void(0)😉\n\n# vim /usr/local/nginx/conf/nginx.conf\n\n添加如下行，将 /group1/m00 映射到 /ljzsg/fastdfs/file/data\nlocation /group1/m00 {\n    alias /ljzsg/fastdfs/file/data;\n}# 重启nginx# /usr/local/nginx/sbin/nginx -s reload\n\n\n1\n2\n3\n4\n5\n6\n\n\n[](javascript:void(0)😉\n\n\n\n② 在浏览器访问之前上传的图片、成功。\n\nhttp://file.ljzsg.com/group1/m00/00/00/wkgz6lnduteamdrcaaeormxzpp870.jpeg\n\n回到顶部\n\n\n# 四、fastdfs 配置 nginx 模块\n\n\n# 1、安装配置nginx模块\n\n① fastdfs-nginx-module 模块说明\n\nfastdfs 通过 tracker 服务器，将文件放在 storage 服务器存储， 但是同组存储服务器之间需要进行文件复制， 有同步延迟的问题。\n\n假设 tracker 服务器将文件上传到了 192.168.51.128，上传成功后文件 id已经返回给客户端。\n\n此时 fastdfs 存储集群机制会将这个文件同步到同组存储 192.168.51.129，在文件还没有复制完成的情况下，客户端如果用这个文件 id 在 192.168.51.129 上取文件,就会出现文件无法访问的错误。\n\n而 fastdfs-nginx-module 可以重定向文件链接到源服务器取文件，避免客户端由于复制延迟导致的文件无法访问错误。\n\n② 下载 fastdfs-nginx-module、解压\n\n[](javascript:void(0)😉\n\n# 这里为啥这么长一串呢，因为最新版的master与当前nginx有些版本问题。\n# wget https://github.com/happyfish100/fastdfs-nginx-module/archive/5e5f3566bbfa57418b5506aaefbe107a42c9fcb1.zip\n\n# 解压\n# unzip 5e5f3566bbfa57418b5506aaefbe107a42c9fcb1.zip\n\n# 重命名\n# mv fastdfs-nginx-module-5e5f3566bbfa57418b5506aaefbe107a42c9fcb1  fastdfs-nginx-module-master\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n[](javascript:void(0)😉\n\n③ 配置nginx\n\n在nginx中添加模块\n\n[](javascript:void(0)😉\n\n# 先停掉nginx服务# /usr/local/nginx/sbin/nginx -s stop进入解压包目录\n# cd /softpackages/nginx-1.12.1/\n\n# 添加模块\n# ./configure --add-module=../fastdfs-nginx-module-master/src\n\n重新编译、安装\n# make && make install\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n[](javascript:void(0)😉\n\n④ 查看nginx的模块\n\n# /usr/local/nginx/sbin/nginx -v\n\n\n1\n\n\n有下面这个就说明添加模块成功\n\n\n\n⑤ 复制 fastdfs-nginx-module 源码中的配置文件到/etc/fdfs 目录， 并修改\n\n# cd /softpackages/fastdfs-nginx-module-master/src\n\n# cp mod_fastdfs.conf /etc/fdfs/\n\n\n1\n2\n3\n\n\n修改如下配置，其它默认\n\n[](javascript:void(0)😉\n\n# 连接超时时间connect_timeout=10\n\n# tracker server\ntracker_server=file.ljzsg.com:22122\n# storageserver 默认端口\nstorage_server_port=23000\n\n# 如果文件id的uri中包含/group**，则要设置为true\nurl_have_group_name = true\n\n# storage 配置的store_path0路径，必须和storage.conf中的一致\nstore_path0=/ljzsg/fastdfs/file\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n[](javascript:void(0)😉\n\n⑥ 复制 fastdfs 的部分配置文件到/etc/fdfs 目录\n\n# cd /softpackages/fastdfs-5.05/conf/\n\n# cp anti-steal.jpg http.conf mime.types /etc/fdfs/\n\n\n1\n2\n3\n\n\n⑦ 配置nginx，修改nginx.conf\n\n# vim /usr/local/nginx/conf/nginx.conf\n\n\n1\n\n\n修改配置，其它的默认\n\n在80端口下添加fastdfs-nginx模块\n\nlocation ~/group([0-9])/m00 {\n    ngx_fastdfs_module;\n}\n\n\n1\n2\n3\n\n\n\n\n注意：\n\nlisten 80 端口值是要与 /etc/fdfs/storage.conf 中的 http.server_port=80 (前面改成80了)相对应。如果改成其它端口，则需要统一，同时在防火墙中打开该端口。\n\nlocation 的配置，如果有多个group则配置location ~/group([0-9])/m00 ，没有则不用配group。\n\n⑧ 在/ljzsg/fastdfs/file 文件存储目录下创建软连接，将其链接到实际存放数据的目录，这一步可以省略。\n\n# ln -s /ljzsg/fastdfs/file/data/ /ljzsg/fastdfs/file/data/m00 \n\n\n1\n\n\n⑨ 启动nginx\n\n# /usr/local/nginx/sbin/nginx\n\n\n1\n\n\n打印处如下就算配置成功\n\n\n\n⑩ 在地址栏访问。\n\n能下载文件就算安装成功。注意和第三点中直接使用nginx路由访问不同的是，这里配置 fastdfs-nginx-module 模块，可以重定向文件链接到源服务器取文件。\n\nhttp://file.ljzsg.com/group1/m00/00/00/wkgz6lnduteamdrcaaeormxzpp870.jpeg\n\n最终部署结构图(盗的图)：可以按照下面的结构搭建环境。\n\n\n\n回到顶部\n\n\n# 五、java客户端\n\n前面文件系统平台搭建好了，现在就要写客户端代码在系统中实现上传下载，这里只是简单的测试代码。\n\n\n# 1、首先需要搭建 fastdfs 客户端java开发环境\n\n① 项目中使用maven进行依赖管理，可以在pom.xml中引入如下依赖即可：\n\n<dependency>\n   <groupid>net.oschina.zcx7878</groupid>\n   <artifactid>fastdfs-client-java</artifactid>\n   <version>1.27.0.0</version>\n</dependency>\n\n\n1\n2\n3\n4\n5\n\n\n其它的方式，参考官方文档：https://github.com/happyfish100/fastdfs-client-java\n\n② 引入配置文件\n\n可直接复制包下的 fastdfs-client.properties.sample 或者 fdfs_client.conf.sample，到你的项目中，去掉.sample。\n\n\n\n我这里直接复制 fastdfs-client.properties.sample 中的配置到项目配置文件 config.properties 中，修改tracker_servers。只需要加载这个配置文件即可\n\n\n\n\n# 2、客户端api\n\n个人封装的fastdfs java api以同步到github：https://github.com/bojiangzhou/lyyzoo-fastdfs-java.git\n\n\n# 六、权限控制\n\n前面使用nginx支持http方式访问文件，但所有人都能直接访问这个文件服务器了，所以做一下权限控制。\n\nfastdfs的权限控制是在服务端开启token验证，客户端根据文件名、当前unix时间戳、秘钥获取token，在地址中带上token参数即可通过http方式访问文件。\n\n① 服务端开启token验证\n\n[](javascript:void(0)😉\n\n修改http.conf\n# vim /etc/fdfs/http.conf\n\n设置为true表示开启token验证\nhttp.anti_steal.check_token=true设置token失效的时间单位为秒(s)http.anti_steal.token_ttl=1800\n密钥，跟客户端配置文件的fastdfs.http_secret_key保持一致\nhttp.anti_steal.secret_key=fastdfs1234567890\n\n如果token检查失败，返回的页面\nhttp.anti_steal.token_check_fail=/ljzsg/fastdfs/page/403.html\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n[](javascript:void(0)😉\n\n记得重启服务。\n\n② 配置客户端\n\n客户端只需要设置如下两个参数即可，两边的密钥保持一致。\n\n# token 防盗链功能\nfastdfs.http_anti_steal_token=true\n# 密钥\nfastdfs.http_secret_key=fastdfs1234567890\n\n\n1\n2\n3\n4\n\n\n③ 客户端生成token\n\n访问文件需要带上生成的token以及unix时间戳，所以返回的token是token和时间戳的拼接。\n\n之后，将token拼接在地址后即可访问：file.ljzsg.com/group1/m00/00/00/wkgzgfnkaxqaifxyaaeormxzpp878.jpeg?token=078d370098b03e9020b82c829c205e1f&ts=1508141521\n\n[](javascript:void(0)😉\n\n 1     /**\n 2      * 获取访问服务器的token，拼接到地址后面\n 3      *\n 4      * @param filepath 文件路径 group1/m00/00/00/wkgzgfnktpyaiaugaaeormxzpp876.jpeg\n 5      * @param httpsecretkey 密钥\n 6      * @return 返回token，如： token=078d370098b03e9020b82c829c205e1f&ts=1508141521\n 7      */\n 8     public static string gettoken(string filepath, string httpsecretkey){\n 9         // unix seconds\n10         int ts = (int) instant.now().getepochsecond();\n11         // token\n12         string token = "null";\n13         try {\n14             token = protocommon.gettoken(getfilename(filepath), ts, httpsecretkey);\n15         } catch (unsupportedencodingexception e) {\n16             e.printstacktrace();\n17         } catch (nosuchalgorithmexception e) {\n18             e.printstacktrace();\n19         } catch (myexception e) {\n20             e.printstacktrace();\n21         }\n22 \n23         stringbuilder sb = new stringbuilder();\n24         sb.append("token=").append(token);\n25         sb.append("&ts=").append(ts);\n26 \n27         return sb.tostring();\n28     }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n[](javascript:void(0)😉\n\n④ 注意事项\n\n如果生成的token验证无法通过，请进行如下两项检查： 　　a. 确认调用token生成函数(protocommon.gettoken)，传递的文件id中没有包含group name。传递的文件id格式形如：m00/00/00/wkgzgfnktpyaiaugaaeormxzpp876.jpeg\n\nb. 确认服务器时间基本是一致的，注意服务器时间不能相差太多，不要相差到分钟级别。\n\n⑤ 对比下发现，如果系统文件隐私性较高，可以直接通过fastdfs-client提供的api去访问即可，不用再配置nginx走http访问。配置nginx的主要目的是为了快速访问服务器的文件(如图片)，如果还要加权限验证，则需要客户端生成token，其实已经没有多大意义。\n\n关键是，这里我没找到fastdfs如何对部分资源加token验证，部分开放。有知道的还请留言。\n\n----------------------------------------\n\nok，以上就是单机中使用fastdfs搭建文件系统并上传下载的过程。\n\n完！！！\n\n原文链接：https://www.cnblogs.com/chiangchou/p/fastdfs.html',charsets:{cjk:!0}},{title:"部署ceph集群 Nautilus版",frontmatter:{title:"部署ceph集群 Nautilus版",date:"2023-03-01T15:47:49.000Z",permalink:"/pages/88a4de/",categories:["运维","存储","ceph"],tags:[null],readingShow:"top",description:"学习ceph第一天，记录下ceph集群部署过程",meta:[{name:"image",content:"http://pic.zzppjj.top/LightPicture/2023/03/60bf4e01f6c60c66.png"},{name:"twitter:title",content:"部署ceph集群 Nautilus版"},{name:"twitter:description",content:"学习ceph第一天，记录下ceph集群部署过程"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://pic.zzppjj.top/LightPicture/2023/03/60bf4e01f6c60c66.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/06.%E5%AD%98%E5%82%A8/03.ceph/01.%E9%83%A8%E7%BD%B2ceph%E9%9B%86%E7%BE%A4%20Nautilus%E7%89%88.html"},{property:"og:type",content:"article"},{property:"og:title",content:"部署ceph集群 Nautilus版"},{property:"og:description",content:"学习ceph第一天，记录下ceph集群部署过程"},{property:"og:image",content:"http://pic.zzppjj.top/LightPicture/2023/03/60bf4e01f6c60c66.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/06.%E5%AD%98%E5%82%A8/03.ceph/01.%E9%83%A8%E7%BD%B2ceph%E9%9B%86%E7%BE%A4%20Nautilus%E7%89%88.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-03-01T15:47:49.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"部署ceph集群 Nautilus版"},{itemprop:"description",content:"学习ceph第一天，记录下ceph集群部署过程"},{itemprop:"image",content:"http://pic.zzppjj.top/LightPicture/2023/03/60bf4e01f6c60c66.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/06.%E5%AD%98%E5%82%A8/03.ceph/01.%E9%83%A8%E7%BD%B2ceph%E9%9B%86%E7%BE%A4%20Nautilus%E7%89%88.html",relativePath:"04.运维/06.存储/03.ceph/01.部署ceph集群 Nautilus版.md",key:"v-0bebf3bd",path:"/pages/88a4de/",headers:[{level:3,title:"ceph简介",slug:"ceph简介",normalizedTitle:"ceph简介",charIndex:86},{level:4,title:"ceph架构",slug:"ceph架构",normalizedTitle:"ceph架构",charIndex:96},{level:4,title:"Ceph 支持三种接口",slug:"ceph-支持三种接口",normalizedTitle:"ceph 支持三种接口",charIndex:108},{level:4,title:"ceph主要特点",slug:"ceph主要特点",normalizedTitle:"ceph主要特点",charIndex:215},{level:4,title:"ceph核心组件及其作用",slug:"ceph核心组件及其作用",normalizedTitle:"ceph核心组件及其作用",charIndex:291},{level:4,title:"ceph应用场景",slug:"ceph应用场景",normalizedTitle:"ceph应用场景",charIndex:1231},{level:3,title:"环境准备",slug:"环境准备",normalizedTitle:"环境准备",charIndex:2156},{level:3,title:"系统初始化",slug:"系统初始化",normalizedTitle:"系统初始化",charIndex:2558},{level:4,title:"关闭防火墙及selinux",slug:"关闭防火墙及selinux",normalizedTitle:"关闭防火墙及selinux",charIndex:2603},{level:4,title:"修改主机名",slug:"修改主机名",normalizedTitle:"修改主机名",charIndex:2855},{level:4,title:"配置hosts解析记录",slug:"配置hosts解析记录",normalizedTitle:"配置hosts解析记录",charIndex:2981},{level:4,title:"调整内核参数",slug:"调整内核参数",normalizedTitle:"调整内核参数",charIndex:3115},{level:4,title:"调整最大可打开文件数",slug:"调整最大可打开文件数",normalizedTitle:"调整最大可打开文件数",charIndex:4326},{level:4,title:"配置ntp",slug:"配置ntp",normalizedTitle:"配置ntp",charIndex:4648},{level:5,title:"配置ntp服务端",slug:"配置ntp服务端",normalizedTitle:"配置ntp服务端",charIndex:4657},{level:5,title:"其他节点同步控制节点时间",slug:"其他节点同步控制节点时间",normalizedTitle:"其他节点同步控制节点时间",charIndex:5034},{level:4,title:"配置yum源",slug:"配置yum源",normalizedTitle:"配置yum源",charIndex:5424},{level:4,title:"安装基础工具",slug:"安装基础工具",normalizedTitle:"安装基础工具",charIndex:6118},{level:4,title:"配置免密登录到其他节点",slug:"配置免密登录到其他节点",normalizedTitle:"配置免密登录到其他节点",charIndex:6246},{level:3,title:"部署ceph集群",slug:"部署ceph集群",normalizedTitle:"部署ceph集群",charIndex:6362},{level:4,title:"部署admin",slug:"部署admin",normalizedTitle:"部署admin",charIndex:6415},{level:5,title:"安装ceph软件包",slug:"安装ceph软件包",normalizedTitle:"安装ceph软件包",charIndex:7258},{level:5,title:"初始化monitor",slug:"初始化monitor",normalizedTitle:"初始化monitor",charIndex:7419},{level:5,title:"拷贝admin的秘钥文件",slug:"拷贝admin的秘钥文件",normalizedTitle:"拷贝admin的秘钥文件",charIndex:7513},{level:5,title:"确认集群状态",slug:"确认集群状态",normalizedTitle:"确认集群状态",charIndex:7627},{level:4,title:"部署manager",slug:"部署manager",normalizedTitle:"部署manager",charIndex:8022},{level:4,title:"创建三个OSD",slug:"创建三个osd",normalizedTitle:"创建三个osd",charIndex:8596},{level:3,title:"ceph集群扩容",slug:"ceph集群扩容",normalizedTitle:"ceph集群扩容",charIndex:9868},{level:4,title:"扩容monitor节点",slug:"扩容monitor节点",normalizedTitle:"扩容monitor节点",charIndex:9880},{level:5,title:"扩容第一个monitor节点",slug:"扩容第一个monitor节点",normalizedTitle:"扩容第一个monitor节点",charIndex:10057},{level:5,title:"扩容第二个monitor节点",slug:"扩容第二个monitor节点",normalizedTitle:"扩容第二个monitor节点",charIndex:12596},{level:4,title:"检查ceph集群状态",slug:"检查ceph集群状态",normalizedTitle:"检查ceph集群状态",charIndex:13080},{level:4,title:"扩容manager节点",slug:"扩容manager节点",normalizedTitle:"扩容manager节点",charIndex:14400}],headersStr:"ceph简介 ceph架构 Ceph 支持三种接口 ceph主要特点 ceph核心组件及其作用 ceph应用场景 环境准备 系统初始化 关闭防火墙及selinux 修改主机名 配置hosts解析记录 调整内核参数 调整最大可打开文件数 配置ntp 配置ntp服务端 其他节点同步控制节点时间 配置yum源 安装基础工具 配置免密登录到其他节点 部署ceph集群 部署admin 安装ceph软件包 初始化monitor 拷贝admin的秘钥文件 确认集群状态 部署manager 创建三个OSD ceph集群扩容 扩容monitor节点 扩容第一个monitor节点 扩容第二个monitor节点 检查ceph集群状态 扩容manager节点",content:'学习ceph第一天，记录下ceph集群部署过程\n\n参考文档：\n\n * 官方文档。\n * ceph基本概念、原理、架构介绍\n * 【分析】Ceph系统架构与基本概念\n\n\n# ceph简介\n\n# ceph架构\n\n\n\n# Ceph 支持三种接口\n\n>  * Object：有原生的API，而且也兼容 Swift 和 S3 的 API\n>  * Block：支持精简配置、快照、克隆\n>  * File：Posix 接口，支持快照\n\n# ceph主要特点\n\n>  * 统一存储\n>  * 无任何单点故障\n>  * 数据多份冗余\n>  * 存储容量可扩展\n>  * 自动容错及故障自愈\n\n# ceph核心组件及其作用\n\n> 在Ceph存储集群中，包含了三大角色组件，他们在Ceph存储集群中表现为3个守护进程，分别是Ceph OSD、Monitor、Managers。当然还有其他的功能组件，但是最主要的是这三个。\n> \n>  * Ceph OSD： Ceph的OSD（Object Storage Device）守护进程。主要功能包括：存储数据、副本数据处理、数据恢复、数据回补、平衡数据分布，并将数据相关的一些监控信息提供给Ceph Moniter,以便Ceph Moniter来检查其他OSD的心跳状态。一个Ceph OSD存储集群，要求至少两个Ceph OSD,才能有效的保存两份数据。注意，这里的两个Ceph OSD是指运行在两台物理服务器上，并不是在一台物理服务器上运行两个Ceph OSD的守护进程。通常，冗余和高可用性至少需要3个Ceph OSD。\n>  * Monitor： Ceph的Monitor守护进程，主要功能是维护集群状态的表组，这个表组中包含了多张表，其中有Moniter map、OSD map、PG(Placement Group) map、CRUSH map。 这些映射是Ceph守护进程之间相互协调的关键簇状态。 监视器还负责管理守护进程和客户端之间的身份验证。 通常需要至少三个监视器来实现冗余和高可用性。\n>  * Managers： Ceph的Managers（Ceph Manager），守护进程（ceph-mgr）负责跟踪运行时间指标和Ceph群集的当前状态，包括存储利用率，当前性能指标和系统负载。 Ceph Manager守护程序还托管基于python的插件来管理和公开Ceph集群信息，包括基于Web的仪表板和REST API。 通常，至少有两名Manager需要高可用性。\n>  * MDS： Ceph的MDS（Metadata Server）守护进程，主要保存的是Ceph文件系统的元数据。注意，对于Ceph的块设备和Ceph对象存储都不需要Ceph MDS守护进程。Ceph MDS为基于POSIX文件系统的用户提供了一些基础命令的执行，比如ls、find等，这样可以很大程度降低Ceph存储集群的压力。\n\n# ceph应用场景\n\n> Ceph的应用场景主要由它的架构确定，Ceph提供对象存储、块存储和文件存储，主要由以下4种应用场景：\n> \n>  * LIBRADOS应用： 通俗的说，Librados提供了应用程序对RADOS的直接访问，目前Librados已经提供了对C、C++、Java、Python、Ruby和PHP的支持。它支持单个单项的原子操作，如同时更新数据和属性、CAS操作，同时有对象粒度的快照操作。它的实现是基于RADOS的插件API，也就是在RADOS上运行的封装库。\n>  * RADOSGW应用： 这类应用基于Librados之上，增加了HTTP协议，提供RESTful接口并且兼容S3、Swfit接口。RADOSGW将Ceph集群作为分布式对象存储，对外提供服务。\n>  * RBD应用： 这类应用也是基于Librados之上的，细分为下面两种应用场景。\n>    * 第一种应用场景为虚拟机提供块设备。通过Librbd可以创建一个块设备（Container），然后通过QEMU/KVM附加到VM上。通过Container和VM的解耦，使得块设备可以被绑定到不同的VM上。\n>    * 第二种应用场景为主机提供块设备。这种场景是传统意义上的理解的块存储。\n>    * 以上两种方式都是将一个虚拟的块设备分片存储在RADOS中，都会利用数据条带化提高数据并行传输，都支持块设备的快照、COW（Copy-On-Write）克隆。最重要的是RBD还支持Live migration。\n>  * CephFS（Ceph文件系统）应用： 这类应用是基于RADOS实现的PB级分布式文件系统，其中引入MDS（Meta Date Server）,它主要为兼容POSIX文件系统提供元数据，比如文件目录和文件元数据。同时MDS会将元数据存储在RADOS中，这样元数据本身也达到了并行化，可以大大加快文件操作的速度。MDS本身不为Client提供数据文件，只为Client提供对元数据的操作。当Client打开一个文件时，会查询并更新MDS相应的元数据（如文件包括的对象信息），然后再根据提供的对象信息直接从RADOS中得到文件数据。\n\n\n# 环境准备\n\nOS           HOSTNAME          IP                              ROLES\nCentos 7.5   pod4-core-20-10   192.168.20.10 / 192.168.30.10   ceph-deploy、monitor、Managers、rgw、mds\nCentos 7.5   pod4-core-20-5    192.168.20.5 / 192.168.30.5     monitor、Managers、rgw、mds\nCentos 7.5   pod4-core-20-6    192.168.20.6 / 192.168.30.6     monitor、Managers、rgw、mds\n\n注：每个节点有两块硬盘、两块网卡以便测试，使用ceph-deploy工具部署ceph存储。\n\n\n# 系统初始化\n\n系统初始化这一小节的所有操作，没有特别注明的，均需在所有节点上执行。\n\n# 关闭防火墙及selinux\n\n# 关防火墙\n$ systemctl stop firewalld && systemctl disable firewalld\n\n# 关selinux\nsetenforce 0\nsed -i \'s#^SELINUX=.*#SELINUX=disabled#g\' /etc/selinux/config\nsed -i \'s#^SELINUX=.*#SELINUX=disabled#g\' /etc/sysconfig/selinux\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 修改主机名\n\n# 将 HOSTNAME 替换为你实际主机名\n$ hostnamectl set-hostname HOSTNAME\n\n\n# 所有节点执行刷新生效\nbash\nsource /etc/profile\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 配置hosts解析记录\n\n$ cat >> /etc/hosts << EOF192.168.20.10 pod4-core-20-10192.168.20.5 pod4-core-20-5192.168.20.6 pod4-core-20-6EOF\n\n\n1\n\n\n# 调整内核参数\n\n$ cat > /etc/sysctl.conf << EOFkernel.sysrq = 0kernel.core_uses_pid = 1fs.file-max=655360kernel.msgmnb = 65536kernel.msgmax = 65536kernel.shmmax = 68719476736kernel.shmall = 4294967296kernel.pid_max = 655360net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_max_tw_buckets = 262144net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_timestamps = 0net.ipv4.tcp_sack = 1net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_ecn = 0net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_max_orphans = 655360net.ipv4.tcp_max_syn_backlog = 262144net.ipv4.tcp_mem = 65536 131072 262144net.ipv4.udp_mem = 65536 131072 262144net.ipv4.tcp_rmem = 4096 87380 16777216net.ipv4.tcp_wmem = 4096 16384 16777216net.ipv4.ip_local_port_range = 1024 65535net.ipv4.route.gc_timeout = 100# 禁止icmp重定向报文net.ipv4.conf.all.accept_redirects = 0# 禁止icmp源路由net.ipv4.conf.all.accept_source_route = 0net.core.somaxconn = 65535net.core.rmem_default = 8388608net.core.wmem_default = 8388608net.core.rmem_max = 16777216net.core.wmem_max = 16777216net.core.netdev_max_backlog = 262144vm.swappiness = 3vm.overcommit_memory = 1vm.max_map_count = 262144EOF\n\n# 刷新生效\nsysctl -p\n\n\n1\n2\n3\n4\n\n\n# 调整最大可打开文件数\n\n$ mv /etc/security/limits.conf{,.bak}\ncat > /etc/security/limits.conf  << EOF*                -       nofile          650000*                -       memlock         unlimited*                -       stack           655360*                -       nproc           unlimitedEOF\n\n\n1\n2\n\n\n> 执行此操作后，需要重新打开终端，才可生效。\n\n# 配置ntp\n\n# 配置ntp服务端\n\n在pod4-core-20-10上执行这些步骤。\n\n# 安装软件包\n$ yum -y install chrony\n\n# 编辑配置文件\n$ mv /etc/chrony.conf{,.bak}\ncat > /etc/chrony.conf << EOFbindcmdaddress 0.0.0.0server ntp.aliyun.com iburstallow 192.168.20.0/24driftfile /var/lib/chrony/driftmakestep 1.0 3rtcsynclogdir /var/log/chronyEOF\n\n# 启动chronyd\nsystemctl enable chronyd && systemctl restart chronyd \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 其他节点同步控制节点时间\n\n在其他所有节点执行此操作。\n\n# 安装软件包\n$ yum -y install chrony\n\n# 编辑配置文件\n$ mv /etc/chrony.conf{,.bak}\ncat > /etc/chrony.conf << EOFserver pod4-core-20-10 iburstdriftfile /var/lib/chrony/driftmakestep 1.0 3rtcsynclogdir /var/log/chronyEOF\n$ systemctl enable chronyd && systemctl restart chronyd \n\n# 最好先手动测试下，可以同步时间\n$ yum -y install ntp\n$ ntpdate -u 192.168.20.10\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# 配置yum源\n\n# 更换为阿里云源\n$ mkdir /etc/yum.repos.d/bak\nmv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/\n\nwget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo\nwget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo\nyum clean all && yum makecache fast\n\n# 配置ceph-nautilus源\ncat > /etc/yum.repos.d/ceph.repo << "EOF"[ceph-norch]name=ceph-norchbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/enabled=1gpgcheck=0[ceph-x86_64]name=ceph-x86_64 baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/enabled=1gpgcheck=0EOF\nyum clean all\nyum makecache fast\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n接下来的操作，如果没有特殊说明，均在 pod4-core-20-10节点执行。\n\n# 安装基础工具\n\n$ yum -y install python-setuptools ceph-deploy\n\n$ ceph-deploy --version     # 确保ceph-deploy版本为2.0.1\n2.0.1\n\n\n1\n2\n3\n4\n\n\n# 配置免密登录到其他节点\n\n$ ssh-keygen -t rsa -q\nfor i in 5 6 10;do ssh-copy-id pod4-core-20-$i;done     # 根据提示输入相应密码\n\n\n1\n2\n\n\n\n# 部署ceph集群\n\n接下来的操作，如果没有特殊说明，均在 pod4-core-20-10节点执行。\n\n# 部署admin\n\n创建一个Ceph Monitor和三个Ceph OSD守护进程的Ceph存储集群。集群达到状态后，通过添加第四个Ceph OSD守护程序，一个元数据服务器和两个以上的Ceph Monitors对其进行扩展。\n\n$ mkdir my-cluster && cd my-cluster\n$ ceph-deploy new --public-network 192.168.20.0/24 \\\n  --cluster-network 192.168.30.0/24 pod4-core-20-10\n\n# --public-network：对外网络，用于客户端访问\n# --cluster-network：集群内部信息同步网络\n# pod4-core-20-10：将此节点设置为Monitor节点\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n上述命令执行后，输出如下：\n\n\n\n$ ls    # 命令执行成功后，会生成配置文件、日志文件、秘钥文件\nceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring\n$ cat ceph.conf        # 生成的配置文件内容如下\n# 如果有错误之处，可以根据自己实际情况修改\n[global]\nfsid = 634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\npublic_network = 192.168.20.0/24\ncluster_network = 192.168.30.0/24\nmon_initial_members = pod4-core-20-10\nmon_host = 192.168.20.10\nauth_cluster_required = cephx\nauth_service_required = cephx\nauth_client_required = cephx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 安装ceph软件包\n\n仅安装ceph软件包此操作在所有节点执行\n\n# 不要使用官方提供的 ceph-deploy install 命令进行安装，它会自己再配置yum源，安装较慢\n$ yum -y install ceph ceph-mon ceph-mgr ceph-radosgw ceph-mds\n\n\n1\n2\n\n\n# 初始化monitor\n\n接下来的操作，如果没有特殊说明，均在 pod4-core-20-10节点执行。\n\n$ ceph-deploy mon create-initial\n\n\n1\n\n\n# 拷贝admin的秘钥文件\n\n# admin后面指定ceph集群中的所有节点\n$ ceph-deploy admin pod4-core-20-10 pod4-core-20-5 pod4-core-20-6\n\n\n1\n2\n\n\n# 确认集群状态\n\n$ ceph -s\n  cluster:\n    id:     634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\n    health: HEALTH_OK\n\n  services:\n    mon: 1 daemons, quorum pod4-core-20-10 (age 2m)  # 包含一个monitor\n    mgr: no daemons active\n    osd: 0 osds: 0 up, 0 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 B\n    usage:   0 B used, 0 B / 0 B avail\n    pgs:     \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# 部署manager\n\n# 将manager守护程序部署在 pod4-core-20-10\n$ ceph-deploy mgr create pod4-core-20-10\n\n\n$ ceph -s     # 查看集群状态\n  cluster:\n    id:     634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\n    health: HEALTH_WARN\n            OSD count 0 < osd_pool_default_size 3\n\n  services:\n    mon: 1 daemons, quorum pod4-core-20-10 (age 8m)\n    mgr: pod4-core-20-10(active, since 41s)     # 新增一个manager节点\n    osd: 0 osds: 0 up, 0 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 B\n    usage:   0 B used, 0 B / 0 B avail\n    pgs:  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 创建三个OSD\n\n每个机器至少需要一块全新硬盘，确保没有被格式化过。\n\n$ ceph-deploy osd create --data /dev/sdb pod4-core-20-10\nceph-deploy osd create --data /dev/sdb pod4-core-20-5\nceph-deploy osd create --data /dev/sdb pod4-core-20-6\n\n$ ceph -s      # 查看集群状态\n  cluster:\n    id:     634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\n    health: HEALTH_OK\n\n  services:\n    mon: 1 daemons, quorum pod4-core-20-10 (age 14m)\n    mgr: pod4-core-20-10(active, since 6m)\n    osd: 3 osds: 3 up (since 30s), 3 in (since 30s)     # 包含了三个osd\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 B\n    usage:   3.0 GiB used, 57 GiB / 60 GiB avail\n    pgs:     \n\n\n$ ceph osd tree       # 查看osd详细信息\nID CLASS WEIGHT  TYPE NAME                STATUS REWEIGHT PRI-AFF \n-1       0.05846 root default                                     \n-3       0.01949     host pod4-core-20-10                         \n 0   hdd 0.01949         osd.0                up  1.00000 1.00000 \n-5       0.01949     host pod4-core-20-5                          \n 1   hdd 0.01949         osd.1                up  1.00000 1.00000 \n-7       0.01949     host pod4-core-20-6                          \n 2   hdd 0.01949         osd.2                up  1.00000 1.00000 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# ceph集群扩容\n\n# 扩容monitor节点\n\n一个Ceph存储集群至少需要一个Ceph Monitor和Ceph Manager才能运行。为了获得高可用性，Ceph存储群集通常运行多个Ceph Monitor，这样可以避免单点故障。\n\nCeph使用Paxos算法，节点数量最好是奇数（即，大于N / 2，其中N是Monitor的数量）才能形成仲裁。尽管这不是必需的。\n\n# 扩容第一个monitor节点\n\n# 添加 pod4-core-20-5为monitor节点\n$ ceph-deploy mon add pod4-core-20-5\n\n$ ceph quorum_status --format json-pretty  # 检查仲裁状态\n\n{\n    "election_epoch": 8,\n    "quorum": [\n        0,\n        1\n    ],\n    "quorum_names": [     # 加入仲裁的节点\n        "pod4-core-20-10",\n        "pod4-core-20-5"\n    ],\n    "quorum_leader_name": "pod4-core-20-10",      # 当前leader\n    "quorum_age": 210,\n    "monmap": {\n        "epoch": 2,\n        "fsid": "634fc0a4-d2bd-4f14-af6b-421ecbb89ba6",\n        "modified": "2021-02-21 15:46:00.889286",\n        "created": "2021-02-21 15:21:30.739161",\n        "min_mon_release": 14,\n        "min_mon_release_name": "nautilus",\n        "features": {\n            "persistent": [\n                "kraken",\n                "luminous",\n                "mimic",\n                "osdmap-prune",\n                "nautilus"\n            ],\n            "optional": []\n        },\n        "mons": [\n            {\n                "rank": 0,\n                "name": "pod4-core-20-10",\n                "public_addrs": {\n                    "addrvec": [\n                        {\n                            "type": "v2",\n                            "addr": "192.168.20.10:3300",\n                            "nonce": 0\n                        },\n                        {\n                            "type": "v1",\n                            "addr": "192.168.20.10:6789",\n                            "nonce": 0\n                        }\n                    ]\n                },\n                "addr": "192.168.20.10:6789/0",\n                "public_addr": "192.168.20.10:6789/0"\n            },\n            {\n                "rank": 1,\n                "name": "pod4-core-20-5",\n                "public_addrs": {\n                    "addrvec": [\n                        {\n                            "type": "v2",\n                            "addr": "192.168.20.5:3300",\n                            "nonce": 0\n                        },\n                        {\n                            "type": "v1",\n                            "addr": "192.168.20.5:6789",\n                            "nonce": 0\n                        }\n                    ]\n                },\n                "addr": "192.168.20.5:6789/0",\n                "public_addr": "192.168.20.5:6789/0"\n            }\n        ]\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n\n\n# 扩容第二个monitor节点\n\n# 添加 pod4-core-20-6为monitor节点\n$ ceph-deploy mon add pod4-core-20-6\n\n# 确认三个节点加入仲裁\n$ ceph quorum_status --format json-pretty\n\n{\n    "election_epoch": 12,\n    "quorum": [\n        0,\n        1,\n        2\n    ],\n    "quorum_names": [\n        "pod4-core-20-10",\n        "pod4-core-20-5",\n        "pod4-core-20-6"\n    ],\n    "quorum_leader_name": "pod4-core-20-10",\n    "quorum_age": 30,\n\n      ....... # 省略部分输出\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 检查ceph集群状态\n\n$ ceph -s\n  cluster:\n    id:     634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\n    health: HEALTH_OK\n\n  services:     #确保有三个monitor\n    mon: 3 daemons, quorum pod4-core-20-10,pod4-core-20-5,pod4-core-20-6 (age 64s)\n    mgr: pod4-core-20-10(active, since 25m)\n    osd: 3 osds: 3 up (since 19m), 3 in (since 19m)\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 B\n    usage:   3.0 GiB used, 57 GiB / 60 GiB avail\n    pgs: \n\n$ ceph mon stat     # 也可以单独查看monitor状态\ne3: 3 mons at {pod4-core-20-10=[v2:192.168.20.10:3300/0,v1:192.168.20.10:6789/0],\npod4-core-20-5=[v2:192.168.20.5:3300/0,v1:192.168.20.5:6789/0],\npod4-core-20-6=[v2:192.168.20.6:3300/0,v1:192.168.20.6:6789/0]}, \nelection epoch 12, leader 0 pod4-core-20-10, quorum 0,1,2 \npod4-core-20-10,pod4-core-20-5,pod4-core-20-6\n\n$ ceph mon dump     # dump查看monitor状态\ndumped monmap epoch 3\nepoch 3\nfsid 634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\nlast_changed 2021-02-21 15:53:36.055686\ncreated 2021-02-21 15:21:30.739161\nmin_mon_release 14 (nautilus)\n0: [v2:192.168.20.10:3300/0,v1:192.168.20.10:6789/0] mon.pod4-core-20-10\n1: [v2:192.168.20.5:3300/0,v1:192.168.20.5:6789/0] mon.pod4-core-20-5\n2: [v2:192.168.20.6:3300/0,v1:192.168.20.6:6789/0] mon.pod4-core-20-6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n# 扩容manager节点\n\nCeph Manager守护进程以 active/standby 模式运行。当前active节点宕机后，处于standby状态的会自动提升为active节点。\n\n# 将pod4-core-20-5、pod4-core-20-6添加到manager集群\n$ ceph-deploy mgr create pod4-core-20-5 pod4-core-20-6\n\n# 添加成功后，确认集群状态\n$ ceph -s      \n  cluster:\n    id:     634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum pod4-core-20-10,pod4-core-20-5,pod4-core-20-6 (age 8m)\n    mgr: pod4-core-20-10(active, since 32m), standbys: pod4-core-20-5, pod4-core-20-6\n    osd: 3 osds: 3 up (since 26m), 3 in (since 26m)\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 B\n    usage:   3.0 GiB used, 57 GiB / 60 GiB avail\n    pgs:     \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n至此，ceph集群部署完成。\n\n----------------------------------------\n\n原文链接：\n\nhttps://lvjianzhao.gitee.io/lvjianzhao/posts/6010b413/?highlight=ceph\n\n> ',normalizedContent:'学习ceph第一天，记录下ceph集群部署过程\n\n参考文档：\n\n * 官方文档。\n * ceph基本概念、原理、架构介绍\n * 【分析】ceph系统架构与基本概念\n\n\n# ceph简介\n\n# ceph架构\n\n\n\n# ceph 支持三种接口\n\n>  * object：有原生的api，而且也兼容 swift 和 s3 的 api\n>  * block：支持精简配置、快照、克隆\n>  * file：posix 接口，支持快照\n\n# ceph主要特点\n\n>  * 统一存储\n>  * 无任何单点故障\n>  * 数据多份冗余\n>  * 存储容量可扩展\n>  * 自动容错及故障自愈\n\n# ceph核心组件及其作用\n\n> 在ceph存储集群中，包含了三大角色组件，他们在ceph存储集群中表现为3个守护进程，分别是ceph osd、monitor、managers。当然还有其他的功能组件，但是最主要的是这三个。\n> \n>  * ceph osd： ceph的osd（object storage device）守护进程。主要功能包括：存储数据、副本数据处理、数据恢复、数据回补、平衡数据分布，并将数据相关的一些监控信息提供给ceph moniter,以便ceph moniter来检查其他osd的心跳状态。一个ceph osd存储集群，要求至少两个ceph osd,才能有效的保存两份数据。注意，这里的两个ceph osd是指运行在两台物理服务器上，并不是在一台物理服务器上运行两个ceph osd的守护进程。通常，冗余和高可用性至少需要3个ceph osd。\n>  * monitor： ceph的monitor守护进程，主要功能是维护集群状态的表组，这个表组中包含了多张表，其中有moniter map、osd map、pg(placement group) map、crush map。 这些映射是ceph守护进程之间相互协调的关键簇状态。 监视器还负责管理守护进程和客户端之间的身份验证。 通常需要至少三个监视器来实现冗余和高可用性。\n>  * managers： ceph的managers（ceph manager），守护进程（ceph-mgr）负责跟踪运行时间指标和ceph群集的当前状态，包括存储利用率，当前性能指标和系统负载。 ceph manager守护程序还托管基于python的插件来管理和公开ceph集群信息，包括基于web的仪表板和rest api。 通常，至少有两名manager需要高可用性。\n>  * mds： ceph的mds（metadata server）守护进程，主要保存的是ceph文件系统的元数据。注意，对于ceph的块设备和ceph对象存储都不需要ceph mds守护进程。ceph mds为基于posix文件系统的用户提供了一些基础命令的执行，比如ls、find等，这样可以很大程度降低ceph存储集群的压力。\n\n# ceph应用场景\n\n> ceph的应用场景主要由它的架构确定，ceph提供对象存储、块存储和文件存储，主要由以下4种应用场景：\n> \n>  * librados应用： 通俗的说，librados提供了应用程序对rados的直接访问，目前librados已经提供了对c、c++、java、python、ruby和php的支持。它支持单个单项的原子操作，如同时更新数据和属性、cas操作，同时有对象粒度的快照操作。它的实现是基于rados的插件api，也就是在rados上运行的封装库。\n>  * radosgw应用： 这类应用基于librados之上，增加了http协议，提供restful接口并且兼容s3、swfit接口。radosgw将ceph集群作为分布式对象存储，对外提供服务。\n>  * rbd应用： 这类应用也是基于librados之上的，细分为下面两种应用场景。\n>    * 第一种应用场景为虚拟机提供块设备。通过librbd可以创建一个块设备（container），然后通过qemu/kvm附加到vm上。通过container和vm的解耦，使得块设备可以被绑定到不同的vm上。\n>    * 第二种应用场景为主机提供块设备。这种场景是传统意义上的理解的块存储。\n>    * 以上两种方式都是将一个虚拟的块设备分片存储在rados中，都会利用数据条带化提高数据并行传输，都支持块设备的快照、cow（copy-on-write）克隆。最重要的是rbd还支持live migration。\n>  * cephfs（ceph文件系统）应用： 这类应用是基于rados实现的pb级分布式文件系统，其中引入mds（meta date server）,它主要为兼容posix文件系统提供元数据，比如文件目录和文件元数据。同时mds会将元数据存储在rados中，这样元数据本身也达到了并行化，可以大大加快文件操作的速度。mds本身不为client提供数据文件，只为client提供对元数据的操作。当client打开一个文件时，会查询并更新mds相应的元数据（如文件包括的对象信息），然后再根据提供的对象信息直接从rados中得到文件数据。\n\n\n# 环境准备\n\nos           hostname          ip                              roles\ncentos 7.5   pod4-core-20-10   192.168.20.10 / 192.168.30.10   ceph-deploy、monitor、managers、rgw、mds\ncentos 7.5   pod4-core-20-5    192.168.20.5 / 192.168.30.5     monitor、managers、rgw、mds\ncentos 7.5   pod4-core-20-6    192.168.20.6 / 192.168.30.6     monitor、managers、rgw、mds\n\n注：每个节点有两块硬盘、两块网卡以便测试，使用ceph-deploy工具部署ceph存储。\n\n\n# 系统初始化\n\n系统初始化这一小节的所有操作，没有特别注明的，均需在所有节点上执行。\n\n# 关闭防火墙及selinux\n\n# 关防火墙\n$ systemctl stop firewalld && systemctl disable firewalld\n\n# 关selinux\nsetenforce 0\nsed -i \'s#^selinux=.*#selinux=disabled#g\' /etc/selinux/config\nsed -i \'s#^selinux=.*#selinux=disabled#g\' /etc/sysconfig/selinux\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 修改主机名\n\n# 将 hostname 替换为你实际主机名\n$ hostnamectl set-hostname hostname\n\n\n# 所有节点执行刷新生效\nbash\nsource /etc/profile\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 配置hosts解析记录\n\n$ cat >> /etc/hosts << eof192.168.20.10 pod4-core-20-10192.168.20.5 pod4-core-20-5192.168.20.6 pod4-core-20-6eof\n\n\n1\n\n\n# 调整内核参数\n\n$ cat > /etc/sysctl.conf << eofkernel.sysrq = 0kernel.core_uses_pid = 1fs.file-max=655360kernel.msgmnb = 65536kernel.msgmax = 65536kernel.shmmax = 68719476736kernel.shmall = 4294967296kernel.pid_max = 655360net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_max_tw_buckets = 262144net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_timestamps = 0net.ipv4.tcp_sack = 1net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_ecn = 0net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_max_orphans = 655360net.ipv4.tcp_max_syn_backlog = 262144net.ipv4.tcp_mem = 65536 131072 262144net.ipv4.udp_mem = 65536 131072 262144net.ipv4.tcp_rmem = 4096 87380 16777216net.ipv4.tcp_wmem = 4096 16384 16777216net.ipv4.ip_local_port_range = 1024 65535net.ipv4.route.gc_timeout = 100# 禁止icmp重定向报文net.ipv4.conf.all.accept_redirects = 0# 禁止icmp源路由net.ipv4.conf.all.accept_source_route = 0net.core.somaxconn = 65535net.core.rmem_default = 8388608net.core.wmem_default = 8388608net.core.rmem_max = 16777216net.core.wmem_max = 16777216net.core.netdev_max_backlog = 262144vm.swappiness = 3vm.overcommit_memory = 1vm.max_map_count = 262144eof\n\n# 刷新生效\nsysctl -p\n\n\n1\n2\n3\n4\n\n\n# 调整最大可打开文件数\n\n$ mv /etc/security/limits.conf{,.bak}\ncat > /etc/security/limits.conf  << eof*                -       nofile          650000*                -       memlock         unlimited*                -       stack           655360*                -       nproc           unlimitedeof\n\n\n1\n2\n\n\n> 执行此操作后，需要重新打开终端，才可生效。\n\n# 配置ntp\n\n# 配置ntp服务端\n\n在pod4-core-20-10上执行这些步骤。\n\n# 安装软件包\n$ yum -y install chrony\n\n# 编辑配置文件\n$ mv /etc/chrony.conf{,.bak}\ncat > /etc/chrony.conf << eofbindcmdaddress 0.0.0.0server ntp.aliyun.com iburstallow 192.168.20.0/24driftfile /var/lib/chrony/driftmakestep 1.0 3rtcsynclogdir /var/log/chronyeof\n\n# 启动chronyd\nsystemctl enable chronyd && systemctl restart chronyd \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 其他节点同步控制节点时间\n\n在其他所有节点执行此操作。\n\n# 安装软件包\n$ yum -y install chrony\n\n# 编辑配置文件\n$ mv /etc/chrony.conf{,.bak}\ncat > /etc/chrony.conf << eofserver pod4-core-20-10 iburstdriftfile /var/lib/chrony/driftmakestep 1.0 3rtcsynclogdir /var/log/chronyeof\n$ systemctl enable chronyd && systemctl restart chronyd \n\n# 最好先手动测试下，可以同步时间\n$ yum -y install ntp\n$ ntpdate -u 192.168.20.10\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# 配置yum源\n\n# 更换为阿里云源\n$ mkdir /etc/yum.repos.d/bak\nmv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/\n\nwget -o /etc/yum.repos.d/centos-base.repo https://mirrors.aliyun.com/repo/centos-7.repo\nwget -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo\nyum clean all && yum makecache fast\n\n# 配置ceph-nautilus源\ncat > /etc/yum.repos.d/ceph.repo << "eof"[ceph-norch]name=ceph-norchbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/enabled=1gpgcheck=0[ceph-x86_64]name=ceph-x86_64 baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/enabled=1gpgcheck=0eof\nyum clean all\nyum makecache fast\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n接下来的操作，如果没有特殊说明，均在 pod4-core-20-10节点执行。\n\n# 安装基础工具\n\n$ yum -y install python-setuptools ceph-deploy\n\n$ ceph-deploy --version     # 确保ceph-deploy版本为2.0.1\n2.0.1\n\n\n1\n2\n3\n4\n\n\n# 配置免密登录到其他节点\n\n$ ssh-keygen -t rsa -q\nfor i in 5 6 10;do ssh-copy-id pod4-core-20-$i;done     # 根据提示输入相应密码\n\n\n1\n2\n\n\n\n# 部署ceph集群\n\n接下来的操作，如果没有特殊说明，均在 pod4-core-20-10节点执行。\n\n# 部署admin\n\n创建一个ceph monitor和三个ceph osd守护进程的ceph存储集群。集群达到状态后，通过添加第四个ceph osd守护程序，一个元数据服务器和两个以上的ceph monitors对其进行扩展。\n\n$ mkdir my-cluster && cd my-cluster\n$ ceph-deploy new --public-network 192.168.20.0/24 \\\n  --cluster-network 192.168.30.0/24 pod4-core-20-10\n\n# --public-network：对外网络，用于客户端访问\n# --cluster-network：集群内部信息同步网络\n# pod4-core-20-10：将此节点设置为monitor节点\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n上述命令执行后，输出如下：\n\n\n\n$ ls    # 命令执行成功后，会生成配置文件、日志文件、秘钥文件\nceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring\n$ cat ceph.conf        # 生成的配置文件内容如下\n# 如果有错误之处，可以根据自己实际情况修改\n[global]\nfsid = 634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\npublic_network = 192.168.20.0/24\ncluster_network = 192.168.30.0/24\nmon_initial_members = pod4-core-20-10\nmon_host = 192.168.20.10\nauth_cluster_required = cephx\nauth_service_required = cephx\nauth_client_required = cephx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 安装ceph软件包\n\n仅安装ceph软件包此操作在所有节点执行\n\n# 不要使用官方提供的 ceph-deploy install 命令进行安装，它会自己再配置yum源，安装较慢\n$ yum -y install ceph ceph-mon ceph-mgr ceph-radosgw ceph-mds\n\n\n1\n2\n\n\n# 初始化monitor\n\n接下来的操作，如果没有特殊说明，均在 pod4-core-20-10节点执行。\n\n$ ceph-deploy mon create-initial\n\n\n1\n\n\n# 拷贝admin的秘钥文件\n\n# admin后面指定ceph集群中的所有节点\n$ ceph-deploy admin pod4-core-20-10 pod4-core-20-5 pod4-core-20-6\n\n\n1\n2\n\n\n# 确认集群状态\n\n$ ceph -s\n  cluster:\n    id:     634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\n    health: health_ok\n\n  services:\n    mon: 1 daemons, quorum pod4-core-20-10 (age 2m)  # 包含一个monitor\n    mgr: no daemons active\n    osd: 0 osds: 0 up, 0 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 b\n    usage:   0 b used, 0 b / 0 b avail\n    pgs:     \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# 部署manager\n\n# 将manager守护程序部署在 pod4-core-20-10\n$ ceph-deploy mgr create pod4-core-20-10\n\n\n$ ceph -s     # 查看集群状态\n  cluster:\n    id:     634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\n    health: health_warn\n            osd count 0 < osd_pool_default_size 3\n\n  services:\n    mon: 1 daemons, quorum pod4-core-20-10 (age 8m)\n    mgr: pod4-core-20-10(active, since 41s)     # 新增一个manager节点\n    osd: 0 osds: 0 up, 0 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 b\n    usage:   0 b used, 0 b / 0 b avail\n    pgs:  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 创建三个osd\n\n每个机器至少需要一块全新硬盘，确保没有被格式化过。\n\n$ ceph-deploy osd create --data /dev/sdb pod4-core-20-10\nceph-deploy osd create --data /dev/sdb pod4-core-20-5\nceph-deploy osd create --data /dev/sdb pod4-core-20-6\n\n$ ceph -s      # 查看集群状态\n  cluster:\n    id:     634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\n    health: health_ok\n\n  services:\n    mon: 1 daemons, quorum pod4-core-20-10 (age 14m)\n    mgr: pod4-core-20-10(active, since 6m)\n    osd: 3 osds: 3 up (since 30s), 3 in (since 30s)     # 包含了三个osd\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 b\n    usage:   3.0 gib used, 57 gib / 60 gib avail\n    pgs:     \n\n\n$ ceph osd tree       # 查看osd详细信息\nid class weight  type name                status reweight pri-aff \n-1       0.05846 root default                                     \n-3       0.01949     host pod4-core-20-10                         \n 0   hdd 0.01949         osd.0                up  1.00000 1.00000 \n-5       0.01949     host pod4-core-20-5                          \n 1   hdd 0.01949         osd.1                up  1.00000 1.00000 \n-7       0.01949     host pod4-core-20-6                          \n 2   hdd 0.01949         osd.2                up  1.00000 1.00000 \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# ceph集群扩容\n\n# 扩容monitor节点\n\n一个ceph存储集群至少需要一个ceph monitor和ceph manager才能运行。为了获得高可用性，ceph存储群集通常运行多个ceph monitor，这样可以避免单点故障。\n\nceph使用paxos算法，节点数量最好是奇数（即，大于n / 2，其中n是monitor的数量）才能形成仲裁。尽管这不是必需的。\n\n# 扩容第一个monitor节点\n\n# 添加 pod4-core-20-5为monitor节点\n$ ceph-deploy mon add pod4-core-20-5\n\n$ ceph quorum_status --format json-pretty  # 检查仲裁状态\n\n{\n    "election_epoch": 8,\n    "quorum": [\n        0,\n        1\n    ],\n    "quorum_names": [     # 加入仲裁的节点\n        "pod4-core-20-10",\n        "pod4-core-20-5"\n    ],\n    "quorum_leader_name": "pod4-core-20-10",      # 当前leader\n    "quorum_age": 210,\n    "monmap": {\n        "epoch": 2,\n        "fsid": "634fc0a4-d2bd-4f14-af6b-421ecbb89ba6",\n        "modified": "2021-02-21 15:46:00.889286",\n        "created": "2021-02-21 15:21:30.739161",\n        "min_mon_release": 14,\n        "min_mon_release_name": "nautilus",\n        "features": {\n            "persistent": [\n                "kraken",\n                "luminous",\n                "mimic",\n                "osdmap-prune",\n                "nautilus"\n            ],\n            "optional": []\n        },\n        "mons": [\n            {\n                "rank": 0,\n                "name": "pod4-core-20-10",\n                "public_addrs": {\n                    "addrvec": [\n                        {\n                            "type": "v2",\n                            "addr": "192.168.20.10:3300",\n                            "nonce": 0\n                        },\n                        {\n                            "type": "v1",\n                            "addr": "192.168.20.10:6789",\n                            "nonce": 0\n                        }\n                    ]\n                },\n                "addr": "192.168.20.10:6789/0",\n                "public_addr": "192.168.20.10:6789/0"\n            },\n            {\n                "rank": 1,\n                "name": "pod4-core-20-5",\n                "public_addrs": {\n                    "addrvec": [\n                        {\n                            "type": "v2",\n                            "addr": "192.168.20.5:3300",\n                            "nonce": 0\n                        },\n                        {\n                            "type": "v1",\n                            "addr": "192.168.20.5:6789",\n                            "nonce": 0\n                        }\n                    ]\n                },\n                "addr": "192.168.20.5:6789/0",\n                "public_addr": "192.168.20.5:6789/0"\n            }\n        ]\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n\n\n# 扩容第二个monitor节点\n\n# 添加 pod4-core-20-6为monitor节点\n$ ceph-deploy mon add pod4-core-20-6\n\n# 确认三个节点加入仲裁\n$ ceph quorum_status --format json-pretty\n\n{\n    "election_epoch": 12,\n    "quorum": [\n        0,\n        1,\n        2\n    ],\n    "quorum_names": [\n        "pod4-core-20-10",\n        "pod4-core-20-5",\n        "pod4-core-20-6"\n    ],\n    "quorum_leader_name": "pod4-core-20-10",\n    "quorum_age": 30,\n\n      ....... # 省略部分输出\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 检查ceph集群状态\n\n$ ceph -s\n  cluster:\n    id:     634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\n    health: health_ok\n\n  services:     #确保有三个monitor\n    mon: 3 daemons, quorum pod4-core-20-10,pod4-core-20-5,pod4-core-20-6 (age 64s)\n    mgr: pod4-core-20-10(active, since 25m)\n    osd: 3 osds: 3 up (since 19m), 3 in (since 19m)\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 b\n    usage:   3.0 gib used, 57 gib / 60 gib avail\n    pgs: \n\n$ ceph mon stat     # 也可以单独查看monitor状态\ne3: 3 mons at {pod4-core-20-10=[v2:192.168.20.10:3300/0,v1:192.168.20.10:6789/0],\npod4-core-20-5=[v2:192.168.20.5:3300/0,v1:192.168.20.5:6789/0],\npod4-core-20-6=[v2:192.168.20.6:3300/0,v1:192.168.20.6:6789/0]}, \nelection epoch 12, leader 0 pod4-core-20-10, quorum 0,1,2 \npod4-core-20-10,pod4-core-20-5,pod4-core-20-6\n\n$ ceph mon dump     # dump查看monitor状态\ndumped monmap epoch 3\nepoch 3\nfsid 634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\nlast_changed 2021-02-21 15:53:36.055686\ncreated 2021-02-21 15:21:30.739161\nmin_mon_release 14 (nautilus)\n0: [v2:192.168.20.10:3300/0,v1:192.168.20.10:6789/0] mon.pod4-core-20-10\n1: [v2:192.168.20.5:3300/0,v1:192.168.20.5:6789/0] mon.pod4-core-20-5\n2: [v2:192.168.20.6:3300/0,v1:192.168.20.6:6789/0] mon.pod4-core-20-6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n# 扩容manager节点\n\nceph manager守护进程以 active/standby 模式运行。当前active节点宕机后，处于standby状态的会自动提升为active节点。\n\n# 将pod4-core-20-5、pod4-core-20-6添加到manager集群\n$ ceph-deploy mgr create pod4-core-20-5 pod4-core-20-6\n\n# 添加成功后，确认集群状态\n$ ceph -s      \n  cluster:\n    id:     634fc0a4-d2bd-4f14-af6b-421ecbb89ba6\n    health: health_ok\n\n  services:\n    mon: 3 daemons, quorum pod4-core-20-10,pod4-core-20-5,pod4-core-20-6 (age 8m)\n    mgr: pod4-core-20-10(active, since 32m), standbys: pod4-core-20-5, pod4-core-20-6\n    osd: 3 osds: 3 up (since 26m), 3 in (since 26m)\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 b\n    usage:   3.0 gib used, 57 gib / 60 gib avail\n    pgs:     \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n至此，ceph集群部署完成。\n\n----------------------------------------\n\n原文链接：\n\nhttps://lvjianzhao.gitee.io/lvjianzhao/posts/6010b413/?highlight=ceph\n\n> ',charsets:{cjk:!0}},{title:"glusterfs",frontmatter:{title:"glusterfs",categories:"glusterfs",tags:["分布式文件系统"],date:"2022-12-09T20:47:55.000Z",permalink:"/pages/70e7ad/",readingShow:"top",description:"GlusterFS文件存储介绍",meta:[{name:"twitter:title",content:"glusterfs"},{name:"twitter:description",content:"GlusterFS文件存储介绍"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/06.%E5%AD%98%E5%82%A8/02.glusterfs.html"},{property:"og:type",content:"article"},{property:"og:title",content:"glusterfs"},{property:"og:description",content:"GlusterFS文件存储介绍"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/06.%E5%AD%98%E5%82%A8/02.glusterfs.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:47:55.000Z"},{property:"article:tag",content:"分布式文件系统"},{itemprop:"name",content:"glusterfs"},{itemprop:"description",content:"GlusterFS文件存储介绍"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/06.%E5%AD%98%E5%82%A8/02.glusterfs.html",relativePath:"04.运维/06.存储/02.glusterfs.md",key:"v-ed9b282c",path:"/pages/70e7ad/",headersStr:null,content:"GlusterFS文件存储介绍\n\n原文链接：https://www.viayc.com/categories/GlusterFS/\n\n说明\n\n简介\n\n1234567891011   GLUSTERFS 是 SCALE-OUT 存储解决方案 GLUSTER 的核心，它是一个开源的分布式文件系统，\n                具有强大的横向扩展能力，通过扩展能够支持数 PB 存储容量和处理数千客户端。 GLUSTERFS 借助 TCP/IP 或\n                INFINIBAND RDMA 网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据。 GLUSTERFS\n                基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。 GLUSTERFS 支持运行在任何 IP\n                网络上的标准应用程序的标准客户端，用户可以在全局统一的命名空间中使用 NFS/CIFS 等标准协议来访问应用数据。\n                GLUSTERFS\n                使得用户可摆脱原有的独立、高成本的封闭存储系统，能够利用普通廉价的存储设备来部署可集中管理、横向扩展、虚拟化的存储池，存储容量可扩展至\n                TB/PB 级。 目前 GLUSTERFS 已被 RED HAT\n                收购，它的官网是：HTTPS://WWW.GLUSTER.ORG/\n                \n\nGlusterFS 在企业中的应用场景\n\n理论和实践上分析，GlusterFS 目前主要适用于大文件存储场景，对于小文件尤其是海量小文件，存储效率和访问性能都表现不佳。建议存放文件大小大于 1MB\n\nGlusterFS 安装\n\n环境说明\n\n * CentOS 7\n * GlusterFS\n\n3 台机器安装 GlusterFS 组成一个集群。\n\n12345678910111213   服务器：10.1.0.1110.1.0.1210.1.0.13 配置 HOSTS 10.1.0.11\n                    MANAGER10.1.0.12 NODE-110.1.0.13 NODE-2 CLIENT:10.1.0.10\n                    CLIENT\n                    \n\n安装 GlusterFS\n\nCentOS 安装 GlusterFS 非常的简单\n\n在三个节点都安装 GlusterFS\n\n12345   # 安装 GLUSTERFS YUM 源文件#YUM INSTALL CENTOS-RELEASE-GLUSTER #\n        安装 GLUSTERFS 软件YUM INSTALL -Y GLUSTERFS GLUSTERFS-SERVER\n        GLUSTERFS-FUSE GLUSTERFS-RDMA\n        \n\n启动 GlusterFS\n\n1234567891011121314   [ROOT@MANAGER ~]#SYSTEMCTL START\n                      GLUSTERD.SERVICE[ROOT@MANAGER ~]#SYSTEMCTL ENABLE\n                      GLUSTERD.SERVICE[ROOT@MANAGER ~]#SYSTEMCTL STATUS\n                      GLUSTERD.SERVICE● GLUSTERD.SERVICE - GLUSTERFS, A CLUSTERED\n                      FILE-SYSTEM SERVER LOADED: LOADED\n                      (/USR/LIB/SYSTEMD/SYSTEM/GLUSTERD.SERVICE; DISABLED; VENDOR\n                      PRESET: DISABLED) ACTIVE: ACTIVE (RUNNING) SINCE 一\n                      2017-02-27 17:55:56 CST; 11S AGO PROCESS: 5476\n                      EXECSTART=/USR/SBIN/GLUSTERD -P /VAR/RUN/GLUSTERD.PID\n                      --LOG-LEVEL $LOG_LEVEL $GLUSTERD_OPTIONS (CODE=EXITED,\n                      STATUS=0/SUCCESS)MAIN PID: 5477 (GLUSTERD) MEMORY: 15.0M\n                      CGROUP: /SYSTEM.SLICE/GLUSTERD.SERVICE └─5477\n                      /USR/SBIN/GLUSTERD -P /VAR/RUN/GLUSTERD.PID --LOG-LEVEL INFO\n                      2 月 27 17:55:56 MEETBILL SYSTEMD[1]: STARTING GLUSTERFS, A\n                      CLUSTERED FILE-SYSTEM SERVER...2 月 27 17:55:56 MEETBILL\n                      SYSTEMD[1]: STARTED GLUSTERFS, A CLUSTERED FILE-SYSTEM\n                      SERVER.\n                      \n\n配置 GlusterFS\n\n在 manager 节点上配置，将 节点 加入到 集群中。\n\n12345678   [ROOT@MANAGER ~]#GLUSTER PEER PROBE MANAGERPEER PROBE:\n           SUCCESS. PROBE ON LOCALHOST NOT NEEDED [ROOT@MANAGER\n           ~]#GLUSTER PEER PROBE NODE-1PEER PROBE: SUCCESS.\n           [ROOT@MANAGER ~]#GLUSTER PEER PROBE NODE-2PEER PROBE:\n           SUCCESS.\n           \n\n查看集群状态：\n\n12345678910   [ROOT@MANAGER ~]#GLUSTER PEER STATUSNUMBER OF PEERS: 2\n              HOSTNAME: NODE-1UUID:\n              41573E8B-EB00-4802-84F0-F923A2C7BE79STATE: PEER IN CLUSTER\n              (CONNECTED) HOSTNAME: NODE-2UUID:\n              DA068E0B-EADA-4A50-94FF-623F630986D7STATE: PEER IN CLUSTER\n              (CONNECTED)\n              \n\n创建数据存储目录：\n\n123   [ROOT@MANAGER ~]#MKDIR -P /OPT/GLUSTER/DATA[ROOT@NODE-1 ~]#\n      MKDIR -P /OPT/GLUSTER/DATA[ROOT@NODE-2 ~]# MKDIR -P\n      /OPT/GLUSTER/DATA\n      \n\n查看 volume 状态：\n\n12   [ROOT@MANAGER ~]#GLUSTER VOLUME INFONO VOLUMES PRESENT\n     \n\nvolume 模式说明\n\n一、 默认模式，既 DHT, 也叫 分布卷：将文件已 hash 算法随机分布到 一台服务器节点中存储。\n\n1   GLUSTER VOLUME CREATE TEST-VOLUME SERVER1:/EXP1\n    SERVER2:/EXP2\n    \n\n二、 复制模式，既 AFR, 创建 volume 时带 replica x 数量：将文件复制到 replica x 个节点中。\n\n1   GLUSTER VOLUME CREATE TEST-VOLUME REPLICA 2 TRANSPORT TCP\n    SERVER1:/EXP1 SERVER2:/EXP2\n    \n\n三、 条带模式，既 Striped, 创建 volume 时带 stripe x 数量： 将文件切割成数据块，分别存储到 stripe x 个节点中 ( 类似 raid 0 )。\n\n1   GLUSTER VOLUME CREATE TEST-VOLUME STRIPE 2 TRANSPORT TCP\n    SERVER1:/EXP1 SERVER2:/EXP2\n    \n\n四、 分布式条带模式（组合型），最少需要 4 台服务器才能创建。 创建 volume 时 stripe 2 server = 4 个节点： 是 DHT 与 Striped 的组合型。\n\n1   GLUSTER VOLUME CREATE TEST-VOLUME STRIPE 2 TRANSPORT TCP\n    SERVER1:/EXP1 SERVER2:/EXP2 SERVER3:/EXP3 SERVER4:/EXP4\n    \n\n五、 分布式复制模式（组合型）, 最少需要 4 台服务器才能创建。 创建 volume 时 replica 2 server = 4 个节点：是 DHT 与 AFR 的组合型。\n\n1   GLUSTER VOLUME CREATE TEST-VOLUME REPLICA 2 TRANSPORT TCP\n    SERVER1:/EXP1 SERVER2:/EXP2　SERVER3:/EXP3 SERVER4:/EXP4\n    \n\n六、 条带复制卷模式（组合型）, 最少需要 4 台服务器才能创建。 创建 volume 时 stripe 2 replica 2 server = 4 个节点： 是 Striped 与 AFR 的组合型。\n\n1   GLUSTER VOLUME CREATE TEST-VOLUME STRIPE 2 REPLICA 2\n    TRANSPORT TCP SERVER1:/EXP1 SERVER2:/EXP2 SERVER3:/EXP3\n    SERVER4:/EXP4\n    \n\n七、 三种模式混合，至少需要 8 台 服务器才能创建。 stripe 2 replica 2 , 每 4 个节点 组成一个 组。\n\n1   GLUSTER VOLUME CREATE TEST-VOLUME STRIPE 2 REPLICA 2\n    TRANSPORT TCP SERVER1:/EXP1 SERVER2:/EXP2 SERVER3:/EXP3\n    SERVER4:/EXP4 SERVER5:/EXP5 SERVER6:/EXP6 SERVER7:/EXP7\n    SERVER8:/EXP8\n    \n\n创建 GlusterFS 磁盘\n\n12   [ROOT@MANAGER ~]#GLUSTER VOLUME CREATE MODELS REPLICA 3\n     MANAGER:/OPT/GLUSTER/DATA NODE-1:/OPT/GLUSTER/DATA\n     NODE-2:/OPT/GLUSTER/DATA FORCEVOLUME CREATE: MODELS:\n     SUCCESS: PLEASE START THE VOLUME TO ACCESS DATA\n     \n\n再查看 volume 状态：\n\n1234567891011121314   [ROOT@MANAGER ~]#GLUSTER VOLUME INFO VOLUME NAME:\n                      MODELSTYPE: REPLICATEVOLUME ID:\n                      E539FF3B-2278-4F3F-A594-1F101EABBF1ESTATUS: CREATEDNUMBER OF\n                      BRICKS: 1 X 3 = 3TRANSPORT-TYPE: TCPBRICKS:BRICK1:\n                      MANAGER:/OPT/GLUSTER/DATABRICK2:\n                      NODE-1:/OPT/GLUSTER/DATABRICK3:\n                      NODE-2:/OPT/GLUSTER/DATAOPTIONS\n                      RECONFIGURED:PERFORMANCE.READDIR-AHEAD: ON\n                      \n\n启动 models\n\n12   [ROOT@MANAGER ~]#GLUSTER VOLUME START MODELSVOLUME START:\n     MODELS: SUCCESS\n     \n\nGlusterFS 客户端\n\n部署 GlusterFS 客户端并 mount GlusterFS 文件系统\n\n123   [ROOT@CLIENT ~]#YUM INSTALL -Y GLUSTERFS\n      GLUSTERFS-FUSE[ROOT@CLIENT ~]#MKDIR -P\n      /OPT/GFSMNT[ROOT@CLIENT ~]#MOUNT -T GLUSTERFS MANAGER:MODELS\n      /OPT/GFSMNT/\n      \n\n1234567891011   [ROOT@NODE-94 ~]#DF -H文件系统 容量 已用 可用 已用 %\n                挂载点/DEV/MAPPER/VG001-ROOT 98G 1.2G 97G 2% /DEVTMPFS 32G 0\n                32G 0% /DEVTMPFS 32G 0 32G 0% /DEV/SHMTMPFS 32G 130M 32G 1%\n                /RUNTMPFS 32G 0 32G 0% /SYS/FS/CGROUP/DEV/MAPPER/VG001-OPT\n                441G 71G 370G 17% /OPT/DEV/SDA2 497M 153M 344M 31%\n                /BOOTTMPFS 6.3G 0 6.3G 0% /RUN/USER/0MANAGER:MODELS 441G 18G\n                424G 4% /OPT/GFSMNT\n                \n\n维护\n\n 1. 查看 GlusterFS 中所有的 volume:\n\n1   [ROOT@MANAGER ~]#GLUSTER VOLUME LIST\n    \n\n 1. 删除 GlusterFS 磁盘：\n\n12   [ROOT@MANAGER ~]#GLUSTER VOLUME STOP MODELS #停止名字为 MODELS\n     的磁盘[ROOT@MANAGER ~]#GLUSTER VOLUME DELETE MODELS #删除名字为\n     MODELS 的磁盘\n     \n\n注： 删除 磁盘 以后，必须删除 磁盘 ( /opt/gluster/data ) 中的 （ .glusterfs/ .trashcan/ ）目录。\n\n否则创建新 volume 相同的 磁盘 会出现文件 不分布，或者 类型 错乱 的问题。\n\n 1. 卸载某个节点 GlusterFS 磁盘\n\n1   [ROOT@MANAGER ~]#GLUSTER PEER DETACH NODE-2\n    \n\n 1. 设置访问限制，按照每个 volume 来限制\n\n1   [ROOT@MANAGER ~]#GLUSTER VOLUME SET MODELS AUTH.ALLOW\n    10.6.0.,10.7.0.\n    \n\n 1. 添加 GlusterFS 节点：\n\n12   [ROOT@MANAGER ~]#GLUSTER PEER PROBE NODE-3[ROOT@MANAGER\n     ~]#GLUSTER VOLUME ADD-BRICK MODELS NODE-3:/OPT/GLUSTER/DATA\n     \n\n注：如果是复制卷或者条带卷，则每次添加的 Brick 数必须是 replica 或者 stripe 的整数倍\n\n 1. 配置卷\n\n1   [ROOT@MANAGER ~]# GLUSTER VOLUME SET\n    \n\n 1. 缩容 volume:\n\n先将数据迁移到其它可用的 Brick，迁移结束后才将该 Brick 移除：\n\n1   [ROOT@MANAGER ~]#GLUSTER VOLUME REMOVE-BRICK MODELS\n    NODE-2:/OPT/GLUSTER/DATA NODE-3:/OPT/GLUSTER/DATA START\n    \n\n在执行了 start 之后，可以使用 status 命令查看移除进度：\n\n1   [ROOT@MANAGER ~]#GLUSTER VOLUME REMOVE-BRICK MODELS\n    NODE-2:/OPT/GLUSTER/DATA NODE-3:/OPT/GLUSTER/DATA STATUS\n    \n\n不进行数据迁移，直接删除该 Brick：\n\n1   [ROOT@MANAGER ~]#GLUSTER VOLUME REMOVE-BRICK MODELS\n    NODE-2:/OPT/GLUSTER/DATA NODE-3:/OPT/GLUSTER/DATA COMMIT\n    \n\n注意，如果是复制卷或者条带卷，则每次移除的 Brick 数必须是 replica 或者 stripe 的整数倍。\n\n扩容：\n\n1   GLUSTER VOLUME ADD-BRICK MODELS NODE-2:/OPT/GLUSTER/DATA\n    \n\n 1. 修复命令：\n\n1   [ROOT@MANAGER ~]#GLUSTER VOLUME REPLACE-BRICK MODELS\n    NODE-2:/OPT/GLUSTER/DATA NODE-3:/OPT/GLUSTER/DATA COMMIT\n    -FORCE\n    \n\n 1. 迁移 volume:\n\n1   [ROOT@MANAGER ~]#GLUSTER VOLUME REPLACE-BRICK MODELS\n    NODE-2:/OPT/GLUSTER/DATA NODE-3:/OPT/GLUSTER/DATA START\n    \n\npause 为暂停迁移\n\n1   [ROOT@MANAGER ~]#GLUSTER VOLUME REPLACE-BRICK MODELS\n    NODE-2:/OPT/GLUSTER/DATA NODE-3:/OPT/GLUSTER/DATA PAUSE\n    \n\nabort 为终止迁移\n\n1   [ROOT@MANAGER ~]#GLUSTER VOLUME REPLACE-BRICK MODELS\n    NODE-2:/OPT/GLUSTER/DATA NODE-3:/OPT/GLUSTER/DATA ABORT\n    \n\nstatus 查看迁移状态\n\n1   [ROOT@MANAGER ~]#GLUSTER VOLUME REPLACE-BRICK MODELS\n    NODE-2:/OPT/GLUSTER/DATA NODE-3:/OPT/GLUSTER/DATA STATUS\n    \n\n迁移结束后使用 commit 来生效\n\n1   [ROOT@MANAGER ~]#GLUSTER VOLUME REPLACE-BRICK MODELS\n    NODE-2:/OPT/GLUSTER/DATA NODE-3:/OPT/GLUSTER/DATA COMMIT\n    \n\n 1. 均衡 volume:\n\n12345   [ROOT@MANAGER ~]#GLUSTER VOLUME MODELS\n        LAY-OUTSTART[ROOT@MANAGER ~]#GLUSTER VOLUME MODELS\n        START[ROOT@MANAGER ~]#GLUSTER VOLUME MODELS\n        STARTFORCE[ROOT@MANAGER ~]#GLUSTER VOLUME MODELS\n        STATUS[ROOT@MANAGER ~]#GLUSTER VOLUME MODELS STOP\n        ",normalizedContent:"glusterfs文件存储介绍\n\n原文链接：https://www.viayc.com/categories/glusterfs/\n\n说明\n\n简介\n\n1234567891011   glusterfs 是 scale-out 存储解决方案 gluster 的核心，它是一个开源的分布式文件系统，\n                具有强大的横向扩展能力，通过扩展能够支持数 pb 存储容量和处理数千客户端。 glusterfs 借助 tcp/ip 或\n                infiniband rdma 网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据。 glusterfs\n                基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。 glusterfs 支持运行在任何 ip\n                网络上的标准应用程序的标准客户端，用户可以在全局统一的命名空间中使用 nfs/cifs 等标准协议来访问应用数据。\n                glusterfs\n                使得用户可摆脱原有的独立、高成本的封闭存储系统，能够利用普通廉价的存储设备来部署可集中管理、横向扩展、虚拟化的存储池，存储容量可扩展至\n                tb/pb 级。 目前 glusterfs 已被 red hat\n                收购，它的官网是：https://www.gluster.org/\n                \n\nglusterfs 在企业中的应用场景\n\n理论和实践上分析，glusterfs 目前主要适用于大文件存储场景，对于小文件尤其是海量小文件，存储效率和访问性能都表现不佳。建议存放文件大小大于 1mb\n\nglusterfs 安装\n\n环境说明\n\n * centos 7\n * glusterfs\n\n3 台机器安装 glusterfs 组成一个集群。\n\n12345678910111213   服务器：10.1.0.1110.1.0.1210.1.0.13 配置 hosts 10.1.0.11\n                    manager10.1.0.12 node-110.1.0.13 node-2 client:10.1.0.10\n                    client\n                    \n\n安装 glusterfs\n\ncentos 安装 glusterfs 非常的简单\n\n在三个节点都安装 glusterfs\n\n12345   # 安装 glusterfs yum 源文件#yum install centos-release-gluster #\n        安装 glusterfs 软件yum install -y glusterfs glusterfs-server\n        glusterfs-fuse glusterfs-rdma\n        \n\n启动 glusterfs\n\n1234567891011121314   [root@manager ~]#systemctl start\n                      glusterd.service[root@manager ~]#systemctl enable\n                      glusterd.service[root@manager ~]#systemctl status\n                      glusterd.service● glusterd.service - glusterfs, a clustered\n                      file-system server loaded: loaded\n                      (/usr/lib/systemd/system/glusterd.service; disabled; vendor\n                      preset: disabled) active: active (running) since 一\n                      2017-02-27 17:55:56 cst; 11s ago process: 5476\n                      execstart=/usr/sbin/glusterd -p /var/run/glusterd.pid\n                      --log-level $log_level $glusterd_options (code=exited,\n                      status=0/success)main pid: 5477 (glusterd) memory: 15.0m\n                      cgroup: /system.slice/glusterd.service └─5477\n                      /usr/sbin/glusterd -p /var/run/glusterd.pid --log-level info\n                      2 月 27 17:55:56 meetbill systemd[1]: starting glusterfs, a\n                      clustered file-system server...2 月 27 17:55:56 meetbill\n                      systemd[1]: started glusterfs, a clustered file-system\n                      server.\n                      \n\n配置 glusterfs\n\n在 manager 节点上配置，将 节点 加入到 集群中。\n\n12345678   [root@manager ~]#gluster peer probe managerpeer probe:\n           success. probe on localhost not needed [root@manager\n           ~]#gluster peer probe node-1peer probe: success.\n           [root@manager ~]#gluster peer probe node-2peer probe:\n           success.\n           \n\n查看集群状态：\n\n12345678910   [root@manager ~]#gluster peer statusnumber of peers: 2\n              hostname: node-1uuid:\n              41573e8b-eb00-4802-84f0-f923a2c7be79state: peer in cluster\n              (connected) hostname: node-2uuid:\n              da068e0b-eada-4a50-94ff-623f630986d7state: peer in cluster\n              (connected)\n              \n\n创建数据存储目录：\n\n123   [root@manager ~]#mkdir -p /opt/gluster/data[root@node-1 ~]#\n      mkdir -p /opt/gluster/data[root@node-2 ~]# mkdir -p\n      /opt/gluster/data\n      \n\n查看 volume 状态：\n\n12   [root@manager ~]#gluster volume infono volumes present\n     \n\nvolume 模式说明\n\n一、 默认模式，既 dht, 也叫 分布卷：将文件已 hash 算法随机分布到 一台服务器节点中存储。\n\n1   gluster volume create test-volume server1:/exp1\n    server2:/exp2\n    \n\n二、 复制模式，既 afr, 创建 volume 时带 replica x 数量：将文件复制到 replica x 个节点中。\n\n1   gluster volume create test-volume replica 2 transport tcp\n    server1:/exp1 server2:/exp2\n    \n\n三、 条带模式，既 striped, 创建 volume 时带 stripe x 数量： 将文件切割成数据块，分别存储到 stripe x 个节点中 ( 类似 raid 0 )。\n\n1   gluster volume create test-volume stripe 2 transport tcp\n    server1:/exp1 server2:/exp2\n    \n\n四、 分布式条带模式（组合型），最少需要 4 台服务器才能创建。 创建 volume 时 stripe 2 server = 4 个节点： 是 dht 与 striped 的组合型。\n\n1   gluster volume create test-volume stripe 2 transport tcp\n    server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4\n    \n\n五、 分布式复制模式（组合型）, 最少需要 4 台服务器才能创建。 创建 volume 时 replica 2 server = 4 个节点：是 dht 与 afr 的组合型。\n\n1   gluster volume create test-volume replica 2 transport tcp\n    server1:/exp1 server2:/exp2　server3:/exp3 server4:/exp4\n    \n\n六、 条带复制卷模式（组合型）, 最少需要 4 台服务器才能创建。 创建 volume 时 stripe 2 replica 2 server = 4 个节点： 是 striped 与 afr 的组合型。\n\n1   gluster volume create test-volume stripe 2 replica 2\n    transport tcp server1:/exp1 server2:/exp2 server3:/exp3\n    server4:/exp4\n    \n\n七、 三种模式混合，至少需要 8 台 服务器才能创建。 stripe 2 replica 2 , 每 4 个节点 组成一个 组。\n\n1   gluster volume create test-volume stripe 2 replica 2\n    transport tcp server1:/exp1 server2:/exp2 server3:/exp3\n    server4:/exp4 server5:/exp5 server6:/exp6 server7:/exp7\n    server8:/exp8\n    \n\n创建 glusterfs 磁盘\n\n12   [root@manager ~]#gluster volume create models replica 3\n     manager:/opt/gluster/data node-1:/opt/gluster/data\n     node-2:/opt/gluster/data forcevolume create: models:\n     success: please start the volume to access data\n     \n\n再查看 volume 状态：\n\n1234567891011121314   [root@manager ~]#gluster volume info volume name:\n                      modelstype: replicatevolume id:\n                      e539ff3b-2278-4f3f-a594-1f101eabbf1estatus: creatednumber of\n                      bricks: 1 x 3 = 3transport-type: tcpbricks:brick1:\n                      manager:/opt/gluster/databrick2:\n                      node-1:/opt/gluster/databrick3:\n                      node-2:/opt/gluster/dataoptions\n                      reconfigured:performance.readdir-ahead: on\n                      \n\n启动 models\n\n12   [root@manager ~]#gluster volume start modelsvolume start:\n     models: success\n     \n\nglusterfs 客户端\n\n部署 glusterfs 客户端并 mount glusterfs 文件系统\n\n123   [root@client ~]#yum install -y glusterfs\n      glusterfs-fuse[root@client ~]#mkdir -p\n      /opt/gfsmnt[root@client ~]#mount -t glusterfs manager:models\n      /opt/gfsmnt/\n      \n\n1234567891011   [root@node-94 ~]#df -h文件系统 容量 已用 可用 已用 %\n                挂载点/dev/mapper/vg001-root 98g 1.2g 97g 2% /devtmpfs 32g 0\n                32g 0% /devtmpfs 32g 0 32g 0% /dev/shmtmpfs 32g 130m 32g 1%\n                /runtmpfs 32g 0 32g 0% /sys/fs/cgroup/dev/mapper/vg001-opt\n                441g 71g 370g 17% /opt/dev/sda2 497m 153m 344m 31%\n                /boottmpfs 6.3g 0 6.3g 0% /run/user/0manager:models 441g 18g\n                424g 4% /opt/gfsmnt\n                \n\n维护\n\n 1. 查看 glusterfs 中所有的 volume:\n\n1   [root@manager ~]#gluster volume list\n    \n\n 1. 删除 glusterfs 磁盘：\n\n12   [root@manager ~]#gluster volume stop models #停止名字为 models\n     的磁盘[root@manager ~]#gluster volume delete models #删除名字为\n     models 的磁盘\n     \n\n注： 删除 磁盘 以后，必须删除 磁盘 ( /opt/gluster/data ) 中的 （ .glusterfs/ .trashcan/ ）目录。\n\n否则创建新 volume 相同的 磁盘 会出现文件 不分布，或者 类型 错乱 的问题。\n\n 1. 卸载某个节点 glusterfs 磁盘\n\n1   [root@manager ~]#gluster peer detach node-2\n    \n\n 1. 设置访问限制，按照每个 volume 来限制\n\n1   [root@manager ~]#gluster volume set models auth.allow\n    10.6.0.,10.7.0.\n    \n\n 1. 添加 glusterfs 节点：\n\n12   [root@manager ~]#gluster peer probe node-3[root@manager\n     ~]#gluster volume add-brick models node-3:/opt/gluster/data\n     \n\n注：如果是复制卷或者条带卷，则每次添加的 brick 数必须是 replica 或者 stripe 的整数倍\n\n 1. 配置卷\n\n1   [root@manager ~]# gluster volume set\n    \n\n 1. 缩容 volume:\n\n先将数据迁移到其它可用的 brick，迁移结束后才将该 brick 移除：\n\n1   [root@manager ~]#gluster volume remove-brick models\n    node-2:/opt/gluster/data node-3:/opt/gluster/data start\n    \n\n在执行了 start 之后，可以使用 status 命令查看移除进度：\n\n1   [root@manager ~]#gluster volume remove-brick models\n    node-2:/opt/gluster/data node-3:/opt/gluster/data status\n    \n\n不进行数据迁移，直接删除该 brick：\n\n1   [root@manager ~]#gluster volume remove-brick models\n    node-2:/opt/gluster/data node-3:/opt/gluster/data commit\n    \n\n注意，如果是复制卷或者条带卷，则每次移除的 brick 数必须是 replica 或者 stripe 的整数倍。\n\n扩容：\n\n1   gluster volume add-brick models node-2:/opt/gluster/data\n    \n\n 1. 修复命令：\n\n1   [root@manager ~]#gluster volume replace-brick models\n    node-2:/opt/gluster/data node-3:/opt/gluster/data commit\n    -force\n    \n\n 1. 迁移 volume:\n\n1   [root@manager ~]#gluster volume replace-brick models\n    node-2:/opt/gluster/data node-3:/opt/gluster/data start\n    \n\npause 为暂停迁移\n\n1   [root@manager ~]#gluster volume replace-brick models\n    node-2:/opt/gluster/data node-3:/opt/gluster/data pause\n    \n\nabort 为终止迁移\n\n1   [root@manager ~]#gluster volume replace-brick models\n    node-2:/opt/gluster/data node-3:/opt/gluster/data abort\n    \n\nstatus 查看迁移状态\n\n1   [root@manager ~]#gluster volume replace-brick models\n    node-2:/opt/gluster/data node-3:/opt/gluster/data status\n    \n\n迁移结束后使用 commit 来生效\n\n1   [root@manager ~]#gluster volume replace-brick models\n    node-2:/opt/gluster/data node-3:/opt/gluster/data commit\n    \n\n 1. 均衡 volume:\n\n12345   [root@manager ~]#gluster volume models\n        lay-outstart[root@manager ~]#gluster volume models\n        start[root@manager ~]#gluster volume models\n        startforce[root@manager ~]#gluster volume models\n        status[root@manager ~]#gluster volume models stop\n        ",charsets:{cjk:!0}},{title:"centos7下配置firewalld实现nat转发软路由",frontmatter:{title:"centos7下配置firewalld实现nat转发软路由",date:"2022-12-15T19:03:12.000Z",permalink:"/pages/ce46b6/",categories:["运维","防火墙"],tags:[null],readingShow:"top",description:"如果配合 DHCP 服务或实现更多功能。",meta:[{name:"twitter:title",content:"centos7下配置firewalld实现nat转发软路由"},{name:"twitter:description",content:"如果配合 DHCP 服务或实现更多功能。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/07.%E9%98%B2%E7%81%AB%E5%A2%99/01.centos7%E4%B8%8B%E9%85%8D%E7%BD%AEfirewalld%E5%AE%9E%E7%8E%B0nat%E8%BD%AC%E5%8F%91%E8%BD%AF%E8%B7%AF%E7%94%B1.html"},{property:"og:type",content:"article"},{property:"og:title",content:"centos7下配置firewalld实现nat转发软路由"},{property:"og:description",content:"如果配合 DHCP 服务或实现更多功能。"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/07.%E9%98%B2%E7%81%AB%E5%A2%99/01.centos7%E4%B8%8B%E9%85%8D%E7%BD%AEfirewalld%E5%AE%9E%E7%8E%B0nat%E8%BD%AC%E5%8F%91%E8%BD%AF%E8%B7%AF%E7%94%B1.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T19:03:12.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"centos7下配置firewalld实现nat转发软路由"},{itemprop:"description",content:"如果配合 DHCP 服务或实现更多功能。"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/07.%E9%98%B2%E7%81%AB%E5%A2%99/01.centos7%E4%B8%8B%E9%85%8D%E7%BD%AEfirewalld%E5%AE%9E%E7%8E%B0nat%E8%BD%AC%E5%8F%91%E8%BD%AF%E8%B7%AF%E7%94%B1.html",relativePath:"04.运维/07.防火墙/01.centos7下配置firewalld实现nat转发软路由.md",key:"v-0a29d12e",path:"/pages/ce46b6/",headersStr:null,content:'如果配合 DHCP 服务或实现更多功能。\n\n☼ NAT 转发软路由\n\n开启 NAT 转发之后，只要本机可以上网，不论是单网卡还是多网卡，局域网内的其他机器可以将默认网关设置为已开启 NAT 转发的服务器 IP ，即可实现上网。\n\n开启 NAT 转发 firewall-cmd --permanent --zone=public --add-masquerade # 开放 DNS 使用的 53 端口，UDP # 必须，否则其他机器无法进行域名解析 firewall-cmd --zone=public --add-port=80/tcp --permanent # 检查是否允许 NAT 转发 firewall-cmd --query-masquerade # 禁止防火墙 NAT 转发 firewall-cmd --remove-masquerade\n\n☼ 端口转发\n\n端口转发可以将指定地址访问指定的端口时，将流量转发至指定地址的指定端口。转发的目的如果不指定 ip 的话就默认为本机，如果指定了 ip 却没指定端口，则默认使用来源端口。\n\n将80端口的流量转发至8080 firewall-cmd --add-forward-port=port=80:proto=tcp:toport=8080 # 将80端口的流量转发至192.168.0.1 firewall-cmd --add-forward-port=proto=80:proto=tcp:toaddr=192.168.0.1 # 将80端口的流量转发至192.168.0.1的8080端口 firewall-cmd --add-forward-port=proto=80:proto=tcp:toaddr=192.168.0.1:toport=8080\n\n1、限制IP地址访问\n\n（1）比如限制IP为192.168.0.200的地址禁止访问80端口即禁止访问机器\n\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" reject"\n\n（2）重新载入一下防火墙设置，使设置生效\n\nfirewall-cmd --reload\n\n（3）查看已经设置的规则\n\nfirewall-cmd --zone=public --list-rich-rules\n\n2、解除IP地址限制\n\n（1）解除刚才被限制的192.168.0.200\n\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" accept"\n\n（2）重新载入一下防火墙设置，使设置生效\n\nfirewall-cmd --reload\n\n（3）再查看规则设置发现已经没有192.168.0.200的限制了\n\nfirewall-cmd --zone=public --list-rich-rules\n\n如设置未生效，可尝试直接编辑规则文件，删掉原来的设置规则，重新载入一下防火墙即可\n\nvi /etc/firewalld/zones/public.xml\n\n限制IP地址段\n\n（1）如我们需要限制10.0.0.0-10.0.0.255这一整个段的IP，禁止他们访问\n\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="10.0.0.0/24" port protocol="tcp" port="80" reject"\n\n其中10.0.0.0/24表示为从10.0.0.0这个IP开始，24代表子网掩码为255.255.255.0，共包含256个地址，即从0-255共256个IP，即正好限制了这一整段的IP地址，具体的设置规则可参考下表\n\n重新载入一下防火墙设置，使设置生效\n\nfirewall-cmd --reload\n\n（3）查看规则，确认是否生效\n\nfirewall-cmd --zone=public --list-rich-rules\n\n（4）同理，打开限制为\n\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="10.0.0.0/24" port protocol="tcp" port="80" accept"\n\nfirewall-cmd --reload\n\n#查看所有的防火墙策略（即显示/etc/firewalld/zones/下的所有策略）\n\nfirewall-cmd --list-all-zones\n\n#允许http服务(对应服务策略目录：/usr/lib/firewalld/services/)\n\nfirewall-cmd --permanent --add-service=http\n\n#关闭http服务(对应服务策略目录：/usr/lib/firewalld/services/)\n\nfirewall-cmd --permanent --remove-service=http\n\n将接口添加到区域，默认接口都在public\n\nfirewall-cmd --zone=public --add-interface=eth0 永久生效再加上 --permanent 然后reload防火墙\n\n设置默认接口区域\n\nfirewall-cmd --set-default-zone=public 立即生效无需重启 firewall-cmd --reload\n\n查看防火墙端口开启状态\n\nfirewall-cmd --list-ports\n\n添加\n\nfirewall-cmd --zone=public --add-port=80/tcp --permanent\n\n删除\n\nfirewall-cmd --zone=public --remove-port=80/tcp --permanent',normalizedContent:'如果配合 dhcp 服务或实现更多功能。\n\n☼ nat 转发软路由\n\n开启 nat 转发之后，只要本机可以上网，不论是单网卡还是多网卡，局域网内的其他机器可以将默认网关设置为已开启 nat 转发的服务器 ip ，即可实现上网。\n\n开启 nat 转发 firewall-cmd --permanent --zone=public --add-masquerade # 开放 dns 使用的 53 端口，udp # 必须，否则其他机器无法进行域名解析 firewall-cmd --zone=public --add-port=80/tcp --permanent # 检查是否允许 nat 转发 firewall-cmd --query-masquerade # 禁止防火墙 nat 转发 firewall-cmd --remove-masquerade\n\n☼ 端口转发\n\n端口转发可以将指定地址访问指定的端口时，将流量转发至指定地址的指定端口。转发的目的如果不指定 ip 的话就默认为本机，如果指定了 ip 却没指定端口，则默认使用来源端口。\n\n将80端口的流量转发至8080 firewall-cmd --add-forward-port=port=80:proto=tcp:toport=8080 # 将80端口的流量转发至192.168.0.1 firewall-cmd --add-forward-port=proto=80:proto=tcp:toaddr=192.168.0.1 # 将80端口的流量转发至192.168.0.1的8080端口 firewall-cmd --add-forward-port=proto=80:proto=tcp:toaddr=192.168.0.1:toport=8080\n\n1、限制ip地址访问\n\n（1）比如限制ip为192.168.0.200的地址禁止访问80端口即禁止访问机器\n\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" reject"\n\n（2）重新载入一下防火墙设置，使设置生效\n\nfirewall-cmd --reload\n\n（3）查看已经设置的规则\n\nfirewall-cmd --zone=public --list-rich-rules\n\n2、解除ip地址限制\n\n（1）解除刚才被限制的192.168.0.200\n\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.200" port protocol="tcp" port="80" accept"\n\n（2）重新载入一下防火墙设置，使设置生效\n\nfirewall-cmd --reload\n\n（3）再查看规则设置发现已经没有192.168.0.200的限制了\n\nfirewall-cmd --zone=public --list-rich-rules\n\n如设置未生效，可尝试直接编辑规则文件，删掉原来的设置规则，重新载入一下防火墙即可\n\nvi /etc/firewalld/zones/public.xml\n\n限制ip地址段\n\n（1）如我们需要限制10.0.0.0-10.0.0.255这一整个段的ip，禁止他们访问\n\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="10.0.0.0/24" port protocol="tcp" port="80" reject"\n\n其中10.0.0.0/24表示为从10.0.0.0这个ip开始，24代表子网掩码为255.255.255.0，共包含256个地址，即从0-255共256个ip，即正好限制了这一整段的ip地址，具体的设置规则可参考下表\n\n重新载入一下防火墙设置，使设置生效\n\nfirewall-cmd --reload\n\n（3）查看规则，确认是否生效\n\nfirewall-cmd --zone=public --list-rich-rules\n\n（4）同理，打开限制为\n\nfirewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="10.0.0.0/24" port protocol="tcp" port="80" accept"\n\nfirewall-cmd --reload\n\n#查看所有的防火墙策略（即显示/etc/firewalld/zones/下的所有策略）\n\nfirewall-cmd --list-all-zones\n\n#允许http服务(对应服务策略目录：/usr/lib/firewalld/services/)\n\nfirewall-cmd --permanent --add-service=http\n\n#关闭http服务(对应服务策略目录：/usr/lib/firewalld/services/)\n\nfirewall-cmd --permanent --remove-service=http\n\n将接口添加到区域，默认接口都在public\n\nfirewall-cmd --zone=public --add-interface=eth0 永久生效再加上 --permanent 然后reload防火墙\n\n设置默认接口区域\n\nfirewall-cmd --set-default-zone=public 立即生效无需重启 firewall-cmd --reload\n\n查看防火墙端口开启状态\n\nfirewall-cmd --list-ports\n\n添加\n\nfirewall-cmd --zone=public --add-port=80/tcp --permanent\n\n删除\n\nfirewall-cmd --zone=public --remove-port=80/tcp --permanent',charsets:{cjk:!0}},{title:"数据库二进制安装",frontmatter:{title:"数据库二进制安装",categories:"mysql",tags:["mysql"],date:"2022-12-09T20:49:19.000Z",permalink:"/pages/1f5460/",readingShow:"top",description:"MySQL Yum仓库提供了用于在Linux平台上安装MySQL服务器，客户端和其他组件的RPM包。mysql-yum安装下载地址",meta:[{name:"image",content:"http://www.sunrisenan.com/uploads/mysql/images/m_f397fc71d88cf7f381987beaeda4b9df_r.png"},{name:"twitter:title",content:"数据库二进制安装"},{name:"twitter:description",content:"MySQL Yum仓库提供了用于在Linux平台上安装MySQL服务器，客户端和其他组件的RPM包。mysql-yum安装下载地址"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://www.sunrisenan.com/uploads/mysql/images/m_f397fc71d88cf7f381987beaeda4b9df_r.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/01.mysql/01.%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85.html"},{property:"og:type",content:"article"},{property:"og:title",content:"数据库二进制安装"},{property:"og:description",content:"MySQL Yum仓库提供了用于在Linux平台上安装MySQL服务器，客户端和其他组件的RPM包。mysql-yum安装下载地址"},{property:"og:image",content:"http://www.sunrisenan.com/uploads/mysql/images/m_f397fc71d88cf7f381987beaeda4b9df_r.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/01.mysql/01.%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:49:19.000Z"},{property:"article:tag",content:"mysql"},{itemprop:"name",content:"数据库二进制安装"},{itemprop:"description",content:"MySQL Yum仓库提供了用于在Linux平台上安装MySQL服务器，客户端和其他组件的RPM包。mysql-yum安装下载地址"},{itemprop:"image",content:"http://www.sunrisenan.com/uploads/mysql/images/m_f397fc71d88cf7f381987beaeda4b9df_r.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/01.mysql/01.%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85.html",relativePath:"04.运维/08.数据库/01.mysql/01.数据库二进制安装.md",key:"v-09ffdab8",path:"/pages/1f5460/",headers:[{level:2,title:"1.安装MySQL仓库源",slug:"_1-安装mysql仓库源",normalizedTitle:"1.安装mysql仓库源",charIndex:153},{level:2,title:"2.选择并启用适合当前平台的发行包",slug:"_2-选择并启用适合当前平台的发行包",normalizedTitle:"2.选择并启用适合当前平台的发行包",charIndex:268},{level:2,title:"3.通过以下命令安装MySQL, 并启动MySQL",slug:"_3-通过以下命令安装mysql-并启动mysql",normalizedTitle:"3.通过以下命令安装mysql, 并启动mysql",charIndex:812},{level:2,title:"1.基础环境准备",slug:"_1-基础环境准备",normalizedTitle:"1.基础环境准备",charIndex:1683},{level:2,title:"2.下载MySQL并安装",slug:"_2-下载mysql并安装",normalizedTitle:"2.下载mysql并安装",charIndex:1972},{level:2,title:"3.进行MySQL初始化",slug:"_3-进行mysql初始化",normalizedTitle:"3.进行mysql初始化",charIndex:2261},{level:2,title:"4.建立MySQL配置文件",slug:"_4-建立mysql配置文件",normalizedTitle:"4.建立mysql配置文件",charIndex:2874},{level:2,title:"5.启动MySQL数据库",slug:"_5-启动mysql数据库",normalizedTitle:"5.启动mysql数据库",charIndex:3060},{level:2,title:"6.连接数据库测试",slug:"_6-连接数据库测试",normalizedTitle:"6.连接数据库测试",charIndex:3967},{level:2,title:"1.源码安装mysql需要依赖cmake、boost",slug:"_1-源码安装mysql需要依赖cmake、boost",normalizedTitle:"1.源码安装mysql需要依赖cmake、boost",charIndex:5341},{level:2,title:"2.下载源码包并编译MySQL",slug:"_2-下载源码包并编译mysql",normalizedTitle:"2.下载源码包并编译mysql",charIndex:5546},{level:2,title:"3.完成后基本优化",slug:"_3-完成后基本优化",normalizedTitle:"3.完成后基本优化",charIndex:6392},{level:2,title:"4.准备MySQL基础配置文件",slug:"_4-准备mysql基础配置文件",normalizedTitle:"4.准备mysql基础配置文件",charIndex:6551},{level:2,title:"5.拷贝MySQL程序启动文件",slug:"_5-拷贝mysql程序启动文件",normalizedTitle:"5.拷贝mysql程序启动文件",charIndex:6667},{level:2,title:"6.初始化MySQL",slug:"_6-初始化mysql",normalizedTitle:"6.初始化mysql",charIndex:6871},{level:2,title:"1.更改root密码",slug:"_1-更改root密码",normalizedTitle:"1.更改root密码",charIndex:7281},{level:2,title:"2.忘记mysql root密码",slug:"_2-忘记mysql-root密码",normalizedTitle:"2.忘记mysql root密码",charIndex:7702}],headersStr:"1.安装MySQL仓库源 2.选择并启用适合当前平台的发行包 3.通过以下命令安装MySQL, 并启动MySQL 1.基础环境准备 2.下载MySQL并安装 3.进行MySQL初始化 4.建立MySQL配置文件 5.启动MySQL数据库 6.连接数据库测试 1.源码安装mysql需要依赖cmake、boost 2.下载源码包并编译MySQL 3.完成后基本优化 4.准备MySQL基础配置文件 5.拷贝MySQL程序启动文件 6.初始化MySQL 1.更改root密码 2.忘记mysql root密码",content:"# 数据库二进制安装\n\n\n# 1.YUM安装数据库\n\nMySQL Yum仓库提供了用于在Linux平台上安装MySQL服务器，客户端和其他组件的RPM包。mysql-yum安装下载地址\n\n\n\n\n\n> 使用MySQL Yum仓库时，默认选择安装最新的MySQL版本。如果需要使用低版本请按如下操作。\n\n\n# 1.安装MySQL仓库源\n\n[root@sql ~]# rpm -ivh https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm\n\n\n1\n\n\n\n# 2.选择并启用适合当前平台的发行包\n\n//列出所有MySQL发行版仓库\n[root@sql ~]# yum repolist all|grep mysql\n\n//禁用8.0发行版仓库, 启用5.7发行版仓库\n[root@sql ~]# yum install yum-utils\n[root@sql ~]# yum-config-manager --disable mysql80-community\n[root@sql ~]# yum-config-manager --enable mysql57-community\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n> 可以手动编辑/etc/yum.repos.d/mysql-community.repo 文件配置仓库\n\n[mysql57-community]\nname=MySQL 5.7 Community Server\nbaseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/6/$basearch/\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3.通过以下命令安装MySQL, 并启动MySQL\n\n[root@sql ~]# yum install -y mysql-community-server\n[root@sql ~]# systemctl start mysqld\n[root@sql ~]# systemctl enable mysqld\n\n\n1\n2\n3\n\n\n> MySQL服务器初始化（仅适用于MySQL 5.7）在服务器初始启动时，如果服务器的数据目录为空，则会发生以下情况： • 服务器已初始化。 • 在数据目录中生成SSL证书和密钥文件。 • validate_password插件安装并启用。 • 超级用户帐户‘root‘@’localhost’已创建。\n\n超级用户的密码被设置并存储在错误日志文件中。要显示它，请使用以下命令:\n\n[root@vm-70-160 ~]# grep \"password\" /var/log/mysqld.log \n2018-04-28T07:11:51.589629Z 1 [Note] A temporary password is generated for root@localhost: jHlRHucap3+7\n通过使用生成的临时密码登录并尽快更改root密码并为超级用户帐户设置自定义密码\n[root@vm-70-160 ~]# mysql -uroot -pjHlRHucap3+7\nmysql> ALTER USER 'root'@'localhost' IDENTIFIED BY 'Bgx123.com';\n\n\n1\n2\n3\n4\n5\n\n\n> MySQL的validate_password插件默认安装。将要求密码至少包含大写、小写、数字、特殊字符、并且总密码长度至少为8个字符。\n\n\n# 2.通用安装数据库\n\n采用二进制免编译方式安装MySQL, 不需要复杂的编译设置和编译时间等待,解压下载的软件包,初始化即可完成MySQL的安装和启动。MySQL二进制包下载地址\n\n\n\nMySQL二进制包官方安装手册\n\n\n# 1.基础环境准备\n\n[root@sql ~]# systemctl stop firewalld\n[root@sql ~]# systemctl disable firewalld\n[root@sql ~]# setenforce 0\n\n//建立用户与相应目录\n[root@sql ~]# groupadd mysql\n[root@sql ~]# useradd -r -g mysql -s /sbin/nologin mysql\n[root@sql ~]# mkdir /soft/src -p && cd /soft/src\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 2.下载MySQL并安装\n\n[root@sql src]# wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz\n[root@sql src]# tar xf mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz -C /soft\n[root@sql src]# ln -s /soft/mysql-5.7.22-linux-glibc2.12-x86_64/ /soft/mysql\n\n\n1\n2\n3\n\n\n\n# 3.进行MySQL初始化\n\n//创建初始化目录以及数据库数据目录\n\n[root@sql ~]# mkdir /soft/mysql/{mysql-file,data}\n[root@sql ~]# chown mysql.mysql /soft/mysql/\n\n//初始化数据库\n[root@sql ~]# /soft/mysql/bin/mysqld --initialize \\\n--user=mysql --basedir=/soft/mysql \\\n--datadir=/soft/mysql/data\n-------\n\n//初始化数据库会告诉默认登陆账户与密码\n2018-04-28T02:30:33.954980Z 1 [Note] A temporary password is generated for'root@localhost: I,isfqnx.0tO'\n-------\n\n//使用ssl连接, 初始化后重新授权目录权限[如不需要可忽略]\n\n[root@sql ~]# /soft/mysql/bin/mysql_ssl_rsa_setup \\\n--datadir=/soft/mysql/data/\n[root@sql ~]# chown -R mysql.mysql /soft/mysql/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 4.建立MySQL配置文件\n\n//mysql安装目录及mysql数据库目录\n[root@sql ~]# cp /etc/my.cnf /etc/my.cnf_bak\n[root@sql ~]# vim /etc/my.cnf\n[mysqld]\nbasedir=/soft/mysql\ndatadir=/soft/mysql/data\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 5.启动MySQL数据库\n\n//方式1,使用 mysqld_safe \n[root@sql ~]# /soft/mysql/bin/mysqld_safe --user=mysql &\n\n//方式2, 使用(systemV)方式管理, [强烈推荐]\n[root@sql ~]# cp /soft/mysql/support-files/mysql.server /etc/init.d/mysqld\n[root@sql ~]# chkconfig --add mysqld\n[root@sql ~]# chkconfig mysqld on\n\n//修改安装目录与数据存放目录\n[root@sql ~]# sed -i '/^basedir=/cbasedir=\\/soft\\/mysql' /etc/init.d/mysqld \n[root@sql ~]# sed -i '/^datadir=/cdatadir=\\/soft\\/mysql\\/data' /etc/init.d/mysqld\n\n//启动数据库\n[root@sql ~]# /etc/init.d/mysqld start\nStarting MySQL. SUCCESS!\n\n//检查进程\n[root@sql ~]# ps aux|grep mysql\n\n//检查端口\n[root@sql ~]# lsof -i :3306\nCOMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\nmysqld  2659 mysql   16u  IPv6  28431      0t0  TCP *:mysql (LISTEN)\n[root@sql ~]# ss -lntup|grep 3306\ntcp    LISTEN     0      80 :::3306  :::*   users:((\"mysqld\",pid=2659,fd=16))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 6.连接数据库测试\n\n//默认情况没有mysql命令, 如果有可能使用过yum安装, 这样容易连错数据库(PATH路径存在命令执行优先级问题)\n[root@sql ~]# mysql\n-bash: mysql: command not found\n\n//可以选择添加路径至PATH中, 或者直接使用绝对路径执行\n[root@sql ~]# echo \"export PATH=$PATH:/soft/mysql/bin\" >> /etc/profile\n[root@sql ~]# source /etc/profile\n\n//登陆数据库\n[root@sql ~]# mysql -uroot -p\"I,isfqnx.0tO\"\n\n//默认系统配置数据库密码必须修改, 否则无法使用数据库\nmysql> show databases;\nERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.\n\n//修改系统默认密码\nmysql> alter user root@'localhost' identified by 'bgx';\nQuery OK, 0 rows affected (0.01 sec)\nmysql> exit;\n\n//退出后使用新密码重新登录数据库\n[root@sql ~]# mysql -uroot -p\"bgx\"\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n4 rows in set (0.02 sec)\nmysql> exit;\n注意: 如果需要重新初始化[可选]\n\n//如果重新初始化会导致数据全部丢失\n[root@sql ~]# yum install -y psmisc\n[root@sql ~]# killall mysqld\n[root@sql ~]# rm -rf /soft/mysql/data/*\n[root@sql ~]# /soft/mysql/bin/mysqld --initialize --user=mysql \\\n--basedir=/soft/mysql --datadir=/soft/mysql/data\n#可不执行\n[root@sql ~]# /soft/mysql/bin/mysql_ssl_rsa_setup --datadir=/soft/mysql/data\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# 3.源码安装数据库\n\n\n# 1.源码安装mysql需要依赖cmake、boost\n\n[root@sql ~]# yum install libaio-devel gcc gcc-c++ ncurses ncurses-devel cmake -y\n[root@sql ~]# useradd -M -s /sbin/nologin mysql\n[root@sql ~]# mkdir /soft/src/ -p\n\n\n1\n2\n3\n\n\n\n# 2.下载源码包并编译MySQL\n\n[root@sql ~]# wget http://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-boost-5.7.22.tar.gz\n[root@sql ~]# tar xf mysql-boost-5.7.22.tar.gz\n[root@sql ~]# cd mysql-5.7.22/\n[root@sql ~]# cmake -DCMAKE_INSTALL_PREFIX=/soft/mysql-5.7.22 \\\n-DMYSQL_UNIX_ADDR=/soft/mysql-5.7.22/data/mysql.sock \\\n-DMYSQL_DATADIR=/soft/mysql-5.7.22/data \\\n-DSYSCONFDIR=/soft/mysql-5.7.22/conf \\\n-DWITH_MYISAM_STORAGE_ENGINE=0 \\\n-DWITH_INNOBASE_STORAGE_ENGINE=1 \\\n-DWITH_MEMORY_STORAGE_ENGINE=0 \\\n-DWITH_READLINE=1 \\\n-DMYSQL_TCP_PORT=3306 \\\n-DENABLED_LOCAL_INFILE=1 \\\n-DWITH_PARTITION_STORAGE_ENGINE=1 \\\n-DEXTRA_CHARSETS=all \\\n-DDEFAULT_CHARSET=utf8 \\\n-DDEFAULT_COLLATION=utf8_general_ci \\\n-DWITH_BOOST=/soft/package/src/mysql-5.7.22/boost/boost_1_59_0\n[root@sql ~]# make\n[root@sql ~]# make install\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 3.完成后基本优化\n\n[root@sql ~]# ln -s /soft/mysql-5.7.22 /soft/mysql\n[root@sql ~]# mkdir /soft/mysql/data\n[root@sql ~]# chown -R mysql.mysql /soft/mysql\n\n\n1\n2\n3\n\n\n\n# 4.准备MySQL基础配置文件\n\n[root@sql ~]# vim /etc/my.cnf\n[mysqld]\nbasedir=/soft/mysql\ndatadir=/soft/mysql/data\n\n\n1\n2\n3\n4\n\n\n\n# 5.拷贝MySQL程序启动文件\n\n//拷贝官方准备的启动脚本\n[root@sql ~]# cp /soft/mysql/support-files/mysql.server /etc/init.d/mysqld\n\n//添加为系统服务, 并设定开机自启动\n[root@sql ~]# chkconfig --add mysqld && chkconfig mysqld on\n\n\n1\n2\n3\n4\n5\n\n\n\n# 6.初始化MySQL\n\n[root@sql ~]# /soft/mysql/bin/mysqld --initialize-insecure --user=mysql --basedir=/soft/mysql --datadir=/soft/mysql/data\n\n//启动MySQL\n[root@sql ~]# /etc/init.d/mysqld start\n\n//为mysql命令添加环境变量,以便后续简化执行命令\n[root@sql ~]# echo >> \"export PATH=/data/soft/mysql/bin:$PATH\" /etc/profile\n[root@sql ~]# source /etc/profile\n\n//源码编译MySQL默认root没有密码\n[root@sql ~]# mysql\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 4.变更数据库密码\n\n\n# 1.更改root密码\n\n//第一种方式, 需要知道密码\n[root@sql ~]# mysqladmin -uroot -pBgx123.com password 'Bgx111.com'\nWarning: Since password will be sent to server in plain text, use ssl connection to ensure password safety.\n[root@sql ~]# mysql -uroot -pBgx111.com\nmysql>\n\n//第二种方式, 登录MySQL, 修改相应表\nmysql> update mysql.user set authentication_string=password('Bjtest123.com') where user='root';\nmysql> flush privileges;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 2.忘记mysql root密码\n\n[root@sql ~]# vim /etc/my.cnf\n[mysqld]\nskip-grant-tables # 新增跳过授权表\n\n//重启数据库生效\n[root@sql ~]# systemctl restart mysqld\n\n//查看表字段\nmysql> select user,host,authentication_string from mysql.user;\n+---------------+-----------+-------------------------------------------+\n| user          | host      | authentication_string                     |\n+---------------+-----------+-------------------------------------------+\n| root          | localhost | *C786BB788F276CD53317C80C1957E5F5696751F0 |\n| mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE |\n| mysql.sys     | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE |\n+---------------+-----------+-------------------------------------------+\n3 rows in set (0.00 sec)\n\n//5.7.6版本后更新密码方式\nmysql> update mysql.user set authentication_string=password('Bgx123.com') where user='root';\nmysql> exit\n\n//5.7.6版本前更新密码方式\nmysql> update mysql.user set password=password('Bgx123.com') where user=\"root\" and host=\"localhost\";\n[root@sql ~]# vim /etc/my.cnf\n[mysqld]\n#skip-grant-tables #注释\n\n//重启数据库生效\n[root@sql ~]# systemctl restart mysqld\n\n//使用新密码登录数据库\n[root@sql ~]# mysql -uroot -pBgx123.com\nmysql> \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n原文链接http://www.sunrisenan.com/docs/mysql/mysql-1aofhklc0is34",normalizedContent:"# 数据库二进制安装\n\n\n# 1.yum安装数据库\n\nmysql yum仓库提供了用于在linux平台上安装mysql服务器，客户端和其他组件的rpm包。mysql-yum安装下载地址\n\n\n\n\n\n> 使用mysql yum仓库时，默认选择安装最新的mysql版本。如果需要使用低版本请按如下操作。\n\n\n# 1.安装mysql仓库源\n\n[root@sql ~]# rpm -ivh https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm\n\n\n1\n\n\n\n# 2.选择并启用适合当前平台的发行包\n\n//列出所有mysql发行版仓库\n[root@sql ~]# yum repolist all|grep mysql\n\n//禁用8.0发行版仓库, 启用5.7发行版仓库\n[root@sql ~]# yum install yum-utils\n[root@sql ~]# yum-config-manager --disable mysql80-community\n[root@sql ~]# yum-config-manager --enable mysql57-community\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n> 可以手动编辑/etc/yum.repos.d/mysql-community.repo 文件配置仓库\n\n[mysql57-community]\nname=mysql 5.7 community server\nbaseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/6/$basearch/\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/rpm-gpg-key-mysql\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3.通过以下命令安装mysql, 并启动mysql\n\n[root@sql ~]# yum install -y mysql-community-server\n[root@sql ~]# systemctl start mysqld\n[root@sql ~]# systemctl enable mysqld\n\n\n1\n2\n3\n\n\n> mysql服务器初始化（仅适用于mysql 5.7）在服务器初始启动时，如果服务器的数据目录为空，则会发生以下情况： • 服务器已初始化。 • 在数据目录中生成ssl证书和密钥文件。 • validate_password插件安装并启用。 • 超级用户帐户‘root‘@’localhost’已创建。\n\n超级用户的密码被设置并存储在错误日志文件中。要显示它，请使用以下命令:\n\n[root@vm-70-160 ~]# grep \"password\" /var/log/mysqld.log \n2018-04-28t07:11:51.589629z 1 [note] a temporary password is generated for root@localhost: jhlrhucap3+7\n通过使用生成的临时密码登录并尽快更改root密码并为超级用户帐户设置自定义密码\n[root@vm-70-160 ~]# mysql -uroot -pjhlrhucap3+7\nmysql> alter user 'root'@'localhost' identified by 'bgx123.com';\n\n\n1\n2\n3\n4\n5\n\n\n> mysql的validate_password插件默认安装。将要求密码至少包含大写、小写、数字、特殊字符、并且总密码长度至少为8个字符。\n\n\n# 2.通用安装数据库\n\n采用二进制免编译方式安装mysql, 不需要复杂的编译设置和编译时间等待,解压下载的软件包,初始化即可完成mysql的安装和启动。mysql二进制包下载地址\n\n\n\nmysql二进制包官方安装手册\n\n\n# 1.基础环境准备\n\n[root@sql ~]# systemctl stop firewalld\n[root@sql ~]# systemctl disable firewalld\n[root@sql ~]# setenforce 0\n\n//建立用户与相应目录\n[root@sql ~]# groupadd mysql\n[root@sql ~]# useradd -r -g mysql -s /sbin/nologin mysql\n[root@sql ~]# mkdir /soft/src -p && cd /soft/src\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 2.下载mysql并安装\n\n[root@sql src]# wget https://dev.mysql.com/get/downloads/mysql-5.7/mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz\n[root@sql src]# tar xf mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz -c /soft\n[root@sql src]# ln -s /soft/mysql-5.7.22-linux-glibc2.12-x86_64/ /soft/mysql\n\n\n1\n2\n3\n\n\n\n# 3.进行mysql初始化\n\n//创建初始化目录以及数据库数据目录\n\n[root@sql ~]# mkdir /soft/mysql/{mysql-file,data}\n[root@sql ~]# chown mysql.mysql /soft/mysql/\n\n//初始化数据库\n[root@sql ~]# /soft/mysql/bin/mysqld --initialize \\\n--user=mysql --basedir=/soft/mysql \\\n--datadir=/soft/mysql/data\n-------\n\n//初始化数据库会告诉默认登陆账户与密码\n2018-04-28t02:30:33.954980z 1 [note] a temporary password is generated for'root@localhost: i,isfqnx.0to'\n-------\n\n//使用ssl连接, 初始化后重新授权目录权限[如不需要可忽略]\n\n[root@sql ~]# /soft/mysql/bin/mysql_ssl_rsa_setup \\\n--datadir=/soft/mysql/data/\n[root@sql ~]# chown -r mysql.mysql /soft/mysql/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 4.建立mysql配置文件\n\n//mysql安装目录及mysql数据库目录\n[root@sql ~]# cp /etc/my.cnf /etc/my.cnf_bak\n[root@sql ~]# vim /etc/my.cnf\n[mysqld]\nbasedir=/soft/mysql\ndatadir=/soft/mysql/data\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 5.启动mysql数据库\n\n//方式1,使用 mysqld_safe \n[root@sql ~]# /soft/mysql/bin/mysqld_safe --user=mysql &\n\n//方式2, 使用(systemv)方式管理, [强烈推荐]\n[root@sql ~]# cp /soft/mysql/support-files/mysql.server /etc/init.d/mysqld\n[root@sql ~]# chkconfig --add mysqld\n[root@sql ~]# chkconfig mysqld on\n\n//修改安装目录与数据存放目录\n[root@sql ~]# sed -i '/^basedir=/cbasedir=\\/soft\\/mysql' /etc/init.d/mysqld \n[root@sql ~]# sed -i '/^datadir=/cdatadir=\\/soft\\/mysql\\/data' /etc/init.d/mysqld\n\n//启动数据库\n[root@sql ~]# /etc/init.d/mysqld start\nstarting mysql. success!\n\n//检查进程\n[root@sql ~]# ps aux|grep mysql\n\n//检查端口\n[root@sql ~]# lsof -i :3306\ncommand  pid  user   fd   type device size/off node name\nmysqld  2659 mysql   16u  ipv6  28431      0t0  tcp *:mysql (listen)\n[root@sql ~]# ss -lntup|grep 3306\ntcp    listen     0      80 :::3306  :::*   users:((\"mysqld\",pid=2659,fd=16))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 6.连接数据库测试\n\n//默认情况没有mysql命令, 如果有可能使用过yum安装, 这样容易连错数据库(path路径存在命令执行优先级问题)\n[root@sql ~]# mysql\n-bash: mysql: command not found\n\n//可以选择添加路径至path中, 或者直接使用绝对路径执行\n[root@sql ~]# echo \"export path=$path:/soft/mysql/bin\" >> /etc/profile\n[root@sql ~]# source /etc/profile\n\n//登陆数据库\n[root@sql ~]# mysql -uroot -p\"i,isfqnx.0to\"\n\n//默认系统配置数据库密码必须修改, 否则无法使用数据库\nmysql> show databases;\nerror 1820 (hy000): you must reset your password using alter user statement before executing this statement.\n\n//修改系统默认密码\nmysql> alter user root@'localhost' identified by 'bgx';\nquery ok, 0 rows affected (0.01 sec)\nmysql> exit;\n\n//退出后使用新密码重新登录数据库\n[root@sql ~]# mysql -uroot -p\"bgx\"\nmysql> show databases;\n+--------------------+\n| database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n4 rows in set (0.02 sec)\nmysql> exit;\n注意: 如果需要重新初始化[可选]\n\n//如果重新初始化会导致数据全部丢失\n[root@sql ~]# yum install -y psmisc\n[root@sql ~]# killall mysqld\n[root@sql ~]# rm -rf /soft/mysql/data/*\n[root@sql ~]# /soft/mysql/bin/mysqld --initialize --user=mysql \\\n--basedir=/soft/mysql --datadir=/soft/mysql/data\n#可不执行\n[root@sql ~]# /soft/mysql/bin/mysql_ssl_rsa_setup --datadir=/soft/mysql/data\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# 3.源码安装数据库\n\n\n# 1.源码安装mysql需要依赖cmake、boost\n\n[root@sql ~]# yum install libaio-devel gcc gcc-c++ ncurses ncurses-devel cmake -y\n[root@sql ~]# useradd -m -s /sbin/nologin mysql\n[root@sql ~]# mkdir /soft/src/ -p\n\n\n1\n2\n3\n\n\n\n# 2.下载源码包并编译mysql\n\n[root@sql ~]# wget http://dev.mysql.com/get/downloads/mysql-5.7/mysql-boost-5.7.22.tar.gz\n[root@sql ~]# tar xf mysql-boost-5.7.22.tar.gz\n[root@sql ~]# cd mysql-5.7.22/\n[root@sql ~]# cmake -dcmake_install_prefix=/soft/mysql-5.7.22 \\\n-dmysql_unix_addr=/soft/mysql-5.7.22/data/mysql.sock \\\n-dmysql_datadir=/soft/mysql-5.7.22/data \\\n-dsysconfdir=/soft/mysql-5.7.22/conf \\\n-dwith_myisam_storage_engine=0 \\\n-dwith_innobase_storage_engine=1 \\\n-dwith_memory_storage_engine=0 \\\n-dwith_readline=1 \\\n-dmysql_tcp_port=3306 \\\n-denabled_local_infile=1 \\\n-dwith_partition_storage_engine=1 \\\n-dextra_charsets=all \\\n-ddefault_charset=utf8 \\\n-ddefault_collation=utf8_general_ci \\\n-dwith_boost=/soft/package/src/mysql-5.7.22/boost/boost_1_59_0\n[root@sql ~]# make\n[root@sql ~]# make install\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 3.完成后基本优化\n\n[root@sql ~]# ln -s /soft/mysql-5.7.22 /soft/mysql\n[root@sql ~]# mkdir /soft/mysql/data\n[root@sql ~]# chown -r mysql.mysql /soft/mysql\n\n\n1\n2\n3\n\n\n\n# 4.准备mysql基础配置文件\n\n[root@sql ~]# vim /etc/my.cnf\n[mysqld]\nbasedir=/soft/mysql\ndatadir=/soft/mysql/data\n\n\n1\n2\n3\n4\n\n\n\n# 5.拷贝mysql程序启动文件\n\n//拷贝官方准备的启动脚本\n[root@sql ~]# cp /soft/mysql/support-files/mysql.server /etc/init.d/mysqld\n\n//添加为系统服务, 并设定开机自启动\n[root@sql ~]# chkconfig --add mysqld && chkconfig mysqld on\n\n\n1\n2\n3\n4\n5\n\n\n\n# 6.初始化mysql\n\n[root@sql ~]# /soft/mysql/bin/mysqld --initialize-insecure --user=mysql --basedir=/soft/mysql --datadir=/soft/mysql/data\n\n//启动mysql\n[root@sql ~]# /etc/init.d/mysqld start\n\n//为mysql命令添加环境变量,以便后续简化执行命令\n[root@sql ~]# echo >> \"export path=/data/soft/mysql/bin:$path\" /etc/profile\n[root@sql ~]# source /etc/profile\n\n//源码编译mysql默认root没有密码\n[root@sql ~]# mysql\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 4.变更数据库密码\n\n\n# 1.更改root密码\n\n//第一种方式, 需要知道密码\n[root@sql ~]# mysqladmin -uroot -pbgx123.com password 'bgx111.com'\nwarning: since password will be sent to server in plain text, use ssl connection to ensure password safety.\n[root@sql ~]# mysql -uroot -pbgx111.com\nmysql>\n\n//第二种方式, 登录mysql, 修改相应表\nmysql> update mysql.user set authentication_string=password('bjtest123.com') where user='root';\nmysql> flush privileges;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 2.忘记mysql root密码\n\n[root@sql ~]# vim /etc/my.cnf\n[mysqld]\nskip-grant-tables # 新增跳过授权表\n\n//重启数据库生效\n[root@sql ~]# systemctl restart mysqld\n\n//查看表字段\nmysql> select user,host,authentication_string from mysql.user;\n+---------------+-----------+-------------------------------------------+\n| user          | host      | authentication_string                     |\n+---------------+-----------+-------------------------------------------+\n| root          | localhost | *c786bb788f276cd53317c80c1957e5f5696751f0 |\n| mysql.session | localhost | *thisisnotavalidpasswordthatcanbeusedhere |\n| mysql.sys     | localhost | *thisisnotavalidpasswordthatcanbeusedhere |\n+---------------+-----------+-------------------------------------------+\n3 rows in set (0.00 sec)\n\n//5.7.6版本后更新密码方式\nmysql> update mysql.user set authentication_string=password('bgx123.com') where user='root';\nmysql> exit\n\n//5.7.6版本前更新密码方式\nmysql> update mysql.user set password=password('bgx123.com') where user=\"root\" and host=\"localhost\";\n[root@sql ~]# vim /etc/my.cnf\n[mysqld]\n#skip-grant-tables #注释\n\n//重启数据库生效\n[root@sql ~]# systemctl restart mysqld\n\n//使用新密码登录数据库\n[root@sql ~]# mysql -uroot -pbgx123.com\nmysql> \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n原文链接http://www.sunrisenan.com/docs/mysql/mysql-1aofhklc0is34",charsets:{cjk:!0}},{title:"oracle数据库安装",frontmatter:{title:"oracle数据库安装",date:"2023-03-01T15:15:52.000Z",permalink:"/pages/e977f3/",categories:["运维","数据库","oracle"],tags:[null],readingShow:"top",description:"1、系统及配置要求",meta:[{name:"image",content:"http://pic.zzppjj.top/LightPicture/2023/03/fccca0cae5d5919e.png"},{name:"twitter:title",content:"oracle数据库安装"},{name:"twitter:description",content:"1、系统及配置要求"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://pic.zzppjj.top/LightPicture/2023/03/fccca0cae5d5919e.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/03.oracle/01.oracle%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%89%E8%A3%85.html"},{property:"og:type",content:"article"},{property:"og:title",content:"oracle数据库安装"},{property:"og:description",content:"1、系统及配置要求"},{property:"og:image",content:"http://pic.zzppjj.top/LightPicture/2023/03/fccca0cae5d5919e.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/03.oracle/01.oracle%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%89%E8%A3%85.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-03-01T15:15:52.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"oracle数据库安装"},{itemprop:"description",content:"1、系统及配置要求"},{itemprop:"image",content:"http://pic.zzppjj.top/LightPicture/2023/03/fccca0cae5d5919e.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/03.oracle/01.oracle%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%89%E8%A3%85.html",relativePath:"04.运维/08.数据库/03.oracle/01.oracle数据库安装.md",key:"v-b2ae320e",path:"/pages/e977f3/",headers:[{level:3,title:"一、安装Oracle 12c",slug:"一、安装oracle-12c",normalizedTitle:"一、安装oracle 12c",charIndex:2},{level:4,title:"1、安装前的注意事项",slug:"_1、安装前的注意事项",normalizedTitle:"1、安装前的注意事项",charIndex:20},{level:4,title:"2、Oracle 12c安装过程",slug:"_2、oracle-12c安装过程",normalizedTitle:"2、oracle 12c安装过程",charIndex:2688},{level:3,title:"二、Oracle 12c数据库的创建",slug:"二、oracle-12c数据库的创建",normalizedTitle:"二、oracle 12c数据库的创建",charIndex:4931},{level:3,title:"三、Oracle 12c数据库的启动与关闭",slug:"三、oracle-12c数据库的启动与关闭",normalizedTitle:"三、oracle 12c数据库的启动与关闭",charIndex:5012},{level:4,title:"1、启动、 关闭 Oracle 数据库",slug:"_1、启动、-关闭-oracle-数据库",normalizedTitle:"1、启动、 关闭 oracle 数据库",charIndex:5037},{level:4,title:"2、管理 Oracle 监听进程",slug:"_2、管理-oracle-监听进程",normalizedTitle:"2、管理 oracle 监听进程",charIndex:5645},{level:3,title:"四、附件",slug:"四、附件",normalizedTitle:"四、附件",charIndex:7189}],headersStr:"一、安装Oracle 12c 1、安装前的注意事项 2、Oracle 12c安装过程 二、Oracle 12c数据库的创建 三、Oracle 12c数据库的启动与关闭 1、启动、 关闭 Oracle 数据库 2、管理 Oracle 监听进程 四、附件",content:'# 一、安装Oracle 12c\n\n# 1、安装前的注意事项\n\n1、系统及配置要求\n\n> 安装有 GNOME 中文桌面环境 防火墙的配置选项设置为禁用 。 SELinux设置为禁用 。 默认安装设置为软件开发 。 内核选择3.10.0.54.0.1.e17.x86_64及以上版本 物理内存必须高于1GB，对于VMware 虚拟机建议不少于2GB . 交换空间物理内存为1-2GB时，交换分区为物理内存的1.5-2倍；物理内存为2-16GB时，交换分区与物理内存大小相同；物理内存超过16GB时，交换分区使用16GB就可以了。\n\n2、修改主机名和IP\n\n# vim /etc/hostname        //修改主机名\nOracle\n# vim /etc/hosts        //添加主机IP映射\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.1.10 Oracle\n\n\n1\n2\n3\n4\n5\n6\n\n\n3、软件环境要求\n\n# yum -y install binutils compat-libcap1 compat-libstdc++-33 gcc gcc-c++ glibc glibc-devel ksh libaio libaio-devel libgcc libgcc libstdc++ libstdc++-devel libXi libXtst make sysstat unixODBC unixODBC-devel\n\n\n1\n\n\n4、内核要求\n\n# vim /etc/sysctl.conf\n......\nfs.aio-max-nr = 1048576\nfs.file-max  = 6815744\nkernel.shmall =  2097152\nkernel.shmmax =  4294967295\nkernel.shmmni =  4096\nkernel.sem  =   250 32000 100 128\nnet.ipv4.ip_local_port_range  =  9000 65500\nnet.core.rmem_max  = 4194304\nnet.core.rmem_default = 262144\nnet.core.wmem_default = 262144\nnet.core.wmem_max = 1048576\n# sysctl -p\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n5、Oracle用户环境要求 创建固定的运行用户oracle，安装组oinstall，管理组dba\n\n# groupadd -g 54321 oinstall        //创建安装组（-g 组ID）\n# groupadd -g 54322 dba        //创建管理组\n# useradd -u 54321 -g oinstall -G dba oracle        //创建运行用户\n# passwd oracle        //设置密码\n# mkdir -p /u01/app/oracle        //建立基本目录\n# chown -R oracle:oinstall /u01/app/\n# chmod -R 755 /u01/app/oracle/\n# vim /home/oracle/.bash_profile        //调整oracle用户的环境配置\n......\numask 022\nORACLE_BASE=/u01/app/oracle        //定义基本目录\nORACLE_HOME=/u01/app/oracle/product/12.2.0/dbhome_1/        //定义安装家目录\nORACLE_SID=orcl        // 定义数据库实例名称\nNLS_LANG="SIMPLIFIED CHINESE_CHINA".UTF8        // 确定使用何种语言环境\nPATH=$PATH:$ORACLE_HOME/bin\nLANG=zh_CN.UTF-8\nexport  DISPLAY=:0.0\nexport PATH LANG NLS_LANG ORACLE_BASE ORACLE_HOME ORACLE_SID\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n6、oracle用户资源限制要求\n\n# vim /etc/pam.d/login\n......\nsession required        /lib/security/pam_limits.so\nsession required        pam_limits.so\n# vim /etc/security/limits.conf\noracle  soft    nproc   2047        //进程数软限制\noracle  hard    nproc   16384        //进程数硬限制\noracle  soft    nofile  1024        //文件数软限制\noracle  hard    nofile  65536        //文件数硬限制\noracle  soft    stack   10240        //Oracle软堆栈限制\n# vim /etc/profile\n......\nif      [ $USER = "oracle" ]; then\n        if [ $SHELL = "/bin/ksh" ]; then\n                ulimit -p 16384\n                ulimit -n 65536\n        else\n                ulimit -u 16384 -n 65536\n        fi\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n7、重新启动系统\n\n# 2、Oracle 12c安装过程\n\nOracle的中文官方网址为Oracle CRM On Demand current release，它支持Oracle12c安装文件的免费下载（需注册账号)。这里我们使用光盘安装。\n\n# mkdir /tmp/abc\n# cd /media/\n# ls\nlinuxx64_12201_database.zip\n# cp linuxx64_12201_database.zip /tmp/abc/\n# cd /tmp/abc/\n# unzip linuxx64_12201_database.zip \n# xhost +        //一 定要以root用户在图形环境中操作\naccess control disabled, clients can connect from any host\n# su - oracle \n$ cd /tmp/abc/database/\n$ export DISPLAY=:0.0        //设置DISPLAY环境变量\n$ ./runInstaller \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n1、典型安装过程 如果已购买Oracle 12c数据库的授权且服务器能够连接Internet，建议正确填写电子邮件地址和Oracle Support口令以便及时接收官方的安全更新通知。 配置安全更新\n\n\n\n安装选项\n\n\n\n系统类\n\n\n\n数据库安装选项\n\n\n\n安装类型\n\n\n\n典型安装配置\n\n\n\n创建产品清单\n\n\n\n先决条件检查及汇总\n\n\n\n安装产品\n\n\n\n安装完成\n\n\n\n执行配置脚本时切换为root用户并依次执行脚本文件\n\n# /u01/app/oraInventory/orainstRoot.sh \n更改权限/u01/app/oraInventory.\n添加组的读取和写入权限。\n删除全局的读取, 写入和执行权限。\n\n更改组名/u01/app/oraInventory 到 oinstall.\n脚本的执行已完成。\n# /u01/app/oracle/product/12.2.0/dbhome_1/root.sh         //按照默认值按两次Enter键\nPerforming root user operation.\n\nThe following environment variables are set as:\n    ORACLE_OWNER= oracle\n    ORACLE_HOME=  /u01/app/oracle/product/12.2.0/dbhome_1\n\nEnter the full pathname of the local bin directory: [/usr/local/bin]: \n   Copying dbhome to /usr/local/bin ...\n   Copying oraenv to /usr/local/bin ...\n   Copying coraenv to /usr/local/bin ...\n\n\nCreating /etc/oratab file...\nEntries will be added to the /etc/oratab file as needed by\nDatabase Configuration Assistant when a database is created\nFinished running generic part of root script.\nNow product-specific root actions will be performed.\nDo you want to setup Oracle Trace File Analyzer (TFA) now ? yes|[no] : \n\nOracle Trace File Analyzer (TFA - User Mode) is available at :\n    /u01/app/oracle/product/12.2.0/dbhome_1/suptools/tfa/release/tfa_home/bin/tfactl\n\nOR\n\nOracle Trace File Analyzer (TFA - Daemon Mode) can be installed by running this script :\n    /u01/app/oracle/product/12.2.0/dbhome_1/suptools/tfa/release/tfa_home/install/roottfa.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n2、安装flash player\n\nwget http://linuxdownload.adobe.com/adobe-release/adobe-release-x86_64-1.0-1.noarch.rpm\nrpm -ivh adobe-release-x86_64-1.0-1.noarch.rpm\nyum -y install flash-plugin\n\n\n1\n2\n3\n\n\n\n# 二、Oracle 12c数据库的创建\n\n1、以oracle用户身份运行命令DBCA\n\n$ dbca\n\n\n1\n\n\n选择数据库操作\n\n\n\n填写数据库信息\n\n\n\n\n# 三、Oracle 12c数据库的启动与关闭\n\n# 1、启动、 关闭 Oracle 数据库\n\n1、数据库的启动\n\n$ sqlplus / as sysdba         //以sysdba用户登录\nSQL> startup nomount         //仅启动一个Oracle实例\nSQL> startup mount         //启动实例并且装载数据库，但没有打开数据库\nSQL> startup         //完成启动实例、装载数据库和打开数据库\n\n\n1\n2\n3\n4\n\n\n必须运行下面的两条命令 ， 数据库才能正确启动\n\nALTER DATABASE MOUNT;         //startup nomount启动时\nALTER DATABASE OPEN;         //startup nomount启动时/startup mount启动时\n\n\n1\n2\n\n\n2、数据库的关闭\n\nSQL> shutdown normal         //同shutdown，目前连接的所有用户都从数据库中退出后才开始关闭数据库\nSQL> shutdown immediate         //安全且相对较快\nSQL> shutdown transactional         //计划关闭数据库\nSQL> shutdown abort         //会造成数据丢失，并且恢复数据库也需要较长时间\n\n\n1\n2\n3\n4\n\n\n# 2、管理 Oracle 监听进程\n\n先启动监听， 后启动数据库 1、启动监听\n\n$ lsnrctl start\n\n\n1\n\n\n2、关闭监听\n\n$ lsnrctl stop\n\n\n1\n\n\n3、其他命令\n\n$ lsnrctl status         //查看监听进程状态\n$ lsnrctl reload         //重新加载监听进程\n$ lsnrctl set         //设置相应参数\n$ lsnrctl show         //查看当前参数的取值\n$ lsnrctl help         //显示帮助信息\n$ lsnrctl version         //显示当前监听进程版本\n$ lsnrctl change_password         //改变口令\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n4、增加新的监听进程\n\n$ vim $ORACLE_HOME/network/admin/listener.ora\nLISTENER =\n  (DESCRIPTION_LIST =\n    (DESCRIPTION =\n      (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.1.1)(PORT = 1521))\n      (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521))\n    )\n  )\nLISTENER1 =\n  (DESCRIPTION_LIST =\n    (DESCRIPTION  =\n      (ADDRESS = (PROTOCOL = TCP) (HOST = Oracle) (PORT = 1522))         //主机为Oracle，端口为1522\n    )\n  )\n#静态注册模块\nSID_LIST_LISTENER =         //SID_LIST_监听名\n  (SID_LIST =         \n    (SID_DESC =         \n      (SID NAME = orcl)         //实例名\n      (ORACLE_HOME = /uO1/app/oracle/product/12.2.0/dbhome_1)         //标识服务主目录位置\n      (GLOBAL_DBNAME = orcl)         //全局数据库名称\n    )\n  )\n$ netstat -lnupt | grep 1522\n(Not all processes could be identified, non-owned process info\n will not be shown, you would have to be root to see it all.)\ntcp6       0      0 :::1522                 :::*                    LISTEN      3725/tnslsnr  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n查询GLOBAL_DBNAME代码\n\nSQL> select global_name from global_name;\n\nGLOBAL_NAME\n--------------------------------------------------------------------------------\nORCL\n\n\n1\n2\n3\n4\n5\n\n\n\n# 四、附件\n\n另：关于sqlplus方向键不能用的解决方法\n\n# yum -y install ncurses* readline*\n# wget https://fossies.org/linux/privat/rlwrap-0.45.2.tar.gz\n# tar zxf rlwrap-0.45.2.tar.gz -C /usr/src/\n# cd /usr/src/rlwrap-0.45.2/\n# ./configure && make && make install\n# vim /home/oracle/.bash_profile\n......\nalias sqlplus=\'rlwrap sqlplus\'\nalias rman=\'rlwrap rman\' \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n----------------------------------------\n\n原文链接：数据库部署 | Gaoyufu \'s blog',normalizedContent:'# 一、安装oracle 12c\n\n# 1、安装前的注意事项\n\n1、系统及配置要求\n\n> 安装有 gnome 中文桌面环境 防火墙的配置选项设置为禁用 。 selinux设置为禁用 。 默认安装设置为软件开发 。 内核选择3.10.0.54.0.1.e17.x86_64及以上版本 物理内存必须高于1gb，对于vmware 虚拟机建议不少于2gb . 交换空间物理内存为1-2gb时，交换分区为物理内存的1.5-2倍；物理内存为2-16gb时，交换分区与物理内存大小相同；物理内存超过16gb时，交换分区使用16gb就可以了。\n\n2、修改主机名和ip\n\n# vim /etc/hostname        //修改主机名\noracle\n# vim /etc/hosts        //添加主机ip映射\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.1.10 oracle\n\n\n1\n2\n3\n4\n5\n6\n\n\n3、软件环境要求\n\n# yum -y install binutils compat-libcap1 compat-libstdc++-33 gcc gcc-c++ glibc glibc-devel ksh libaio libaio-devel libgcc libgcc libstdc++ libstdc++-devel libxi libxtst make sysstat unixodbc unixodbc-devel\n\n\n1\n\n\n4、内核要求\n\n# vim /etc/sysctl.conf\n......\nfs.aio-max-nr = 1048576\nfs.file-max  = 6815744\nkernel.shmall =  2097152\nkernel.shmmax =  4294967295\nkernel.shmmni =  4096\nkernel.sem  =   250 32000 100 128\nnet.ipv4.ip_local_port_range  =  9000 65500\nnet.core.rmem_max  = 4194304\nnet.core.rmem_default = 262144\nnet.core.wmem_default = 262144\nnet.core.wmem_max = 1048576\n# sysctl -p\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n5、oracle用户环境要求 创建固定的运行用户oracle，安装组oinstall，管理组dba\n\n# groupadd -g 54321 oinstall        //创建安装组（-g 组id）\n# groupadd -g 54322 dba        //创建管理组\n# useradd -u 54321 -g oinstall -g dba oracle        //创建运行用户\n# passwd oracle        //设置密码\n# mkdir -p /u01/app/oracle        //建立基本目录\n# chown -r oracle:oinstall /u01/app/\n# chmod -r 755 /u01/app/oracle/\n# vim /home/oracle/.bash_profile        //调整oracle用户的环境配置\n......\numask 022\noracle_base=/u01/app/oracle        //定义基本目录\noracle_home=/u01/app/oracle/product/12.2.0/dbhome_1/        //定义安装家目录\noracle_sid=orcl        // 定义数据库实例名称\nnls_lang="simplified chinese_china".utf8        // 确定使用何种语言环境\npath=$path:$oracle_home/bin\nlang=zh_cn.utf-8\nexport  display=:0.0\nexport path lang nls_lang oracle_base oracle_home oracle_sid\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n6、oracle用户资源限制要求\n\n# vim /etc/pam.d/login\n......\nsession required        /lib/security/pam_limits.so\nsession required        pam_limits.so\n# vim /etc/security/limits.conf\noracle  soft    nproc   2047        //进程数软限制\noracle  hard    nproc   16384        //进程数硬限制\noracle  soft    nofile  1024        //文件数软限制\noracle  hard    nofile  65536        //文件数硬限制\noracle  soft    stack   10240        //oracle软堆栈限制\n# vim /etc/profile\n......\nif      [ $user = "oracle" ]; then\n        if [ $shell = "/bin/ksh" ]; then\n                ulimit -p 16384\n                ulimit -n 65536\n        else\n                ulimit -u 16384 -n 65536\n        fi\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n7、重新启动系统\n\n# 2、oracle 12c安装过程\n\noracle的中文官方网址为oracle crm on demand current release，它支持oracle12c安装文件的免费下载（需注册账号)。这里我们使用光盘安装。\n\n# mkdir /tmp/abc\n# cd /media/\n# ls\nlinuxx64_12201_database.zip\n# cp linuxx64_12201_database.zip /tmp/abc/\n# cd /tmp/abc/\n# unzip linuxx64_12201_database.zip \n# xhost +        //一 定要以root用户在图形环境中操作\naccess control disabled, clients can connect from any host\n# su - oracle \n$ cd /tmp/abc/database/\n$ export display=:0.0        //设置display环境变量\n$ ./runinstaller \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n1、典型安装过程 如果已购买oracle 12c数据库的授权且服务器能够连接internet，建议正确填写电子邮件地址和oracle support口令以便及时接收官方的安全更新通知。 配置安全更新\n\n\n\n安装选项\n\n\n\n系统类\n\n\n\n数据库安装选项\n\n\n\n安装类型\n\n\n\n典型安装配置\n\n\n\n创建产品清单\n\n\n\n先决条件检查及汇总\n\n\n\n安装产品\n\n\n\n安装完成\n\n\n\n执行配置脚本时切换为root用户并依次执行脚本文件\n\n# /u01/app/orainventory/orainstroot.sh \n更改权限/u01/app/orainventory.\n添加组的读取和写入权限。\n删除全局的读取, 写入和执行权限。\n\n更改组名/u01/app/orainventory 到 oinstall.\n脚本的执行已完成。\n# /u01/app/oracle/product/12.2.0/dbhome_1/root.sh         //按照默认值按两次enter键\nperforming root user operation.\n\nthe following environment variables are set as:\n    oracle_owner= oracle\n    oracle_home=  /u01/app/oracle/product/12.2.0/dbhome_1\n\nenter the full pathname of the local bin directory: [/usr/local/bin]: \n   copying dbhome to /usr/local/bin ...\n   copying oraenv to /usr/local/bin ...\n   copying coraenv to /usr/local/bin ...\n\n\ncreating /etc/oratab file...\nentries will be added to the /etc/oratab file as needed by\ndatabase configuration assistant when a database is created\nfinished running generic part of root script.\nnow product-specific root actions will be performed.\ndo you want to setup oracle trace file analyzer (tfa) now ? yes|[no] : \n\noracle trace file analyzer (tfa - user mode) is available at :\n    /u01/app/oracle/product/12.2.0/dbhome_1/suptools/tfa/release/tfa_home/bin/tfactl\n\nor\n\noracle trace file analyzer (tfa - daemon mode) can be installed by running this script :\n    /u01/app/oracle/product/12.2.0/dbhome_1/suptools/tfa/release/tfa_home/install/roottfa.sh\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n2、安装flash player\n\nwget http://linuxdownload.adobe.com/adobe-release/adobe-release-x86_64-1.0-1.noarch.rpm\nrpm -ivh adobe-release-x86_64-1.0-1.noarch.rpm\nyum -y install flash-plugin\n\n\n1\n2\n3\n\n\n\n# 二、oracle 12c数据库的创建\n\n1、以oracle用户身份运行命令dbca\n\n$ dbca\n\n\n1\n\n\n选择数据库操作\n\n\n\n填写数据库信息\n\n\n\n\n# 三、oracle 12c数据库的启动与关闭\n\n# 1、启动、 关闭 oracle 数据库\n\n1、数据库的启动\n\n$ sqlplus / as sysdba         //以sysdba用户登录\nsql> startup nomount         //仅启动一个oracle实例\nsql> startup mount         //启动实例并且装载数据库，但没有打开数据库\nsql> startup         //完成启动实例、装载数据库和打开数据库\n\n\n1\n2\n3\n4\n\n\n必须运行下面的两条命令 ， 数据库才能正确启动\n\nalter database mount;         //startup nomount启动时\nalter database open;         //startup nomount启动时/startup mount启动时\n\n\n1\n2\n\n\n2、数据库的关闭\n\nsql> shutdown normal         //同shutdown，目前连接的所有用户都从数据库中退出后才开始关闭数据库\nsql> shutdown immediate         //安全且相对较快\nsql> shutdown transactional         //计划关闭数据库\nsql> shutdown abort         //会造成数据丢失，并且恢复数据库也需要较长时间\n\n\n1\n2\n3\n4\n\n\n# 2、管理 oracle 监听进程\n\n先启动监听， 后启动数据库 1、启动监听\n\n$ lsnrctl start\n\n\n1\n\n\n2、关闭监听\n\n$ lsnrctl stop\n\n\n1\n\n\n3、其他命令\n\n$ lsnrctl status         //查看监听进程状态\n$ lsnrctl reload         //重新加载监听进程\n$ lsnrctl set         //设置相应参数\n$ lsnrctl show         //查看当前参数的取值\n$ lsnrctl help         //显示帮助信息\n$ lsnrctl version         //显示当前监听进程版本\n$ lsnrctl change_password         //改变口令\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n4、增加新的监听进程\n\n$ vim $oracle_home/network/admin/listener.ora\nlistener =\n  (description_list =\n    (description =\n      (address = (protocol = tcp)(host = 192.168.1.1)(port = 1521))\n      (address = (protocol = ipc)(key = extproc1521))\n    )\n  )\nlistener1 =\n  (description_list =\n    (description  =\n      (address = (protocol = tcp) (host = oracle) (port = 1522))         //主机为oracle，端口为1522\n    )\n  )\n#静态注册模块\nsid_list_listener =         //sid_list_监听名\n  (sid_list =         \n    (sid_desc =         \n      (sid name = orcl)         //实例名\n      (oracle_home = /uo1/app/oracle/product/12.2.0/dbhome_1)         //标识服务主目录位置\n      (global_dbname = orcl)         //全局数据库名称\n    )\n  )\n$ netstat -lnupt | grep 1522\n(not all processes could be identified, non-owned process info\n will not be shown, you would have to be root to see it all.)\ntcp6       0      0 :::1522                 :::*                    listen      3725/tnslsnr  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n查询global_dbname代码\n\nsql> select global_name from global_name;\n\nglobal_name\n--------------------------------------------------------------------------------\norcl\n\n\n1\n2\n3\n4\n5\n\n\n\n# 四、附件\n\n另：关于sqlplus方向键不能用的解决方法\n\n# yum -y install ncurses* readline*\n# wget https://fossies.org/linux/privat/rlwrap-0.45.2.tar.gz\n# tar zxf rlwrap-0.45.2.tar.gz -c /usr/src/\n# cd /usr/src/rlwrap-0.45.2/\n# ./configure && make && make install\n# vim /home/oracle/.bash_profile\n......\nalias sqlplus=\'rlwrap sqlplus\'\nalias rman=\'rlwrap rman\' \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n----------------------------------------\n\n原文链接：数据库部署 | gaoyufu \'s blog',charsets:{cjk:!0}},{title:"mongodb相关概念",frontmatter:{title:"mongodb相关概念",categories:"mongodb",tags:["mongodb"],date:"2022-12-09T20:50:33.000Z",permalink:"/pages/598be1/",readingShow:"top",description:"传统的关系型数据库（如MySQL），在数据操作的“三高”需求以及应对Web2.0的网站需求面前，显得力不从心。",meta:[{name:"image",content:"https://img-blog.csdnimg.cn/20200912222258123.png"},{name:"twitter:title",content:"mongodb相关概念"},{name:"twitter:description",content:"传统的关系型数据库（如MySQL），在数据操作的“三高”需求以及应对Web2.0的网站需求面前，显得力不从心。"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://img-blog.csdnimg.cn/20200912222258123.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/02.mongodb/01.mongodb%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5.html"},{property:"og:type",content:"article"},{property:"og:title",content:"mongodb相关概念"},{property:"og:description",content:"传统的关系型数据库（如MySQL），在数据操作的“三高”需求以及应对Web2.0的网站需求面前，显得力不从心。"},{property:"og:image",content:"https://img-blog.csdnimg.cn/20200912222258123.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/02.mongodb/01.mongodb%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:50:33.000Z"},{property:"article:tag",content:"mongodb"},{itemprop:"name",content:"mongodb相关概念"},{itemprop:"description",content:"传统的关系型数据库（如MySQL），在数据操作的“三高”需求以及应对Web2.0的网站需求面前，显得力不从心。"},{itemprop:"image",content:"https://img-blog.csdnimg.cn/20200912222258123.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/02.mongodb/01.mongodb%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5.html",relativePath:"04.运维/08.数据库/02.mongodb/01.mongodb相关概念.md",key:"v-1ebf1e36",path:"/pages/598be1/",headers:[{level:2,title:"1.1 业务应用场景",slug:"_1-1-业务应用场景",normalizedTitle:"1.1 业务应用场景",charIndex:22},{level:2,title:"1.2 MongoDB 简介",slug:"_1-2-mongodb-简介",normalizedTitle:"1.2 mongodb 简介",charIndex:978},{level:2,title:"1.3 体系结构",slug:"_1-3-体系结构",normalizedTitle:"1.3 体系结构",charIndex:1280},{level:2,title:"1.4 数据模型",slug:"_1-4-数据模型",normalizedTitle:"1.4 数据模型",charIndex:1672},{level:2,title:"1.5 MongoDB 的特点",slug:"_1-5-mongodb-的特点",normalizedTitle:"1.5 mongodb 的特点",charIndex:3437},{level:2,title:"2.1 Windows 系统中的安装启动",slug:"_2-1-windows-系统中的安装启动",normalizedTitle:"2.1 windows 系统中的安装启动",charIndex:4075},{level:3,title:"2.1.1 下载安装包",slug:"_2-1-1-下载安装包",normalizedTitle:"2.1.1 下载安装包",charIndex:4100},{level:3,title:"2.1.2 解压安装启动",slug:"_2-1-2-解压安装启动",normalizedTitle:"2.1.2 解压安装启动",charIndex:4452},{level:2,title:"2.2 Shell 连接(mongo命令)",slug:"_2-2-shell-连接-mongo命令",normalizedTitle:"2.2 shell 连接(mongo命令)",charIndex:6015},{level:3,title:"2.2.1 查看已经有的数据库",slug:"_2-2-1-查看已经有的数据库",normalizedTitle:"2.2.1 查看已经有的数据库",charIndex:6120},{level:3,title:"2.2.2 退出 mongodb",slug:"_2-2-2-退出-mongodb",normalizedTitle:"2.2.2 退出 mongodb",charIndex:6161},{level:2,title:"2.3 Compass- 图形化界面客户端",slug:"_2-3-compass-图形化界面客户端",normalizedTitle:"2.3 compass- 图形化界面客户端",charIndex:6288},{level:2,title:"2.4 Linux 系统中的安装启动和连接",slug:"_2-4-linux-系统中的安装启动和连接",normalizedTitle:"2.4 linux 系统中的安装启动和连接",charIndex:6515},{level:2,title:"3.1 案例需求",slug:"_3-1-案例需求",normalizedTitle:"3.1 案例需求",charIndex:9319},{level:2,title:"3.2 数据库操作",slug:"_3-2-数据库操作",normalizedTitle:"3.2 数据库操作",charIndex:9969},{level:3,title:"3.2.1 选择和创建数据库",slug:"_3-2-1-选择和创建数据库",normalizedTitle:"3.2.1 选择和创建数据库",charIndex:9983},{level:3,title:"3.2.2 数据库的删除",slug:"_3-2-2-数据库的删除",normalizedTitle:"3.2.2 数据库的删除",charIndex:10639},{level:2,title:"3.3 集合操作",slug:"_3-3-集合操作",normalizedTitle:"3.3 集合操作",charIndex:10723},{level:3,title:"3.3.1 集合的显式创建",slug:"_3-3-1-集合的显式创建",normalizedTitle:"3.3.1 集合的显式创建",charIndex:10772},{level:3,title:"3.3.2 集合的隐式创建",slug:"_3-3-2-集合的隐式创建",normalizedTitle:"3.3.2 集合的隐式创建",charIndex:11187},{level:3,title:"3.3.3 集合的删除",slug:"_3-3-3-集合的删除",normalizedTitle:"3.3.3 集合的删除",charIndex:11275},{level:2,title:"3.4 文档基本CRUD",slug:"_3-4-文档基本crud",normalizedTitle:"3.4 文档基本crud",charIndex:11449},{level:3,title:"3.4.1 文档的插入",slug:"_3-4-1-文档的插入",normalizedTitle:"3.4.1 文档的插入",charIndex:11521},{level:4,title:"3.4.1.1 单个文档插入",slug:"_3-4-1-1-单个文档插入",normalizedTitle:"3.4.1.1 单个文档插入",charIndex:11536},{level:4,title:"3.4.1.2 批量插入",slug:"_3-4-1-2-批量插入",normalizedTitle:"3.4.1.2 批量插入",charIndex:13147},{level:3,title:"3.4.2 文档的基本查询",slug:"_3-4-2-文档的基本查询",normalizedTitle:"3.4.2 文档的基本查询",charIndex:15612},{level:4,title:"3.4.2.1 查询所有",slug:"_3-4-2-1-查询所有",normalizedTitle:"3.4.2.1 查询所有",charIndex:15890},{level:4,title:"3.4.2.2 投影查询（PROJECTION QUERY）",slug:"_3-4-2-2-投影查询-projection-query",normalizedTitle:"3.4.2.2 投影查询（projection query）",charIndex:16381},{level:3,title:"3.4.3 文档的更新",slug:"_3-4-3-文档的更新",normalizedTitle:"3.4.3 文档的更新",charIndex:16763},{level:4,title:"3.4.3.1 覆盖的修改",slug:"_3-4-3-1-覆盖的修改",normalizedTitle:"3.4.3.1 覆盖的修改",charIndex:17511},{level:4,title:"3.4.3.2 局部修改",slug:"_3-4-3-2-局部修改",normalizedTitle:"3.4.3.2 局部修改",charIndex:17660},{level:4,title:"3.4.3.3 批量的修改",slug:"_3-4-3-3-批量的修改",normalizedTitle:"3.4.3.3 批量的修改",charIndex:17807},{level:4,title:"3.4.3.4 列值增长的修改",slug:"_3-4-3-4-列值增长的修改",normalizedTitle:"3.4.3.4 列值增长的修改",charIndex:18053},{level:3,title:"3.4.4 删除文档",slug:"_3-4-4-删除文档",normalizedTitle:"3.4.4 删除文档",charIndex:18204},{level:2,title:"3.5 文档的分页查询",slug:"_3-5-文档的分页查询",normalizedTitle:"3.5 文档的分页查询",charIndex:18360},{level:3,title:"3.5.1 统计查询",slug:"_3-5-1-统计查询",normalizedTitle:"3.5.1 统计查询",charIndex:18376},{level:4,title:"3.5.1.1 统计所有记录数",slug:"_3-5-1-1-统计所有记录数",normalizedTitle:"3.5.1.1 统计所有记录数",charIndex:18570},{level:4,title:"3.5.1.2 按条件统计记录数",slug:"_3-5-1-2-按条件统计记录数",normalizedTitle:"3.5.1.2 按条件统计记录数",charIndex:18635},{level:3,title:"3.5.2 分页列表查询",slug:"_3-5-2-分页列表查询",normalizedTitle:"3.5.2 分页列表查询",charIndex:18756},{level:3,title:"3.5.3 排序查询",slug:"_3-5-3-排序查询",normalizedTitle:"3.5.3 排序查询",charIndex:19230},{level:2,title:"3.6 文档的更多查询",slug:"_3-6-文档的更多查询",normalizedTitle:"3.6 文档的更多查询",charIndex:19594},{level:3,title:"3.6.1 正则的复杂条件查询",slug:"_3-6-1-正则的复杂条件查询",normalizedTitle:"3.6.1 正则的复杂条件查询",charIndex:19610},{level:3,title:"3.6.2 比较查询",slug:"_3-6-2-比较查询",normalizedTitle:"3.6.2 比较查询",charIndex:19896},{level:3,title:"3.6.3 包含查询",slug:"_3-6-3-包含查询",normalizedTitle:"3.6.3 包含查询",charIndex:20358},{level:3,title:"3.6.4 条件连接查询",slug:"_3-6-4-条件连接查询",normalizedTitle:"3.6.4 条件连接查询",charIndex:20579},{level:2,title:"3.7 常用命令小结",slug:"_3-7-常用命令小结",normalizedTitle:"3.7 常用命令小结",charIndex:21009},{level:2,title:"4.1 概述",slug:"_4-1-概述",normalizedTitle:"4.1 概述",charIndex:21748},{level:2,title:"4.2 索引的类型",slug:"_4-2-索引的类型",normalizedTitle:"4.2 索引的类型",charIndex:22141},{level:3,title:"4.2.1 单字段索引",slug:"_4-2-1-单字段索引",normalizedTitle:"4.2.1 单字段索引",charIndex:22155},{level:3,title:"4.2.2 复合索引",slug:"_4-2-2-复合索引",normalizedTitle:"4.2.2 复合索引",charIndex:22295},{level:3,title:"4.2.3 其他索引",slug:"_4-2-3-其他索引",normalizedTitle:"4.2.3 其他索引",charIndex:22463},{level:2,title:"4.3 索引的管理操作",slug:"_4-3-索引的管理操作",normalizedTitle:"4.3 索引的管理操作",charIndex:22887},{level:3,title:"4.3.1 索引的查看",slug:"_4-3-1-索引的查看",normalizedTitle:"4.3.1 索引的查看",charIndex:22903},{level:3,title:"4.3.2 索引的创建",slug:"_4-3-2-索引的创建",normalizedTitle:"4.3.2 索引的创建",charIndex:23400},{level:4,title:"4.3.2.1 单字段索引",slug:"_4-3-2-1-单字段索引",normalizedTitle:"4.3.2.1 单字段索引",charIndex:24965},{level:4,title:"4.3.2.2 复合索引",slug:"_4-3-2-2-复合索引",normalizedTitle:"4.3.2.2 复合索引",charIndex:25072},{level:3,title:"4.3.3 索引的移除",slug:"_4-3-3-索引的移除",normalizedTitle:"4.3.3 索引的移除",charIndex:25197},{level:4,title:"4.3.3.1 指定索引的移除",slug:"_4-3-3-1-指定索引的移除",normalizedTitle:"4.3.3.1 指定索引的移除",charIndex:25234},{level:4,title:"4.3.3.2 所有索引的移除",slug:"_4-3-3-2-所有索引的移除",normalizedTitle:"4.3.3.2 所有索引的移除",charIndex:25505},{level:2,title:"4.4 索引的使用",slug:"_4-4-索引的使用",normalizedTitle:"4.4 索引的使用",charIndex:25661},{level:3,title:"4.4.1 执行计划",slug:"_4-4-1-执行计划",normalizedTitle:"4.4.1 执行计划",charIndex:25675},{level:3,title:"4.4.2 涵盖的查询",slug:"_4-4-2-涵盖的查询",normalizedTitle:"4.4.2 涵盖的查询",charIndex:28277},{level:2,title:"5.1 需求分析",slug:"_5-1-需求分析",normalizedTitle:"5.1 需求分析",charIndex:30143},{level:2,title:"5.2 表结构分析",slug:"_5-2-表结构分析",normalizedTitle:"5.2 表结构分析",charIndex:30288},{level:2,title:"5.3 技术选型",slug:"_5-3-技术选型",normalizedTitle:"5.3 技术选型",charIndex:30907},{level:3,title:"5.3.1 mongodb-driver",slug:"_5-3-1-mongodb-driver",normalizedTitle:"5.3.1 mongodb-driver",charIndex:30920},{level:3,title:"5.3.2 SpringDataMongoDB",slug:"_5-3-2-springdatamongodb",normalizedTitle:"5.3.2 springdatamongodb",charIndex:31183},{level:2,title:"5.4 文章微服务模块搭建",slug:"_5-4-文章微服务模块搭建",normalizedTitle:"5.4 文章微服务模块搭建",charIndex:31362},{level:2,title:"5.5 文章评论实体类的编写",slug:"_5-5-文章评论实体类的编写",normalizedTitle:"5.5 文章评论实体类的编写",charIndex:33252},{level:2,title:"5.6 文章评论的基本增删改查",slug:"_5-6-文章评论的基本增删改查",normalizedTitle:"5.6 文章评论的基本增删改查",charIndex:34817},{level:2,title:"5.7 根据上级ID查询文章评论的分页列表",slug:"_5-7-根据上级id查询文章评论的分页列表",normalizedTitle:"5.7 根据上级id查询文章评论的分页列表",charIndex:37557},{level:2,title:"5.8 MongoTemplate 实现评论点赞",slug:"_5-8-mongotemplate-实现评论点赞",normalizedTitle:"5.8 mongotemplate 实现评论点赞",charIndex:38705}],headersStr:"1.1 业务应用场景 1.2 MongoDB 简介 1.3 体系结构 1.4 数据模型 1.5 MongoDB 的特点 2.1 Windows 系统中的安装启动 2.1.1 下载安装包 2.1.2 解压安装启动 2.2 Shell 连接(mongo命令) 2.2.1 查看已经有的数据库 2.2.2 退出 mongodb 2.3 Compass- 图形化界面客户端 2.4 Linux 系统中的安装启动和连接 3.1 案例需求 3.2 数据库操作 3.2.1 选择和创建数据库 3.2.2 数据库的删除 3.3 集合操作 3.3.1 集合的显式创建 3.3.2 集合的隐式创建 3.3.3 集合的删除 3.4 文档基本CRUD 3.4.1 文档的插入 3.4.1.1 单个文档插入 3.4.1.2 批量插入 3.4.2 文档的基本查询 3.4.2.1 查询所有 3.4.2.2 投影查询（PROJECTION QUERY） 3.4.3 文档的更新 3.4.3.1 覆盖的修改 3.4.3.2 局部修改 3.4.3.3 批量的修改 3.4.3.4 列值增长的修改 3.4.4 删除文档 3.5 文档的分页查询 3.5.1 统计查询 3.5.1.1 统计所有记录数 3.5.1.2 按条件统计记录数 3.5.2 分页列表查询 3.5.3 排序查询 3.6 文档的更多查询 3.6.1 正则的复杂条件查询 3.6.2 比较查询 3.6.3 包含查询 3.6.4 条件连接查询 3.7 常用命令小结 4.1 概述 4.2 索引的类型 4.2.1 单字段索引 4.2.2 复合索引 4.2.3 其他索引 4.3 索引的管理操作 4.3.1 索引的查看 4.3.2 索引的创建 4.3.2.1 单字段索引 4.3.2.2 复合索引 4.3.3 索引的移除 4.3.3.1 指定索引的移除 4.3.3.2 所有索引的移除 4.4 索引的使用 4.4.1 执行计划 4.4.2 涵盖的查询 5.1 需求分析 5.2 表结构分析 5.3 技术选型 5.3.1 mongodb-driver 5.3.2 SpringDataMongoDB 5.4 文章微服务模块搭建 5.5 文章评论实体类的编写 5.6 文章评论的基本增删改查 5.7 根据上级ID查询文章评论的分页列表 5.8 MongoTemplate 实现评论点赞",content:'# 1. MongoDB 相关概念\n\n\n# 1.1 业务应用场景\n\n传统的关系型数据库（如MySQL），在数据操作的“三高”需求以及应对Web2.0的网站需求面前，显得力不从心。\n\n解释：“三高”需求：\n\n * High performance - 对数据库高并发读写的需求。\n * Huge Storage - 对海量数据的高效率存储和访问的需求。\n * High Scalability && High Availability- 对数据库的高可扩展性和高可用性的需求。\n\n而MongoDB可应对“三高”需求。\n\n具体的应用场景如：\n\n * 社交场景，使用 MongoDB 存储存储用户信息，以及用户发表的朋友圈信息，通过地理位置索引实现附近的人、地点等功能。\n * 游戏场景，使用 MongoDB 存储游戏用户信息，用户的装备、积分等直接以内嵌文档的形式存储，方便查询、高效率存储和访问。\n * 物流场景，使用 MongoDB 存储订单信息，订单状态在运送过程中会不断更新，以 MongoDB 内嵌数组的形式来存储，一次查询就能将订单所有的变更读取出来。\n * 物联网场景，使用 MongoDB 存储所有接入的智能设备信息，以及设备汇报的日志信息，并对这些信息进行多维度的分析。\n * 视频直播，使用 MongoDB 存储用户信息、点赞互动信息等。\n\n这些应用场景中，数据操作方面的共同特点是：\n\n * 数据量大\n * 写入操作频繁（读写都很频繁）\n * 价值较低的数据，对事务性要求不高\n\n对于这样的数据，我们更适合使用MongoDB来实现数据的存储。\n\n什么时候选择MongoDB\n\n在架构选型上，除了上述的三个特点外，如果你还犹豫是否要选择它？可以考虑以下的一些问题：\n\n * 应用不需要事务及复杂 join 支持\n * 新应用，需求会变，数据模型无法确定，想快速迭代开发\n * 应用需要2000-3000以上的读写QPS（更高也可以）\n * 应用需要TB甚至 PB 级别数据存储\n * 应用发展迅速，需要能快速水平扩展\n * 应用要求存储的数据不丢失\n * 应用需要99.999%高可用\n * 应用需要大量的地理位置查询、文本查询\n\n如果上述有1个符合，可以考虑 MongoDB，2个及以上的符合，选择 MongoDB 绝不会后悔。\n\n\n# 1.2 MongoDB 简介\n\nMongoDB是一个开源、高性能、无模式的文档型数据库，当初的设计就是用于简化开发和方便扩展，是NoSQL数据库产品中的一种。是最像关系型数据库（MySQL）的非关系型数据库。\n\n它支持的数据结构非常松散，是一种类似于 JSON 的 格式叫BSON，所以它既可以存储比较复杂的数据类型，又相当的灵活。\n\nMongoDB中的记录是一个文档，它是一个由字段和值对（field:value）组成的数据结构。MongoDB文档类似于JSON对象，即一个文档认为就是一个对象。字段的数据类型是字符型，它的值除了使用基本的一些类型外，还可以包括其他文档、普通数组和文档数组。\n\n\n# 1.3 体系结构\n\nMySQL和MongoDB对比\n\n\n\nSQL 术语/概念     MONGODB术语/概念   解释/说明\ndatabase      database       数据库\ntable         collection     数据库表/集合\nrow           document       数据记录行/文档\ncolumn        field          数据字段/域\nindex         index          索引\ntable joins                  表连接,MongoDB不支持\n              嵌入文档           MongoDB通过嵌入式文档来替代多表连接\nprimary key   primary key    主键,MongoDB自动将_id字段设置为主键\n\n\n# 1.4 数据模型\n\nMongoDB的最小存储单位就是文档(document)对象。文档(document)对象对应于关系型数据库的行。数据在MongoDB中以BSON（Binary-JSON）文档的格式存储在磁盘上。\n\nBSON（Binary Serialized Document Format）是一种类json的一种二进制形式的存储格式，简称Binary JSON。BSON和JSON一样，支持内嵌的文档对象和数组对象，但是BSON有JSON没有的一些数据类型，如Date和BinData类型。\n\nBSON采用了类似于 C 语言结构体的名称、对表示方法，支持内嵌的文档对象和数组对象，具有轻量性、可遍历性、高效性的三个特点，可以有效描述非结构化数据和结构化数据。这种格式的优点是灵活性高，但它的缺点是空间利用率不是很理想。\n\nBson中，除了基本的JSON类型：string,integer,boolean,double,null,array和object，mongo还使用了特殊的数据类型。这些类型包括date,object id,binary data,regular expression 和code。每一个驱动都以特定语言的方式实现了这些类型，查看你的驱动的文档来获取详细信息。\n\nBSON数据类型参考列表：\n\n数据类型        描述                                         举例\n字符串         UTF-8字符串都可表示为字符串类型的数据                      {“x” : “foobar”}\n对象id        对象id是文档的12字节的唯一 ID                         {“X” :ObjectId() }\n布尔值         真或者假：true或者false                           {“x”:true}\n数组          值的集合或者列表可以表示成数组                            {“x” ： [“a”, “b”, “c”]}\n32位整数       类型不可用。JavaScript仅支持64位浮点数，所以32位整数会被自动转换。   shell是不支持该类型的，shell中默认会转换成64位浮点数\n64位整数       不支持这个类型。shell会使用一个特殊的内嵌文档来显示64位整数          shell是不支持该类型的，shell中默认会转换成64位浮点数\n64位浮点数      shell中的数字就是这一种类型                           {“x”：3.14159，”y”：3}\nnull        表示空值或者未定义的对象                               {“x”:null}\nundefined   文档中也可以使用未定义类型                              {“x”:undefined}\n符号          shell不支持，shell会将数据库中的符号类型的数据自动转换成字符串       \n正则表达式       文档中可以包含正则表达式，采用JavaScript的正则表达式语法          {“x” ： /foobar/i}\n代码          文档中还可以包含JavaScript代码                       {“x” ： function() { /* …… */ }}\n二进制数据       二进制数据可以由任意字节的串组成，不过shell中无法使用              \n最大值/最小值     BSON包括一个特殊类型，表示可能的最大值。shell中没有这个类型。        \n\n提示：\n\nshell默认使用64位浮点型数值。{“x”：3.14}或{“x”：3}。对于整型值，可以使用NumberInt（4字节符号整数）或NumberLong（8字节符号整数），{“x”:NumberInt(“3”)}{“x”:NumberLong(“3”)}\n\n\n# 1.5 MongoDB 的特点\n\nMongoDB主要有如下特点：\n\n * 高性能：\n   \n   MongoDB提供高性能的数据持久性。特别是\n   \n   对嵌入式数据模型的支持减少了数据库系统上的I/O活动。\n   \n   索引支持更快的查询，并且可以包含来自嵌入式文档和数组的键。（文本索引解决搜索的需求、TTL索引解决历史数据自动过期的需求、地理位置索引可用于构建各种 O2O 应用）\n   \n   mmapv1、wiredtiger、mongorocks（rocksdb）、in-memory 等多引擎支持满足各种场景需求。\n   \n   Gridfs解决文件存储的需求。\n\n * 高可用性：\n   \n   MongoDB的复制工具称为副本集（replica set），它可提供自动故障转移和数据冗余。\n\n * 高扩展性：\n   \n   MongoDB提供了水平可扩展性作为其核心功能的一部分。\n   \n   分片将数据分布在一组集群的机器上。（海量数据存储，服务能力水平扩展）\n   \n   从3.4开始，MongoDB支持基于片键创建数据区域。在一个平衡的集群中，MongoDB将一个区域所覆盖的读写只定向到该区域内的那些片。\n\n * 丰富的查询支持：\n   \n   MongoDB支持丰富的查询语言，支持读和写操作(CRUD)，比如数据聚合、文本搜索和地理空间查询等。\n\n * 其他特点：如无模式（动态模式）、灵活的文档模型、\n\n\n# 2. 单机部署\n\n\n# 2.1 Windows 系统中的安装启动\n\n\n# 2.1.1 下载安装包\n\nMongoDB 提供了可用于 32 位和 64 位系统的预编译二进制包，你可以从MongoDB官网下载安装，MongoDB 预编译二进制包下地址：https://www.mongodb.com/download-center#community\n\n\n\n根据上图所示下载 zip 包。\n\n提示：版本的选择：\n\nMongoDB的版本命名规范如：x.y.z；\n\ny为奇数时表示当前版本为开发版，如：1.5.2、4.1.13；\n\ny为偶数时表示当前版本为稳定版，如：1.6.3、4.0.10；\n\nz是修正版本号，数字越大越好。\n\n详情： http://docs.mongodb.org/manual/release-notes/#release-version-numbers\n\n\n# 2.1.2 解压安装启动\n\n将压缩包解压到一个目录中。在解压目录中，手动建立一个目录用于存放数据文件，如 data/db\n\n方式1：命令行参数方式启动服务\n\n在 bin 目录中打开命令行提示符，输入如下命令：\n\nmongod --dbpath=..\\data\\db\n\n\n1\n\n\n我们在启动信息中可以看到， mongoDB的默认端口是27017，如果我们想改变默认的启动端口，可以通过–port来指定端口。\n\n为了方便我们每次启动，可以将安装目录的bin目录设置到环境变量的path中， bin 目录下是一些常用命令，比如 mongod 启动服务用的，mongo 客户端连接服务用的。\n\n方式2：配置文件方式启动服务\n\n在解压目录中新建 config 文件夹，该文件夹中新建配置文件 mongod.conf ，内如参考如下：\n\nstorage:\n    #The directory where the mongod instance stores its data.Default Value is "\\data\\db" on Windows.\n    dbPath: D:\\mongodb-win32-x86_64-2008plus-ssl-4.0.1\\data\n\n\n1\n2\n3\n\n\n详细配置项内容可以参考官方文档： https://docs.mongodb.com/manual/reference/configuration-options/\n\n【注意】\n\n * 配置文件中如果使用双引号，比如路径地址，自动会将双引号的内容转义。如果不转义，则会报错：\n   \n   error-parsing-yaml-config-file-yaml-cpp-error-at-line-3-column-15-unknown-escape-character-d\n   \n   \n   1\n   \n   \n   解决：\n   \n   a. 对 \\ 换成 / 或 \\\n   \n   b. 如果路径中没有空格，则无需加引号。\n\n * 配置文件中不能以Tab分割字段\n   \n   解决：\n   \n   将其转换成空格。\n\n启动方式：\n\nmongod -f ../config/mongod.conf\n或\nmongod --config ../config/mongod.conf\n\n\n1\n2\n3\n\n\n更多参数配置：\n\nsystemLog:\n    destination: file\n    #The path of the log file to which mongod or mongos should send all diagnostic logging information\n    path: "D:/mongodb-win32-x86_64-2008plus-ssl-4.0.1/log/mongod.log"\n    logAppend: true\nstorage:\n    journal:\n      enabled: true\n    #The directory where the mongod instance stores its data.Default Value is "/data/db".\n    dbPath: "D:/mongodb-win32-x86_64-2008plus-ssl-4.0.1/data"\nnet:\n    #bindIp: 127.0.0.1\n    port: 27017\nsetParameter:\n    enableLocalhostAuthBypass: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 2.2 Shell 连接(mongo命令)\n\n在命令提示符输入以下shell命令即可完成登陆\n\nmongo\n或\nmongo --host=127.0.0.1 --port=27017\n\n\n1\n2\n3\n\n\n\n# 2.2.1 查看已经有的数据库\n\nshow databases\n\n\n1\n\n\n\n# 2.2.2 退出 mongodb\n\nexit\n\n\n1\n\n\n更多参数可以通过帮助查看：\n\nmongo --help\n\n\n1\n\n\n提示：\n\nMongoDB javascript shell是一个基于javascript的解释器，故是支持js程序的。\n\n\n# 2.3 Compass- 图形化界面客户端\n\n到MongoDB官网下载MongoDB Compass，地址： https://www.mongodb.com/download-center/v2/compass?initial=true\n\n如果是下载安装版，则按照步骤安装；如果是下载加压缩版，直接解压，执行里面的MongoDBCompassCommunity.exe 文件即可。\n\n在打开的界面中，输入主机地址、端口等相关信息，点击连接：\n\n\n\n\n# 2.4 Linux 系统中的安装启动和连接\n\n目标：在Linux中部署一个单机的MongoDB，作为生产环境下使用。\n\n提示：和Windows下操作差不多。\n\n步骤如下：\n\n * 先到官网下载压缩包 mongod -linux-x86_64-4.0.10.tgz 。\n\n * 上传压缩包到Linux中，解压到当前目录：\n   \n   tar -xvf mongodb-linux-x86_64-4.0.10.tgz\n   \n   \n   1\n   \n\n * 移动解压后的文件夹到指定的目录中：\n   \n   mv mongodb-linux-x86_64-4.0.10 /usr/local/mongodb\n   \n   \n   1\n   \n\n * 新建几个目录，分别用来存储数据和日志：\n   \n   #数据存储目录\n   mkdir -p /mongodb/single/data/db\n   #日志存储目录\n   mkdir -p /mongodb/single/log\n   \n   \n   1\n   2\n   3\n   4\n   \n\n * 新建并修改配置文件\n   \n   vi /mongodb/single/mongod.conf\n   \n   \n   1\n   \n   \n   配置文件的内容如下：\n   \n   systemLog:\n         #MongoDB发送所有日志输出的目标指定为文件\n         # #The path of the log file to which mongod or mongos should send all diagnostic logging information\n         destination: file\n         #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n         path: "/mongodb/single/log/mongod.log"\n         #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n         logAppend: true\n   storage:\n         #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n         ##The directory where the mongod instance stores its data.Default Value is "/data/db".\n         dbPath: "/mongodb/single/data/db"\n         journal:\n             #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n             enabled: true\n   processManagement:\n         #启用在后台运行mongos或mongod进程的守护进程模式。\n         fork: true\n   net:\n         #服务实例绑定的IP，默认是localhost\n         bindIp: localhost,192.168.142.128\n         #bindIp\n         #绑定的端口，默认是27017\n         port: 27017\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   17\n   18\n   19\n   20\n   21\n   22\n   23\n   24\n   \n\n * 启动MongoDB服务\n   \n   /usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf\n   \n   \n   1\n   \n   \n   注意：\n   \n   如果启动后不是 successfully ，则是启动失败了。原因基本上就是配置文件有问题。\n   \n   通过进程来查看服务是否启动了：\n   \n   ps -ef |grep mongod\n   \n   \n   1\n   \n\n * 分别使用mongo命令和compass工具来连接测试。\n   \n   提示：如果远程连接不上，需要配置防火墙放行，或直接关闭linux防火墙\n   \n   #查看防火墙状态\n   systemctl status firewalld\n   #临时关闭防火墙\n   systemctl stop firewalld\n   #开机禁止启动防火墙\n   systemctl disable firewalld\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   \n\n * 停止关闭服务\n   \n   停止服务的方式有两种：快速关闭和标准关闭，下面依次说明：\n   \n   （一）快速关闭方法（快速，简单，数据可能会出错）\n   \n   目标：通过系统的kill命令直接杀死进程：\n   \n   杀完要检查一下，避免有的没有杀掉。\n   \n   #通过进程编号关闭节点\n   kill -2 54410\n   \n   \n   1\n   2\n   \n   \n   【补充】\n   \n   如果一旦是因为数据损坏，则需要进行如下操作（了解）：\n   \n   1）删除lock文件：\n   \n   rm -f /mongodb/single/data/db/*.lock\n   \n   \n   1\n   \n   \n   2 ）修复数据：\n   \n   /usr/local/mongdb/bin/mongod --repair --dbpath=/mongodb/single/data/db\n   \n   \n   1\n   \n   \n   （二）标准的关闭方法（数据不容易出错，但麻烦）：\n   \n   目标：通过mongo客户端中的shutdownServer命令来关闭服务\n   \n   主要的操作步骤参考如下：\n   \n   //客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\n   mongo --port 27017\n   //#切换到admin库\n   use admin\n   //关闭服务\n   db.shutdownServer()\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   \n\n\n# 3. 基本常用命令\n\n\n# 3.1 案例需求\n\n存放文章评论的数据存放到MongoDB中，数据结构参考如下：\n\n数据库：articledb\n\n专栏文章评论           COMMENT                     \n字段名称             字段含义      字段类型              备注\n_id              ID        ObjectId或String   Mongo的主键的字段\narticleid        文章ID      String            \ncontent          评论内容      String            \nuserid           评论人ID     String            \nnickname         评论人昵称     String            \ncreatedatetime   评论的日期时间   Date              \nlikenum          点赞数       Int32             \nreplynum         回复数       Int32             \nstate            状态        String            0：不可见；1：可见；\nparentid         上级ID      String            如果为0表示文章的顶级评论\n\n\n# 3.2 数据库操作\n\n\n# 3.2.1 选择和创建数据库\n\n选择和创建数据库的语法格式：\n\nuse 数据库名称\n\n\n1\n\n\n如果数据库不存在则自动创建，例如，以下语句创建 articledb 数据库：\n\nuse articledb\n\n\n1\n\n\n查看有权限的所有的数据库命令\n\nshow dbs\n或\nshow databases\n\n\n1\n2\n3\n\n\n> 注意 : 在 MongoDB 中，集合只有在内容插入后才会创建! 就是说，创建集合(数据表)后要再插入一个文档(记录)，集合才会真正创建。\n\n查看当前正在使用的数据库命令\n\ndb\n\n\n1\n\n\nMongoDB 中默认的数据库为 test，如果你没有选择数据库，集合将存放在 test 数据库中。\n\n另外：\n\n数据库名可以是满足以下条件的任意UTF-8字符串。\n\n * 不能是空字符串（ “”)。\n * 不得含有 ‘ ‘（空格)、.、$、/、\\和\\0 (空字符)。\n * 应全部小写。\n * 最多 64字节。\n\n有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。\n\n * admin ： 从权限的角度来看，这是”root”数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。\n * local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合\n * config : 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。\n\n\n# 3.2.2 数据库的删除\n\nMongoDB 删除数据库的语法格式如下：\n\ndb.dropDatabase()\n\n\n1\n\n\n提示：主要用来删除已经持久化的数据库\n\n\n# 3.3 集合操作\n\n集合，类似关系型数据库中的表。\n\n可以显示的创建，也可以隐式的创建。\n\n\n# 3.3.1 集合的显式创建\n\n基本语法格式：\n\ndb.createCollection(name)\n\n\n1\n\n\n参数说明：\n\n * name: 要创建的集合名称\n\n例如：创建一个名为 mycollection 的普通集合。\n\ndb.createCollection("mycollection")\n\n\n1\n\n\n查看当前库中的表： show tables命令\n\nshow collections\n或\nshow tables\n\n\n1\n2\n3\n\n\n集合的命名规范：\n\n * 集合名不能是空字符串 “”。\n * 集合名不能含有 \\0字符（空字符)，这个字符表示集合名的结尾。\n * 集合名不能以 “system.”开头，这是为系统集合保留的前缀。\n * 用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现$。\n\n\n# 3.3.2 集合的隐式创建\n\n当向一个集合中插入一个文档的时候，如果集合不存在，则会自动创建集合。\n\n详见 文档的插入 章节。\n\n提示：通常我们使用隐式创建文档即可。\n\n\n# 3.3.3 集合的删除\n\n集合删除语法格式如下：\n\ndb.collection.drop()\n或\ndb.集合.drop()\n\n\n1\n2\n3\n\n\n返回值\n\n如果成功删除选定集合，则 drop() 方法返回 true，否则返回 false。\n\n例如：要删除mycollection集合\n\ndb.mycollection.drop()\n\n\n1\n\n\n\n# 3.4 文档基本CRUD\n\n文档（document）的数据结构和 JSON 基本一样。\n\n所有存储在集合中的数据都是 BSON 格式。\n\n\n# 3.4.1 文档的插入\n\n# 3.4.1.1 单个文档插入\n\n使用insert() 或 save() 方法向集合中插入文档，语法如下：\n\ndb.collection.insert(\n    <document or array of documents>,\n    {\n        writeConcern: <document>,\n        ordered: <boolean>\n    }\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n参数：\n\nPARAMETER      TYPE                DESCRIPTION\ndocument       document or array   要插入到集合中的文档或文档数组。（(json格式）\nwriteConcern   document            Optional. A document expressing the write concern . Omit to\n                                   use the default write concern. See Write Concern .Do not\n                                   explicitly set the write concern for the operation if run in\n                                   a transaction. To use write concern with transactions, see\n                                   Transactions and Write Concern\nordered        boolean             可选。如果为真，则按顺序插入数组中的文档，如果其中一个文档出现错误，MongoDB将返回而不处理数组中的其余文档。如果为假，则执行无序插入，如果其中一个文档出现错误，则继续处理数组中的主文档。在版本2.6+中默认为true\n\n【示例】\n\n要向comment的集合(表)中插入一条测试数据：\n\ndb.comment.insert({"articleid":"100000","content":"今天天气真好，阳光明媚","userid":"1001","nickname":"Rose","createdatetime":new Date(),"likenum":NumberInt(10),"state":null})\n\n\n1\n\n\n提示：\n\n * comment集合如果不存在，则会隐式创建\n * mongo中的数字，默认情况下是double类型，如果要存整型，必须使用函数NumberInt(整型数字)，否则取出来就有问题了。\n * 插入当前日期使用 new Date()\n * 插入的数据没有指定 _id ，会自动生成主键值\n * 如果某字段没值，可以赋值为null，或不写该字段。\n\n执行后，如下，说明插入一个数据成功了。\n\nWriteResult({ "nInserted" : 1 })\n\n\n1\n\n\n注意：\n\n * 文档中的键/值对是有序的。\n * 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。\n * MongoDB区分类型和大小写。\n * MongoDB的文档不能有重复的键。\n * 文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。\n\n文档键命名规范：\n\n * 键不能含有 \\0 (空字符)。这个字符用来表示键的结尾。\n * . 和$有特别的意义，只有在特定环境下才能使用。\n * 以下划线 “_”开头的键是保留的(不是严格要求的)。\n\n# 3.4.1.2 批量插入\n\n语法：\n\ndb.collection.insertMany(\n     [ <document 1> , <document 2>, ... ],\n     {\n         writeConcern: <document>,\n         ordered: <boolean>\n     }\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n【示例】\n\n批量插入多条文章评论：\n\ndb.comment.insertMany([\n    {"_id":"1","articleid":"100001","content":"我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。","userid":"1002","nickname":"相忘于江湖","createdatetime":new Date("2019-08-05T22:08:15.522Z"),"likenum":NumberInt(1000),"state":"1"},\n    {"_id":"2","articleid":"100001","content":"我夏天空腹喝凉开水，冬天喝温开水","userid":"1005","nickname":"伊人憔悴","createdatetime":new Date("2019-08-05T23:58:51.485Z"),"likenum":NumberInt(888),"state":"1"},\n    {"_id":"3","articleid":"100001","content":"我一直喝凉开水，冬天夏天都喝。","userid":"1004","nickname":"杰克船长","createdatetime":new Date("2019-08-06T01:05:06.321Z"),"likenum":NumberInt(666),"state":"1"},\n    {"_id":"4","articleid":"100001","content":"专家说不能空腹吃饭，影响健康。","userid":"1003","nickname":"凯撒","createdatetime":new Date("2019-08-06T08:18:35.288Z"),"likenum":NumberInt(2000),"state":"1"},\n    {"_id":"5","articleid":"100001","content":"研究表明，刚烧开的水千万不能喝，因为烫嘴。","userid":"1003","nickname":"凯撒","createdatetime":new Date("2019-08-06T11:01:02.521Z"),"likenum":NumberInt(3000),"state":"1"}\n\n]);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n提示：\n\n插入时指定了 _id ，则主键就是该值。\n\n如果某条数据插入失败，将会终止插入，但已经插入成功的数据不会回滚掉。\n\n因为批量插入由于数据较多容易出现失败，因此，可以使用try catch进行异常捕捉处理，测试的时候可以不处理。如（了解）：\n\ntry {\n    db.comment.insertMany([\n        {"_id":"1","articleid":"100001","content":"我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。","userid":"1002","nickname":"相忘于江湖","createdatetime":new Date("2019-08-05T22:08:15.522Z"),"likenum":NumberInt(1000),"state":"1"},\n        {"_id":"2","articleid":"100001","content":"我夏天空腹喝凉开水，冬天喝温开水","userid":"1005","nickname":"伊人憔悴","createdatetime":new Date("2019-08-05T23:58:51.485Z"),"likenum":NumberInt(888),"state":"1"},\n        {"_id":"3","articleid":"100001","content":"我一直喝凉开水，冬天夏天都喝。","userid":"1004","nickname":"杰克船长","createdatetime":new Date("2019-08-06T01:05:06.321Z"),"likenum":NumberInt(666),"state":"1"},\n        {"_id":"4","articleid":"100001","content":"专家说不能空腹吃饭，影响健康。","userid":"1003","nickname":"凯撒","createdatetime":new Date("2019-08-06T08:18:35.288Z"),"likenum":NumberInt(2000),"state":"1"},\n        {"_id":"5","articleid":"100001","content":"研究表明，刚烧开的水千万不能喝，因为烫嘴。","userid":"1003","nickname":"凯撒","createdatetime":new Date("2019-08-06T11:01:02.521Z"),"likenum":NumberInt(3000),"state":"1"}\n\n    ]);\n\n} catch (e) {\n    print (e);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 3.4.2 文档的基本查询\n\n查询数据的语法格式如下：\n\ndb.collection.find(<query>, [projection])\n\n\n1\n\n\n参数：\n\nPARAMETER    TYPE       DESCRIPTION\nquery        document   可选。使用查询运算符指定选择筛选器。若要返回集合中的所有文档，请省略此参数或传递空文档( {} )。\nprojection   document   可选。指定要在与查询筛选器匹配的文档中返回的字段（投影）。若要返回匹配文档中的所有字段，请省略此参数。\n\n【示例】\n\n# 3.4.2.1 查询所有\n\n如果我们要查询comment集合的所有文档，我们输入以下命令\n\ndb.comment.find()\n或\ndb.comment.find({})\n\n\n1\n2\n3\n\n\n这里你会发现每条文档会有一个叫 _id的字段，这个相当于我们原来关系数据库中表的主键，当你在插入文档记录时没有指定该字段，MongoDB会自动创建，其类型是ObjectID类型。\n\n如果我们在插入文档记录时指定该字段也可以，其类型可以是ObjectID类型，也可以是MongoDB支持的任意类型。\n\n如果我想按一定条件来查询，比如我想查询userid为1003的记录，怎么办？很简单！只 要在find()中添加参数即可，参数也是json格式，如下：\n\ndb.comment.find({userid:\'1003\'})\n\n\n1\n\n\n如果你只需要返回符合条件的第一条数据，我们可以使用findOne命令来实现，语法和find一样。\n\n如：查询用户编号是1003的记录，但只最多返回符合条件的第一条记录：\n\ndb.comment.findOne({userid:\'1003\'})\n\n\n1\n\n\n# 3.4.2.2 投影查询（PROJECTION QUERY）\n\n如果要查询结果返回部分字段，则需要使用投影查询（不显示所有字段，只显示指定的字段）。\n\n如：查询结果只显示 _id 、userid、nickname :\n\ndb.comment.find({userid:"1003"},{userid:1,nickname:1})\n\n\n1\n\n\n默认 _id 会显示。\n\n如：查询结果只显示 、 userid、nickname ，不显示 _id ：\n\ndb.comment.find({userid:"1003"},{userid:1,nickname:1,_id:0})\n\n\n1\n\n\n再例如：查询所有数据，但只显示 _id 、userid、nickname :\n\ndb.comment.find({},{userid:1,nickname:1})\n\n\n1\n\n\n\n# 3.4.3 文档的更新\n\n更新文档的语法：\n\ndb.collection.update(query, update, options)\n//或\ndb.collection.update(\n    <query>,\n    <update>,\n    {\n        upsert: <boolean>,\n        multi: <boolean>,\n        writeConcern: <document>,\n        collation: <document>,\n        arrayFilters: [ <filterdocument1>, ... ],\n        hint:  <document|string>     // Available starting in MongoDB 4.2\n    }\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n参数：\n\nPARAMETER   TYPE       DESCRIPTION\nquery       document   更新的选择条件。可以使用与find（）方法中相同的查询选择器，类似sql\n                       update查询内where后面的。。在3.0版中进行了更改：当使用upsert:true执行update（）时，如果查询使用点表示法在_id\n                       字段上指定条件，则MongoDB将拒绝插入新文档。\nmulti       boolean    可选。如果设置为true，则更新符合查询条件的多个文档。如果设置为false，则更新一个文档。默认值为false。\n\n【示例】\n\n# 3.4.3.1 覆盖的修改\n\n如果我们想修改_id为1的记录，点赞量为1001，输入以下语句：\n\ndb.comment.update({_id:"1"},{likenum:NumberInt(1001)})\n\n\n1\n\n\n执行后，我们会发现，这条文档除了 likenum字段其它字段都不见了，\n\n# 3.4.3.2 局部修改\n\n为了解决这个问题，我们需要使用修改器$set来实现，命令如下：\n\n我们想修改_id为2的记录，浏览量为889，输入以下语句：\n\ndb.comment.update({_id:"2"},{$set:{likenum:NumberInt(889)}})\n\n\n1\n\n\n# 3.4.3.3 批量的修改\n\n更新所有用户为 1003 的用户的昵称为 凯撒大帝 。\n\n//默认只修改第一条数据\ndb.comment.update({userid:"1003"},{$set:{nickname:"凯撒2"}})\n//修改所有符合条件的数据\ndb.comment.update({userid:"1003"},{$set:{nickname:"凯撒大帝"}},{multi:true})\n\n\n1\n2\n3\n4\n\n\n提示：如果不加后面的参数，则只更新符合条件的第一条记录\n\n# 3.4.3.4 列值增长的修改\n\n如果我们想实现对某列值在原有值的基础上进行增加或减少，可以使用 $inc 运算符来实现。\n\n需求：对3号数据的点赞数，每次递增1\n\ndb.comment.update({_id:"3"},{$inc:{likenum:NumberInt(1)}})\n\n\n1\n\n\n\n# 3.4.4 删除文档\n\n删除文档的语法结构：\n\ndb.集合名称.remove(条件)\n\n\n1\n\n\n以下语句可以将数据全部删除，请慎用\n\ndb.comment.remove({})\n\n\n1\n\n\n如果删除 _id=1的记录，输入以下语句\n\ndb.comment.remove({_id:"1"})\n\n\n1\n\n\n\n# 3.5 文档的分页查询\n\n\n# 3.5.1 统计查询\n\n统计查询使用count()方法，语法如下：\n\ndb.collection.count(query, options)\n\n\n1\n\n\n参数：\n\nPARAMETER   TYPE       DESCRIPTION\nquery       document   查询选择条件\noptions     document   可选。用于修改计数的额外选项\n\n【示例】\n\n# 3.5.1.1 统计所有记录数\n\n统计comment集合的所有的记录数：\n\ndb.comment.count()\n\n\n1\n\n\n# 3.5.1.2 按条件统计记录数\n\n例如：统计userid为1003的记录条数\n\ndb.comment.count({userid:"1003"})\n\n\n1\n\n\n提示：\n\n默认情况下 count() 方法返回符合条件的全部记录条数。\n\n\n# 3.5.2 分页列表查询\n\n可以使用limit()方法来读取指定数量的数据，使用skip()方法来跳过指定数量的数据。\n\n基本语法如下所示：\n\ndb.COLLECTION_NAME.find().limit(NUMBER).skip(NUMBER)\n\n\n1\n\n\n如果你想返回指定条数的记录，可以在find方法后调用limit来返回结果(TopN)，默认值20，例如：\n\ndb.comment.find().limit(3)\n\n\n1\n\n\nskip 方法同样接受一个数字参数作为跳过的记录条数。（前N个不要）,默认值是0\n\ndb.comment.find().skip(3)\n\n\n1\n\n\n分页查询：需求：每页2个，第二页开始：跳过前两条数据，接着值显示3和4条数据\n\n//第一页\ndb.comment.find().skip(0).limit(2)\n//第二页\ndb.comment.find().skip(2).limit(2)\n//第三页\ndb.comment.find().skip(4).limit(2)\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3.5.3 排序查询\n\nsort() 方法对数据进行排序，sort() 方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1为升序排列，而 -1 是用于降序排列。\n\n语法如下所示：\n\ndb.COLLECTION_NAME.find().sort({KEY:1})\n或\ndb.集合名称.find().sort(排序方式)\n\n\n1\n2\n3\n\n\n例如：\n\n对userid降序排列，并对访问量进行升序排列\n\ndb.comment.find().sort({userid:-1,likenum:1})\n\n\n1\n\n\n提示：\n\nskip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()，和命令编写顺序无关。\n\n\n# 3.6 文档的更多查询\n\n\n# 3.6.1 正则的复杂条件查询\n\nMongoDB的模糊查询是通过正则表达式的方式实现的。格式为：\n\ndb.collection.find({field:/正则表达式/})\n或\ndb.集合.find({字段:/正则表达式/})\n\n\n1\n2\n3\n\n\n提示：正则表达式是 js的语法，直接量的写法。\n\n例如，我要查询评论内容包含“开水”的所有文档，代码如下：\n\ndb.comment.find({content:/开水/})\n\n\n1\n\n\n如果要查询评论的内容中以 “专家”开头的，代码如下：\n\ndb.comment.find({content:/^专家/})\n\n\n1\n\n\n\n# 3.6.2 比较查询\n\n<, <=, >, >= 这个操作符也是很常用的，格式如下:\n\ndb.集合名称.find({ "field" : { $gt: value }}) // 大于: field > value\ndb.集合名称.find({ "field" : { $lt: value }}) // 小于: field < value\ndb.集合名称.find({ "field" : { $gte: value }}) // 大于等于: field >= value\ndb.集合名称.find({ "field" : { $lte: value }}) // 小于等于: field <= value\ndb.集合名称.find({ "field" : { $ne: value }}) // 不等于: field != value\n\n\n1\n2\n3\n4\n5\n\n\n示例：查询评论点赞数量大于 700的记录\n\ndb.comment.find({likenum:{$gt:NumberInt(700)}})\n\n\n1\n\n\n\n# 3.6.3 包含查询\n\n包含使用$in操作符。 示例：查询评论的集合中userid字段包含1003或1004的文档\n\ndb.comment.find({userid:{$in:["1003","1004"]}})\n\n\n1\n\n\n不包含使用 $nin操作符。 示例：查询评论集合中userid字段不包含1003和1004的文档\n\ndb.comment.find({userid:{$nin:["1003","1004"]}})\n\n\n1\n\n\n\n# 3.6.4 条件连接查询\n\n我们如果需要查询同时满足两个以上条件，需要使用$and操作符将条件进行关联。（相当于SQL的and） 格式为：\n\n$and:[ {  },{  },{ } ]\n\n\n1\n\n\n示例：查询评论集合中 likenum大于等于700 并且小于2000的文档：\n\ndb.comment.find({$and:[{likenum:{$gte:NumberInt(700)}},{likenum:{$lt:NumberInt(2000)}}]})\n\n\n1\n\n\n如果两个以上条件之间是或者的关系，我们使用操作符进行关联，与前面 and的使用方式相同格式为：\n\n$or:[ {  },{  },{   } ]\n\n\n1\n\n\n示例：查询评论集合中 userid为1003，或者点赞数小于1000的文档记录\n\ndb.comment.find({$or:[ {userid:"1003"} ,{likenum:{$lt:1000} }]})\n\n\n1\n\n\n\n# 3.7 常用命令小结\n\n选择切换数据库：use articledb\n插入数据：db.comment.insert({bson数据})\n查询所有数据：db.comment.find();\n条件查询数据：db.comment.find({条件})\n查询符合条件的第一条记录：db.comment.findOne({条件})\n查询符合条件的前几条记录：db.comment.find({条件}).limit(条数)\n查询符合条件的跳过的记录：db.comment.find({条件}).skip(条数)\n修改数据：db.comment.update({条件},{修改后的数据}) 或db.comment.update({条件},{$set:{要修改部分的字段:数据})\n修改数据并自增某字段值：db.comment.update({条件},{$inc:{自增的字段:步进值}})\n删除数据：db.comment.remove({条件})\n统计查询：db.comment.count({条件})\n模糊查询：db.comment.find({字段名:/正则表达式/})\n条件比较运算：db.comment.find({字段名:{$gt:值}})\n包含查询：db.comment.find({字段名:{$in:[值1，值2]}})或db.comment.find({字段名:{$nin:[值1，值2]}})\n条件连接查询：db.comment.find({$and:[{条件1},{条件2}]})或db.comment.find({$or:[{条件1},{条件2}]})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 4. 索引-Index\n\n\n# 4.1 概述\n\n索引支持在MongoDB中高效地执行查询。如果没有索引，MongoDB必须执行全集合扫描，即扫描集合中的每个文档，以选择与查询语句匹配的文档。这种扫描全集合的查询效率是非常低的，特别在处理大量的数据时，查询可以要花费几十秒甚至几分钟，这对网站的性能是非常致命的。\n\n如果查询存在适当的索引，MongoDB可以使用该索引限制必须检查的文档数。\n\n索引是特殊的数据结构，它以易于遍历的形式存储集合数据集的一小部分。索引存储特定字段或一组字段的值，按字段值排序。索引项的排序支持有效的相等匹配和基于范围的查询操作。此外，MongoDB还可以使用索引中的排序返回排序结果。\n\n官网文档： https://docs.mongodb.com/manual/indexes/\n\n了解：\n\nMongoDB索引使用B树数据结构（确切的说是B-Tree，MySQL是B+Tree）\n\n\n# 4.2 索引的类型\n\n\n# 4.2.1 单字段索引\n\nMongoDB支持在文档的单个字段上创建用户定义的升序/降序索引，称为单字段索引（Single Field Index）。\n\n对于单个字段索引和排序操作，索引键的排序顺序（即升序或降序）并不重要，因为MongoDB可以在任何方向上遍历索引。\n\n\n\n\n# 4.2.2 复合索引\n\nMongoDB还支持多个字段的用户定义索引，即复合索引（Compound Index）。\n\n复合索引中列出的字段顺序具有重要意义。例如，如果复合索引由 { userid: 1, score: -1 } 组成，则索引首先按userid正序排序，然后在每个userid的值内，再在按score倒序排序。\n\n\n\n\n# 4.2.3 其他索引\n\n地理空间索引（Geospatial Index）、文本索引（Text Indexes）、哈希索引（Hashed Indexes）。\n\n * 地理空间索引（Geospatial Index）\n   \n   为了支持对地理空间坐标数据的有效查询，MongoDB提供了两种特殊的索引：返回结果时使用平面几何的二维索引和返回结果时使用球面几何的二维球面索引。\n\n * 文本索引（Text Indexes）\n   \n   MongoDB提供了一种文本索引类型，支持在集合中搜索字符串内容。这些文本索引不存储特定于语言的停止词（例如“the”、“a”、“or”），而将集合中的词作为词干，只存储根词。\n\n * 哈希索引（Hashed Indexes）\n   \n   为了支持基于散列的分片，MongoDB提供了散列索引类型，它对字段值的散列进行索引。这些索引在其范围内的值分布更加随机，但只支持相等匹配，不支持基于范围的查询。\n\n\n# 4.3 索引的管理操作\n\n\n# 4.3.1 索引的查看\n\n说明：\n\n返回一个集合中的所有索引的数组。\n\n语法：\n\ndb.collection.getIndexes()\n\n\n1\n\n\n提示：该语法命令运行要求是 MongoDB 3.0+\n\n【示例】\n\n查看comment集合中所有的索引情况\n\n> db.comment.getIndexes()\n[\n   {\n        "v" : 2,\n        "key" : {\n            "_id" : 1\n       },\n        "name" : "_id_",\n        "ns" : "articledb.comment"\n   }\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n结果中显示的是默认 _id 索引。\n\n默认_id索引：\n\nMongoDB在创建集合的过程中，在_id 字段上创建一个唯一的索引，默认名字为_id_，该索引可防止客户端插入两个具有相同值的文档，您不能在_id字段上删除此索引。\n\n注意：该索引是唯一索引，因此值不能重复，即 _id 值不能重复的。在分片集群中，通常使用 _id 作为片键。\n\n\n# 4.3.2 索引的创建\n\n说明：在集合上创建索引。\n\n语法：\n\ndb.collection.createIndex(keys, options)\n\n\n1\n\n\n参数：\n\nPARAMETER   TYPE       DESCRIPTION\nkeys        document   包含字段和值对的文档，其中字段是索引键，值描述该字段的索引类型。对于字段上的升序索引，请\n                       指定值1；对于降序索引，请指定值-1。比如： { 字段:1或-1} ，其中1 为指定按升序创建索引，如果你\n                       想按降序来创建索引指定为 -1 即可。另外，MongoDB支持几种不同的索引类型，包括文本、地理空 间和哈希索引。\noptions     document   可选。包含一组控制索引创建的选项的文档。有关详细信息，请参见选项详情列表。\n\noptions（更多选项）列表：\n\nPARAMETER            TYPE            DESCRIPTION\nbackground           Boolean         建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 “background”\n                                     可选参数。 “background” 默认值为false。\nunique               Boolean         建立的索引是否唯一。指定为true创建唯一索引。默认值为false.\nname                 string          索引的名称。如果未指定，MongoDB的通过连接索引的字段名和排序顺序生成一个索引名 称。\ndropDups             Boolean         3.0+版本已废弃。在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false.\nsparse               Boolean         对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为true的话，在索\n                                     引字段中不会查询出不包含对应字段的文档.。默认值为 false.\nexpireAfterSeconds   integer         指定一个以秒为单位的数值，完成 TTL设定，设定集合的生存时间。\nv                    index version   索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本。\nweights              document        索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。\ndefault_language     string          对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语\nlanguage_override    string          对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language.\n\n提示： 注意在 3.0.0 版本前创建索引方法为 db.collection.ensureIndex() ，之后的版本使用了 db.collection.createIndex() 方法，ensureIndex() 还能用，但只是 createIndex() 的别名。\n\n【示例】\n\n# 4.3.2.1 单字段索引\n\n示例：对 userid 字段建立索引：\n\ndb.comment.createIndex({userid:1})\n\n\n1\n\n\n参数 1：按升序创建索引\n\ncompass查看：\n\n\n\n# 4.3.2.2 复合索引\n\n对 userid 和 nickname 同时建立复合（Compound）索引：\n\ndb.comment.createIndex({userid:1,nickname:-1})\n\n\n1\n\n\ncompass 中：\n\n\n\n\n# 4.3.3 索引的移除\n\n说明：可以移除指定的索引，或移除所有索引\n\n# 4.3.3.1 指定索引的移除\n\n语法：\n\ndb.collection.dropIndex(index)\n\n\n1\n\n\n参数：\n\nPARAMETER   TYPE                 DESCRIPTION\nindex       string or document   指定要删除的索引。可以通过索引名称或索引规范文档指定索引。若要删除文本索引，请指定 索引名称。\n\n【示例】\n\n删除 comment 集合中 userid 字段上的升序索引：\n\ndb.comment.dropIndex({userid:1})\n\n\n1\n\n\n# 4.3.3.2 所有索引的移除\n\n语法：\n\ndb.collection.dropIndexes()\n\n\n1\n\n\n【示例】\n\n删除 comment 集合中所有索引。\n\ndb.comment.dropIndexes()\n\n\n1\n\n\n提示： _id 的字段的索引是无法删除的，只能删除非 _id 字段的索引。\n\n\n# 4.4 索引的使用\n\n\n# 4.4.1 执行计划\n\n分析查询性能（Analyze Query Performance）通常使用执行计划（解释计划、Explain Plan）来查看查询的情况，如查询耗费的时间、是否基于索引查询等。\n\n那么，通常，我们想知道，建立的索引是否有效，效果如何，都需要通过执行计划查看。\n\n语法：\n\ndb.collection.find(query,options).explain(options)\n\n\n1\n\n\n【示例】\n\n查看根据userid查询数据的情况：\n\n> db.comment.find({userid:"1003"}).explain()\n{\n    "queryPlanner" : {\n        "plannerVersion" : 1,\n        "namespace" : "articledb.comment",\n        "indexFilterSet" : false,\n        "parsedQuery" : {\n            "userid" : {\n                "$eq" : "1003"\n            }\n        },\n        "winningPlan" : {\n            "stage" : "COLLSCAN",\n            "filter" : {\n                "userid" : {\n                    "$eq" : "1003"\n                }\n            },\n            "direction" : "forward"\n        },\n        "rejectedPlans" : [ ]\n   },\n   "serverInfo" : {\n       "host" : "9ef3740277ad",\n       "port" : 27017,\n       "version" : "4.0.10",\n       "gitVersion" : "c389e7f69f637f7a1ac3cc9fae843b635f20b766"\n   },\n   "ok" : 1\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n关键点看： “stage” : “COLLSCAN”, 表示全集合扫描\n\n\n\n下面对userid建立索引\n\ndb.comment.createIndex({userid:1})\n\n\n1\n\n\n再次查看执行计划：\n\n> db.comment.find({userid:"1013"}).explain()\n{\n    "queryPlanner" : {\n        "plannerVersion" : 1,\n        "namespace" : "articledb.comment",\n        "indexFilterSet" : false,\n        "parsedQuery" : {\n            "userid" : {\n                "$eq" : "1013"\n            }\n        },\n        "winningPlan" : {\n            "stage" : "FETCH",\n            "inputStage" : {\n                "stage" : "IXSCAN",\n                "keyPattern" : {\n                    "userid" : 1\n                },\n                "indexName" : "userid_1",\n                "isMultiKey" : false,\n                "multiKeyPaths" : {\n                    "userid" : [ ]\n                 },\n                "isUnique" : false,\n                "isSparse" : false,\n                "isPartial" : false,\n                "indexVersion" : 2,\n                "direction" : "forward",\n                "indexBounds" : {\n                    "userid" : [\n                        "[\\"1013\\", \\"1013\\"]"\n                    ]\n                }\n            }\n        },\n        "rejectedPlans" : [ ]\n    },\n    "serverInfo" : {\n        "host" : "9ef3740277ad",\n        "port" : 27017,\n        "version" : "4.0.10",\n        "gitVersion" : "c389e7f69f637f7a1ac3cc9fae843b635f20b766"\n    },\n    "ok" : 1\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n关键点看： “stage” : “IXSCAN” ,基于索引的扫描\n\ncompass查看：\n\n\n\n\n# 4.4.2 涵盖的查询\n\nCovered Queries\n\n当查询条件和查询的投影仅包含索引字段时，MongoDB直接从索引返回结果，而不扫描任何文档或将文档带入内存。 这些覆盖的查询可以非常有效。\n\n\n\n更多：https://docs.mongodb.com/manual/core/query-optimization/#read-operations-covered-query\n\n【示例】\n\n> db.comment.find({userid:"1003"},{userid:1,_id:0})\n{ "userid" : "1003" }\n{ "userid" : "1003" }\n> db.comment.find({userid:"1003"},{userid:1,_id:0}).explain()\n{\n    "queryPlanner" : {\n        "plannerVersion" : 1,\n        "namespace" : "articledb.comment",\n        "indexFilterSet" : false,\n        "parsedQuery" : {\n            "userid" : {\n                "$eq" : "1003"\n            }\n        },\n        "winningPlan" : {\n            "stage" : "PROJECTION",\n            "transformBy" : {\n                "userid" : 1,\n                "_id" : 0\n            },\n            "inputStage" : {\n                "stage" : "IXSCAN",\n                "keyPattern" : {\n                    "userid" : 1\n                },\n                "indexName" : "userid_1",\n                "isMultiKey" : false,\n                "multiKeyPaths" : {\n                    "userid" : [ ]\n                },\n                "isUnique" : false,\n                "isSparse" : false,\n                "isPartial" : false,\n                "indexVersion" : 2,\n                "direction" : "forward",\n                "indexBounds" : {\n                    "userid" : [\n                        "[\\"1003\\", \\"1003\\"]"\n                    ]\n                }\n            }\n        },\n        "rejectedPlans" : [ ]\n    },\n    "serverInfo" : {\n        "host" : "bobohost.localdomain",\n        "port" : 27017,\n        "version" : "4.0.10",\n        "gitVersion" : "c389e7f69f637f7a1ac3cc9fae843b635f20b766"\n    },\n    "ok" : 1\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\nCompass 中：\n\n\n\n\n# 5. 文章评论\n\n\n# 5.1 需求分析\n\n某头条的文章评论业务如下：\n\n\n\n文章示例参考：早晨空腹喝水，是对还是错？ https://www.toutiao.com/a6721476546088927748/\n\n需要实现以下功能：\n\n * 基本增删改查API\n * 根据文章id查询评论\n * 评论点赞\n\n\n# 5.2 表结构分析\n\n数据库：articledb\n\n专栏文章评论           COMMENT                     \n字段名称             字段含义      字段类型              备注\n_id              ID        ObjectId或String   Mongo的主键的字段\narticleid        文章ID      String            \ncontent          评论内容      String            \nuserid           评论人ID     String            \nnickname         评论人昵称     String            \ncreatedatetime   评论的日期时间   Date              \nlikenum          点赞数       Int32             \nreplynum         回复数       Int32             \nstate            状态        String            0：不可见；1：可见；\nparentid         上级ID      String            如果为0表示文章的顶级评论\n\n\n# 5.3 技术选型\n\n\n# 5.3.1 mongodb-driver\n\nmongodb-driver是mongo官方推出的java连接mongoDB的驱动包，相当于JDBC驱动。我们通过一个入门的案例来了解mongodb-driver的基本使用。\n\n官方驱动说明和下载： http://mongodb.github.io/mongo-java-driver/\n\n官方驱动示例文档：http://mongodb.github.io/mongo-java-driver/3.8/driver/getting-started/quick-start/\n\n\n# 5.3.2 SpringDataMongoDB\n\nSpringData家族成员之一，用于操作MongoDB的持久层框架，封装了底层的mongodb-driver。\n\n官网主页： https://projects.spring.io/spring-data-mongodb/\n\n我们十次方项目的吐槽微服务就采用SpringDataMongoDB框架。\n\n\n# 5.4 文章微服务模块搭建\n\n（1）搭建项目工程article，pom.xml引入依赖：\n\n<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.1.6.RELEASE</version>\n        <relativePath/>\n    </parent>\n    <groupId>com.wgy</groupId>\n    <artifactId>article</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-data-mongodb</artifactId>\n        </dependency>\n    </dependencies>\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n（ 2）创建application.yml\n\nspring:\n  #数据源配置\n  data:\n    mongodb:\n      # 主机地址\n      #host: 192.168.142.128\n      # 数据库\n      #database: articledb\n      # 默认端口是27017\n      #port: 27017\n      #也可以使用uri连接\n      uri: mongodb://192.168.142.128:27017/articledb\nserver:\n  # 端口\n  port: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n（ 3）创建启动类\n\npackage com.wgy;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n/**\n * 启动类\n *\n * @author wgy\n */\n@SpringBootApplication\npublic class ArticleApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(ArticleApplication.class, args);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n（4）启动项目，看是否能正常启动，控制台没有错误。\n\n\n# 5.5 文章评论实体类的编写\n\n创建实体类 创建包com.wgy.article，包下建包po用于存放实体类，创建实体类\n\n/**\n * 评论实体类\n *\n * @author wgy\n */\n//把一个java类声明为mongodb的文档，可以通过collection参数指定这个类对应的文档。\n//@Document(collection="mongodb 对应 collection 名")\n// 若未加 @Document ，该 bean save 到 mongo 的 comment collection\n// 若添加 @Document ，则 save 到 comment collection\n@Document(collection = "comment")//可以省略，如果省略，则默认使用类名小写映射集合\n//复合索引\n// @CompoundIndex( def = "{\'userid\': 1, \'nickname\': -1}")\npublic class Comment implements Serializable {\n\n    //主键标识，该属性的值会自动对应mongodb的主键字段"_id"，如果该属性名就叫“id”,则该注解可以省略，否则必须写\n    @Id\n    private String id;//主键    //该属性对应mongodb的字段的名字，如果一致，则无需该注解\n    @Field("content")\n    private String content;//吐槽内容\n    private Date publishtime;//发布日期    //添加了一个单字段的索引\n    @Indexed\n    private String userid;//发布人ID\n    private String nickname;//昵称\n    private LocalDateTime createdatetime;//评论的日期时间\n    private Integer likenum;//点赞数\n    private Integer replynum;//回复数\n    private String state;//状态\n    private String parentid;//上级ID\n    private String articleid;\n\n    //set/get/toString...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n说明：\n\n索引可以大大提升查询效率，一般在查询字段上添加索引，索引的添加可以通过Mongo的命令来添加，也可以在Java的实体类中通过注解添加。\n\n1）单字段索引注解@Indexed\n\norg.springframework.data.mongodb.core.index.Indexed.class\n\n声明该字段需要索引，建索引可以大大的提高查询效率。\n\nMongo命令参考：\n\ndb.comment.createIndex({"userid":1})\n\n\n1\n\n\n2 ）复合索引注解@CompoundIndex\n\norg.springframework.data.mongodb.core.index.CompoundIndex.class\n\n复合索引的声明，建复合索引可以有效地提高多字段的查询效率。\n\nMongo命令参考：\n\ndb.comment.createIndex({"userid":1,"nickname":-1})\n\n\n1\n\n\n\n# 5.6 文章评论的基本增删改查\n\n（1）创建数据访问接口 com.wgy.article包下创建dao包，包下创建接口\n\n/**\n * 评论的持久层接口\n *\n * @author wgy\n */\npublic interface CommentRepository extends MongoRepository<Comment, String> {\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n（ 2）创建业务逻辑类 com.wgy.article包下创建service包，包下创建类\n\n/**\n * 评论的业务层\n *\n * @author wgy\n */\n@Service\npublic class CommentService {\n\n    //注入dao\n    @Autowired\n    private CommentRepository commentRepository;\n\n    /**\n     * 保存一个评论\n     *\n     * @param comment\n     */\n    public void saveComment(Comment comment) {\n        //如果需要自定义主键，可以在这里指定主键；如果不指定主键，MongoDB会自动生成主键\n        //设置一些默认初始值。。。\n        //调用dao\n        commentRepository.save(comment);\n    }\n\n    /**\n     * 更新评论\n     *\n     * @param comment\n     */\n    public void updateComment(Comment comment) {\n        //调用dao\n        commentRepository.save(comment);\n    }\n\n    /**\n     * 根据id删除评论\n     *\n     * @param id\n     */\n    public void deleteCommentById(String id) {\n        //调用dao\n        commentRepository.deleteById(id);\n    }\n\n    /**\n     * 查询所有评论\n     *\n     * @return\n     */\n    public List<Comment> findCommentList() {\n        //调用dao\n        return commentRepository.findAll();\n    }\n\n    /**\n     * 根据id查询评论\n     *\n     * @param id\n     * @return\n     */\n    public Comment findCommentById(String id) {\n        //调用dao\n        return commentRepository.findById(id).get();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\n（ 3）新建Junit测试类，测试保存和查询所有：\n\n/**\n * 评论测试类\n *\n * @author wgy\n */\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class CommentServiceTest {\n\n    @Autowired\n    private CommentService commentService;\n\n    /**\n     * 查询所有数据\n     */\n    @Test\n    public void testFindCommentList() {\n        List<Comment> commentList = commentService.findCommentList();\n        System.out.println(commentList);\n    }\n\n    /**\n     * 测试根据id查询\n     */\n    @Test\n    public void testFindCommentById() {\n        Comment commentById = commentService.findCommentById("1");\n        System.out.println(commentById);\n    }\n\n    /**\n     * 保存一个评论\n     */\n    @Test\n    public void testSaveComment() {\n        Comment comment = new Comment();\n        comment.setArticleid("100000");\n        comment.setContent("测试添加的数据");\n        comment.setCreatedatetime(LocalDateTime.now());\n        comment.setUserid("1003");\n        comment.setNickname("凯撒大帝");\n        comment.setState("1");\n        comment.setLikenum(0);\n        comment.setReplynum(0);\n        commentService.saveComment(comment);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# 5.7 根据上级ID查询文章评论的分页列表\n\n（1）CommentRepository新增方法定义\n\n/**\n * 根据父id，查询子评论的分页列表\n *\n * @param parentid\n * @param pageable\n * @return\n */\nPage<Comment> findByParentid(String parentid, Pageable pageable);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n（2）CommentService新增方法\n\n/**\n * 根据父id查询分页列表\n *\n * @param parentid\n * @param page\n * @param size\n * @return\n */\npublic Page<Comment> findCommentListByParentid(String parentid, int page, int size) {\n    return commentRepository.findByParentid(parentid, PageRequest.of(page - 1, size));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n（3）junit测试用例：\n\n/**\n * 测试根据父id查询子评论的分页列表\n */\n@Test\npublic void testFindCommentListByParentid() {\n    Page<Comment> pageResponse = commentService.findCommentListByParentid("3", 1, 2);\n    System.out.println("----总记录数：" + pageResponse.getTotalElements());\n    System.out.println("----当前页数据：" + pageResponse.getContent());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n（4）测试\n\n使用compass快速插入一条测试数据，数据的内容是对3号评论内容进行评论。\n\n\n\n执行测试，结果：\n\n----总记录数：1\n----当前页数据：[Comment{id=\'33\', content=\'你年轻，火力大\', publishtime=null, userid=\'1003\', nickname=\'凯撒大帝\',createdatetime=null, likenum=null, replynum=null, state=\'null\', parentid=\'3\', articleid=\'100001\'}]\n\n\n1\n2\n\n\n\n# 5.8 MongoTemplate 实现评论点赞\n\n我们看一下以下点赞的临时示例代码： CommentService 新增updateThumbup方法\n\n/**\n * 点赞-效率低\n * @param id\n */\npublic void updateCommentThumbupToIncrementingOld(String id){\n    Comment comment = CommentRepository.findById(id).get();\n    comment.setLikenum(comment.getLikenum()+1);\n    CommentRepository.save(comment);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n以上方法虽然实现起来比较简单，但是执行效率并不高，因为我只需要将点赞数加 1就可以了，没必要查询出所有字段修改后再更新所有字段。(蝴蝶效应)\n\n我们可以使用MongoTemplate类来实现对某列的操作。 （1）修改CommentService\n\n//注入MongoTemplate\n@Autowired\nprivate MongoTemplate mongoTemplate;\n\n/**\n * 点赞数+1\n *\n * @param id\n */\npublic void updateCommentLikenum(String id) {\n\n    //  查询条件\n    Query query = Query.query(Criteria.where("_id").is(id));\n    //  更新条件\n    Update update = new Update();\n    //局部更新，相当于$set\n//        update.set(key, value)\n    //递增$inc\n    update.inc("likenum");\n    //参数3：集合的名字或实体类的类型Comment.class\n    mongoTemplate.updateFirst(query, update, Comment.class);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n（ 2）测试用例：\n\n/**\n * 测试点赞数+1\n */\n@Test\npublic void testUpdateCommentLikenum() {\n    commentService.updateCommentLikenum("3");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n执行测试用例后，发现点赞数+1了：\n\n\n\n原文链接：https://wgy1993.gitee.io/archives/d69f1e.html',normalizedContent:'# 1. mongodb 相关概念\n\n\n# 1.1 业务应用场景\n\n传统的关系型数据库（如mysql），在数据操作的“三高”需求以及应对web2.0的网站需求面前，显得力不从心。\n\n解释：“三高”需求：\n\n * high performance - 对数据库高并发读写的需求。\n * huge storage - 对海量数据的高效率存储和访问的需求。\n * high scalability && high availability- 对数据库的高可扩展性和高可用性的需求。\n\n而mongodb可应对“三高”需求。\n\n具体的应用场景如：\n\n * 社交场景，使用 mongodb 存储存储用户信息，以及用户发表的朋友圈信息，通过地理位置索引实现附近的人、地点等功能。\n * 游戏场景，使用 mongodb 存储游戏用户信息，用户的装备、积分等直接以内嵌文档的形式存储，方便查询、高效率存储和访问。\n * 物流场景，使用 mongodb 存储订单信息，订单状态在运送过程中会不断更新，以 mongodb 内嵌数组的形式来存储，一次查询就能将订单所有的变更读取出来。\n * 物联网场景，使用 mongodb 存储所有接入的智能设备信息，以及设备汇报的日志信息，并对这些信息进行多维度的分析。\n * 视频直播，使用 mongodb 存储用户信息、点赞互动信息等。\n\n这些应用场景中，数据操作方面的共同特点是：\n\n * 数据量大\n * 写入操作频繁（读写都很频繁）\n * 价值较低的数据，对事务性要求不高\n\n对于这样的数据，我们更适合使用mongodb来实现数据的存储。\n\n什么时候选择mongodb\n\n在架构选型上，除了上述的三个特点外，如果你还犹豫是否要选择它？可以考虑以下的一些问题：\n\n * 应用不需要事务及复杂 join 支持\n * 新应用，需求会变，数据模型无法确定，想快速迭代开发\n * 应用需要2000-3000以上的读写qps（更高也可以）\n * 应用需要tb甚至 pb 级别数据存储\n * 应用发展迅速，需要能快速水平扩展\n * 应用要求存储的数据不丢失\n * 应用需要99.999%高可用\n * 应用需要大量的地理位置查询、文本查询\n\n如果上述有1个符合，可以考虑 mongodb，2个及以上的符合，选择 mongodb 绝不会后悔。\n\n\n# 1.2 mongodb 简介\n\nmongodb是一个开源、高性能、无模式的文档型数据库，当初的设计就是用于简化开发和方便扩展，是nosql数据库产品中的一种。是最像关系型数据库（mysql）的非关系型数据库。\n\n它支持的数据结构非常松散，是一种类似于 json 的 格式叫bson，所以它既可以存储比较复杂的数据类型，又相当的灵活。\n\nmongodb中的记录是一个文档，它是一个由字段和值对（field:value）组成的数据结构。mongodb文档类似于json对象，即一个文档认为就是一个对象。字段的数据类型是字符型，它的值除了使用基本的一些类型外，还可以包括其他文档、普通数组和文档数组。\n\n\n# 1.3 体系结构\n\nmysql和mongodb对比\n\n\n\nsql 术语/概念     mongodb术语/概念   解释/说明\ndatabase      database       数据库\ntable         collection     数据库表/集合\nrow           document       数据记录行/文档\ncolumn        field          数据字段/域\nindex         index          索引\ntable joins                  表连接,mongodb不支持\n              嵌入文档           mongodb通过嵌入式文档来替代多表连接\nprimary key   primary key    主键,mongodb自动将_id字段设置为主键\n\n\n# 1.4 数据模型\n\nmongodb的最小存储单位就是文档(document)对象。文档(document)对象对应于关系型数据库的行。数据在mongodb中以bson（binary-json）文档的格式存储在磁盘上。\n\nbson（binary serialized document format）是一种类json的一种二进制形式的存储格式，简称binary json。bson和json一样，支持内嵌的文档对象和数组对象，但是bson有json没有的一些数据类型，如date和bindata类型。\n\nbson采用了类似于 c 语言结构体的名称、对表示方法，支持内嵌的文档对象和数组对象，具有轻量性、可遍历性、高效性的三个特点，可以有效描述非结构化数据和结构化数据。这种格式的优点是灵活性高，但它的缺点是空间利用率不是很理想。\n\nbson中，除了基本的json类型：string,integer,boolean,double,null,array和object，mongo还使用了特殊的数据类型。这些类型包括date,object id,binary data,regular expression 和code。每一个驱动都以特定语言的方式实现了这些类型，查看你的驱动的文档来获取详细信息。\n\nbson数据类型参考列表：\n\n数据类型        描述                                         举例\n字符串         utf-8字符串都可表示为字符串类型的数据                      {“x” : “foobar”}\n对象id        对象id是文档的12字节的唯一 id                         {“x” :objectid() }\n布尔值         真或者假：true或者false                           {“x”:true}\n数组          值的集合或者列表可以表示成数组                            {“x” ： [“a”, “b”, “c”]}\n32位整数       类型不可用。javascript仅支持64位浮点数，所以32位整数会被自动转换。   shell是不支持该类型的，shell中默认会转换成64位浮点数\n64位整数       不支持这个类型。shell会使用一个特殊的内嵌文档来显示64位整数          shell是不支持该类型的，shell中默认会转换成64位浮点数\n64位浮点数      shell中的数字就是这一种类型                           {“x”：3.14159，”y”：3}\nnull        表示空值或者未定义的对象                               {“x”:null}\nundefined   文档中也可以使用未定义类型                              {“x”:undefined}\n符号          shell不支持，shell会将数据库中的符号类型的数据自动转换成字符串       \n正则表达式       文档中可以包含正则表达式，采用javascript的正则表达式语法          {“x” ： /foobar/i}\n代码          文档中还可以包含javascript代码                       {“x” ： function() { /* …… */ }}\n二进制数据       二进制数据可以由任意字节的串组成，不过shell中无法使用              \n最大值/最小值     bson包括一个特殊类型，表示可能的最大值。shell中没有这个类型。        \n\n提示：\n\nshell默认使用64位浮点型数值。{“x”：3.14}或{“x”：3}。对于整型值，可以使用numberint（4字节符号整数）或numberlong（8字节符号整数），{“x”:numberint(“3”)}{“x”:numberlong(“3”)}\n\n\n# 1.5 mongodb 的特点\n\nmongodb主要有如下特点：\n\n * 高性能：\n   \n   mongodb提供高性能的数据持久性。特别是\n   \n   对嵌入式数据模型的支持减少了数据库系统上的i/o活动。\n   \n   索引支持更快的查询，并且可以包含来自嵌入式文档和数组的键。（文本索引解决搜索的需求、ttl索引解决历史数据自动过期的需求、地理位置索引可用于构建各种 o2o 应用）\n   \n   mmapv1、wiredtiger、mongorocks（rocksdb）、in-memory 等多引擎支持满足各种场景需求。\n   \n   gridfs解决文件存储的需求。\n\n * 高可用性：\n   \n   mongodb的复制工具称为副本集（replica set），它可提供自动故障转移和数据冗余。\n\n * 高扩展性：\n   \n   mongodb提供了水平可扩展性作为其核心功能的一部分。\n   \n   分片将数据分布在一组集群的机器上。（海量数据存储，服务能力水平扩展）\n   \n   从3.4开始，mongodb支持基于片键创建数据区域。在一个平衡的集群中，mongodb将一个区域所覆盖的读写只定向到该区域内的那些片。\n\n * 丰富的查询支持：\n   \n   mongodb支持丰富的查询语言，支持读和写操作(crud)，比如数据聚合、文本搜索和地理空间查询等。\n\n * 其他特点：如无模式（动态模式）、灵活的文档模型、\n\n\n# 2. 单机部署\n\n\n# 2.1 windows 系统中的安装启动\n\n\n# 2.1.1 下载安装包\n\nmongodb 提供了可用于 32 位和 64 位系统的预编译二进制包，你可以从mongodb官网下载安装，mongodb 预编译二进制包下地址：https://www.mongodb.com/download-center#community\n\n\n\n根据上图所示下载 zip 包。\n\n提示：版本的选择：\n\nmongodb的版本命名规范如：x.y.z；\n\ny为奇数时表示当前版本为开发版，如：1.5.2、4.1.13；\n\ny为偶数时表示当前版本为稳定版，如：1.6.3、4.0.10；\n\nz是修正版本号，数字越大越好。\n\n详情： http://docs.mongodb.org/manual/release-notes/#release-version-numbers\n\n\n# 2.1.2 解压安装启动\n\n将压缩包解压到一个目录中。在解压目录中，手动建立一个目录用于存放数据文件，如 data/db\n\n方式1：命令行参数方式启动服务\n\n在 bin 目录中打开命令行提示符，输入如下命令：\n\nmongod --dbpath=..\\data\\db\n\n\n1\n\n\n我们在启动信息中可以看到， mongodb的默认端口是27017，如果我们想改变默认的启动端口，可以通过–port来指定端口。\n\n为了方便我们每次启动，可以将安装目录的bin目录设置到环境变量的path中， bin 目录下是一些常用命令，比如 mongod 启动服务用的，mongo 客户端连接服务用的。\n\n方式2：配置文件方式启动服务\n\n在解压目录中新建 config 文件夹，该文件夹中新建配置文件 mongod.conf ，内如参考如下：\n\nstorage:\n    #the directory where the mongod instance stores its data.default value is "\\data\\db" on windows.\n    dbpath: d:\\mongodb-win32-x86_64-2008plus-ssl-4.0.1\\data\n\n\n1\n2\n3\n\n\n详细配置项内容可以参考官方文档： https://docs.mongodb.com/manual/reference/configuration-options/\n\n【注意】\n\n * 配置文件中如果使用双引号，比如路径地址，自动会将双引号的内容转义。如果不转义，则会报错：\n   \n   error-parsing-yaml-config-file-yaml-cpp-error-at-line-3-column-15-unknown-escape-character-d\n   \n   \n   1\n   \n   \n   解决：\n   \n   a. 对 \\ 换成 / 或 \\\n   \n   b. 如果路径中没有空格，则无需加引号。\n\n * 配置文件中不能以tab分割字段\n   \n   解决：\n   \n   将其转换成空格。\n\n启动方式：\n\nmongod -f ../config/mongod.conf\n或\nmongod --config ../config/mongod.conf\n\n\n1\n2\n3\n\n\n更多参数配置：\n\nsystemlog:\n    destination: file\n    #the path of the log file to which mongod or mongos should send all diagnostic logging information\n    path: "d:/mongodb-win32-x86_64-2008plus-ssl-4.0.1/log/mongod.log"\n    logappend: true\nstorage:\n    journal:\n      enabled: true\n    #the directory where the mongod instance stores its data.default value is "/data/db".\n    dbpath: "d:/mongodb-win32-x86_64-2008plus-ssl-4.0.1/data"\nnet:\n    #bindip: 127.0.0.1\n    port: 27017\nsetparameter:\n    enablelocalhostauthbypass: false\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 2.2 shell 连接(mongo命令)\n\n在命令提示符输入以下shell命令即可完成登陆\n\nmongo\n或\nmongo --host=127.0.0.1 --port=27017\n\n\n1\n2\n3\n\n\n\n# 2.2.1 查看已经有的数据库\n\nshow databases\n\n\n1\n\n\n\n# 2.2.2 退出 mongodb\n\nexit\n\n\n1\n\n\n更多参数可以通过帮助查看：\n\nmongo --help\n\n\n1\n\n\n提示：\n\nmongodb javascript shell是一个基于javascript的解释器，故是支持js程序的。\n\n\n# 2.3 compass- 图形化界面客户端\n\n到mongodb官网下载mongodb compass，地址： https://www.mongodb.com/download-center/v2/compass?initial=true\n\n如果是下载安装版，则按照步骤安装；如果是下载加压缩版，直接解压，执行里面的mongodbcompasscommunity.exe 文件即可。\n\n在打开的界面中，输入主机地址、端口等相关信息，点击连接：\n\n\n\n\n# 2.4 linux 系统中的安装启动和连接\n\n目标：在linux中部署一个单机的mongodb，作为生产环境下使用。\n\n提示：和windows下操作差不多。\n\n步骤如下：\n\n * 先到官网下载压缩包 mongod -linux-x86_64-4.0.10.tgz 。\n\n * 上传压缩包到linux中，解压到当前目录：\n   \n   tar -xvf mongodb-linux-x86_64-4.0.10.tgz\n   \n   \n   1\n   \n\n * 移动解压后的文件夹到指定的目录中：\n   \n   mv mongodb-linux-x86_64-4.0.10 /usr/local/mongodb\n   \n   \n   1\n   \n\n * 新建几个目录，分别用来存储数据和日志：\n   \n   #数据存储目录\n   mkdir -p /mongodb/single/data/db\n   #日志存储目录\n   mkdir -p /mongodb/single/log\n   \n   \n   1\n   2\n   3\n   4\n   \n\n * 新建并修改配置文件\n   \n   vi /mongodb/single/mongod.conf\n   \n   \n   1\n   \n   \n   配置文件的内容如下：\n   \n   systemlog:\n         #mongodb发送所有日志输出的目标指定为文件\n         # #the path of the log file to which mongod or mongos should send all diagnostic logging information\n         destination: file\n         #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n         path: "/mongodb/single/log/mongod.log"\n         #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n         logappend: true\n   storage:\n         #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n         ##the directory where the mongod instance stores its data.default value is "/data/db".\n         dbpath: "/mongodb/single/data/db"\n         journal:\n             #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n             enabled: true\n   processmanagement:\n         #启用在后台运行mongos或mongod进程的守护进程模式。\n         fork: true\n   net:\n         #服务实例绑定的ip，默认是localhost\n         bindip: localhost,192.168.142.128\n         #bindip\n         #绑定的端口，默认是27017\n         port: 27017\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   17\n   18\n   19\n   20\n   21\n   22\n   23\n   24\n   \n\n * 启动mongodb服务\n   \n   /usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf\n   \n   \n   1\n   \n   \n   注意：\n   \n   如果启动后不是 successfully ，则是启动失败了。原因基本上就是配置文件有问题。\n   \n   通过进程来查看服务是否启动了：\n   \n   ps -ef |grep mongod\n   \n   \n   1\n   \n\n * 分别使用mongo命令和compass工具来连接测试。\n   \n   提示：如果远程连接不上，需要配置防火墙放行，或直接关闭linux防火墙\n   \n   #查看防火墙状态\n   systemctl status firewalld\n   #临时关闭防火墙\n   systemctl stop firewalld\n   #开机禁止启动防火墙\n   systemctl disable firewalld\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   \n\n * 停止关闭服务\n   \n   停止服务的方式有两种：快速关闭和标准关闭，下面依次说明：\n   \n   （一）快速关闭方法（快速，简单，数据可能会出错）\n   \n   目标：通过系统的kill命令直接杀死进程：\n   \n   杀完要检查一下，避免有的没有杀掉。\n   \n   #通过进程编号关闭节点\n   kill -2 54410\n   \n   \n   1\n   2\n   \n   \n   【补充】\n   \n   如果一旦是因为数据损坏，则需要进行如下操作（了解）：\n   \n   1）删除lock文件：\n   \n   rm -f /mongodb/single/data/db/*.lock\n   \n   \n   1\n   \n   \n   2 ）修复数据：\n   \n   /usr/local/mongdb/bin/mongod --repair --dbpath=/mongodb/single/data/db\n   \n   \n   1\n   \n   \n   （二）标准的关闭方法（数据不容易出错，但麻烦）：\n   \n   目标：通过mongo客户端中的shutdownserver命令来关闭服务\n   \n   主要的操作步骤参考如下：\n   \n   //客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\n   mongo --port 27017\n   //#切换到admin库\n   use admin\n   //关闭服务\n   db.shutdownserver()\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   \n\n\n# 3. 基本常用命令\n\n\n# 3.1 案例需求\n\n存放文章评论的数据存放到mongodb中，数据结构参考如下：\n\n数据库：articledb\n\n专栏文章评论           comment                     \n字段名称             字段含义      字段类型              备注\n_id              id        objectid或string   mongo的主键的字段\narticleid        文章id      string            \ncontent          评论内容      string            \nuserid           评论人id     string            \nnickname         评论人昵称     string            \ncreatedatetime   评论的日期时间   date              \nlikenum          点赞数       int32             \nreplynum         回复数       int32             \nstate            状态        string            0：不可见；1：可见；\nparentid         上级id      string            如果为0表示文章的顶级评论\n\n\n# 3.2 数据库操作\n\n\n# 3.2.1 选择和创建数据库\n\n选择和创建数据库的语法格式：\n\nuse 数据库名称\n\n\n1\n\n\n如果数据库不存在则自动创建，例如，以下语句创建 articledb 数据库：\n\nuse articledb\n\n\n1\n\n\n查看有权限的所有的数据库命令\n\nshow dbs\n或\nshow databases\n\n\n1\n2\n3\n\n\n> 注意 : 在 mongodb 中，集合只有在内容插入后才会创建! 就是说，创建集合(数据表)后要再插入一个文档(记录)，集合才会真正创建。\n\n查看当前正在使用的数据库命令\n\ndb\n\n\n1\n\n\nmongodb 中默认的数据库为 test，如果你没有选择数据库，集合将存放在 test 数据库中。\n\n另外：\n\n数据库名可以是满足以下条件的任意utf-8字符串。\n\n * 不能是空字符串（ “”)。\n * 不得含有 ‘ ‘（空格)、.、$、/、\\和\\0 (空字符)。\n * 应全部小写。\n * 最多 64字节。\n\n有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。\n\n * admin ： 从权限的角度来看，这是”root”数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。\n * local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合\n * config : 当mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。\n\n\n# 3.2.2 数据库的删除\n\nmongodb 删除数据库的语法格式如下：\n\ndb.dropdatabase()\n\n\n1\n\n\n提示：主要用来删除已经持久化的数据库\n\n\n# 3.3 集合操作\n\n集合，类似关系型数据库中的表。\n\n可以显示的创建，也可以隐式的创建。\n\n\n# 3.3.1 集合的显式创建\n\n基本语法格式：\n\ndb.createcollection(name)\n\n\n1\n\n\n参数说明：\n\n * name: 要创建的集合名称\n\n例如：创建一个名为 mycollection 的普通集合。\n\ndb.createcollection("mycollection")\n\n\n1\n\n\n查看当前库中的表： show tables命令\n\nshow collections\n或\nshow tables\n\n\n1\n2\n3\n\n\n集合的命名规范：\n\n * 集合名不能是空字符串 “”。\n * 集合名不能含有 \\0字符（空字符)，这个字符表示集合名的结尾。\n * 集合名不能以 “system.”开头，这是为系统集合保留的前缀。\n * 用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现$。\n\n\n# 3.3.2 集合的隐式创建\n\n当向一个集合中插入一个文档的时候，如果集合不存在，则会自动创建集合。\n\n详见 文档的插入 章节。\n\n提示：通常我们使用隐式创建文档即可。\n\n\n# 3.3.3 集合的删除\n\n集合删除语法格式如下：\n\ndb.collection.drop()\n或\ndb.集合.drop()\n\n\n1\n2\n3\n\n\n返回值\n\n如果成功删除选定集合，则 drop() 方法返回 true，否则返回 false。\n\n例如：要删除mycollection集合\n\ndb.mycollection.drop()\n\n\n1\n\n\n\n# 3.4 文档基本crud\n\n文档（document）的数据结构和 json 基本一样。\n\n所有存储在集合中的数据都是 bson 格式。\n\n\n# 3.4.1 文档的插入\n\n# 3.4.1.1 单个文档插入\n\n使用insert() 或 save() 方法向集合中插入文档，语法如下：\n\ndb.collection.insert(\n    <document or array of documents>,\n    {\n        writeconcern: <document>,\n        ordered: <boolean>\n    }\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n参数：\n\nparameter      type                description\ndocument       document or array   要插入到集合中的文档或文档数组。（(json格式）\nwriteconcern   document            optional. a document expressing the write concern . omit to\n                                   use the default write concern. see write concern .do not\n                                   explicitly set the write concern for the operation if run in\n                                   a transaction. to use write concern with transactions, see\n                                   transactions and write concern\nordered        boolean             可选。如果为真，则按顺序插入数组中的文档，如果其中一个文档出现错误，mongodb将返回而不处理数组中的其余文档。如果为假，则执行无序插入，如果其中一个文档出现错误，则继续处理数组中的主文档。在版本2.6+中默认为true\n\n【示例】\n\n要向comment的集合(表)中插入一条测试数据：\n\ndb.comment.insert({"articleid":"100000","content":"今天天气真好，阳光明媚","userid":"1001","nickname":"rose","createdatetime":new date(),"likenum":numberint(10),"state":null})\n\n\n1\n\n\n提示：\n\n * comment集合如果不存在，则会隐式创建\n * mongo中的数字，默认情况下是double类型，如果要存整型，必须使用函数numberint(整型数字)，否则取出来就有问题了。\n * 插入当前日期使用 new date()\n * 插入的数据没有指定 _id ，会自动生成主键值\n * 如果某字段没值，可以赋值为null，或不写该字段。\n\n执行后，如下，说明插入一个数据成功了。\n\nwriteresult({ "ninserted" : 1 })\n\n\n1\n\n\n注意：\n\n * 文档中的键/值对是有序的。\n * 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。\n * mongodb区分类型和大小写。\n * mongodb的文档不能有重复的键。\n * 文档的键是字符串。除了少数例外情况，键可以使用任意utf-8字符。\n\n文档键命名规范：\n\n * 键不能含有 \\0 (空字符)。这个字符用来表示键的结尾。\n * . 和$有特别的意义，只有在特定环境下才能使用。\n * 以下划线 “_”开头的键是保留的(不是严格要求的)。\n\n# 3.4.1.2 批量插入\n\n语法：\n\ndb.collection.insertmany(\n     [ <document 1> , <document 2>, ... ],\n     {\n         writeconcern: <document>,\n         ordered: <boolean>\n     }\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n【示例】\n\n批量插入多条文章评论：\n\ndb.comment.insertmany([\n    {"_id":"1","articleid":"100001","content":"我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。","userid":"1002","nickname":"相忘于江湖","createdatetime":new date("2019-08-05t22:08:15.522z"),"likenum":numberint(1000),"state":"1"},\n    {"_id":"2","articleid":"100001","content":"我夏天空腹喝凉开水，冬天喝温开水","userid":"1005","nickname":"伊人憔悴","createdatetime":new date("2019-08-05t23:58:51.485z"),"likenum":numberint(888),"state":"1"},\n    {"_id":"3","articleid":"100001","content":"我一直喝凉开水，冬天夏天都喝。","userid":"1004","nickname":"杰克船长","createdatetime":new date("2019-08-06t01:05:06.321z"),"likenum":numberint(666),"state":"1"},\n    {"_id":"4","articleid":"100001","content":"专家说不能空腹吃饭，影响健康。","userid":"1003","nickname":"凯撒","createdatetime":new date("2019-08-06t08:18:35.288z"),"likenum":numberint(2000),"state":"1"},\n    {"_id":"5","articleid":"100001","content":"研究表明，刚烧开的水千万不能喝，因为烫嘴。","userid":"1003","nickname":"凯撒","createdatetime":new date("2019-08-06t11:01:02.521z"),"likenum":numberint(3000),"state":"1"}\n\n]);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n提示：\n\n插入时指定了 _id ，则主键就是该值。\n\n如果某条数据插入失败，将会终止插入，但已经插入成功的数据不会回滚掉。\n\n因为批量插入由于数据较多容易出现失败，因此，可以使用try catch进行异常捕捉处理，测试的时候可以不处理。如（了解）：\n\ntry {\n    db.comment.insertmany([\n        {"_id":"1","articleid":"100001","content":"我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。","userid":"1002","nickname":"相忘于江湖","createdatetime":new date("2019-08-05t22:08:15.522z"),"likenum":numberint(1000),"state":"1"},\n        {"_id":"2","articleid":"100001","content":"我夏天空腹喝凉开水，冬天喝温开水","userid":"1005","nickname":"伊人憔悴","createdatetime":new date("2019-08-05t23:58:51.485z"),"likenum":numberint(888),"state":"1"},\n        {"_id":"3","articleid":"100001","content":"我一直喝凉开水，冬天夏天都喝。","userid":"1004","nickname":"杰克船长","createdatetime":new date("2019-08-06t01:05:06.321z"),"likenum":numberint(666),"state":"1"},\n        {"_id":"4","articleid":"100001","content":"专家说不能空腹吃饭，影响健康。","userid":"1003","nickname":"凯撒","createdatetime":new date("2019-08-06t08:18:35.288z"),"likenum":numberint(2000),"state":"1"},\n        {"_id":"5","articleid":"100001","content":"研究表明，刚烧开的水千万不能喝，因为烫嘴。","userid":"1003","nickname":"凯撒","createdatetime":new date("2019-08-06t11:01:02.521z"),"likenum":numberint(3000),"state":"1"}\n\n    ]);\n\n} catch (e) {\n    print (e);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 3.4.2 文档的基本查询\n\n查询数据的语法格式如下：\n\ndb.collection.find(<query>, [projection])\n\n\n1\n\n\n参数：\n\nparameter    type       description\nquery        document   可选。使用查询运算符指定选择筛选器。若要返回集合中的所有文档，请省略此参数或传递空文档( {} )。\nprojection   document   可选。指定要在与查询筛选器匹配的文档中返回的字段（投影）。若要返回匹配文档中的所有字段，请省略此参数。\n\n【示例】\n\n# 3.4.2.1 查询所有\n\n如果我们要查询comment集合的所有文档，我们输入以下命令\n\ndb.comment.find()\n或\ndb.comment.find({})\n\n\n1\n2\n3\n\n\n这里你会发现每条文档会有一个叫 _id的字段，这个相当于我们原来关系数据库中表的主键，当你在插入文档记录时没有指定该字段，mongodb会自动创建，其类型是objectid类型。\n\n如果我们在插入文档记录时指定该字段也可以，其类型可以是objectid类型，也可以是mongodb支持的任意类型。\n\n如果我想按一定条件来查询，比如我想查询userid为1003的记录，怎么办？很简单！只 要在find()中添加参数即可，参数也是json格式，如下：\n\ndb.comment.find({userid:\'1003\'})\n\n\n1\n\n\n如果你只需要返回符合条件的第一条数据，我们可以使用findone命令来实现，语法和find一样。\n\n如：查询用户编号是1003的记录，但只最多返回符合条件的第一条记录：\n\ndb.comment.findone({userid:\'1003\'})\n\n\n1\n\n\n# 3.4.2.2 投影查询（projection query）\n\n如果要查询结果返回部分字段，则需要使用投影查询（不显示所有字段，只显示指定的字段）。\n\n如：查询结果只显示 _id 、userid、nickname :\n\ndb.comment.find({userid:"1003"},{userid:1,nickname:1})\n\n\n1\n\n\n默认 _id 会显示。\n\n如：查询结果只显示 、 userid、nickname ，不显示 _id ：\n\ndb.comment.find({userid:"1003"},{userid:1,nickname:1,_id:0})\n\n\n1\n\n\n再例如：查询所有数据，但只显示 _id 、userid、nickname :\n\ndb.comment.find({},{userid:1,nickname:1})\n\n\n1\n\n\n\n# 3.4.3 文档的更新\n\n更新文档的语法：\n\ndb.collection.update(query, update, options)\n//或\ndb.collection.update(\n    <query>,\n    <update>,\n    {\n        upsert: <boolean>,\n        multi: <boolean>,\n        writeconcern: <document>,\n        collation: <document>,\n        arrayfilters: [ <filterdocument1>, ... ],\n        hint:  <document|string>     // available starting in mongodb 4.2\n    }\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n参数：\n\nparameter   type       description\nquery       document   更新的选择条件。可以使用与find（）方法中相同的查询选择器，类似sql\n                       update查询内where后面的。。在3.0版中进行了更改：当使用upsert:true执行update（）时，如果查询使用点表示法在_id\n                       字段上指定条件，则mongodb将拒绝插入新文档。\nmulti       boolean    可选。如果设置为true，则更新符合查询条件的多个文档。如果设置为false，则更新一个文档。默认值为false。\n\n【示例】\n\n# 3.4.3.1 覆盖的修改\n\n如果我们想修改_id为1的记录，点赞量为1001，输入以下语句：\n\ndb.comment.update({_id:"1"},{likenum:numberint(1001)})\n\n\n1\n\n\n执行后，我们会发现，这条文档除了 likenum字段其它字段都不见了，\n\n# 3.4.3.2 局部修改\n\n为了解决这个问题，我们需要使用修改器$set来实现，命令如下：\n\n我们想修改_id为2的记录，浏览量为889，输入以下语句：\n\ndb.comment.update({_id:"2"},{$set:{likenum:numberint(889)}})\n\n\n1\n\n\n# 3.4.3.3 批量的修改\n\n更新所有用户为 1003 的用户的昵称为 凯撒大帝 。\n\n//默认只修改第一条数据\ndb.comment.update({userid:"1003"},{$set:{nickname:"凯撒2"}})\n//修改所有符合条件的数据\ndb.comment.update({userid:"1003"},{$set:{nickname:"凯撒大帝"}},{multi:true})\n\n\n1\n2\n3\n4\n\n\n提示：如果不加后面的参数，则只更新符合条件的第一条记录\n\n# 3.4.3.4 列值增长的修改\n\n如果我们想实现对某列值在原有值的基础上进行增加或减少，可以使用 $inc 运算符来实现。\n\n需求：对3号数据的点赞数，每次递增1\n\ndb.comment.update({_id:"3"},{$inc:{likenum:numberint(1)}})\n\n\n1\n\n\n\n# 3.4.4 删除文档\n\n删除文档的语法结构：\n\ndb.集合名称.remove(条件)\n\n\n1\n\n\n以下语句可以将数据全部删除，请慎用\n\ndb.comment.remove({})\n\n\n1\n\n\n如果删除 _id=1的记录，输入以下语句\n\ndb.comment.remove({_id:"1"})\n\n\n1\n\n\n\n# 3.5 文档的分页查询\n\n\n# 3.5.1 统计查询\n\n统计查询使用count()方法，语法如下：\n\ndb.collection.count(query, options)\n\n\n1\n\n\n参数：\n\nparameter   type       description\nquery       document   查询选择条件\noptions     document   可选。用于修改计数的额外选项\n\n【示例】\n\n# 3.5.1.1 统计所有记录数\n\n统计comment集合的所有的记录数：\n\ndb.comment.count()\n\n\n1\n\n\n# 3.5.1.2 按条件统计记录数\n\n例如：统计userid为1003的记录条数\n\ndb.comment.count({userid:"1003"})\n\n\n1\n\n\n提示：\n\n默认情况下 count() 方法返回符合条件的全部记录条数。\n\n\n# 3.5.2 分页列表查询\n\n可以使用limit()方法来读取指定数量的数据，使用skip()方法来跳过指定数量的数据。\n\n基本语法如下所示：\n\ndb.collection_name.find().limit(number).skip(number)\n\n\n1\n\n\n如果你想返回指定条数的记录，可以在find方法后调用limit来返回结果(topn)，默认值20，例如：\n\ndb.comment.find().limit(3)\n\n\n1\n\n\nskip 方法同样接受一个数字参数作为跳过的记录条数。（前n个不要）,默认值是0\n\ndb.comment.find().skip(3)\n\n\n1\n\n\n分页查询：需求：每页2个，第二页开始：跳过前两条数据，接着值显示3和4条数据\n\n//第一页\ndb.comment.find().skip(0).limit(2)\n//第二页\ndb.comment.find().skip(2).limit(2)\n//第三页\ndb.comment.find().skip(4).limit(2)\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3.5.3 排序查询\n\nsort() 方法对数据进行排序，sort() 方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1为升序排列，而 -1 是用于降序排列。\n\n语法如下所示：\n\ndb.collection_name.find().sort({key:1})\n或\ndb.集合名称.find().sort(排序方式)\n\n\n1\n2\n3\n\n\n例如：\n\n对userid降序排列，并对访问量进行升序排列\n\ndb.comment.find().sort({userid:-1,likenum:1})\n\n\n1\n\n\n提示：\n\nskip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()，和命令编写顺序无关。\n\n\n# 3.6 文档的更多查询\n\n\n# 3.6.1 正则的复杂条件查询\n\nmongodb的模糊查询是通过正则表达式的方式实现的。格式为：\n\ndb.collection.find({field:/正则表达式/})\n或\ndb.集合.find({字段:/正则表达式/})\n\n\n1\n2\n3\n\n\n提示：正则表达式是 js的语法，直接量的写法。\n\n例如，我要查询评论内容包含“开水”的所有文档，代码如下：\n\ndb.comment.find({content:/开水/})\n\n\n1\n\n\n如果要查询评论的内容中以 “专家”开头的，代码如下：\n\ndb.comment.find({content:/^专家/})\n\n\n1\n\n\n\n# 3.6.2 比较查询\n\n<, <=, >, >= 这个操作符也是很常用的，格式如下:\n\ndb.集合名称.find({ "field" : { $gt: value }}) // 大于: field > value\ndb.集合名称.find({ "field" : { $lt: value }}) // 小于: field < value\ndb.集合名称.find({ "field" : { $gte: value }}) // 大于等于: field >= value\ndb.集合名称.find({ "field" : { $lte: value }}) // 小于等于: field <= value\ndb.集合名称.find({ "field" : { $ne: value }}) // 不等于: field != value\n\n\n1\n2\n3\n4\n5\n\n\n示例：查询评论点赞数量大于 700的记录\n\ndb.comment.find({likenum:{$gt:numberint(700)}})\n\n\n1\n\n\n\n# 3.6.3 包含查询\n\n包含使用$in操作符。 示例：查询评论的集合中userid字段包含1003或1004的文档\n\ndb.comment.find({userid:{$in:["1003","1004"]}})\n\n\n1\n\n\n不包含使用 $nin操作符。 示例：查询评论集合中userid字段不包含1003和1004的文档\n\ndb.comment.find({userid:{$nin:["1003","1004"]}})\n\n\n1\n\n\n\n# 3.6.4 条件连接查询\n\n我们如果需要查询同时满足两个以上条件，需要使用$and操作符将条件进行关联。（相当于sql的and） 格式为：\n\n$and:[ {  },{  },{ } ]\n\n\n1\n\n\n示例：查询评论集合中 likenum大于等于700 并且小于2000的文档：\n\ndb.comment.find({$and:[{likenum:{$gte:numberint(700)}},{likenum:{$lt:numberint(2000)}}]})\n\n\n1\n\n\n如果两个以上条件之间是或者的关系，我们使用操作符进行关联，与前面 and的使用方式相同格式为：\n\n$or:[ {  },{  },{   } ]\n\n\n1\n\n\n示例：查询评论集合中 userid为1003，或者点赞数小于1000的文档记录\n\ndb.comment.find({$or:[ {userid:"1003"} ,{likenum:{$lt:1000} }]})\n\n\n1\n\n\n\n# 3.7 常用命令小结\n\n选择切换数据库：use articledb\n插入数据：db.comment.insert({bson数据})\n查询所有数据：db.comment.find();\n条件查询数据：db.comment.find({条件})\n查询符合条件的第一条记录：db.comment.findone({条件})\n查询符合条件的前几条记录：db.comment.find({条件}).limit(条数)\n查询符合条件的跳过的记录：db.comment.find({条件}).skip(条数)\n修改数据：db.comment.update({条件},{修改后的数据}) 或db.comment.update({条件},{$set:{要修改部分的字段:数据})\n修改数据并自增某字段值：db.comment.update({条件},{$inc:{自增的字段:步进值}})\n删除数据：db.comment.remove({条件})\n统计查询：db.comment.count({条件})\n模糊查询：db.comment.find({字段名:/正则表达式/})\n条件比较运算：db.comment.find({字段名:{$gt:值}})\n包含查询：db.comment.find({字段名:{$in:[值1，值2]}})或db.comment.find({字段名:{$nin:[值1，值2]}})\n条件连接查询：db.comment.find({$and:[{条件1},{条件2}]})或db.comment.find({$or:[{条件1},{条件2}]})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 4. 索引-index\n\n\n# 4.1 概述\n\n索引支持在mongodb中高效地执行查询。如果没有索引，mongodb必须执行全集合扫描，即扫描集合中的每个文档，以选择与查询语句匹配的文档。这种扫描全集合的查询效率是非常低的，特别在处理大量的数据时，查询可以要花费几十秒甚至几分钟，这对网站的性能是非常致命的。\n\n如果查询存在适当的索引，mongodb可以使用该索引限制必须检查的文档数。\n\n索引是特殊的数据结构，它以易于遍历的形式存储集合数据集的一小部分。索引存储特定字段或一组字段的值，按字段值排序。索引项的排序支持有效的相等匹配和基于范围的查询操作。此外，mongodb还可以使用索引中的排序返回排序结果。\n\n官网文档： https://docs.mongodb.com/manual/indexes/\n\n了解：\n\nmongodb索引使用b树数据结构（确切的说是b-tree，mysql是b+tree）\n\n\n# 4.2 索引的类型\n\n\n# 4.2.1 单字段索引\n\nmongodb支持在文档的单个字段上创建用户定义的升序/降序索引，称为单字段索引（single field index）。\n\n对于单个字段索引和排序操作，索引键的排序顺序（即升序或降序）并不重要，因为mongodb可以在任何方向上遍历索引。\n\n\n\n\n# 4.2.2 复合索引\n\nmongodb还支持多个字段的用户定义索引，即复合索引（compound index）。\n\n复合索引中列出的字段顺序具有重要意义。例如，如果复合索引由 { userid: 1, score: -1 } 组成，则索引首先按userid正序排序，然后在每个userid的值内，再在按score倒序排序。\n\n\n\n\n# 4.2.3 其他索引\n\n地理空间索引（geospatial index）、文本索引（text indexes）、哈希索引（hashed indexes）。\n\n * 地理空间索引（geospatial index）\n   \n   为了支持对地理空间坐标数据的有效查询，mongodb提供了两种特殊的索引：返回结果时使用平面几何的二维索引和返回结果时使用球面几何的二维球面索引。\n\n * 文本索引（text indexes）\n   \n   mongodb提供了一种文本索引类型，支持在集合中搜索字符串内容。这些文本索引不存储特定于语言的停止词（例如“the”、“a”、“or”），而将集合中的词作为词干，只存储根词。\n\n * 哈希索引（hashed indexes）\n   \n   为了支持基于散列的分片，mongodb提供了散列索引类型，它对字段值的散列进行索引。这些索引在其范围内的值分布更加随机，但只支持相等匹配，不支持基于范围的查询。\n\n\n# 4.3 索引的管理操作\n\n\n# 4.3.1 索引的查看\n\n说明：\n\n返回一个集合中的所有索引的数组。\n\n语法：\n\ndb.collection.getindexes()\n\n\n1\n\n\n提示：该语法命令运行要求是 mongodb 3.0+\n\n【示例】\n\n查看comment集合中所有的索引情况\n\n> db.comment.getindexes()\n[\n   {\n        "v" : 2,\n        "key" : {\n            "_id" : 1\n       },\n        "name" : "_id_",\n        "ns" : "articledb.comment"\n   }\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n结果中显示的是默认 _id 索引。\n\n默认_id索引：\n\nmongodb在创建集合的过程中，在_id 字段上创建一个唯一的索引，默认名字为_id_，该索引可防止客户端插入两个具有相同值的文档，您不能在_id字段上删除此索引。\n\n注意：该索引是唯一索引，因此值不能重复，即 _id 值不能重复的。在分片集群中，通常使用 _id 作为片键。\n\n\n# 4.3.2 索引的创建\n\n说明：在集合上创建索引。\n\n语法：\n\ndb.collection.createindex(keys, options)\n\n\n1\n\n\n参数：\n\nparameter   type       description\nkeys        document   包含字段和值对的文档，其中字段是索引键，值描述该字段的索引类型。对于字段上的升序索引，请\n                       指定值1；对于降序索引，请指定值-1。比如： { 字段:1或-1} ，其中1 为指定按升序创建索引，如果你\n                       想按降序来创建索引指定为 -1 即可。另外，mongodb支持几种不同的索引类型，包括文本、地理空 间和哈希索引。\noptions     document   可选。包含一组控制索引创建的选项的文档。有关详细信息，请参见选项详情列表。\n\noptions（更多选项）列表：\n\nparameter            type            description\nbackground           boolean         建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 “background”\n                                     可选参数。 “background” 默认值为false。\nunique               boolean         建立的索引是否唯一。指定为true创建唯一索引。默认值为false.\nname                 string          索引的名称。如果未指定，mongodb的通过连接索引的字段名和排序顺序生成一个索引名 称。\ndropdups             boolean         3.0+版本已废弃。在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false.\nsparse               boolean         对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为true的话，在索\n                                     引字段中不会查询出不包含对应字段的文档.。默认值为 false.\nexpireafterseconds   integer         指定一个以秒为单位的数值，完成 ttl设定，设定集合的生存时间。\nv                    index version   索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本。\nweights              document        索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。\ndefault_language     string          对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语\nlanguage_override    string          对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language.\n\n提示： 注意在 3.0.0 版本前创建索引方法为 db.collection.ensureindex() ，之后的版本使用了 db.collection.createindex() 方法，ensureindex() 还能用，但只是 createindex() 的别名。\n\n【示例】\n\n# 4.3.2.1 单字段索引\n\n示例：对 userid 字段建立索引：\n\ndb.comment.createindex({userid:1})\n\n\n1\n\n\n参数 1：按升序创建索引\n\ncompass查看：\n\n\n\n# 4.3.2.2 复合索引\n\n对 userid 和 nickname 同时建立复合（compound）索引：\n\ndb.comment.createindex({userid:1,nickname:-1})\n\n\n1\n\n\ncompass 中：\n\n\n\n\n# 4.3.3 索引的移除\n\n说明：可以移除指定的索引，或移除所有索引\n\n# 4.3.3.1 指定索引的移除\n\n语法：\n\ndb.collection.dropindex(index)\n\n\n1\n\n\n参数：\n\nparameter   type                 description\nindex       string or document   指定要删除的索引。可以通过索引名称或索引规范文档指定索引。若要删除文本索引，请指定 索引名称。\n\n【示例】\n\n删除 comment 集合中 userid 字段上的升序索引：\n\ndb.comment.dropindex({userid:1})\n\n\n1\n\n\n# 4.3.3.2 所有索引的移除\n\n语法：\n\ndb.collection.dropindexes()\n\n\n1\n\n\n【示例】\n\n删除 comment 集合中所有索引。\n\ndb.comment.dropindexes()\n\n\n1\n\n\n提示： _id 的字段的索引是无法删除的，只能删除非 _id 字段的索引。\n\n\n# 4.4 索引的使用\n\n\n# 4.4.1 执行计划\n\n分析查询性能（analyze query performance）通常使用执行计划（解释计划、explain plan）来查看查询的情况，如查询耗费的时间、是否基于索引查询等。\n\n那么，通常，我们想知道，建立的索引是否有效，效果如何，都需要通过执行计划查看。\n\n语法：\n\ndb.collection.find(query,options).explain(options)\n\n\n1\n\n\n【示例】\n\n查看根据userid查询数据的情况：\n\n> db.comment.find({userid:"1003"}).explain()\n{\n    "queryplanner" : {\n        "plannerversion" : 1,\n        "namespace" : "articledb.comment",\n        "indexfilterset" : false,\n        "parsedquery" : {\n            "userid" : {\n                "$eq" : "1003"\n            }\n        },\n        "winningplan" : {\n            "stage" : "collscan",\n            "filter" : {\n                "userid" : {\n                    "$eq" : "1003"\n                }\n            },\n            "direction" : "forward"\n        },\n        "rejectedplans" : [ ]\n   },\n   "serverinfo" : {\n       "host" : "9ef3740277ad",\n       "port" : 27017,\n       "version" : "4.0.10",\n       "gitversion" : "c389e7f69f637f7a1ac3cc9fae843b635f20b766"\n   },\n   "ok" : 1\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n关键点看： “stage” : “collscan”, 表示全集合扫描\n\n\n\n下面对userid建立索引\n\ndb.comment.createindex({userid:1})\n\n\n1\n\n\n再次查看执行计划：\n\n> db.comment.find({userid:"1013"}).explain()\n{\n    "queryplanner" : {\n        "plannerversion" : 1,\n        "namespace" : "articledb.comment",\n        "indexfilterset" : false,\n        "parsedquery" : {\n            "userid" : {\n                "$eq" : "1013"\n            }\n        },\n        "winningplan" : {\n            "stage" : "fetch",\n            "inputstage" : {\n                "stage" : "ixscan",\n                "keypattern" : {\n                    "userid" : 1\n                },\n                "indexname" : "userid_1",\n                "ismultikey" : false,\n                "multikeypaths" : {\n                    "userid" : [ ]\n                 },\n                "isunique" : false,\n                "issparse" : false,\n                "ispartial" : false,\n                "indexversion" : 2,\n                "direction" : "forward",\n                "indexbounds" : {\n                    "userid" : [\n                        "[\\"1013\\", \\"1013\\"]"\n                    ]\n                }\n            }\n        },\n        "rejectedplans" : [ ]\n    },\n    "serverinfo" : {\n        "host" : "9ef3740277ad",\n        "port" : 27017,\n        "version" : "4.0.10",\n        "gitversion" : "c389e7f69f637f7a1ac3cc9fae843b635f20b766"\n    },\n    "ok" : 1\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n\n\n关键点看： “stage” : “ixscan” ,基于索引的扫描\n\ncompass查看：\n\n\n\n\n# 4.4.2 涵盖的查询\n\ncovered queries\n\n当查询条件和查询的投影仅包含索引字段时，mongodb直接从索引返回结果，而不扫描任何文档或将文档带入内存。 这些覆盖的查询可以非常有效。\n\n\n\n更多：https://docs.mongodb.com/manual/core/query-optimization/#read-operations-covered-query\n\n【示例】\n\n> db.comment.find({userid:"1003"},{userid:1,_id:0})\n{ "userid" : "1003" }\n{ "userid" : "1003" }\n> db.comment.find({userid:"1003"},{userid:1,_id:0}).explain()\n{\n    "queryplanner" : {\n        "plannerversion" : 1,\n        "namespace" : "articledb.comment",\n        "indexfilterset" : false,\n        "parsedquery" : {\n            "userid" : {\n                "$eq" : "1003"\n            }\n        },\n        "winningplan" : {\n            "stage" : "projection",\n            "transformby" : {\n                "userid" : 1,\n                "_id" : 0\n            },\n            "inputstage" : {\n                "stage" : "ixscan",\n                "keypattern" : {\n                    "userid" : 1\n                },\n                "indexname" : "userid_1",\n                "ismultikey" : false,\n                "multikeypaths" : {\n                    "userid" : [ ]\n                },\n                "isunique" : false,\n                "issparse" : false,\n                "ispartial" : false,\n                "indexversion" : 2,\n                "direction" : "forward",\n                "indexbounds" : {\n                    "userid" : [\n                        "[\\"1003\\", \\"1003\\"]"\n                    ]\n                }\n            }\n        },\n        "rejectedplans" : [ ]\n    },\n    "serverinfo" : {\n        "host" : "bobohost.localdomain",\n        "port" : 27017,\n        "version" : "4.0.10",\n        "gitversion" : "c389e7f69f637f7a1ac3cc9fae843b635f20b766"\n    },\n    "ok" : 1\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\ncompass 中：\n\n\n\n\n# 5. 文章评论\n\n\n# 5.1 需求分析\n\n某头条的文章评论业务如下：\n\n\n\n文章示例参考：早晨空腹喝水，是对还是错？ https://www.toutiao.com/a6721476546088927748/\n\n需要实现以下功能：\n\n * 基本增删改查api\n * 根据文章id查询评论\n * 评论点赞\n\n\n# 5.2 表结构分析\n\n数据库：articledb\n\n专栏文章评论           comment                     \n字段名称             字段含义      字段类型              备注\n_id              id        objectid或string   mongo的主键的字段\narticleid        文章id      string            \ncontent          评论内容      string            \nuserid           评论人id     string            \nnickname         评论人昵称     string            \ncreatedatetime   评论的日期时间   date              \nlikenum          点赞数       int32             \nreplynum         回复数       int32             \nstate            状态        string            0：不可见；1：可见；\nparentid         上级id      string            如果为0表示文章的顶级评论\n\n\n# 5.3 技术选型\n\n\n# 5.3.1 mongodb-driver\n\nmongodb-driver是mongo官方推出的java连接mongodb的驱动包，相当于jdbc驱动。我们通过一个入门的案例来了解mongodb-driver的基本使用。\n\n官方驱动说明和下载： http://mongodb.github.io/mongo-java-driver/\n\n官方驱动示例文档：http://mongodb.github.io/mongo-java-driver/3.8/driver/getting-started/quick-start/\n\n\n# 5.3.2 springdatamongodb\n\nspringdata家族成员之一，用于操作mongodb的持久层框架，封装了底层的mongodb-driver。\n\n官网主页： https://projects.spring.io/spring-data-mongodb/\n\n我们十次方项目的吐槽微服务就采用springdatamongodb框架。\n\n\n# 5.4 文章微服务模块搭建\n\n（1）搭建项目工程article，pom.xml引入依赖：\n\n<?xml version="1.0" encoding="utf-8"?>\n<project xmlns="http://maven.apache.org/pom/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n         xsi:schemalocation="http://maven.apache.org/pom/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelversion>4.0.0</modelversion>\n    <parent>\n        <groupid>org.springframework.boot</groupid>\n        <artifactid>spring-boot-starter-parent</artifactid>\n        <version>2.1.6.release</version>\n        <relativepath/>\n    </parent>\n    <groupid>com.wgy</groupid>\n    <artifactid>article</artifactid>\n    <version>1.0-snapshot</version>\n    <dependencies>\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-test</artifactid>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupid>org.springframework.boot</groupid>\n            <artifactid>spring-boot-starter-data-mongodb</artifactid>\n        </dependency>\n    </dependencies>\n</project>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n（ 2）创建application.yml\n\nspring:\n  #数据源配置\n  data:\n    mongodb:\n      # 主机地址\n      #host: 192.168.142.128\n      # 数据库\n      #database: articledb\n      # 默认端口是27017\n      #port: 27017\n      #也可以使用uri连接\n      uri: mongodb://192.168.142.128:27017/articledb\nserver:\n  # 端口\n  port: 80\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n（ 3）创建启动类\n\npackage com.wgy;\n\nimport org.springframework.boot.springapplication;\nimport org.springframework.boot.autoconfigure.springbootapplication;\n\n/**\n * 启动类\n *\n * @author wgy\n */\n@springbootapplication\npublic class articleapplication {\n\n    public static void main(string[] args) {\n        springapplication.run(articleapplication.class, args);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n（4）启动项目，看是否能正常启动，控制台没有错误。\n\n\n# 5.5 文章评论实体类的编写\n\n创建实体类 创建包com.wgy.article，包下建包po用于存放实体类，创建实体类\n\n/**\n * 评论实体类\n *\n * @author wgy\n */\n//把一个java类声明为mongodb的文档，可以通过collection参数指定这个类对应的文档。\n//@document(collection="mongodb 对应 collection 名")\n// 若未加 @document ，该 bean save 到 mongo 的 comment collection\n// 若添加 @document ，则 save 到 comment collection\n@document(collection = "comment")//可以省略，如果省略，则默认使用类名小写映射集合\n//复合索引\n// @compoundindex( def = "{\'userid\': 1, \'nickname\': -1}")\npublic class comment implements serializable {\n\n    //主键标识，该属性的值会自动对应mongodb的主键字段"_id"，如果该属性名就叫“id”,则该注解可以省略，否则必须写\n    @id\n    private string id;//主键    //该属性对应mongodb的字段的名字，如果一致，则无需该注解\n    @field("content")\n    private string content;//吐槽内容\n    private date publishtime;//发布日期    //添加了一个单字段的索引\n    @indexed\n    private string userid;//发布人id\n    private string nickname;//昵称\n    private localdatetime createdatetime;//评论的日期时间\n    private integer likenum;//点赞数\n    private integer replynum;//回复数\n    private string state;//状态\n    private string parentid;//上级id\n    private string articleid;\n\n    //set/get/tostring...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n说明：\n\n索引可以大大提升查询效率，一般在查询字段上添加索引，索引的添加可以通过mongo的命令来添加，也可以在java的实体类中通过注解添加。\n\n1）单字段索引注解@indexed\n\norg.springframework.data.mongodb.core.index.indexed.class\n\n声明该字段需要索引，建索引可以大大的提高查询效率。\n\nmongo命令参考：\n\ndb.comment.createindex({"userid":1})\n\n\n1\n\n\n2 ）复合索引注解@compoundindex\n\norg.springframework.data.mongodb.core.index.compoundindex.class\n\n复合索引的声明，建复合索引可以有效地提高多字段的查询效率。\n\nmongo命令参考：\n\ndb.comment.createindex({"userid":1,"nickname":-1})\n\n\n1\n\n\n\n# 5.6 文章评论的基本增删改查\n\n（1）创建数据访问接口 com.wgy.article包下创建dao包，包下创建接口\n\n/**\n * 评论的持久层接口\n *\n * @author wgy\n */\npublic interface commentrepository extends mongorepository<comment, string> {\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n（ 2）创建业务逻辑类 com.wgy.article包下创建service包，包下创建类\n\n/**\n * 评论的业务层\n *\n * @author wgy\n */\n@service\npublic class commentservice {\n\n    //注入dao\n    @autowired\n    private commentrepository commentrepository;\n\n    /**\n     * 保存一个评论\n     *\n     * @param comment\n     */\n    public void savecomment(comment comment) {\n        //如果需要自定义主键，可以在这里指定主键；如果不指定主键，mongodb会自动生成主键\n        //设置一些默认初始值。。。\n        //调用dao\n        commentrepository.save(comment);\n    }\n\n    /**\n     * 更新评论\n     *\n     * @param comment\n     */\n    public void updatecomment(comment comment) {\n        //调用dao\n        commentrepository.save(comment);\n    }\n\n    /**\n     * 根据id删除评论\n     *\n     * @param id\n     */\n    public void deletecommentbyid(string id) {\n        //调用dao\n        commentrepository.deletebyid(id);\n    }\n\n    /**\n     * 查询所有评论\n     *\n     * @return\n     */\n    public list<comment> findcommentlist() {\n        //调用dao\n        return commentrepository.findall();\n    }\n\n    /**\n     * 根据id查询评论\n     *\n     * @param id\n     * @return\n     */\n    public comment findcommentbyid(string id) {\n        //调用dao\n        return commentrepository.findbyid(id).get();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\n（ 3）新建junit测试类，测试保存和查询所有：\n\n/**\n * 评论测试类\n *\n * @author wgy\n */\n@runwith(springrunner.class)\n@springboottest\npublic class commentservicetest {\n\n    @autowired\n    private commentservice commentservice;\n\n    /**\n     * 查询所有数据\n     */\n    @test\n    public void testfindcommentlist() {\n        list<comment> commentlist = commentservice.findcommentlist();\n        system.out.println(commentlist);\n    }\n\n    /**\n     * 测试根据id查询\n     */\n    @test\n    public void testfindcommentbyid() {\n        comment commentbyid = commentservice.findcommentbyid("1");\n        system.out.println(commentbyid);\n    }\n\n    /**\n     * 保存一个评论\n     */\n    @test\n    public void testsavecomment() {\n        comment comment = new comment();\n        comment.setarticleid("100000");\n        comment.setcontent("测试添加的数据");\n        comment.setcreatedatetime(localdatetime.now());\n        comment.setuserid("1003");\n        comment.setnickname("凯撒大帝");\n        comment.setstate("1");\n        comment.setlikenum(0);\n        comment.setreplynum(0);\n        commentservice.savecomment(comment);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# 5.7 根据上级id查询文章评论的分页列表\n\n（1）commentrepository新增方法定义\n\n/**\n * 根据父id，查询子评论的分页列表\n *\n * @param parentid\n * @param pageable\n * @return\n */\npage<comment> findbyparentid(string parentid, pageable pageable);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n（2）commentservice新增方法\n\n/**\n * 根据父id查询分页列表\n *\n * @param parentid\n * @param page\n * @param size\n * @return\n */\npublic page<comment> findcommentlistbyparentid(string parentid, int page, int size) {\n    return commentrepository.findbyparentid(parentid, pagerequest.of(page - 1, size));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n（3）junit测试用例：\n\n/**\n * 测试根据父id查询子评论的分页列表\n */\n@test\npublic void testfindcommentlistbyparentid() {\n    page<comment> pageresponse = commentservice.findcommentlistbyparentid("3", 1, 2);\n    system.out.println("----总记录数：" + pageresponse.gettotalelements());\n    system.out.println("----当前页数据：" + pageresponse.getcontent());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n（4）测试\n\n使用compass快速插入一条测试数据，数据的内容是对3号评论内容进行评论。\n\n\n\n执行测试，结果：\n\n----总记录数：1\n----当前页数据：[comment{id=\'33\', content=\'你年轻，火力大\', publishtime=null, userid=\'1003\', nickname=\'凯撒大帝\',createdatetime=null, likenum=null, replynum=null, state=\'null\', parentid=\'3\', articleid=\'100001\'}]\n\n\n1\n2\n\n\n\n# 5.8 mongotemplate 实现评论点赞\n\n我们看一下以下点赞的临时示例代码： commentservice 新增updatethumbup方法\n\n/**\n * 点赞-效率低\n * @param id\n */\npublic void updatecommentthumbuptoincrementingold(string id){\n    comment comment = commentrepository.findbyid(id).get();\n    comment.setlikenum(comment.getlikenum()+1);\n    commentrepository.save(comment);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n以上方法虽然实现起来比较简单，但是执行效率并不高，因为我只需要将点赞数加 1就可以了，没必要查询出所有字段修改后再更新所有字段。(蝴蝶效应)\n\n我们可以使用mongotemplate类来实现对某列的操作。 （1）修改commentservice\n\n//注入mongotemplate\n@autowired\nprivate mongotemplate mongotemplate;\n\n/**\n * 点赞数+1\n *\n * @param id\n */\npublic void updatecommentlikenum(string id) {\n\n    //  查询条件\n    query query = query.query(criteria.where("_id").is(id));\n    //  更新条件\n    update update = new update();\n    //局部更新，相当于$set\n//        update.set(key, value)\n    //递增$inc\n    update.inc("likenum");\n    //参数3：集合的名字或实体类的类型comment.class\n    mongotemplate.updatefirst(query, update, comment.class);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n（ 2）测试用例：\n\n/**\n * 测试点赞数+1\n */\n@test\npublic void testupdatecommentlikenum() {\n    commentservice.updatecommentlikenum("3");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n执行测试用例后，发现点赞数+1了：\n\n\n\n原文链接：https://wgy1993.gitee.io/archives/d69f1e.html',charsets:{cjk:!0}},{title:"服务器虚拟化VMware ESXI搭建集群",frontmatter:{title:"服务器虚拟化VMware ESXI搭建集群",date:"2022-12-16T15:57:11.000Z",permalink:"/pages/26f193/",categories:["运维","系统","vmware"],tags:[null],readingShow:"top",description:"VMwarev Center Server 提供了一个可伸缩、可扩展的平台，为虚拟化管理奠定了基础。可集中管理VMware vSphere环境，与其他管理平台相比，极大地提高了 IT 管理员对虚拟环境的控制。",meta:[{name:"image",content:"https://img-blog.csdnimg.cn/20201106091846329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F5eTk3MDUyNQ==,size_16,color_FFFFFF,t_70#pic_center"},{name:"twitter:title",content:"服务器虚拟化VMware ESXI搭建集群"},{name:"twitter:description",content:"VMwarev Center Server 提供了一个可伸缩、可扩展的平台，为虚拟化管理奠定了基础。可集中管理VMware vSphere环境，与其他管理平台相比，极大地提高了 IT 管理员对虚拟环境的控制。"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://img-blog.csdnimg.cn/20201106091846329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F5eTk3MDUyNQ==,size_16,color_FFFFFF,t_70#pic_center"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/01.vmware/01.%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%99%9A%E6%8B%9F%E5%8C%96VMware%20ESXI%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4.html"},{property:"og:type",content:"article"},{property:"og:title",content:"服务器虚拟化VMware ESXI搭建集群"},{property:"og:description",content:"VMwarev Center Server 提供了一个可伸缩、可扩展的平台，为虚拟化管理奠定了基础。可集中管理VMware vSphere环境，与其他管理平台相比，极大地提高了 IT 管理员对虚拟环境的控制。"},{property:"og:image",content:"https://img-blog.csdnimg.cn/20201106091846329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F5eTk3MDUyNQ==,size_16,color_FFFFFF,t_70#pic_center"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/01.vmware/01.%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%99%9A%E6%8B%9F%E5%8C%96VMware%20ESXI%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-16T15:57:11.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"服务器虚拟化VMware ESXI搭建集群"},{itemprop:"description",content:"VMwarev Center Server 提供了一个可伸缩、可扩展的平台，为虚拟化管理奠定了基础。可集中管理VMware vSphere环境，与其他管理平台相比，极大地提高了 IT 管理员对虚拟环境的控制。"},{itemprop:"image",content:"https://img-blog.csdnimg.cn/20201106091846329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F5eTk3MDUyNQ==,size_16,color_FFFFFF,t_70#pic_center"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/01.vmware/01.%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%99%9A%E6%8B%9F%E5%8C%96VMware%20ESXI%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4.html",relativePath:"04.运维/09.系统/01.vmware/01.服务器虚拟化VMware ESXI搭建集群.md",key:"v-447429d0",path:"/pages/26f193/",headers:[{level:2,title:"服务器虚拟化VMware ESXI搭建集群",slug:"服务器虚拟化vmware-esxi搭建集群",normalizedTitle:"服务器虚拟化vmware esxi搭建集群",charIndex:2},{level:2,title:"摘要",slug:"摘要",normalizedTitle:"摘要",charIndex:28},{level:2,title:"环境和工具准备",slug:"环境和工具准备",normalizedTitle:"环境和工具准备",charIndex:35},{level:2,title:"服务器安装虚拟化 VMware ESXI",slug:"服务器安装虚拟化-vmware-esxi",normalizedTitle:"服务器安装虚拟化 vmware esxi",charIndex:47},{level:2,title:"创建虚拟机操作系统",slug:"创建虚拟机操作系统",normalizedTitle:"创建虚拟机操作系统",charIndex:72},{level:2,title:"Windows server 2012 R2安装",slug:"windows-server-2012-r2安装",normalizedTitle:"windows server 2012 r2安装",charIndex:86},{level:2,title:"在Windows Server 2012 R2 上面安装vcenter",slug:"在windows-server-2012-r2-上面安装vcenter",normalizedTitle:"在windows server 2012 r2 上面安装vcenter",charIndex:115}],headersStr:"服务器虚拟化VMware ESXI搭建集群 摘要 环境和工具准备 服务器安装虚拟化 VMware ESXI 创建虚拟机操作系统 Windows server 2012 R2安装 在Windows Server 2012 R2 上面安装vcenter",content:"# 服务器虚拟化VMware ESXI搭建集群\n\n * 摘要\n\n * 环境和工具准备\n\n * 服务器安装虚拟化 VMware ESXI\n\n * 创建虚拟机操作系统\n\n * Windows server 2012 R2安装\n\n * 在Windows Server 2012 R2 上面安装vcenter\n\n\n# 摘要\n\nVMwarev Center Server 提供了一个可伸缩、可扩展的平台，为虚拟化管理奠定了基础。可集中管理VMware vSphere环境，与其他管理平台相比，极大地提高了 IT 管理员对虚拟环境的控制。\n\nVMware vCenter Server：提高在虚拟基础架构每个级别上的集中控制和可见性，通过主动管理发挥 vSphere 潜能，是一个具有广泛合作伙伴体系支持的可伸缩、可扩展平台。\n\n无论您拥有十几个虚拟机，还是几千个虚拟机，VMware vCenter Server 都是管理 VMware vSphere 最简单、最有效的方法。借助 VMware vCenter Server，可从单个控制台统一管理数据中心的所有主机和虚拟机，该控制台聚合了集群、主机和虚拟机的性能监控功能。 VMware vCenter Server 使管理员能够从一个位置深入了解虚拟基础架构的集群、主机、虚拟机、存储、客户操作系统和其他关键组件等所有信息。\n\n借助VMware vCenter Server，虚拟化环境变得更易于管理，一个管理员就能管理 100 个以上的工作负载，在管理物理基础架构方面的工作效率比通常情况提高了一倍。\n\n\n# 环境和工具准备\n\n1.服务器（空机）\n\n2.VMware ESXI 6.7.0 刻录的光盘\n\n3.显示屏（接线带全）\n\n4.Windows Server 2012 R2的刻录光盘\n\n5.VMware-VIM-all-6.7.0.iso\n\n6.最好带一个U盘（最好是16GB的，越大反而越好）\n\n\n# 服务器安装虚拟化 VMware ESXI\n\n1.进服务器的光盘驱动模式进入，一般会输入BISO密码，进去之后就是此页面\n\n\n\n2.进入缓冲区，等待\n\n\n\n3.进入欢迎界面，按回车继续\n\n\n\n4.进入安装许可协议界面，按F11继续\n\n\n\n5.系统会自动检查可用存储设备，如果有需要，自己手动选择，按上下键，选中之后按回车继续，注意ESXI会格式化整个硬盘\n\n\n\n6.这里一般选择US default（美式）键盘，按回车继续\n\n\n\n7.设置root密码，注意这里必须是标准化的密码，简单的通过不了，然后按回车继续\n\n\n\n8.配置信息完成后，来到安装界面，然后按F11开始安装\n\n\n\n9.等待安装过程\n\n\n\n10.安装完成，按回车重启\n\n\n\n11.进入登录界面\n\n\n\n12.然后按F2键，进入root账号登录界面\n\n\n\n13.进入选项界面：Configure Password （配置root密码） Configure lockdown mode（配置锁定模式）Configure Management Network （配置网络）Restart Management Network （重启网络）Test Management Network （使用ping测试网络）Network Restore Options （还原配置）Troubleshooting Options （故障排查选项）View System Logs （查看系统日志） Reset System Conf iguration ESXi （出厂设置）\n\n\n\n14.首先我们重置密码,选中然后回车\n\n\n\n15.进入网络配置里面，进行配置，选项有：Network Adapters （网络适配器） VLAN（opt ional）IPv4 Configuration （IPv4 地址配置） IPv6 Configuration （IPv6地址 配置） DNS Configuration （DNS配置） Custom DNS Suffixes （自定义DNS）\n\n\n\n16.首先我们配置网络适配器，上下键移动，空格选中目标，回车确认，自动返回网络配置页面\n\n\n\n17.进入IPv4地址配置，配置完成后，按回车确定\n\n\n\n18.配置完成后，网卡配置需要确认，按Y确认，\n\n\n\n19.然后去网页上登录，输入你自己的IP地址即可，输入用户（root）和密码\n\n\n\n20.输入用户和密码，进入虚拟化平台\n\n\n\n\n# 创建虚拟机操作系统\n\n1.我们进入虚拟化平台，首先要分配许可证，点击主机管理>许可>分配许可证，许可证书可在网上查询\n\n许可证：VMware vSphere 6 Enterprise Plus ：0A65P-00HD0-3Z5M1-M097M-22P7H\n\n\n\n2.上传系统的镜像，点击存储>数据存储>数据存储浏览器>上载>选择镜像\n\n\n\n3.创建虚拟机\n\n\n\n\n\n\n\n\n\n\n\n\n\n创建完成后，打开虚拟机，进行操作系统的安装\n\n\n# Windows server 2012 R2安装\n\n因为安装过程过于简单，所以只说一下步骤操作\n\n1.将Windows Server 2012 R2的光盘放入光驱，打开服务器，选择从光驱启动，然后进入启动界面\n\n2.等待安装程序启动之后进入输入语言和其他选项的设置页面，要安装的语言、时间和货币都选择中文，键盘和输入方法选微软拼音，点击下一步\n\n3.进入现在安装界面，点击现在安装\n\n4.进入产品安装密钥，在网页上查找一个，手动输入即可，点击下一步\n\n5.进入安装操作系统版本界面，因为我们是要使用的vcenter图形版进行管理，所以我们选择带有GUI的服务器，点击下一步\n\n6.进入许可条款，点击我接受许可条款，点击下一步\n\n7.进入安装类型界面，我们选择自定义安装，以便我们来合理的给硬盘分区\n\n8.进入硬盘分区界面，我们选中要从哪里分空间，然后新建一个分区，然后格式化，然后点击下一步\n\n9.进入安装界面，等待安装结束即可，安装完成点击立即重启\n\n10.重启进入设置密码的界面，一默认的用户都是Administrator，设置完成点击确定\n\n11.通过提示的进入登录页面，输入密码，进入系统\n\n\n# 在Windows Server 2012 R2 上面安装vcenter\n\n1.把vcenter镜像拷贝到桌面上，然后双击，镜像就会到我的电脑的出现\n\n\n\n2.安装会出现错误，首先安装一些插件和补丁，经验所得，安装一下的东西\n\nwindows8.1-kb2919355-x64_e6f4da4d33564419065a7370865faacf9b40ff72.msu\n\nwindows8.1-kb2919442-x64_f97d8290d9d75d96f163095c4cb05e1b9f6986e0.msu\n\nwindows8.1-kb2932046-x64_6aee5fda6e2a6729d1fbae6eac08693acd70d985.msu\n\nWindows8.1-KB2934018-x64.msu\n\nwindows8.1-kb2937592-x64_4abc0a39c9e500c0fbe9c41282169c92315cafc2.msu\n\nwindows8.1-kb2938439-x64_3ed1574369e36b11f37af41aa3a875a115a3eac1 (1).msu\n\nWindows8.1-KB2959977-x64.msu\n\nWindows8.1-KB2999226-x64.msu\n\n安装C++环境\n\n网页版的flashpayerpp插件\n\n3.下载完成即可安装\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n输入刚才设置的IP地址访问\n\n\n\n原文链接：https://blog.csdn.net/qyy970525/article/details/109844335",normalizedContent:"# 服务器虚拟化vmware esxi搭建集群\n\n * 摘要\n\n * 环境和工具准备\n\n * 服务器安装虚拟化 vmware esxi\n\n * 创建虚拟机操作系统\n\n * windows server 2012 r2安装\n\n * 在windows server 2012 r2 上面安装vcenter\n\n\n# 摘要\n\nvmwarev center server 提供了一个可伸缩、可扩展的平台，为虚拟化管理奠定了基础。可集中管理vmware vsphere环境，与其他管理平台相比，极大地提高了 it 管理员对虚拟环境的控制。\n\nvmware vcenter server：提高在虚拟基础架构每个级别上的集中控制和可见性，通过主动管理发挥 vsphere 潜能，是一个具有广泛合作伙伴体系支持的可伸缩、可扩展平台。\n\n无论您拥有十几个虚拟机，还是几千个虚拟机，vmware vcenter server 都是管理 vmware vsphere 最简单、最有效的方法。借助 vmware vcenter server，可从单个控制台统一管理数据中心的所有主机和虚拟机，该控制台聚合了集群、主机和虚拟机的性能监控功能。 vmware vcenter server 使管理员能够从一个位置深入了解虚拟基础架构的集群、主机、虚拟机、存储、客户操作系统和其他关键组件等所有信息。\n\n借助vmware vcenter server，虚拟化环境变得更易于管理，一个管理员就能管理 100 个以上的工作负载，在管理物理基础架构方面的工作效率比通常情况提高了一倍。\n\n\n# 环境和工具准备\n\n1.服务器（空机）\n\n2.vmware esxi 6.7.0 刻录的光盘\n\n3.显示屏（接线带全）\n\n4.windows server 2012 r2的刻录光盘\n\n5.vmware-vim-all-6.7.0.iso\n\n6.最好带一个u盘（最好是16gb的，越大反而越好）\n\n\n# 服务器安装虚拟化 vmware esxi\n\n1.进服务器的光盘驱动模式进入，一般会输入biso密码，进去之后就是此页面\n\n\n\n2.进入缓冲区，等待\n\n\n\n3.进入欢迎界面，按回车继续\n\n\n\n4.进入安装许可协议界面，按f11继续\n\n\n\n5.系统会自动检查可用存储设备，如果有需要，自己手动选择，按上下键，选中之后按回车继续，注意esxi会格式化整个硬盘\n\n\n\n6.这里一般选择us default（美式）键盘，按回车继续\n\n\n\n7.设置root密码，注意这里必须是标准化的密码，简单的通过不了，然后按回车继续\n\n\n\n8.配置信息完成后，来到安装界面，然后按f11开始安装\n\n\n\n9.等待安装过程\n\n\n\n10.安装完成，按回车重启\n\n\n\n11.进入登录界面\n\n\n\n12.然后按f2键，进入root账号登录界面\n\n\n\n13.进入选项界面：configure password （配置root密码） configure lockdown mode（配置锁定模式）configure management network （配置网络）restart management network （重启网络）test management network （使用ping测试网络）network restore options （还原配置）troubleshooting options （故障排查选项）view system logs （查看系统日志） reset system conf iguration esxi （出厂设置）\n\n\n\n14.首先我们重置密码,选中然后回车\n\n\n\n15.进入网络配置里面，进行配置，选项有：network adapters （网络适配器） vlan（opt ional）ipv4 configuration （ipv4 地址配置） ipv6 configuration （ipv6地址 配置） dns configuration （dns配置） custom dns suffixes （自定义dns）\n\n\n\n16.首先我们配置网络适配器，上下键移动，空格选中目标，回车确认，自动返回网络配置页面\n\n\n\n17.进入ipv4地址配置，配置完成后，按回车确定\n\n\n\n18.配置完成后，网卡配置需要确认，按y确认，\n\n\n\n19.然后去网页上登录，输入你自己的ip地址即可，输入用户（root）和密码\n\n\n\n20.输入用户和密码，进入虚拟化平台\n\n\n\n\n# 创建虚拟机操作系统\n\n1.我们进入虚拟化平台，首先要分配许可证，点击主机管理>许可>分配许可证，许可证书可在网上查询\n\n许可证：vmware vsphere 6 enterprise plus ：0a65p-00hd0-3z5m1-m097m-22p7h\n\n\n\n2.上传系统的镜像，点击存储>数据存储>数据存储浏览器>上载>选择镜像\n\n\n\n3.创建虚拟机\n\n\n\n\n\n\n\n\n\n\n\n\n\n创建完成后，打开虚拟机，进行操作系统的安装\n\n\n# windows server 2012 r2安装\n\n因为安装过程过于简单，所以只说一下步骤操作\n\n1.将windows server 2012 r2的光盘放入光驱，打开服务器，选择从光驱启动，然后进入启动界面\n\n2.等待安装程序启动之后进入输入语言和其他选项的设置页面，要安装的语言、时间和货币都选择中文，键盘和输入方法选微软拼音，点击下一步\n\n3.进入现在安装界面，点击现在安装\n\n4.进入产品安装密钥，在网页上查找一个，手动输入即可，点击下一步\n\n5.进入安装操作系统版本界面，因为我们是要使用的vcenter图形版进行管理，所以我们选择带有gui的服务器，点击下一步\n\n6.进入许可条款，点击我接受许可条款，点击下一步\n\n7.进入安装类型界面，我们选择自定义安装，以便我们来合理的给硬盘分区\n\n8.进入硬盘分区界面，我们选中要从哪里分空间，然后新建一个分区，然后格式化，然后点击下一步\n\n9.进入安装界面，等待安装结束即可，安装完成点击立即重启\n\n10.重启进入设置密码的界面，一默认的用户都是administrator，设置完成点击确定\n\n11.通过提示的进入登录页面，输入密码，进入系统\n\n\n# 在windows server 2012 r2 上面安装vcenter\n\n1.把vcenter镜像拷贝到桌面上，然后双击，镜像就会到我的电脑的出现\n\n\n\n2.安装会出现错误，首先安装一些插件和补丁，经验所得，安装一下的东西\n\nwindows8.1-kb2919355-x64_e6f4da4d33564419065a7370865faacf9b40ff72.msu\n\nwindows8.1-kb2919442-x64_f97d8290d9d75d96f163095c4cb05e1b9f6986e0.msu\n\nwindows8.1-kb2932046-x64_6aee5fda6e2a6729d1fbae6eac08693acd70d985.msu\n\nwindows8.1-kb2934018-x64.msu\n\nwindows8.1-kb2937592-x64_4abc0a39c9e500c0fbe9c41282169c92315cafc2.msu\n\nwindows8.1-kb2938439-x64_3ed1574369e36b11f37af41aa3a875a115a3eac1 (1).msu\n\nwindows8.1-kb2959977-x64.msu\n\nwindows8.1-kb2999226-x64.msu\n\n安装c++环境\n\n网页版的flashpayerpp插件\n\n3.下载完成即可安装\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n输入刚才设置的ip地址访问\n\n\n\n原文链接：https://blog.csdn.net/qyy970525/article/details/109844335",charsets:{cjk:!0}},{title:"proftpd环境部署",frontmatter:{title:"proftpd环境部署",date:"2022-12-21T17:59:19.000Z",permalink:"/pages/affcf2/",categories:["运维","系统","ftp"],tags:[null],readingShow:"top",description:"Proftpd的全称是Professional FTP daemon，是针对Wu-FTP的弱项而开发的，软件在经过多年的发展之后完善了很多功能，ProFTP已经成为继Wu-FTP之后最为流行的FTP服务器软件，越来越多的站点选用它构筑安全高效的FTP站点。Proftpd软件和vsftpd一样是一个开放源代码的ftp服务器软件，但是可配置项比vsftpd要多，是目前比较流行的ftp软件，Proftpd的配置和apache的配置相似，因此该软件也十分容易配置和管理。",meta:[{name:"image",content:"https://img2018.cnblogs.com/blog/907596/201907/907596-20190712120948336-1260009529.png"},{name:"twitter:title",content:"proftpd环境部署"},{name:"twitter:description",content:"Proftpd的全称是Professional FTP daemon，是针对Wu-FTP的弱项而开发的，软件在经过多年的发展之后完善了很多功能，ProFTP已经成为继Wu-FTP之后最为流行的FTP服务器软件，越来越多的站点选用它构筑安全高效的FTP站点。Proftpd软件和vsftpd一样是一个开放源代码的ftp服务器软件，但是可配置项比vsftpd要多，是目前比较流行的ftp软件，Proftpd的配置和apache的配置相似，因此该软件也十分容易配置和管理。"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://img2018.cnblogs.com/blog/907596/201907/907596-20190712120948336-1260009529.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/02.ftp/01.proftpd%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2.html"},{property:"og:type",content:"article"},{property:"og:title",content:"proftpd环境部署"},{property:"og:description",content:"Proftpd的全称是Professional FTP daemon，是针对Wu-FTP的弱项而开发的，软件在经过多年的发展之后完善了很多功能，ProFTP已经成为继Wu-FTP之后最为流行的FTP服务器软件，越来越多的站点选用它构筑安全高效的FTP站点。Proftpd软件和vsftpd一样是一个开放源代码的ftp服务器软件，但是可配置项比vsftpd要多，是目前比较流行的ftp软件，Proftpd的配置和apache的配置相似，因此该软件也十分容易配置和管理。"},{property:"og:image",content:"https://img2018.cnblogs.com/blog/907596/201907/907596-20190712120948336-1260009529.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/02.ftp/01.proftpd%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-21T17:59:19.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"proftpd环境部署"},{itemprop:"description",content:"Proftpd的全称是Professional FTP daemon，是针对Wu-FTP的弱项而开发的，软件在经过多年的发展之后完善了很多功能，ProFTP已经成为继Wu-FTP之后最为流行的FTP服务器软件，越来越多的站点选用它构筑安全高效的FTP站点。Proftpd软件和vsftpd一样是一个开放源代码的ftp服务器软件，但是可配置项比vsftpd要多，是目前比较流行的ftp软件，Proftpd的配置和apache的配置相似，因此该软件也十分容易配置和管理。"},{itemprop:"image",content:"https://img2018.cnblogs.com/blog/907596/201907/907596-20190712120948336-1260009529.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/02.ftp/01.proftpd%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2.html",relativePath:"04.运维/09.系统/02.ftp/01.proftpd环境部署.md",key:"v-64021198",path:"/pages/affcf2/",headers:[{level:2,title:"proftpd介绍",slug:"proftpd介绍",normalizedTitle:"proftpd介绍",charIndex:2},{level:2,title:"proftp安装记录",slug:"proftp安装记录",normalizedTitle:"proftp安装记录",charIndex:1141}],headersStr:"proftpd介绍 proftp安装记录",content:'# proftpd介绍\n\nProftpd的全称是Professional FTP daemon，是针对Wu-FTP的弱项而开发的，软件在经过多年的发展之后完善了很多功能，ProFTP已经成为继Wu-FTP之后最为流行的FTP服务器软件，越来越多的站点选用它构筑安全高效的FTP站点。Proftpd软件和vsftpd一样是一个开放源代码的ftp服务器软件，但是可配置项比vsftpd要多，是目前比较流行的ftp软件，Proftpd的配置和apache的配置相似，因此该软件也十分容易配置和管理。\n\nProftpd和VSftpd二者区别\n===== vsftpd =====  \n全称Very secure FTP daemon。比ProFTPD 具有更高的安全性。vsftpd使用一般身份启动服务，降低了FTP服务的PID权限，使该服务即使被入侵也无法得到有效的系统管理权限。同时vsftpd利用chroot软件来改变登录者的根目录，使登陆者只能在这个目录中活动，限制了登录者的执行权限。vsftpd通过配置vsftpd.conf文件来完成部署，设定简单，登录者仅分为anonymous和real user 两种。可以使用standalone和super daemon的方式启动。vsftpd无法控制每个目录的流量、不能控制上传和下载的比例、不能针对不同的登陆者进行不同的权限设定。\n\n===== Proftpd=====\n由于Proftpd在自身的原始码中已经包含了所需要的执行指令，不需要Linux系统本机的执行程序的支持，所以在系统安全上更为安全。配置简单且灵活，安装后只需要设定proftpd.conf一个配置文件即可，可配置性更强。可以使用stand-alone或者super daemon方式来启动ftp服务。Proftpd可以控制上下传比例，实现流量控制，针对不同的目录设定不同的权限。登录者分为anonymous和real user两种。\n\nProftpd比较好用的功能有以下几点：\n1）目录访问权限配置灵活，配置简单。\n2）能够不依赖系统用户，可以使用独立的虚拟用户系统（使用过Serv-U的朋友应该深有体会，配置非常方便，对原有系统环境影响较小）\n3）对中文的支持良好，完美解决vsftpd中文引号bug。因为vsftpd在中文支持方面存在bug，对中文中一些字符的支持不是很好（比如对中文的双引号支持不是很好）。\n\nFTP部署的背景\n公司四个部门分别为运维部、开发部、销售部、行政部：\n1）各部门用户访问FTP后可以看到所有目录，仅可以访问本部门的目录；\n2）需要FTP日志功能；\n3）FTP认证方式基于文件认证方式；\n4）共享目录：/var/ftp;\n\n\n# proftp安装记录\n\n测试机器为Centos7.5，iptables和selinux均关闭\n \n1）下载ProFTP\n[root@localhost ~]# wget ftp://ftp.proftpd.org/distrib/source/proftpd-1.3.6.tar.gz\n[root@localhost ~]# tar -zvxf proftpd-1.3.6.tar.gz -C /usr/src/\n[root@localhost ~]# cd  /usr/src/proftpd-1.3.6/\n[root@localhost proftpd-1.3.6]# ./configure  --prefix=/usr/local/proftpd  --sysconfdir=/etc/  --enable-nls  --enable-openssl  --enable-shadow\n \n==================================================================================================================\n编译参数说明（可通过"./configure --help" 查看帮助选项）：\n--prefix=PREFIX         指定安装路径（--prefix=/usr/local/)              \n--sysconfdir=DIR        指定FTP服务配置文件路径(--sysconfdir=/etc)                  \n--localstatedir=DIR     指定运行状态的文件存放位置(默认/var/proftpd)                 \n--with-modules=mod_ldap 指定加载功能模块                        \n--enable-memcache       支持缓存功能                          \n--enable-nls            支持多语言环境（如中文），安装完成后在主配置文件中需要指定字符编码（UseEncoding UTF-8 CP936） \n--enable-openssl        支持TLS加密FTP服务                        \n--enable-shadow         支持使用/etc/shadow验证用户密码\n \n==================================================================================================================\n \n注意需要GCC编译器 \n[root@localhost proftpd-1.3.6]# make\n[root@localhost proftpd-1.3.6]# make install\n \n \n2）添加环境变量\n[root@localhost proftpd-1.3.6]# vim /etc/profile\n........\nPATH=$PATH:/usr/local/proftpd/bin\n \n[root@localhost proftpd-1.3.6]# source /etc/profile\n \n3）创建启动用户及组(该用户无法登录系统，没有宿主目录)\n[root@localhost ~]# useradd proftp -s /sbin/nologin -M\n \n4）建立共享目录\n[root@localhost ~]# mkdir -p /var/ftp/运维部\n[root@localhost ~]# mkdir -p /var/ftp/开发部\n[root@localhost ~]# mkdir -p /var/ftp/销售部\n[root@localhost ~]# mkdir -p /var/ftp/行政部\n[root@localhost ~]# useradd -M -s /sbin/nologin  yunwei\n[root@localhost ~]# useradd -M -s /sbin/nologin  kaifa\n[root@localhost ~]# useradd -M -s /sbin/nologin  xiaoshou\n[root@localhost ~]# useradd -M -s /sbin/nologin  xingzheng\n[root@localhost ~]# chmod 777  /var/ftp/运维部\n[root@localhost ~]# chmod 777  /var/ftp/开发部\n[root@localhost ~]# chmod 777  /var/ftp/销售部\n[root@localhost ~]# chmod 777  /var/ftp/行政部\n \n================proftpd配置原文件解释==============\n[root@localhost ~]# cat /etc/proftpd.conf|grep -v "#"|grep -v "^$"\nServerName                      "ProFTPD Default Installation"     #客户端连接后显示的字符\nServerType                      standalone                         #服务启动模式\nDefaultServer                   on                                \nPort                            21          #端口\nUseIPv6                         off         #禁用IPv6\nUmask                           022         #权限掩码\nMaxInstances                    30          #并发进程30个（防DoS攻击)\nUser                            nobody      #启动服务的用户\nGroup                           nogroup     #启动服务的组\n#DefaultRoot ~                              #共享根目录（默认为用户家目录）\nAllowOverwrite          on                  #是否允许使用文件覆写权限\n<Limit SITE_CHMOD>                          #权限设置\n  DenyAll\n</Limit>\n<Anonymous ~ftp>\n  User                          ftp\n  Group                         ftp\n  UserAlias                     anonymous ftp     #用户别名\n  MaxClients                    10                #最大客户端连接数\n  DisplayLogin                  welcome.msg       #显示登录信息\n  DisplayChdir                  .message\n  <Limit WRITE>                                   #权限设置\n    DenyAll\n  </Limit>\n</Anonymous>\n \n=================该文件格式===================\n##########################################################################\n#   全局设置  参数值                                                                                                  \n#                                                                       \n#   <Directory  "路径"> 指定路径相关设置，可以使用Limit语法限制目录权限 \n#         ... ...                                                       \n#   ... ...                                                       \n#   </Directory>                                                        \n#                             \n#                                                                       \n#                 \n#   <anonymouse "路径">   匿名共享路径相关设置（包括权限设置）    \n#   </anonymouse>                   \n#########################################################################\n \n==============Limit权限说明================\n#########################################################################\n#  CWD:改变所在目录 （即Change Working Directory   表示进入该目录）\n#  MKD/XMKD:新建目录\n#  RNFR/RNTO:重命名目录的(一起使用) ，即更名\n#  DELE:删除文件\n#  RMD/XRMD:删除目录 （即Remove Directory）\n#  RETR:下载\n#  STOR:上传\n#  LOGIN:允许登陆\n#  READ: 可读，包括了RETR,SITE,SIZE,STAT\n#  WRITE: 可写，包括包括了APPE, DELE, MKD, RMD, RNTO, STOR, XMKD, XRMD\n#  DIRS: 允许列出目录，包括了DUP, CWD, LIST, MDTM, NLST, PWD, RNFR, XCUP, XCWD, XPWD\n#  ALL:包括了READ WRITE DIRS\n#######################以上权限结合动作一起使用#####################\n#  AllowUser:允许某个用户\n#  DenyUser:禁止某个用户\n#  AllowGroup:允许某个用户组\n#  DenyGroup:禁止某个用户组\n#  AllowAll:允许所有用户\n#  DenyAll:禁止所有用户\n#########################################################################\n \n5）修改/etc/proftpd.conf文件，部分内容为添加内容\n[root@localhost ~]# cat /etc/proftpd.conf\nServerName                      "ProFTPD Default Installation"\nServerType                      standalone\nDefaultServer                   on\nUseEncoding UTF-8 CP936                    #支持的编码格式(中文)\nPort                            21\nAllowRetrieveRestart            on         #允许断点继传（上传） \nAllowStoreRestart               on         #允许断点继传（下载） \nUseIPv6                         off\nUmask                           022\nRootLogin                       off        #禁止root登录ftp\nMaxInstances                    30\nSystemLog                       /var/log/proftp.log   #产生独立的日志文件. (如果想指定自己的日志格式可以结合（ExtendLog，LogFormat）两个选项设置)\nTransferLog                     /var/log/proftp.log   #记录用户下载的日志信息\nUser                            proftp     #设置启动用户为proftp\nGroup                           proftp     #设置启动组为proftp\nDefaultRoot /var/ftp                       #指定共享根目录为/var/ftp\nAllowOverwrite                  on\n#<Anonymous ~ftp>                          #该部分全部#注释，取消匿名访问功能\n#  User       ftp\n#  Group      ftp\n#  UserAlias      anonymous ftp        \n#  MaxClients     10           \n#  DisplayLogin     welcome.msg        \n#  DisplayChdir     .message\n#  <Limit WRITE>                 \n#    DenyAll\n#  </Limit>\n#</Anonymous>\n \n#以下内容为设置权限，为手动添加内容     \n#所有用户可以看到所有部门的文件夹，仅可以访问自己部门的目录\nRequireValidShell off                            #用户登录是否需要shell（对虚拟用户很重要）\nAuthUserFile /usr/local/proftpd/ftpd.passwd      #通过文件认证用户登录，需要ftpasswd命令创建该文件\n<Directory "/var/ftp/*">\n<Limit CWD READ>                                 #允许所有人可以查看根目录\n    AllowAll\n</Limit>\n</Directory>\n<Directory "/var/ftp/运维部">\n<Limit CWD MKD RNFR READ WRITE STOR RETR>\n    DenyAll                                      #拒绝所有人往该目录下执行Limit后的操作指令\n</Limit>\n<Limit DELE>\n    DenyAll                                      #禁止任何人在该目录下删除文件\n</Limit>\n<Limit CWD MKD RNFR READ WRITE STOR RETR>\n    AllowUser yunwei                             #仅允许yunwei用户可以执行Limit后的所有指令\n</Limit>\n</Directory>\n <Directory "/var/ftp/开发部">\n <Limit CWD MKD RNFR READ WRITE STOR RETR>\n    DenyAll\n</Limit>\n<Limit DELE>\n    DenyAll\n</Limit>\n<Limit CWD MKD RNFR READ WRITE STOR RETR>\n    AllowUser kaifa\n</Limit>\n</Directory>\n<Directory "/var/ftp/行政部">\n<Limit CWD MKD RNFR READ WRITE STOR RETR>\n    DenyAll\n</Limit>\n<Limit DELE>\n   DenyAll\n</Limit>\n<Limit CWD MKD RNFR READ WRITE STOR RETR>\n   AllowUser xingzheng\n</Limit>\n</Directory>\n<Directory "/var/ftp/销售部">\n<Limit CWD MKD RNFR READ WRITE STOR RETR>\n   DenyAll\n</Limit>\n<Limit DELE>\n   DenyAll\n</Limit>\n<Limit CWD MKD RNFR READ WRITE STOR RETR>\n   AllowUser xiaoshou\n</Limit>\n</Directory>\n \n6）用ftpasswd命令建立虚拟账号（下面命令也可以直接用于修改用户密码）\nftpasswd命令格式说明 （该命令可以创建用户文件、组文件，默认创建的用户文件为ftpd.passwd）：\n--passwd  创建密码文件，即AuthUserFile指定的文件\n--group   创建组文件\n--name    指定创建的用户名\n--uid     指定用户虚拟UID\n--gid     指定虚拟GID\n--home    指定用户家目录\n--shell   指定用户Shell\n--file    指定创建的文件名\n \n[root@localhost ~]# ftpasswd  --passwd --file=/usr/local/proftpd/ftpd.passwd --name=yunwei  --uid=1000  --home=/home/nohome  --shell=/bin/false\nftpasswd: using alternate file: /usr/local/proftpd/ftpd.passwd\nftpasswd: --passwd: missing --gid argument: default gid set to uid\nftpasswd: creating passwd entry for user yunwei\n \nftpasswd: /bin/false is not among the valid system shells.  Use of\nftpasswd: "RequireValidShell off" may be required, and the PAM\nftpasswd: module configuration may need to be adjusted.\n \n \nPassword:\nRe-type password:\n \nftpasswd: entry created\n \n用户认证文件创建后的权限是440\n[root@localhost ~]# ll /usr/local/proftpd/ftpd.passwd\n-r--r-----. 1 root root 77 Jul 12 10:59 /usr/local/proftpd/ftpd.passwd\n[root@localhost ~]# cat /usr/local/proftpd/ftpd.passwd\nyunwei:$1$UEKjLwfY$FXV4SHlLeAOGEc2wrZa.M/:1000:1000::/home/nohome:/bin/false\n \n7）启动FTP服务\n检查配置文件是否正常\n[root@localhost ~]# /usr/local/proftpd/sbin/proftpd -t6\nChecking syntax of configuration file\nSyntax check complete.\n \n[root@localhost ~]# /usr/local/proftpd/sbin/proftpd\n \n[root@localhost ~]# ps -ef|grep proftpd\nproftp   13438     1  0 11:01 ?        00:00:00 proftpd: (accepting connections)\nroot     13440 13349  0 11:01 pts/2    00:00:00 grep --color=auto proftpd\n \n[root@localhost ~]# lsof -i:21\nCOMMAND   PID   USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\nproftpd 13438 proftp    0u  IPv4 2066796      0t0  TCP *:ftp (LISTEN)\n \n这里仅以用户为实验环境，还可以实现组功能，这里就不做过多介绍了！\n=====================================================================================\n如果配置组功能，则创建虚拟账号组的命令如下\n# ftpasswd --group --file=/usr/local/proftpd/ftpd.group --name=admin --gid=99\n# ftpasswd --group --name=admin --gid=99 --member=ftpadmin\n=====================================================================================\n \n8）ProFtpd启动脚本\n[root@localhost ~]# mkdir /usr/local/proftpd/etc\n[root@localhost ~]# ln -s /etc/proftpd.conf /usr/local/proftpd/etc/\n[root@localhost ~]# ll /usr/local/proftpd/etc/\ntotal 0\nlrwxrwxrwx. 1 root root 17 Jul 12 11:21 proftpd.conf -> /etc/proftpd.conf\n \n[root@localhost ~]# cat /etc/rc.d/init.d/proftpd\n#!/bin/bash \n# \n# chkconfig: 2345 85 15 \n# description: ProFTPd is an FTP server \n# processname: proftpd \n   \n# Author:   jingyihome \n# E-mail:   webmaster@zhanghaijun.com \n# Website:  https://www.zhanghaijun.com \n   \n# ProFTPd Settings \nPROFTPD="/usr/local/proftpd/sbin/proftpd" \nPROCONF="/usr/local/proftpd/etc/proftpd.conf" \nPROPID="/usr/local/proftpd/var/proftpd.pid" \nRETVAL=0 \nprog="ProFTPd" \n   \nstart() { \n    echo -n $"Starting $prog... " \n    $PROFTPD -c $PROCONF \n    if [ "$?" = 0 ] ; then \n        echo " done" \n    else \n        echo " failed" \n    fi \n} \n   \nstop() { \n    echo -n $"Stopping $prog...  " \n    if [ ! -e $PROPID ]; then \n        echo -n $"$prog is not running." \n        exit 1 \n    fi \n    kill `cat $PROPID` \n    if [ "$?" = 0 ] ; then \n        echo " done" \n    else \n        echo " failed" \n    fi \n} \n   \nrestart(){ \n    echo $"Restarting $prog..." \n    $0 stop \n    sleep 2 \n    $0 start \n} \n   \nstatus(){ \n    if [ -e $PROPID ]; then \n        echo $"$prog is running." \n    else \n        echo $"$prog is not running." \n    fi \n} \n   \ncase "$1" in \n    start) \n        start \n        ;; \n    stop) \n        stop \n        ;; \n    restart) \n        restart \n        ;; \n    status) \n        status \n        ;; \n  *) \n        echo $"Usage: $0 {start|stop|restart|status}" \nesac \n \n授予执行权限\n[root@localhost ~]# chmod 755 /etc/rc.d/init.d/proftpd\n[root@localhost ~]# ll /etc/rc.d/init.d/proftpd\n-rwxr-xr-x. 1 root root 1370 Jul 12 11:20 /etc/rc.d/init.d/proftpd\n[root@localhost ~]# ll /etc/init.d/proftpd\n-rwxr-xr-x. 1 root root 1370 Jul 12 11:20 /etc/init.d/proftpd\n \n测试proftpd脚本启停\n[root@localhost ~]# /etc/init.d/proftpd stop\nStopping ProFTPd...   done\n[root@localhost ~]# lsof -i:21\n[root@localhost ~]#\n \n[root@localhost ~]# /etc/init.d/proftpd start\nStarting ProFTPd...  done\n[root@localhost ~]# lsof -i:21              \nCOMMAND   PID   USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\nproftpd 13503 proftp    0u  IPv4 2066913      0t0  TCP *:ftp (LISTEN)\n \n \n9）如果连接FTP时速度慢，可以在proftpd配置文件proftpd.conf中加入以下内容：\n# Slow logins\n# This is probably caused by a firewall or DNS timeout. By default ProFTPD will try to do both DNS and ident lookups against the\n# incoming connection. If these are blocked or excessively delayed a slower than normal login will result. To turn off DNS and ident\n# use:\nUseReverseDNS off\nIdentLookups off\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n\n\n\n\n\n\n参考链接\n\nCentOS下Proftpd环境部署并使用虚拟用户登录 - 运维笔记 - 散尽浮华 - 博客园',normalizedContent:'# proftpd介绍\n\nproftpd的全称是professional ftp daemon，是针对wu-ftp的弱项而开发的，软件在经过多年的发展之后完善了很多功能，proftp已经成为继wu-ftp之后最为流行的ftp服务器软件，越来越多的站点选用它构筑安全高效的ftp站点。proftpd软件和vsftpd一样是一个开放源代码的ftp服务器软件，但是可配置项比vsftpd要多，是目前比较流行的ftp软件，proftpd的配置和apache的配置相似，因此该软件也十分容易配置和管理。\n\nproftpd和vsftpd二者区别\n===== vsftpd =====  \n全称very secure ftp daemon。比proftpd 具有更高的安全性。vsftpd使用一般身份启动服务，降低了ftp服务的pid权限，使该服务即使被入侵也无法得到有效的系统管理权限。同时vsftpd利用chroot软件来改变登录者的根目录，使登陆者只能在这个目录中活动，限制了登录者的执行权限。vsftpd通过配置vsftpd.conf文件来完成部署，设定简单，登录者仅分为anonymous和real user 两种。可以使用standalone和super daemon的方式启动。vsftpd无法控制每个目录的流量、不能控制上传和下载的比例、不能针对不同的登陆者进行不同的权限设定。\n\n===== proftpd=====\n由于proftpd在自身的原始码中已经包含了所需要的执行指令，不需要linux系统本机的执行程序的支持，所以在系统安全上更为安全。配置简单且灵活，安装后只需要设定proftpd.conf一个配置文件即可，可配置性更强。可以使用stand-alone或者super daemon方式来启动ftp服务。proftpd可以控制上下传比例，实现流量控制，针对不同的目录设定不同的权限。登录者分为anonymous和real user两种。\n\nproftpd比较好用的功能有以下几点：\n1）目录访问权限配置灵活，配置简单。\n2）能够不依赖系统用户，可以使用独立的虚拟用户系统（使用过serv-u的朋友应该深有体会，配置非常方便，对原有系统环境影响较小）\n3）对中文的支持良好，完美解决vsftpd中文引号bug。因为vsftpd在中文支持方面存在bug，对中文中一些字符的支持不是很好（比如对中文的双引号支持不是很好）。\n\nftp部署的背景\n公司四个部门分别为运维部、开发部、销售部、行政部：\n1）各部门用户访问ftp后可以看到所有目录，仅可以访问本部门的目录；\n2）需要ftp日志功能；\n3）ftp认证方式基于文件认证方式；\n4）共享目录：/var/ftp;\n\n\n# proftp安装记录\n\n测试机器为centos7.5，iptables和selinux均关闭\n \n1）下载proftp\n[root@localhost ~]# wget ftp://ftp.proftpd.org/distrib/source/proftpd-1.3.6.tar.gz\n[root@localhost ~]# tar -zvxf proftpd-1.3.6.tar.gz -c /usr/src/\n[root@localhost ~]# cd  /usr/src/proftpd-1.3.6/\n[root@localhost proftpd-1.3.6]# ./configure  --prefix=/usr/local/proftpd  --sysconfdir=/etc/  --enable-nls  --enable-openssl  --enable-shadow\n \n==================================================================================================================\n编译参数说明（可通过"./configure --help" 查看帮助选项）：\n--prefix=prefix         指定安装路径（--prefix=/usr/local/)              \n--sysconfdir=dir        指定ftp服务配置文件路径(--sysconfdir=/etc)                  \n--localstatedir=dir     指定运行状态的文件存放位置(默认/var/proftpd)                 \n--with-modules=mod_ldap 指定加载功能模块                        \n--enable-memcache       支持缓存功能                          \n--enable-nls            支持多语言环境（如中文），安装完成后在主配置文件中需要指定字符编码（useencoding utf-8 cp936） \n--enable-openssl        支持tls加密ftp服务                        \n--enable-shadow         支持使用/etc/shadow验证用户密码\n \n==================================================================================================================\n \n注意需要gcc编译器 \n[root@localhost proftpd-1.3.6]# make\n[root@localhost proftpd-1.3.6]# make install\n \n \n2）添加环境变量\n[root@localhost proftpd-1.3.6]# vim /etc/profile\n........\npath=$path:/usr/local/proftpd/bin\n \n[root@localhost proftpd-1.3.6]# source /etc/profile\n \n3）创建启动用户及组(该用户无法登录系统，没有宿主目录)\n[root@localhost ~]# useradd proftp -s /sbin/nologin -m\n \n4）建立共享目录\n[root@localhost ~]# mkdir -p /var/ftp/运维部\n[root@localhost ~]# mkdir -p /var/ftp/开发部\n[root@localhost ~]# mkdir -p /var/ftp/销售部\n[root@localhost ~]# mkdir -p /var/ftp/行政部\n[root@localhost ~]# useradd -m -s /sbin/nologin  yunwei\n[root@localhost ~]# useradd -m -s /sbin/nologin  kaifa\n[root@localhost ~]# useradd -m -s /sbin/nologin  xiaoshou\n[root@localhost ~]# useradd -m -s /sbin/nologin  xingzheng\n[root@localhost ~]# chmod 777  /var/ftp/运维部\n[root@localhost ~]# chmod 777  /var/ftp/开发部\n[root@localhost ~]# chmod 777  /var/ftp/销售部\n[root@localhost ~]# chmod 777  /var/ftp/行政部\n \n================proftpd配置原文件解释==============\n[root@localhost ~]# cat /etc/proftpd.conf|grep -v "#"|grep -v "^$"\nservername                      "proftpd default installation"     #客户端连接后显示的字符\nservertype                      standalone                         #服务启动模式\ndefaultserver                   on                                \nport                            21          #端口\nuseipv6                         off         #禁用ipv6\numask                           022         #权限掩码\nmaxinstances                    30          #并发进程30个（防dos攻击)\nuser                            nobody      #启动服务的用户\ngroup                           nogroup     #启动服务的组\n#defaultroot ~                              #共享根目录（默认为用户家目录）\nallowoverwrite          on                  #是否允许使用文件覆写权限\n<limit site_chmod>                          #权限设置\n  denyall\n</limit>\n<anonymous ~ftp>\n  user                          ftp\n  group                         ftp\n  useralias                     anonymous ftp     #用户别名\n  maxclients                    10                #最大客户端连接数\n  displaylogin                  welcome.msg       #显示登录信息\n  displaychdir                  .message\n  <limit write>                                   #权限设置\n    denyall\n  </limit>\n</anonymous>\n \n=================该文件格式===================\n##########################################################################\n#   全局设置  参数值                                                                                                  \n#                                                                       \n#   <directory  "路径"> 指定路径相关设置，可以使用limit语法限制目录权限 \n#         ... ...                                                       \n#   ... ...                                                       \n#   </directory>                                                        \n#                             \n#                                                                       \n#                 \n#   <anonymouse "路径">   匿名共享路径相关设置（包括权限设置）    \n#   </anonymouse>                   \n#########################################################################\n \n==============limit权限说明================\n#########################################################################\n#  cwd:改变所在目录 （即change working directory   表示进入该目录）\n#  mkd/xmkd:新建目录\n#  rnfr/rnto:重命名目录的(一起使用) ，即更名\n#  dele:删除文件\n#  rmd/xrmd:删除目录 （即remove directory）\n#  retr:下载\n#  stor:上传\n#  login:允许登陆\n#  read: 可读，包括了retr,site,size,stat\n#  write: 可写，包括包括了appe, dele, mkd, rmd, rnto, stor, xmkd, xrmd\n#  dirs: 允许列出目录，包括了dup, cwd, list, mdtm, nlst, pwd, rnfr, xcup, xcwd, xpwd\n#  all:包括了read write dirs\n#######################以上权限结合动作一起使用#####################\n#  allowuser:允许某个用户\n#  denyuser:禁止某个用户\n#  allowgroup:允许某个用户组\n#  denygroup:禁止某个用户组\n#  allowall:允许所有用户\n#  denyall:禁止所有用户\n#########################################################################\n \n5）修改/etc/proftpd.conf文件，部分内容为添加内容\n[root@localhost ~]# cat /etc/proftpd.conf\nservername                      "proftpd default installation"\nservertype                      standalone\ndefaultserver                   on\nuseencoding utf-8 cp936                    #支持的编码格式(中文)\nport                            21\nallowretrieverestart            on         #允许断点继传（上传） \nallowstorerestart               on         #允许断点继传（下载） \nuseipv6                         off\numask                           022\nrootlogin                       off        #禁止root登录ftp\nmaxinstances                    30\nsystemlog                       /var/log/proftp.log   #产生独立的日志文件. (如果想指定自己的日志格式可以结合（extendlog，logformat）两个选项设置)\ntransferlog                     /var/log/proftp.log   #记录用户下载的日志信息\nuser                            proftp     #设置启动用户为proftp\ngroup                           proftp     #设置启动组为proftp\ndefaultroot /var/ftp                       #指定共享根目录为/var/ftp\nallowoverwrite                  on\n#<anonymous ~ftp>                          #该部分全部#注释，取消匿名访问功能\n#  user       ftp\n#  group      ftp\n#  useralias      anonymous ftp        \n#  maxclients     10           \n#  displaylogin     welcome.msg        \n#  displaychdir     .message\n#  <limit write>                 \n#    denyall\n#  </limit>\n#</anonymous>\n \n#以下内容为设置权限，为手动添加内容     \n#所有用户可以看到所有部门的文件夹，仅可以访问自己部门的目录\nrequirevalidshell off                            #用户登录是否需要shell（对虚拟用户很重要）\nauthuserfile /usr/local/proftpd/ftpd.passwd      #通过文件认证用户登录，需要ftpasswd命令创建该文件\n<directory "/var/ftp/*">\n<limit cwd read>                                 #允许所有人可以查看根目录\n    allowall\n</limit>\n</directory>\n<directory "/var/ftp/运维部">\n<limit cwd mkd rnfr read write stor retr>\n    denyall                                      #拒绝所有人往该目录下执行limit后的操作指令\n</limit>\n<limit dele>\n    denyall                                      #禁止任何人在该目录下删除文件\n</limit>\n<limit cwd mkd rnfr read write stor retr>\n    allowuser yunwei                             #仅允许yunwei用户可以执行limit后的所有指令\n</limit>\n</directory>\n <directory "/var/ftp/开发部">\n <limit cwd mkd rnfr read write stor retr>\n    denyall\n</limit>\n<limit dele>\n    denyall\n</limit>\n<limit cwd mkd rnfr read write stor retr>\n    allowuser kaifa\n</limit>\n</directory>\n<directory "/var/ftp/行政部">\n<limit cwd mkd rnfr read write stor retr>\n    denyall\n</limit>\n<limit dele>\n   denyall\n</limit>\n<limit cwd mkd rnfr read write stor retr>\n   allowuser xingzheng\n</limit>\n</directory>\n<directory "/var/ftp/销售部">\n<limit cwd mkd rnfr read write stor retr>\n   denyall\n</limit>\n<limit dele>\n   denyall\n</limit>\n<limit cwd mkd rnfr read write stor retr>\n   allowuser xiaoshou\n</limit>\n</directory>\n \n6）用ftpasswd命令建立虚拟账号（下面命令也可以直接用于修改用户密码）\nftpasswd命令格式说明 （该命令可以创建用户文件、组文件，默认创建的用户文件为ftpd.passwd）：\n--passwd  创建密码文件，即authuserfile指定的文件\n--group   创建组文件\n--name    指定创建的用户名\n--uid     指定用户虚拟uid\n--gid     指定虚拟gid\n--home    指定用户家目录\n--shell   指定用户shell\n--file    指定创建的文件名\n \n[root@localhost ~]# ftpasswd  --passwd --file=/usr/local/proftpd/ftpd.passwd --name=yunwei  --uid=1000  --home=/home/nohome  --shell=/bin/false\nftpasswd: using alternate file: /usr/local/proftpd/ftpd.passwd\nftpasswd: --passwd: missing --gid argument: default gid set to uid\nftpasswd: creating passwd entry for user yunwei\n \nftpasswd: /bin/false is not among the valid system shells.  use of\nftpasswd: "requirevalidshell off" may be required, and the pam\nftpasswd: module configuration may need to be adjusted.\n \n \npassword:\nre-type password:\n \nftpasswd: entry created\n \n用户认证文件创建后的权限是440\n[root@localhost ~]# ll /usr/local/proftpd/ftpd.passwd\n-r--r-----. 1 root root 77 jul 12 10:59 /usr/local/proftpd/ftpd.passwd\n[root@localhost ~]# cat /usr/local/proftpd/ftpd.passwd\nyunwei:$1$uekjlwfy$fxv4shlleaogec2wrza.m/:1000:1000::/home/nohome:/bin/false\n \n7）启动ftp服务\n检查配置文件是否正常\n[root@localhost ~]# /usr/local/proftpd/sbin/proftpd -t6\nchecking syntax of configuration file\nsyntax check complete.\n \n[root@localhost ~]# /usr/local/proftpd/sbin/proftpd\n \n[root@localhost ~]# ps -ef|grep proftpd\nproftp   13438     1  0 11:01 ?        00:00:00 proftpd: (accepting connections)\nroot     13440 13349  0 11:01 pts/2    00:00:00 grep --color=auto proftpd\n \n[root@localhost ~]# lsof -i:21\ncommand   pid   user   fd   type  device size/off node name\nproftpd 13438 proftp    0u  ipv4 2066796      0t0  tcp *:ftp (listen)\n \n这里仅以用户为实验环境，还可以实现组功能，这里就不做过多介绍了！\n=====================================================================================\n如果配置组功能，则创建虚拟账号组的命令如下\n# ftpasswd --group --file=/usr/local/proftpd/ftpd.group --name=admin --gid=99\n# ftpasswd --group --name=admin --gid=99 --member=ftpadmin\n=====================================================================================\n \n8）proftpd启动脚本\n[root@localhost ~]# mkdir /usr/local/proftpd/etc\n[root@localhost ~]# ln -s /etc/proftpd.conf /usr/local/proftpd/etc/\n[root@localhost ~]# ll /usr/local/proftpd/etc/\ntotal 0\nlrwxrwxrwx. 1 root root 17 jul 12 11:21 proftpd.conf -> /etc/proftpd.conf\n \n[root@localhost ~]# cat /etc/rc.d/init.d/proftpd\n#!/bin/bash \n# \n# chkconfig: 2345 85 15 \n# description: proftpd is an ftp server \n# processname: proftpd \n   \n# author:   jingyihome \n# e-mail:   webmaster@zhanghaijun.com \n# website:  https://www.zhanghaijun.com \n   \n# proftpd settings \nproftpd="/usr/local/proftpd/sbin/proftpd" \nproconf="/usr/local/proftpd/etc/proftpd.conf" \npropid="/usr/local/proftpd/var/proftpd.pid" \nretval=0 \nprog="proftpd" \n   \nstart() { \n    echo -n $"starting $prog... " \n    $proftpd -c $proconf \n    if [ "$?" = 0 ] ; then \n        echo " done" \n    else \n        echo " failed" \n    fi \n} \n   \nstop() { \n    echo -n $"stopping $prog...  " \n    if [ ! -e $propid ]; then \n        echo -n $"$prog is not running." \n        exit 1 \n    fi \n    kill `cat $propid` \n    if [ "$?" = 0 ] ; then \n        echo " done" \n    else \n        echo " failed" \n    fi \n} \n   \nrestart(){ \n    echo $"restarting $prog..." \n    $0 stop \n    sleep 2 \n    $0 start \n} \n   \nstatus(){ \n    if [ -e $propid ]; then \n        echo $"$prog is running." \n    else \n        echo $"$prog is not running." \n    fi \n} \n   \ncase "$1" in \n    start) \n        start \n        ;; \n    stop) \n        stop \n        ;; \n    restart) \n        restart \n        ;; \n    status) \n        status \n        ;; \n  *) \n        echo $"usage: $0 {start|stop|restart|status}" \nesac \n \n授予执行权限\n[root@localhost ~]# chmod 755 /etc/rc.d/init.d/proftpd\n[root@localhost ~]# ll /etc/rc.d/init.d/proftpd\n-rwxr-xr-x. 1 root root 1370 jul 12 11:20 /etc/rc.d/init.d/proftpd\n[root@localhost ~]# ll /etc/init.d/proftpd\n-rwxr-xr-x. 1 root root 1370 jul 12 11:20 /etc/init.d/proftpd\n \n测试proftpd脚本启停\n[root@localhost ~]# /etc/init.d/proftpd stop\nstopping proftpd...   done\n[root@localhost ~]# lsof -i:21\n[root@localhost ~]#\n \n[root@localhost ~]# /etc/init.d/proftpd start\nstarting proftpd...  done\n[root@localhost ~]# lsof -i:21              \ncommand   pid   user   fd   type  device size/off node name\nproftpd 13503 proftp    0u  ipv4 2066913      0t0  tcp *:ftp (listen)\n \n \n9）如果连接ftp时速度慢，可以在proftpd配置文件proftpd.conf中加入以下内容：\n# slow logins\n# this is probably caused by a firewall or dns timeout. by default proftpd will try to do both dns and ident lookups against the\n# incoming connection. if these are blocked or excessively delayed a slower than normal login will result. to turn off dns and ident\n# use:\nusereversedns off\nidentlookups off\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n\n\n\n\n\n\n参考链接\n\ncentos下proftpd环境部署并使用虚拟用户登录 - 运维笔记 - 散尽浮华 - 博客园',charsets:{cjk:!0}},{title:"nexus安装",frontmatter:{title:"nexus安装",date:"2023-02-17T10:48:27.000Z",permalink:"/pages/af4fce/",categories:["运维","系统","nexus"],tags:[null],readingShow:"top",description:"Nexus 是一个强大的maven仓库管理器，它极大地简化了本地内部仓库的维护和外部仓库的访问。",meta:[{name:"image",content:"http://t.eryajf.net/imgs/2021/09/11d4a8c868c78c35.jpg"},{name:"twitter:title",content:"nexus安装"},{name:"twitter:description",content:"Nexus 是一个强大的maven仓库管理器，它极大地简化了本地内部仓库的维护和外部仓库的访问。"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://t.eryajf.net/imgs/2021/09/11d4a8c868c78c35.jpg"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/03.nexus/01.nexus%E5%AE%89%E8%A3%85.html"},{property:"og:type",content:"article"},{property:"og:title",content:"nexus安装"},{property:"og:description",content:"Nexus 是一个强大的maven仓库管理器，它极大地简化了本地内部仓库的维护和外部仓库的访问。"},{property:"og:image",content:"http://t.eryajf.net/imgs/2021/09/11d4a8c868c78c35.jpg"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/03.nexus/01.nexus%E5%AE%89%E8%A3%85.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-17T10:48:27.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"nexus安装"},{itemprop:"description",content:"Nexus 是一个强大的maven仓库管理器，它极大地简化了本地内部仓库的维护和外部仓库的访问。"},{itemprop:"image",content:"http://t.eryajf.net/imgs/2021/09/11d4a8c868c78c35.jpg"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/03.nexus/01.nexus%E5%AE%89%E8%A3%85.html",relativePath:"04.运维/09.系统/03.nexus/01.nexus安装.md",key:"v-c4242b76",path:"/pages/af4fce/",headers:[{level:2,title:"1. nexus介绍",slug:"_1-nexus介绍",normalizedTitle:"1. nexus介绍",charIndex:2},{level:3,title:"nexus是什么？",slug:"nexus是什么",normalizedTitle:"nexus是什么？",charIndex:17},{level:3,title:"为什么要构建 Nexus 私服？",slug:"为什么要构建-nexus-私服",normalizedTitle:"为什么要构建 nexus 私服？",charIndex:598},{level:3,title:"2.版本选择及下载",slug:"_2-版本选择及下载",normalizedTitle:"2.版本选择及下载",charIndex:1069},{level:4,title:"关于版本选择",slug:"关于版本选择",normalizedTitle:"关于版本选择",charIndex:1124},{level:4,title:"关于下载",slug:"关于下载",normalizedTitle:"关于下载",charIndex:1285},{level:3,title:"3.安装jdk-1.8",slug:"_3-安装jdk-1-8",normalizedTitle:"3.安装jdk-1.8",charIndex:1591},{level:3,title:"4.部署nexus",slug:"_4-部署nexus",normalizedTitle:"4.部署nexus",charIndex:1659},{level:3,title:"2.启动",slug:"_2-启动",normalizedTitle:"2.启动",charIndex:1815},{level:3,title:"3.访问",slug:"_3-访问",normalizedTitle:"3.访问",charIndex:1953},{level:3,title:"4.优化配置",slug:"_4-优化配置",normalizedTitle:"4.优化配置",charIndex:2013},{level:4,title:"1.设置开机启动",slug:"_1-设置开机启动",normalizedTitle:"1.设置开机启动",charIndex:2023},{level:4,title:"2.配置运行用户",slug:"_2-配置运行用户",normalizedTitle:"2.配置运行用户",charIndex:2144},{level:4,title:"3.配置jdk",slug:"_3-配置jdk",normalizedTitle:"3.配置jdk",charIndex:2453},{level:4,title:"4.修改端口",slug:"_4-修改端口",normalizedTitle:"4.修改端口",charIndex:2614},{level:4,title:"5.配置存储及日志位置",slug:"_5-配置存储及日志位置",normalizedTitle:"5.配置存储及日志位置",charIndex:2709},{level:4,title:"6.配置完毕之后，重启一下服务。",slug:"_6-配置完毕之后-重启一下服务。",normalizedTitle:"6.配置完毕之后，重启一下服务。",charIndex:2965},{level:4,title:"7.解决nexus下载依赖过多报错问题",slug:"_7-解决nexus下载依赖过多报错问题",normalizedTitle:"7.解决nexus下载依赖过多报错问题",charIndex:3140},{level:3,title:"5.docker方式安装",slug:"_5-docker方式安装",normalizedTitle:"5.docker方式安装",charIndex:3600}],headersStr:"1. nexus介绍 nexus是什么？ 为什么要构建 Nexus 私服？ 2.版本选择及下载 关于版本选择 关于下载 3.安装jdk-1.8 4.部署nexus 2.启动 3.访问 4.优化配置 1.设置开机启动 2.配置运行用户 3.配置jdk 4.修改端口 5.配置存储及日志位置 6.配置完毕之后，重启一下服务。 7.解决nexus下载依赖过多报错问题 5.docker方式安装",content:'# 1. nexus介绍\n\n\n# nexus是什么？\n\nNexus 是一个强大的maven仓库管理器，它极大地简化了本地内部仓库的维护和外部仓库的访问。\n\n不仅如此，他还可以用来创建yum、pypi、npm、docker、nuget、rubygems 等各种私有仓库。\n\n如果使用了公共的 Maven 仓库服务器，可以从 Maven 中央仓库下载所需要的构件（Artifact），但这通常不是一个好的做法。 正常做法是在本地架设一个 Maven 仓库服务器，即利用 Nexus 私服可以只在一个地方就能够完全控制访问和部署在你所维护仓库中的每个 Artifact。 Nexus 在代理远程仓库的同时维护本地仓库，以降低中央仓库的负荷, 节省外网带宽和时间，Nexus 私服就可以满足这样的需要。 Nexus 是一套 “开箱即用” 的系统不需要数据库，它使用文件系统加 Lucene 来组织数据。 Nexus 使用 ExtJS 来开发界面，利用 Restlet 来提供完整的 REST APIs，通过 m2eclipse 与 Eclipse 集成使用。 Nexus 支持 WebDAV 与 LDAP 安全身份认证。 Nexus 还提供了强大的仓库管理功能，构件搜索功能，它基于 REST，友好的 UI 是一个 extjs 的 REST 客户端，它占用较少的内存，基于简单文件系统而非数据库\n\n\n# 为什么要构建 Nexus 私服？\n\n如果没有 Nexus 私服，我们所需的所有构件都需要通过 maven 的中央仓库和第三方的 Maven 仓库下载到本地，而一个团队中的所有人都重复的从 maven 仓库下载构件无疑加大了仓库的负载和浪费了外网带宽，如果网速慢的话，还会影响项目的进程。很多情况下项目的开发都是在内网进行的，连接不到 maven 仓库怎么办呢？开发的公共构件怎么让其它项目使用？这个时候我们不得不为自己的团队搭建属于自己的 maven 私服，这样既节省了网络带宽也会加速项目搭建的进程，当然前提条件就是你的私服中拥有项目所需的所有构件。\n\n总之，在本地构建 nexus私服的好处有：\n\n>  1. 加速构建\n>  2. 节省带宽\n>  3. 节省中央 maven 仓库的带宽\n>  4. 稳定（应付一旦中央服务器出问题的情况）\n>  5. 控制和审计\n>  6. 能够部署第三方构件\n>  7. 可以建立本地内部仓库\n>  8. 可以建立公共仓库\n\n这些优点使得 Nexus 日趋成为最流行的 Maven 仓库管理器\n\n\n# 2.版本选择及下载\n\n----------------------------------------\n\n# 关于版本选择\n\n以前公司使用的版本一直是用的一个2.x的版本，旧的版本各种陈旧，在这种陈旧的情况下，许多地方已经破破烂烂了，比如最重要的功能之一，自动同步远程仓库的依赖，就已经无法使用，看到版本已经更新到3.x，因此就想着搭一个高版本的来体验一下。\n\n这么一体验，发现这个东东，真真的是一个特别好的利器，神器。\n\n\n\n# 关于下载\n\n官网地址：https://www.sonatype.com/\n\n * 可以直接通过下边链接下载最新版本：https://www.sonatype.com/oss-thank-you-tar.gz\n * 可以通过后边的这个链接选择历史版本：https://help.sonatype.com/repomanager3/download/download-archives—repository-manager-3(opens new window)\n\n可能一般网络下在浏览器里边下载比较慢，那么可以复制链接用迅雷下载，就会好一些。\n\n接下来开始整个安装的流程介绍，我们先来介绍普通方式的安装。\n\n\n# 3.安装jdk-1.8\n\nnexus的安装依赖jdk环境。最好安装1.8版本的，否则可能会遇到其他不可知问题。此安装不再赘述。\n\n\n# 4.部署nexus\n\n[root@nexus mnt]$tar xf nexus-3.12.1-01-unix.tar.gz\n[root@nexus mnt]$ls\nnexus-3.12.1-01  nexus-3.12.1-01-unix.tar.gz  sonatype-work\n\n\n1\n2\n3\n\n\n\n# 2.启动\n\ncd /mnt/nexus-3.12.1-01/bin\n./nexus run &\n\n\n1\n2\n\n\n大概等待一分钟左右，如果在日志输出当中看到如下显示，则说明启动成功。\n\nStarted Sonatype Nexus OSS 3.12.1-01\n\n\n1\n\n\n\n# 3.访问\n\n默认监听端口为8081，默认用户名密码为admin/admin123，因此可以访问以下首页并登陆。\n\n\n# 4.优化配置\n\n# 1.设置开机启动\n\nln -s /mnt/nexus-3.12.1-01/bin/nexus /etc/init.d/nexus3\nchkconfig --add nexus3\nchkconfig nexus3 on\n\n\n1\n2\n3\n\n\n# 2.配置运行用户\n\n这个地方可以使用root运行，不过官方文档里边也不建议使用root来运行，因此使用普通用户来运行。\n\n[root@nexus ~]$useradd nexus\n[root@nexus bin]$vim nexus.rc\n\nrun_as_user="nexus"\n\n配置之后记得更改目录权限，否则下次启动会没有权限。\n\n[root@nexus mnt]$chown -R nexus.nexus /mnt/nexus-3.12.1-01\n[root@nexus mnt]$chown -R nexus.nexus /mnt/sonatype-work\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 3.配置jdk\n\n如果这里不配置，一般会使用默认的JAVA_HOME的变量，如果系统中有多个，那么可以进行配置。\n\n[root@nexus bin]$vim nexus\n\n修改第14行：\nINSTALL4J_JAVA_HOME_OVERRIDE=/usr/local/jdk1.8.0_144\n\n\n1\n2\n3\n4\n\n\n# 4.修改端口\n\n一般使用默认的，如果需要修改，则更改/mnt/nexus-3.12.1-01/etc/nexus-default.properties里边的配置。\n\n这里不做修改了。\n\n# 5.配置存储及日志位置\n\n[root@nexus bin]$vim /mnt/nexus-3.12.1-01/bin/nexus.vmoptions\n\n一般都不做修改，使用默认即可，这里列出是为了了解这个点。\n\n-XX:LogFile=../sonatype-work/nexus3/log/jvm.log\n-Dkaraf.data=../sonatype-work/nexus3\n-Djava.io.tmpdir=../sonatype-work/nexus3/tmp\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 6.配置完毕之后，重启一下服务。\n\n注意左上角会有一个告警，这个告警的意思是系统默认的最大文件句柄太小了。\n\n解决办法官网也已经给出了：解决文件句柄限制。\n\n因此我们照官方解决办法来做：\n\necho "nexus  -  nofile 65536" >> /etc/security/limits.conf\n\n\n1\n\n\n然后再次重启服务即可。\n\n# 7.解决nexus下载依赖过多报错问题\n\n执行nexus私库地址，并且npm login完成后，进行npm i下载依赖时，当依赖过多或者执行时间过长时，出现错误提示\n\n重点是 java.io.IOException: java.util.concurrent.TimeoutException: Idle timeout expired: 30001/30000 ms，\n\n由此可知，错误原因是网络连接超时。分析应该是因为第三方依赖过多，导致下载依赖的时间超过了http请求的默认等待时间30000ms导致下载请求被掐断了。\n\n知道原因就好解决了。\n\n通过错误提示发现nexus是采用了 jetty，所以进入nexus根目录\\etc\\jetty，找到jetty-http.xml和jetty-https.xml\n\n打开xml搜索Set name="idleTimeout"\n\n将后面的default=30000改成1200000\n\n重启服务（一定要重启服务）\n\n再次执行npm i指令。\n\n下载正常，问题解决。\n\n\n# 5.docker方式安装\n\n最后介绍一些使用docker的方式如何安装。\n\n如果想使用docker来启动，那么可以参考：官方镜像介绍。(opens new window)\n\n或用下边方式直接启动。\n\nmkdir /mnt/nexus-data\ndocker run -d -p 8081:8081 --name nexus -v --ulimit nofile=65536：65536 /mnt/nexus-data:/nexus-data sonatype/nexus3\n\n\n1\n2\n\n\n原文链接：nexus的安装 | 二丫讲梵',normalizedContent:'# 1. nexus介绍\n\n\n# nexus是什么？\n\nnexus 是一个强大的maven仓库管理器，它极大地简化了本地内部仓库的维护和外部仓库的访问。\n\n不仅如此，他还可以用来创建yum、pypi、npm、docker、nuget、rubygems 等各种私有仓库。\n\n如果使用了公共的 maven 仓库服务器，可以从 maven 中央仓库下载所需要的构件（artifact），但这通常不是一个好的做法。 正常做法是在本地架设一个 maven 仓库服务器，即利用 nexus 私服可以只在一个地方就能够完全控制访问和部署在你所维护仓库中的每个 artifact。 nexus 在代理远程仓库的同时维护本地仓库，以降低中央仓库的负荷, 节省外网带宽和时间，nexus 私服就可以满足这样的需要。 nexus 是一套 “开箱即用” 的系统不需要数据库，它使用文件系统加 lucene 来组织数据。 nexus 使用 extjs 来开发界面，利用 restlet 来提供完整的 rest apis，通过 m2eclipse 与 eclipse 集成使用。 nexus 支持 webdav 与 ldap 安全身份认证。 nexus 还提供了强大的仓库管理功能，构件搜索功能，它基于 rest，友好的 ui 是一个 extjs 的 rest 客户端，它占用较少的内存，基于简单文件系统而非数据库\n\n\n# 为什么要构建 nexus 私服？\n\n如果没有 nexus 私服，我们所需的所有构件都需要通过 maven 的中央仓库和第三方的 maven 仓库下载到本地，而一个团队中的所有人都重复的从 maven 仓库下载构件无疑加大了仓库的负载和浪费了外网带宽，如果网速慢的话，还会影响项目的进程。很多情况下项目的开发都是在内网进行的，连接不到 maven 仓库怎么办呢？开发的公共构件怎么让其它项目使用？这个时候我们不得不为自己的团队搭建属于自己的 maven 私服，这样既节省了网络带宽也会加速项目搭建的进程，当然前提条件就是你的私服中拥有项目所需的所有构件。\n\n总之，在本地构建 nexus私服的好处有：\n\n>  1. 加速构建\n>  2. 节省带宽\n>  3. 节省中央 maven 仓库的带宽\n>  4. 稳定（应付一旦中央服务器出问题的情况）\n>  5. 控制和审计\n>  6. 能够部署第三方构件\n>  7. 可以建立本地内部仓库\n>  8. 可以建立公共仓库\n\n这些优点使得 nexus 日趋成为最流行的 maven 仓库管理器\n\n\n# 2.版本选择及下载\n\n----------------------------------------\n\n# 关于版本选择\n\n以前公司使用的版本一直是用的一个2.x的版本，旧的版本各种陈旧，在这种陈旧的情况下，许多地方已经破破烂烂了，比如最重要的功能之一，自动同步远程仓库的依赖，就已经无法使用，看到版本已经更新到3.x，因此就想着搭一个高版本的来体验一下。\n\n这么一体验，发现这个东东，真真的是一个特别好的利器，神器。\n\n\n\n# 关于下载\n\n官网地址：https://www.sonatype.com/\n\n * 可以直接通过下边链接下载最新版本：https://www.sonatype.com/oss-thank-you-tar.gz\n * 可以通过后边的这个链接选择历史版本：https://help.sonatype.com/repomanager3/download/download-archives—repository-manager-3(opens new window)\n\n可能一般网络下在浏览器里边下载比较慢，那么可以复制链接用迅雷下载，就会好一些。\n\n接下来开始整个安装的流程介绍，我们先来介绍普通方式的安装。\n\n\n# 3.安装jdk-1.8\n\nnexus的安装依赖jdk环境。最好安装1.8版本的，否则可能会遇到其他不可知问题。此安装不再赘述。\n\n\n# 4.部署nexus\n\n[root@nexus mnt]$tar xf nexus-3.12.1-01-unix.tar.gz\n[root@nexus mnt]$ls\nnexus-3.12.1-01  nexus-3.12.1-01-unix.tar.gz  sonatype-work\n\n\n1\n2\n3\n\n\n\n# 2.启动\n\ncd /mnt/nexus-3.12.1-01/bin\n./nexus run &\n\n\n1\n2\n\n\n大概等待一分钟左右，如果在日志输出当中看到如下显示，则说明启动成功。\n\nstarted sonatype nexus oss 3.12.1-01\n\n\n1\n\n\n\n# 3.访问\n\n默认监听端口为8081，默认用户名密码为admin/admin123，因此可以访问以下首页并登陆。\n\n\n# 4.优化配置\n\n# 1.设置开机启动\n\nln -s /mnt/nexus-3.12.1-01/bin/nexus /etc/init.d/nexus3\nchkconfig --add nexus3\nchkconfig nexus3 on\n\n\n1\n2\n3\n\n\n# 2.配置运行用户\n\n这个地方可以使用root运行，不过官方文档里边也不建议使用root来运行，因此使用普通用户来运行。\n\n[root@nexus ~]$useradd nexus\n[root@nexus bin]$vim nexus.rc\n\nrun_as_user="nexus"\n\n配置之后记得更改目录权限，否则下次启动会没有权限。\n\n[root@nexus mnt]$chown -r nexus.nexus /mnt/nexus-3.12.1-01\n[root@nexus mnt]$chown -r nexus.nexus /mnt/sonatype-work\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 3.配置jdk\n\n如果这里不配置，一般会使用默认的java_home的变量，如果系统中有多个，那么可以进行配置。\n\n[root@nexus bin]$vim nexus\n\n修改第14行：\ninstall4j_java_home_override=/usr/local/jdk1.8.0_144\n\n\n1\n2\n3\n4\n\n\n# 4.修改端口\n\n一般使用默认的，如果需要修改，则更改/mnt/nexus-3.12.1-01/etc/nexus-default.properties里边的配置。\n\n这里不做修改了。\n\n# 5.配置存储及日志位置\n\n[root@nexus bin]$vim /mnt/nexus-3.12.1-01/bin/nexus.vmoptions\n\n一般都不做修改，使用默认即可，这里列出是为了了解这个点。\n\n-xx:logfile=../sonatype-work/nexus3/log/jvm.log\n-dkaraf.data=../sonatype-work/nexus3\n-djava.io.tmpdir=../sonatype-work/nexus3/tmp\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 6.配置完毕之后，重启一下服务。\n\n注意左上角会有一个告警，这个告警的意思是系统默认的最大文件句柄太小了。\n\n解决办法官网也已经给出了：解决文件句柄限制。\n\n因此我们照官方解决办法来做：\n\necho "nexus  -  nofile 65536" >> /etc/security/limits.conf\n\n\n1\n\n\n然后再次重启服务即可。\n\n# 7.解决nexus下载依赖过多报错问题\n\n执行nexus私库地址，并且npm login完成后，进行npm i下载依赖时，当依赖过多或者执行时间过长时，出现错误提示\n\n重点是 java.io.ioexception: java.util.concurrent.timeoutexception: idle timeout expired: 30001/30000 ms，\n\n由此可知，错误原因是网络连接超时。分析应该是因为第三方依赖过多，导致下载依赖的时间超过了http请求的默认等待时间30000ms导致下载请求被掐断了。\n\n知道原因就好解决了。\n\n通过错误提示发现nexus是采用了 jetty，所以进入nexus根目录\\etc\\jetty，找到jetty-http.xml和jetty-https.xml\n\n打开xml搜索set name="idletimeout"\n\n将后面的default=30000改成1200000\n\n重启服务（一定要重启服务）\n\n再次执行npm i指令。\n\n下载正常，问题解决。\n\n\n# 5.docker方式安装\n\n最后介绍一些使用docker的方式如何安装。\n\n如果想使用docker来启动，那么可以参考：官方镜像介绍。(opens new window)\n\n或用下边方式直接启动。\n\nmkdir /mnt/nexus-data\ndocker run -d -p 8081:8081 --name nexus -v --ulimit nofile=65536：65536 /mnt/nexus-data:/nexus-data sonatype/nexus3\n\n\n1\n2\n\n\n原文链接：nexus的安装 | 二丫讲梵',charsets:{cjk:!0}},{title:"使用nexus3配置npm私有仓库",frontmatter:{title:"使用nexus3配置npm私有仓库",date:"2023-02-20T10:57:40.000Z",permalink:"/pages/b851fe/",categories:["运维","系统","nexus"],tags:[null],readingShow:"top",description:"当我们运行前端项目的时候，常常在解决依赖的时候会加上一个参数npm install --registry=https://registry.npm.taobao.org将源指定为淘宝的源，以期让速度加快起来，事实上这种的确能够让速度变快，但是长久来看，如果想真正的快速敏捷开发部署，搭建企业内部的私服，则会让速度更上一个台阶。",meta:[{name:"image",content:"http://pic.zzppjj.top/LightPicture/2023/02/013b7fc775dcf453.jpg"},{name:"twitter:title",content:"使用nexus3配置npm私有仓库"},{name:"twitter:description",content:"当我们运行前端项目的时候，常常在解决依赖的时候会加上一个参数npm install --registry=https://registry.npm.taobao.org将源指定为淘宝的源，以期让速度加快起来，事实上这种的确能够让速度变快，但是长久来看，如果想真正的快速敏捷开发部署，搭建企业内部的私服，则会让速度更上一个台阶。"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://pic.zzppjj.top/LightPicture/2023/02/013b7fc775dcf453.jpg"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/03.nexus/02.%E4%BD%BF%E7%94%A8nexus3%E9%85%8D%E7%BD%AEnpm%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93.html"},{property:"og:type",content:"article"},{property:"og:title",content:"使用nexus3配置npm私有仓库"},{property:"og:description",content:"当我们运行前端项目的时候，常常在解决依赖的时候会加上一个参数npm install --registry=https://registry.npm.taobao.org将源指定为淘宝的源，以期让速度加快起来，事实上这种的确能够让速度变快，但是长久来看，如果想真正的快速敏捷开发部署，搭建企业内部的私服，则会让速度更上一个台阶。"},{property:"og:image",content:"http://pic.zzppjj.top/LightPicture/2023/02/013b7fc775dcf453.jpg"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/03.nexus/02.%E4%BD%BF%E7%94%A8nexus3%E9%85%8D%E7%BD%AEnpm%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-20T10:57:40.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"使用nexus3配置npm私有仓库"},{itemprop:"description",content:"当我们运行前端项目的时候，常常在解决依赖的时候会加上一个参数npm install --registry=https://registry.npm.taobao.org将源指定为淘宝的源，以期让速度加快起来，事实上这种的确能够让速度变快，但是长久来看，如果想真正的快速敏捷开发部署，搭建企业内部的私服，则会让速度更上一个台阶。"},{itemprop:"image",content:"http://pic.zzppjj.top/LightPicture/2023/02/013b7fc775dcf453.jpg"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/03.nexus/02.%E4%BD%BF%E7%94%A8nexus3%E9%85%8D%E7%BD%AEnpm%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93.html",relativePath:"04.运维/09.系统/03.nexus/02.使用nexus3配置npm私有仓库.md",key:"v-6feefc5e",path:"/pages/b851fe/",headers:[{level:2,title:"1，创建blob存储。",slug:"_1-创建blob存储。",normalizedTitle:"1，创建blob存储。",charIndex:340},{level:2,title:"2，创建hosted类型的npm。",slug:"_2-创建hosted类型的npm。",normalizedTitle:"2，创建hosted类型的npm。",charIndex:374},{level:2,title:"3，创建一个proxy类型的npm仓库。",slug:"_3-创建一个proxy类型的npm仓库。",normalizedTitle:"3，创建一个proxy类型的npm仓库。",charIndex:544},{level:2,title:"4，创建一个group类型的npm仓库。",slug:"_4-创建一个group类型的npm仓库。",normalizedTitle:"4，创建一个group类型的npm仓库。",charIndex:700},{level:2,title:"5，验证使用。",slug:"_5-验证使用。",normalizedTitle:"5，验证使用。",charIndex:844},{level:3,title:"1，首先获取默认的仓库地址：",slug:"_1-首先获取默认的仓库地址",normalizedTitle:"1，首先获取默认的仓库地址：",charIndex:1383},{level:3,title:"2，配置为私服地址。",slug:"_2-配置为私服地址。",normalizedTitle:"2，配置为私服地址。",charIndex:1488},{level:3,title:"3，安装编译。",slug:"_3-安装编译。",normalizedTitle:"3，安装编译。",charIndex:1769},{level:3,title:"4，再一次安装编译。",slug:"_4-再一次安装编译。",normalizedTitle:"4，再一次安装编译。",charIndex:2373}],headersStr:"1，创建blob存储。 2，创建hosted类型的npm。 3，创建一个proxy类型的npm仓库。 4，创建一个group类型的npm仓库。 5，验证使用。 1，首先获取默认的仓库地址： 2，配置为私服地址。 3，安装编译。 4，再一次安装编译。",content:'当我们运行前端项目的时候，常常在解决依赖的时候会加上一个参数npm install --registry=https://registry.npm.taobao.org将源指定为淘宝的源，以期让速度加快起来，事实上这种的确能够让速度变快，但是长久来看，如果想真正的快速敏捷开发部署，搭建企业内部的私服，则会让速度更上一个台阶。\n\n搭建npm私服，我们依旧使用nexus3。\n\n与其他私服一样的，npm私服同样有三种类型：\n\n * hosted : 本地存储，即同 docker 官方仓库一样提供本地私服功能\n * proxy : 提供代理其他仓库的类型，如 docker 中央仓库\n * group : 组类型，实质作用是组合多个仓库为一个地址\n\n那么就来一个一个创建。\n\n\n# 1，创建blob存储。\n\n为其创建一个单独的存储空间。\n\n\n\n\n# 2，创建hosted类型的npm。\n\n * Name: 定义一个名称local-npm\n * Storage：Blob store，我们下拉选择前面创建好的专用blob：npm-hub。\n * Hosted：开发环境，我们运行重复发布，因此Delpoyment policy 我们选择Allow redeploy。这个很重要！\n\n\n\n\n# 3，创建一个proxy类型的npm仓库。\n\n * Name: proxy-npm\n * Proxy：Remote Storage: 远程仓库地址，这里填写: https://registry.npmjs.org\n * Storage: npm-hub。\n\n其他的均是默认。\n\n整体配置截图如下：\n\n\n\n\n\n\n# 4，创建一个group类型的npm仓库。\n\n * Name：group-npm\n * Storage：选择专用的blob存储npm-hub。\n * group : 将左边可选的2个仓库，添加到右边的members下。\n\n整体配置截图如下：\n\n\n\n这些配置完成之后，就可以使用了。\n\n\n# 5，验证使用。\n\n新建一台环境干净的主机，安装好node环境。\n\n首先通过curl 192.168.106.10/a | sh安装好node环境。\n\n如果看不懂这是什么鬼，可以点击这篇文章了解：构建运维外挂。\n\n此脚本我已经开源在GitHub之中，感兴趣的同学可以点击下边跳转参观。\n\nmagic-of-sysuse-scripts\n\n运维外挂小工具\n\n- name: magic-of-sysuse-scripts\n  desc: 运维外挂小工具\n  avatar: https://avatars2.githubusercontent.com/u/416130?s=460&u=8753e86600e300a9811cdc539aa158deec2e2724&v=4 # 可选\n  link: https://github.com/eryajf/magic-of-sysuse-scripts # 可选\n  bgColor: \'#FBDE4B\' # 可选，默认var(--bodyBg)。颜色值有#号时请添加单引号\n  textColor: \'#fff\' # 可选，默认var(--textColor)\n\n\n1\n2\n3\n4\n5\n6\n\n\n然后拷贝一份前端项目的源码。\n\n\n# 1，首先获取默认的仓库地址：\n\n[root@moban business_jsdweb]$npm config get registryhttps://registry.npmjs.org/\n\n\n1\n\n\n\n# 2，配置为私服地址。\n\n从如下截图中查看(其实就是创建的组对外的地址)。\n\n\n\n通过如下命令配置：\n\n[root@moban business_jsdweb]$npm config set registry http://192.168.112.214:8081/repository/group-npm/\n[root@moban business_jsdweb]$npm config get registry\nhttp://192.168.112.214:8081/repository/group-npm/\n\n\n1\n2\n3\n\n\n可以看到还是空的。\n\n\n# 3，安装编译。\n\nnpm install\n\n\n1\n\n\n在编译的过程中，我们已经可以看看组里的变化了：\n\n\n\n安装完成，整个过程如下，可以看到一共花费了82秒。\n\n[root@moban business_jsdweb]$npm install\n\n> uglifyjs-webpack-plugin@0.4.6 postinstall /root/business_jsdweb/node_modules/webpack/node_modules/uglifyjs-webpack-plugin\n> node lib/post_install.js\n\nnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents):\nnpm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})\n\nadded 1216 packages from 717 contributors in 82.171s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 4，再一次安装编译。\n\n这里再准备一台环境干净的主机，然后进行一次编译安装，看看效果。\n\n编译之前，先将远程地址配置为我们自己的：\n\n[root@7-3 business_jsdweb]$npm config get registry\nhttps://registry.npmjs.org/\n[root@7-3 business_jsdweb]$npm config set registry http://192.168.112.214:8081/repository/group-npm/\n[root@7-3 business_jsdweb]$npm config get registry\nhttp://192.168.112.214:8081/repository/group-npm/\n\n\n1\n2\n3\n4\n5\n\n\n然后编译，看效果：\n\n[root@7-3 business_jsdweb]$npm install\n\n> uglifyjs-webpack-plugin@0.4.6 postinstall /root/business_jsdweb/node_modules/webpack/node_modules/uglifyjs-webpack-plugin\n> node lib/post_install.js\n\nnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents):\nnpm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})\n\nadded 1216 packages from 717 contributors in 31.693s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n可以看到，同样是全新的环境下，因为第一次已经将依赖从远程缓存到本地私服，那么在第二次安装编译的时候，用时31秒。\n\n私服的重要性，以及便捷性，高下立见！\n\n原文链接：使用nexus3配置npm私有仓库 | 二丫讲梵',normalizedContent:'当我们运行前端项目的时候，常常在解决依赖的时候会加上一个参数npm install --registry=https://registry.npm.taobao.org将源指定为淘宝的源，以期让速度加快起来，事实上这种的确能够让速度变快，但是长久来看，如果想真正的快速敏捷开发部署，搭建企业内部的私服，则会让速度更上一个台阶。\n\n搭建npm私服，我们依旧使用nexus3。\n\n与其他私服一样的，npm私服同样有三种类型：\n\n * hosted : 本地存储，即同 docker 官方仓库一样提供本地私服功能\n * proxy : 提供代理其他仓库的类型，如 docker 中央仓库\n * group : 组类型，实质作用是组合多个仓库为一个地址\n\n那么就来一个一个创建。\n\n\n# 1，创建blob存储。\n\n为其创建一个单独的存储空间。\n\n\n\n\n# 2，创建hosted类型的npm。\n\n * name: 定义一个名称local-npm\n * storage：blob store，我们下拉选择前面创建好的专用blob：npm-hub。\n * hosted：开发环境，我们运行重复发布，因此delpoyment policy 我们选择allow redeploy。这个很重要！\n\n\n\n\n# 3，创建一个proxy类型的npm仓库。\n\n * name: proxy-npm\n * proxy：remote storage: 远程仓库地址，这里填写: https://registry.npmjs.org\n * storage: npm-hub。\n\n其他的均是默认。\n\n整体配置截图如下：\n\n\n\n\n\n\n# 4，创建一个group类型的npm仓库。\n\n * name：group-npm\n * storage：选择专用的blob存储npm-hub。\n * group : 将左边可选的2个仓库，添加到右边的members下。\n\n整体配置截图如下：\n\n\n\n这些配置完成之后，就可以使用了。\n\n\n# 5，验证使用。\n\n新建一台环境干净的主机，安装好node环境。\n\n首先通过curl 192.168.106.10/a | sh安装好node环境。\n\n如果看不懂这是什么鬼，可以点击这篇文章了解：构建运维外挂。\n\n此脚本我已经开源在github之中，感兴趣的同学可以点击下边跳转参观。\n\nmagic-of-sysuse-scripts\n\n运维外挂小工具\n\n- name: magic-of-sysuse-scripts\n  desc: 运维外挂小工具\n  avatar: https://avatars2.githubusercontent.com/u/416130?s=460&u=8753e86600e300a9811cdc539aa158deec2e2724&v=4 # 可选\n  link: https://github.com/eryajf/magic-of-sysuse-scripts # 可选\n  bgcolor: \'#fbde4b\' # 可选，默认var(--bodybg)。颜色值有#号时请添加单引号\n  textcolor: \'#fff\' # 可选，默认var(--textcolor)\n\n\n1\n2\n3\n4\n5\n6\n\n\n然后拷贝一份前端项目的源码。\n\n\n# 1，首先获取默认的仓库地址：\n\n[root@moban business_jsdweb]$npm config get registryhttps://registry.npmjs.org/\n\n\n1\n\n\n\n# 2，配置为私服地址。\n\n从如下截图中查看(其实就是创建的组对外的地址)。\n\n\n\n通过如下命令配置：\n\n[root@moban business_jsdweb]$npm config set registry http://192.168.112.214:8081/repository/group-npm/\n[root@moban business_jsdweb]$npm config get registry\nhttp://192.168.112.214:8081/repository/group-npm/\n\n\n1\n2\n3\n\n\n可以看到还是空的。\n\n\n# 3，安装编译。\n\nnpm install\n\n\n1\n\n\n在编译的过程中，我们已经可以看看组里的变化了：\n\n\n\n安装完成，整个过程如下，可以看到一共花费了82秒。\n\n[root@moban business_jsdweb]$npm install\n\n> uglifyjs-webpack-plugin@0.4.6 postinstall /root/business_jsdweb/node_modules/webpack/node_modules/uglifyjs-webpack-plugin\n> node lib/post_install.js\n\nnpm warn optional skipping optional dependency: fsevents@1.2.4 (node_modules/fsevents):\nnpm warn notsup skipping optional dependency: unsupported platform for fsevents@1.2.4: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})\n\nadded 1216 packages from 717 contributors in 82.171s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 4，再一次安装编译。\n\n这里再准备一台环境干净的主机，然后进行一次编译安装，看看效果。\n\n编译之前，先将远程地址配置为我们自己的：\n\n[root@7-3 business_jsdweb]$npm config get registry\nhttps://registry.npmjs.org/\n[root@7-3 business_jsdweb]$npm config set registry http://192.168.112.214:8081/repository/group-npm/\n[root@7-3 business_jsdweb]$npm config get registry\nhttp://192.168.112.214:8081/repository/group-npm/\n\n\n1\n2\n3\n4\n5\n\n\n然后编译，看效果：\n\n[root@7-3 business_jsdweb]$npm install\n\n> uglifyjs-webpack-plugin@0.4.6 postinstall /root/business_jsdweb/node_modules/webpack/node_modules/uglifyjs-webpack-plugin\n> node lib/post_install.js\n\nnpm warn optional skipping optional dependency: fsevents@1.2.4 (node_modules/fsevents):\nnpm warn notsup skipping optional dependency: unsupported platform for fsevents@1.2.4: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})\n\nadded 1216 packages from 717 contributors in 31.693s\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n可以看到，同样是全新的环境下，因为第一次已经将依赖从远程缓存到本地私服，那么在第二次安装编译的时候，用时31秒。\n\n私服的重要性，以及便捷性，高下立见！\n\n原文链接：使用nexus3配置npm私有仓库 | 二丫讲梵',charsets:{cjk:!0}},{title:"使用nexus3配置maven私有仓库",frontmatter:{title:"使用nexus3配置maven私有仓库",date:"2023-02-20T12:03:31.000Z",permalink:"/pages/12e16a/",categories:["运维","系统","nexus"],tags:[null],readingShow:"top",description:"配置之前，我们先来看看系统默认创建的都有哪些？",meta:[{name:"image",content:"http://pic.zzppjj.top/LightPicture/2023/02/40cd9e21fa166d75.jpg"},{name:"twitter:title",content:"使用nexus3配置maven私有仓库"},{name:"twitter:description",content:"配置之前，我们先来看看系统默认创建的都有哪些？"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://pic.zzppjj.top/LightPicture/2023/02/40cd9e21fa166d75.jpg"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/03.nexus/03.%E4%BD%BF%E7%94%A8nexus3%E9%85%8D%E7%BD%AEmaven%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93.html"},{property:"og:type",content:"article"},{property:"og:title",content:"使用nexus3配置maven私有仓库"},{property:"og:description",content:"配置之前，我们先来看看系统默认创建的都有哪些？"},{property:"og:image",content:"http://pic.zzppjj.top/LightPicture/2023/02/40cd9e21fa166d75.jpg"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/03.nexus/03.%E4%BD%BF%E7%94%A8nexus3%E9%85%8D%E7%BD%AEmaven%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-20T12:03:31.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"使用nexus3配置maven私有仓库"},{itemprop:"description",content:"配置之前，我们先来看看系统默认创建的都有哪些？"},{itemprop:"image",content:"http://pic.zzppjj.top/LightPicture/2023/02/40cd9e21fa166d75.jpg"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/09.%E7%B3%BB%E7%BB%9F/03.nexus/03.%E4%BD%BF%E7%94%A8nexus3%E9%85%8D%E7%BD%AEmaven%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93.html",relativePath:"04.运维/09.系统/03.nexus/03.使用nexus3配置maven私有仓库.md",key:"v-28dc669e",path:"/pages/12e16a/",headers:[{level:2,title:"1，创建blob存储。",slug:"_1-创建blob存储。",normalizedTitle:"1，创建blob存储。",charIndex:71},{level:2,title:"2，创建hosted类型的maven。",slug:"_2-创建hosted类型的maven。",normalizedTitle:"2，创建hosted类型的maven。",charIndex:89},{level:2,title:"3，创建一个proxy类型的maven仓库。",slug:"_3-创建一个proxy类型的maven仓库。",normalizedTitle:"3，创建一个proxy类型的maven仓库。",charIndex:533},{level:2,title:"4，创建一个group类型的maven仓库。",slug:"_4-创建一个group类型的maven仓库。",normalizedTitle:"4，创建一个group类型的maven仓库。",charIndex:845},{level:2,title:"5，验证使用。",slug:"_5-验证使用。",normalizedTitle:"5，验证使用。",charIndex:1114},{level:3,title:"1，安装jdk。",slug:"_1-安装jdk。",normalizedTitle:"1，安装jdk。",charIndex:1245},{level:3,title:"2，安装maven。",slug:"_2-安装maven。",normalizedTitle:"2，安装maven。",charIndex:1284},{level:3,title:"3，更改maven的配置。",slug:"_3-更改maven的配置。",normalizedTitle:"3，更改maven的配置。",charIndex:1309},{level:3,title:"4，拉取项目编译。",slug:"_4-拉取项目编译。",normalizedTitle:"4，拉取项目编译。",charIndex:13331}],headersStr:"1，创建blob存储。 2，创建hosted类型的maven。 3，创建一个proxy类型的maven仓库。 4，创建一个group类型的maven仓库。 5，验证使用。 1，安装jdk。 2，安装maven。 3，更改maven的配置。 4，拉取项目编译。",content:"配置之前，我们先来看看系统默认创建的都有哪些？\n\n其中圈起来的都是系统原有的，用不到，就全删掉，重新创建。\n\n\n\n老规矩，开始创建。\n\n\n# 1，创建blob存储。\n\n\n\n\n# 2，创建hosted类型的maven。\n\n点击 Repository下面的 Repositories – Create repository – maven2(hosted) :\n\n * Name: 定义一个名称maven-local\n\n * Online: 勾选。这个开关可以设置这个maven repo是在线还是离线。\n\n * Maven2：这里有三种方式，Releases、SNAPSHOT、Mixed。\n   \n   * Releases: 一般是已经发布的Jar包\n   * Snapshot: 未发布的版本\n   * Mixed：混合的\n\n这里不做设置，默认即可。\n\n * Storage\n   \n   Blob store，我们下拉选择前面创建好的专用blob：maven-use。\n\n * Hosted\n   \n   开发环境，我们运行重复发布，因此Delpoyment policy 我们选择Allow redeploy。这个很重要！\n\n整体配置截图如下：\n\n\n\n\n# 3，创建一个proxy类型的maven仓库。\n\nproxy的功能就是代理中央Maven仓库，当PC访问中央库的时候，先通过Proxy下载到Nexus仓库，然后再从Nexus仓库下载到PC本地。 这样的优势只要其中一个人从中央库下来了，以后大家都是从Nexus私服上进行下来，私服一般部署在内网，这样大大节约的宽带。\n\n * Name: proxy-maven\n\n * Maven 2: 不设置，使用默认。\n\n * Proxy\n   \n   Remote Storage: 远程仓库地址，这里填写: Central Repository:\n\n * Storage: maven-use。\n\n整体配置截图如下：\n\n\n\n\n\n\n# 4，创建一个group类型的maven仓库。\n\n提示\n\ngroup类型的maven仓库，是一个聚合类型的仓库。它可以将前面我们创建的2个仓库聚合成一个URL对外提供服务，可以屏蔽后端的差异性，实现类似透明代理的功能。后面通过一些配置，大家可能会对这个group有更加深入的了解。\n\n * Name：group-maven\n\n * Storage：选择专用的blob存储maven-use。\n\n * group : 将左边可选的2个仓库，添加到右边的members下。\n\n整体配置截图如下：\n\n这样配置以后，我们就可以使用了。\n\n\n\n\n# 5，验证使用。\n\n使用起来其实非常简单，就是在测试机器上安装maven工具，然后再其配置当中，将地址指向我们的私服地址，然后编译项目，这个时候就会通过私服来拉取jar包了，以后再编译的时候，就可以直接从本地私服拉取了。\n\n现在找一台测试机器，进行验证。\n\n\n# 1，安装jdk。\n\n因为maven依赖于jdk，所以要先安装jdk。\n\n\n# 2，安装maven。\n\n安装maven。\n\n\n# 3，更改maven的配置。\n\n将项目编译依赖地址指向改成私服的配置，需要通过更改maven的配置实现，也就是更改maven/conf/settings.xml的仓库地址。\n\n说实话，我尝试安装一个全新的maven，从而通过修改配置来实现走私服拉取依赖，但是，经过这样的操作，我失败了。\n\n因此这里将现役正用的配置拿出来，仅修改其中的核心处（私服链接）吧：\n\n点击查看\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n\x3c!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n--\x3e\n\n\x3c!--\n | This is the configuration file for Maven. It can be specified at two levels:\n |\n |  1. User Level. This settings.xml file provides configuration for a single user,\n |                 and is normally provided in ${user.home}/.m2/settings.xml.\n |\n |                 NOTE: This location can be overridden with the CLI option:\n |\n |                 -s /path/to/user/settings.xml\n |\n |  2. Global Level. This settings.xml file provides configuration for all Maven\n |                 users on a machine (assuming they're all using the same Maven\n |                 installation). It's normally provided in\n |                 ${maven.home}/conf/settings.xml.\n |\n |                 NOTE: This location can be overridden with the CLI option:\n |\n |                 -gs /path/to/global/settings.xml\n |\n | The sections in this sample file are intended to give you a running start at\n | getting the most out of your Maven installation. Where appropriate, the default\n | values (values used when the setting is not specified) are provided.\n |\n |--\x3e\n<settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n          xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\">\n  \x3c!-- localRepository\n   | The path to the local repository maven will use to store artifacts.\n   |\n   | Default: ${user.home}/.m2/repository\n  <localRepository>/path/to/local/repo</localRepository>\n  --\x3e\n\n  \x3c!-- interactiveMode\n   | This will determine whether maven prompts you when it needs input. If set to false,\n   | maven will use a sensible default value, perhaps based on some other setting, for\n   | the parameter in question.\n   |\n   | Default: true\n  <interactiveMode>true</interactiveMode>\n  --\x3e\n  <interactiveMode>false</interactiveMode>\n\n  \x3c!-- offline\n   | Determines whether maven should attempt to connect to the network when executing a build.\n   | This will have an effect on artifact downloads, artifact deployment, and others.\n   |\n   | Default: false\n  <offline>false</offline>\n  --\x3e\n\n  \x3c!-- pluginGroups\n   | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e.\n   | when invoking a command line like \"mvn prefix:goal\". Maven will automatically add thegroup identifiers\n   | \"org.apache.maven.plugins\" and \"org.codehaus.mojo\" if these are not already containedin the list.\n   |--\x3e\n  <pluginGroups>\n    \x3c!-- pluginGroup\n     | Specifies a further group identifier to use for plugin lookup.\n    <pluginGroup>com.your.plugins</pluginGroup>\n    --\x3e\n  </pluginGroups>\n\n  \x3c!-- proxies\n   | This is a list of proxies which can be used on this machine to connect to the network.\n   | Unless otherwise specified (by system property or command-line switch), the first proxy\n   | specification in this list marked as active will be used.\n   |--\x3e\n  <proxies>\n    \x3c!-- proxy\n     | Specification for one proxy, to be used in connecting to the network.\n     |\n    <proxy>\n      <id>optional</id>\n      <active>true</active>\n      <protocol>http</protocol>\n      <username>proxyuser</username>\n      <password>proxypass</password>\n      <host>proxy.host.net</host>\n      <port>80</port>\n      <nonProxyHosts>local.net|some.host.com</nonProxyHosts>\n    </proxy>\n    --\x3e\n  </proxies>\n\n  \x3c!-- servers\n   | This is a list of authentication profiles, keyed by the server-id used within the system.\n   | Authentication profiles can be used whenever maven must make a connection to a remoteserver.\n   |--\x3e\n  <servers>\n    \x3c!-- server\n     | Specifies the authentication information to use when connecting to a particular server, identified by\n     | a unique name within the system (referred to by the 'id' attribute below).\n     |\n     | NOTE: You should either specify username/password OR privateKey/passphrase, since these pairings are\n     |       used together.\n     |\n    <server>\n      <id>deploymentRepo</id>\n      <username>repouser</username>\n      <password>repopwd</password>\n    </server>\n    --\x3e\n\n    \x3c!-- Another sample, using keys to authenticate.\n    <server>\n      <id>siteServer</id>\n      <privateKey>/path/to/private/key</privateKey>\n      <passphrase>optional; leave empty if not used.</passphrase>\n    </server>\n    --\x3e\n  </servers>\n\n  \x3c!-- mirrors\n   | This is a list of mirrors to be used in downloading artifacts from remote repositories.\n   |\n   | It works like this: a POM may declare a repository to use in resolving certain artifacts.\n   | However, this repository may have problems with heavy traffic at times, so people have mirrored\n   | it to several places.\n   |\n   | That repository definition will have a unique id, so we can create a mirror referencefor that\n   | repository, to be used as an alternate download site. The mirror site will be the preferred\n   | server for that repository.\n   |--\x3e\n  <mirrors>\n    \x3c!-- mirror\n     | Specifies a repository mirror site to use instead of a given repository. The repository that\n     | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used\n     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.\n     |\n    <mirror>\n      <id>mirrorId</id>\n      <mirrorOf>repositoryId</mirrorOf>\n      <name>Human Readable Name for this Mirror.</name>\n      <url>http://my.repository.com/repo/path</url>\n    </mirror>\n     --\x3e\n\n<mirror>\n<id>nexus-osc</id>\n<mirrorOf>*</mirrorOf>\n<name>Nexus osc</name>\n<url>http://192.168.106.21:8081/repository/maven-group/</url>\n</mirror>\n\n\n  </mirrors>\n\n  \x3c!-- profiles\n   | This is a list of profiles which can be activated in a variety of ways, and which canmodify\n   | the build process. Profiles provided in the settings.xml are intended to provide local machine-\n   | specific paths and repository locations which allow the build to work in the local environment.\n   |\n   | For example, if you have an integration testing plugin - like cactus - that needs to know where\n   | your Tomcat instance is installed, you can provide a variable here such that the variable is\n   | dereferenced during the build process to configure the cactus plugin.\n   |\n   | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles\n   | section of this document (settings.xml) - will be discussed later. Another way essentially\n   | relies on the detection of a system property, either matching a particular value for the property,\n   | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a\n   | value of '1.4' might activate a profile when the build is executed on a JDK version of '1.4.2_07'.\n   | Finally, the list of active profiles can be specified directly from the command line.\n   |\n   | NOTE: For profiles defined in the settings.xml, you are restricted to specifying onlyartifact\n   |       repositories, plugin repositories, and free-form properties to be used as configuration\n   |       variables for plugins in the POM.\n   |\n   |--\x3e\n  <profiles>\n    \x3c!-- profile\n     | Specifies a set of introductions to the build process, to be activated using one ormore of the\n     | mechanisms described above. For inheritance purposes, and to activate profiles via <activatedProfiles/>\n     | or the command line, profiles have to have an ID that is unique.\n     |\n     | An encouraged best practice for profile identification is to use a consistent naming convention\n     | for profiles, such as 'env-dev', 'env-test', 'env-production', 'user-jdcasey', 'user-brett', etc.\n     | This will make it more intuitive to understand what the set of introduced profiles is attempting\n     | to accomplish, particularly when you only have a list of profile id's for debug.\n     |\n     | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo.\n    <profile>\n      <id>jdk-1.4</id>\n\n      <activation>\n        <jdk>1.4</jdk>\n      </activation>\n\n      <repositories>\n        <repository>\n          <id>jdk14</id>\n          <name>Repository for JDK 1.4 builds</name>\n          <url>http://www.myhost.com/maven/jdk14</url>\n          <layout>default</layout>\n          <snapshotPolicy>always</snapshotPolicy>\n        </repository>\n      </repositories>\n    </profile>\n    --\x3e\n        <profile>\n<id>dev</id>\n<repositories>\n  <repository>\n    <id>central</id>\n    <name>Central</name>\n    <layout>default</layout>\n    <url>http://my.repository.com/repo/path</url>\n    <snapshots>\n      <enabled> false</enabled>\n    </snapshots>\n  </repository>\n</repositories>\n</profile>\n\n    \x3c!--\n     | Here is another profile, activated by the system property 'target-env' with a valueof 'dev',\n     | which provides a specific path to the Tomcat instance. To use this, your plugin configuration\n     | might hypothetically look like:\n     |\n     | ...\n     | <plugin>\n     |   <groupId>org.myco.myplugins</groupId>\n     |   <artifactId>myplugin</artifactId>\n     |\n     |   <configuration>\n     |     <tomcatLocation>${tomcatPath}</tomcatLocation>\n     |   </configuration>\n     | </plugin>\n     | ...\n     |\n     | NOTE: If you just wanted to inject this configuration whenever someone set 'target-env' to\n     |       anything, you could just leave off the <value/> inside the activation-property.\n     |\n    <profile>\n      <id>env-dev</id>\n\n      <activation>\n        <property>\n          <name>target-env</name>\n          <value>dev</value>\n        </property>\n      </activation>\n\n      <properties>\n        <tomcatPath>/path/to/tomcat/instance</tomcatPath>\n      </properties>\n    </profile>\n    --\x3e\n  </profiles>\n\n  \x3c!-- activeProfiles\n   | List of profiles that are active for all builds.\n   |\n  <activeProfiles>\n    <activeProfile>alwaysActiveProfile</activeProfile>\n    <activeProfile>anotherAlwaysActiveProfile</activeProfile>\n  </activeProfiles>\n  --\x3e\n  <activeProfiles>\n    <activeProfile>dev</activeProfile>\n\n  </activeProfiles>\n</settings>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n\n\n然后修改其中的第165行为我新建的私服地址\n\n\n\n\n# 4，拉取项目编译。\n\n拉取之后，进到项目里边，开始编译。\n\nmvn clean package -U -Dmaven.test.skip=true\n\n原文链接：使用nexus3配置maven私有仓库 | 二丫讲梵",normalizedContent:"配置之前，我们先来看看系统默认创建的都有哪些？\n\n其中圈起来的都是系统原有的，用不到，就全删掉，重新创建。\n\n\n\n老规矩，开始创建。\n\n\n# 1，创建blob存储。\n\n\n\n\n# 2，创建hosted类型的maven。\n\n点击 repository下面的 repositories – create repository – maven2(hosted) :\n\n * name: 定义一个名称maven-local\n\n * online: 勾选。这个开关可以设置这个maven repo是在线还是离线。\n\n * maven2：这里有三种方式，releases、snapshot、mixed。\n   \n   * releases: 一般是已经发布的jar包\n   * snapshot: 未发布的版本\n   * mixed：混合的\n\n这里不做设置，默认即可。\n\n * storage\n   \n   blob store，我们下拉选择前面创建好的专用blob：maven-use。\n\n * hosted\n   \n   开发环境，我们运行重复发布，因此delpoyment policy 我们选择allow redeploy。这个很重要！\n\n整体配置截图如下：\n\n\n\n\n# 3，创建一个proxy类型的maven仓库。\n\nproxy的功能就是代理中央maven仓库，当pc访问中央库的时候，先通过proxy下载到nexus仓库，然后再从nexus仓库下载到pc本地。 这样的优势只要其中一个人从中央库下来了，以后大家都是从nexus私服上进行下来，私服一般部署在内网，这样大大节约的宽带。\n\n * name: proxy-maven\n\n * maven 2: 不设置，使用默认。\n\n * proxy\n   \n   remote storage: 远程仓库地址，这里填写: central repository:\n\n * storage: maven-use。\n\n整体配置截图如下：\n\n\n\n\n\n\n# 4，创建一个group类型的maven仓库。\n\n提示\n\ngroup类型的maven仓库，是一个聚合类型的仓库。它可以将前面我们创建的2个仓库聚合成一个url对外提供服务，可以屏蔽后端的差异性，实现类似透明代理的功能。后面通过一些配置，大家可能会对这个group有更加深入的了解。\n\n * name：group-maven\n\n * storage：选择专用的blob存储maven-use。\n\n * group : 将左边可选的2个仓库，添加到右边的members下。\n\n整体配置截图如下：\n\n这样配置以后，我们就可以使用了。\n\n\n\n\n# 5，验证使用。\n\n使用起来其实非常简单，就是在测试机器上安装maven工具，然后再其配置当中，将地址指向我们的私服地址，然后编译项目，这个时候就会通过私服来拉取jar包了，以后再编译的时候，就可以直接从本地私服拉取了。\n\n现在找一台测试机器，进行验证。\n\n\n# 1，安装jdk。\n\n因为maven依赖于jdk，所以要先安装jdk。\n\n\n# 2，安装maven。\n\n安装maven。\n\n\n# 3，更改maven的配置。\n\n将项目编译依赖地址指向改成私服的配置，需要通过更改maven的配置实现，也就是更改maven/conf/settings.xml的仓库地址。\n\n说实话，我尝试安装一个全新的maven，从而通过修改配置来实现走私服拉取依赖，但是，经过这样的操作，我失败了。\n\n因此这里将现役正用的配置拿出来，仅修改其中的核心处（私服链接）吧：\n\n点击查看\n\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n\x3c!--\nlicensed to the apache software foundation (asf) under one\nor more contributor license agreements.  see the notice file\ndistributed with this work for additional information\nregarding copyright ownership.  the asf licenses this file\nto you under the apache license, version 2.0 (the\n\"license\"); you may not use this file except in compliance\nwith the license.  you may obtain a copy of the license at\n\n    http://www.apache.org/licenses/license-2.0\n\nunless required by applicable law or agreed to in writing,\nsoftware distributed under the license is distributed on an\n\"as is\" basis, without warranties or conditions of any\nkind, either express or implied.  see the license for the\nspecific language governing permissions and limitations\nunder the license.\n--\x3e\n\n\x3c!--\n | this is the configuration file for maven. it can be specified at two levels:\n |\n |  1. user level. this settings.xml file provides configuration for a single user,\n |                 and is normally provided in ${user.home}/.m2/settings.xml.\n |\n |                 note: this location can be overridden with the cli option:\n |\n |                 -s /path/to/user/settings.xml\n |\n |  2. global level. this settings.xml file provides configuration for all maven\n |                 users on a machine (assuming they're all using the same maven\n |                 installation). it's normally provided in\n |                 ${maven.home}/conf/settings.xml.\n |\n |                 note: this location can be overridden with the cli option:\n |\n |                 -gs /path/to/global/settings.xml\n |\n | the sections in this sample file are intended to give you a running start at\n | getting the most out of your maven installation. where appropriate, the default\n | values (values used when the setting is not specified) are provided.\n |\n |--\x3e\n<settings xmlns=\"http://maven.apache.org/settings/1.0.0\"\n          xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\"\n          xsi:schemalocation=\"http://maven.apache.org/settings/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\">\n  \x3c!-- localrepository\n   | the path to the local repository maven will use to store artifacts.\n   |\n   | default: ${user.home}/.m2/repository\n  <localrepository>/path/to/local/repo</localrepository>\n  --\x3e\n\n  \x3c!-- interactivemode\n   | this will determine whether maven prompts you when it needs input. if set to false,\n   | maven will use a sensible default value, perhaps based on some other setting, for\n   | the parameter in question.\n   |\n   | default: true\n  <interactivemode>true</interactivemode>\n  --\x3e\n  <interactivemode>false</interactivemode>\n\n  \x3c!-- offline\n   | determines whether maven should attempt to connect to the network when executing a build.\n   | this will have an effect on artifact downloads, artifact deployment, and others.\n   |\n   | default: false\n  <offline>false</offline>\n  --\x3e\n\n  \x3c!-- plugingroups\n   | this is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e.\n   | when invoking a command line like \"mvn prefix:goal\". maven will automatically add thegroup identifiers\n   | \"org.apache.maven.plugins\" and \"org.codehaus.mojo\" if these are not already containedin the list.\n   |--\x3e\n  <plugingroups>\n    \x3c!-- plugingroup\n     | specifies a further group identifier to use for plugin lookup.\n    <plugingroup>com.your.plugins</plugingroup>\n    --\x3e\n  </plugingroups>\n\n  \x3c!-- proxies\n   | this is a list of proxies which can be used on this machine to connect to the network.\n   | unless otherwise specified (by system property or command-line switch), the first proxy\n   | specification in this list marked as active will be used.\n   |--\x3e\n  <proxies>\n    \x3c!-- proxy\n     | specification for one proxy, to be used in connecting to the network.\n     |\n    <proxy>\n      <id>optional</id>\n      <active>true</active>\n      <protocol>http</protocol>\n      <username>proxyuser</username>\n      <password>proxypass</password>\n      <host>proxy.host.net</host>\n      <port>80</port>\n      <nonproxyhosts>local.net|some.host.com</nonproxyhosts>\n    </proxy>\n    --\x3e\n  </proxies>\n\n  \x3c!-- servers\n   | this is a list of authentication profiles, keyed by the server-id used within the system.\n   | authentication profiles can be used whenever maven must make a connection to a remoteserver.\n   |--\x3e\n  <servers>\n    \x3c!-- server\n     | specifies the authentication information to use when connecting to a particular server, identified by\n     | a unique name within the system (referred to by the 'id' attribute below).\n     |\n     | note: you should either specify username/password or privatekey/passphrase, since these pairings are\n     |       used together.\n     |\n    <server>\n      <id>deploymentrepo</id>\n      <username>repouser</username>\n      <password>repopwd</password>\n    </server>\n    --\x3e\n\n    \x3c!-- another sample, using keys to authenticate.\n    <server>\n      <id>siteserver</id>\n      <privatekey>/path/to/private/key</privatekey>\n      <passphrase>optional; leave empty if not used.</passphrase>\n    </server>\n    --\x3e\n  </servers>\n\n  \x3c!-- mirrors\n   | this is a list of mirrors to be used in downloading artifacts from remote repositories.\n   |\n   | it works like this: a pom may declare a repository to use in resolving certain artifacts.\n   | however, this repository may have problems with heavy traffic at times, so people have mirrored\n   | it to several places.\n   |\n   | that repository definition will have a unique id, so we can create a mirror referencefor that\n   | repository, to be used as an alternate download site. the mirror site will be the preferred\n   | server for that repository.\n   |--\x3e\n  <mirrors>\n    \x3c!-- mirror\n     | specifies a repository mirror site to use instead of a given repository. the repository that\n     | this mirror serves has an id that matches the mirrorof element of this mirror. ids are used\n     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.\n     |\n    <mirror>\n      <id>mirrorid</id>\n      <mirrorof>repositoryid</mirrorof>\n      <name>human readable name for this mirror.</name>\n      <url>http://my.repository.com/repo/path</url>\n    </mirror>\n     --\x3e\n\n<mirror>\n<id>nexus-osc</id>\n<mirrorof>*</mirrorof>\n<name>nexus osc</name>\n<url>http://192.168.106.21:8081/repository/maven-group/</url>\n</mirror>\n\n\n  </mirrors>\n\n  \x3c!-- profiles\n   | this is a list of profiles which can be activated in a variety of ways, and which canmodify\n   | the build process. profiles provided in the settings.xml are intended to provide local machine-\n   | specific paths and repository locations which allow the build to work in the local environment.\n   |\n   | for example, if you have an integration testing plugin - like cactus - that needs to know where\n   | your tomcat instance is installed, you can provide a variable here such that the variable is\n   | dereferenced during the build process to configure the cactus plugin.\n   |\n   | as noted above, profiles can be activated in a variety of ways. one way - the activeprofiles\n   | section of this document (settings.xml) - will be discussed later. another way essentially\n   | relies on the detection of a system property, either matching a particular value for the property,\n   | or merely testing its existence. profiles can also be activated by jdk version prefix, where a\n   | value of '1.4' might activate a profile when the build is executed on a jdk version of '1.4.2_07'.\n   | finally, the list of active profiles can be specified directly from the command line.\n   |\n   | note: for profiles defined in the settings.xml, you are restricted to specifying onlyartifact\n   |       repositories, plugin repositories, and free-form properties to be used as configuration\n   |       variables for plugins in the pom.\n   |\n   |--\x3e\n  <profiles>\n    \x3c!-- profile\n     | specifies a set of introductions to the build process, to be activated using one ormore of the\n     | mechanisms described above. for inheritance purposes, and to activate profiles via <activatedprofiles/>\n     | or the command line, profiles have to have an id that is unique.\n     |\n     | an encouraged best practice for profile identification is to use a consistent naming convention\n     | for profiles, such as 'env-dev', 'env-test', 'env-production', 'user-jdcasey', 'user-brett', etc.\n     | this will make it more intuitive to understand what the set of introduced profiles is attempting\n     | to accomplish, particularly when you only have a list of profile id's for debug.\n     |\n     | this profile example uses the jdk version to trigger activation, and provides a jdk-specific repo.\n    <profile>\n      <id>jdk-1.4</id>\n\n      <activation>\n        <jdk>1.4</jdk>\n      </activation>\n\n      <repositories>\n        <repository>\n          <id>jdk14</id>\n          <name>repository for jdk 1.4 builds</name>\n          <url>http://www.myhost.com/maven/jdk14</url>\n          <layout>default</layout>\n          <snapshotpolicy>always</snapshotpolicy>\n        </repository>\n      </repositories>\n    </profile>\n    --\x3e\n        <profile>\n<id>dev</id>\n<repositories>\n  <repository>\n    <id>central</id>\n    <name>central</name>\n    <layout>default</layout>\n    <url>http://my.repository.com/repo/path</url>\n    <snapshots>\n      <enabled> false</enabled>\n    </snapshots>\n  </repository>\n</repositories>\n</profile>\n\n    \x3c!--\n     | here is another profile, activated by the system property 'target-env' with a valueof 'dev',\n     | which provides a specific path to the tomcat instance. to use this, your plugin configuration\n     | might hypothetically look like:\n     |\n     | ...\n     | <plugin>\n     |   <groupid>org.myco.myplugins</groupid>\n     |   <artifactid>myplugin</artifactid>\n     |\n     |   <configuration>\n     |     <tomcatlocation>${tomcatpath}</tomcatlocation>\n     |   </configuration>\n     | </plugin>\n     | ...\n     |\n     | note: if you just wanted to inject this configuration whenever someone set 'target-env' to\n     |       anything, you could just leave off the <value/> inside the activation-property.\n     |\n    <profile>\n      <id>env-dev</id>\n\n      <activation>\n        <property>\n          <name>target-env</name>\n          <value>dev</value>\n        </property>\n      </activation>\n\n      <properties>\n        <tomcatpath>/path/to/tomcat/instance</tomcatpath>\n      </properties>\n    </profile>\n    --\x3e\n  </profiles>\n\n  \x3c!-- activeprofiles\n   | list of profiles that are active for all builds.\n   |\n  <activeprofiles>\n    <activeprofile>alwaysactiveprofile</activeprofile>\n    <activeprofile>anotheralwaysactiveprofile</activeprofile>\n  </activeprofiles>\n  --\x3e\n  <activeprofiles>\n    <activeprofile>dev</activeprofile>\n\n  </activeprofiles>\n</settings>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n\n\n然后修改其中的第165行为我新建的私服地址\n\n\n\n\n# 4，拉取项目编译。\n\n拉取之后，进到项目里边，开始编译。\n\nmvn clean package -u -dmaven.test.skip=true\n\n原文链接：使用nexus3配置maven私有仓库 | 二丫讲梵",charsets:{cjk:!0}},{title:"docker和docker-compose安装",frontmatter:{title:"docker和docker-compose安装",date:"2022-12-15T18:52:58.000Z",permalink:"/pages/217e32/",categories:["运维","docker"],tags:[null],readingShow:"top",description:"docker-compose下载地址\n>\n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\n>\n> docker下载地址\n>\n> Index of linux/static/stable/x86_64/docker-compose下载地址\n>\n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\n>\n> docker下载地址\n>\n> Index of linux/static/stable/x86_64/",meta:[{name:"twitter:title",content:"docker和docker-compose安装"},{name:"twitter:description",content:"docker-compose下载地址\n>\n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\n>\n> docker下载地址\n>\n> Index of linux/static/stable/x86_64/docker-compose下载地址\n>\n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\n>\n> docker下载地址\n>\n> Index of linux/static/stable/x86_64/"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/10.docker/02.docker%E5%92%8Cdocker-compose%E5%AE%89%E8%A3%85.html"},{property:"og:type",content:"article"},{property:"og:title",content:"docker和docker-compose安装"},{property:"og:description",content:"docker-compose下载地址\n>\n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\n>\n> docker下载地址\n>\n> Index of linux/static/stable/x86_64/docker-compose下载地址\n>\n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\n>\n> docker下载地址\n>\n> Index of linux/static/stable/x86_64/"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/10.docker/02.docker%E5%92%8Cdocker-compose%E5%AE%89%E8%A3%85.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T18:52:58.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"docker和docker-compose安装"},{itemprop:"description",content:"docker-compose下载地址\n>\n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\n>\n> docker下载地址\n>\n> Index of linux/static/stable/x86_64/docker-compose下载地址\n>\n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\n>\n> docker下载地址\n>\n> Index of linux/static/stable/x86_64/"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/10.docker/02.docker%E5%92%8Cdocker-compose%E5%AE%89%E8%A3%85.html",relativePath:"04.运维/10.docker/02.docker和docker-compose安装.md",key:"v-65a31f9a",path:"/pages/217e32/",headers:[{level:2,title:"docker和docker-compose二进制安装",slug:"docker和docker-compose二进制安装",normalizedTitle:"docker和docker-compose二进制安装",charIndex:2},{level:2,title:"yum安装社区版",slug:"yum安装社区版",normalizedTitle:"yum安装社区版",charIndex:963},{level:4,title:"Fedora/CentOS/RHEL",slug:"fedora-centos-rhel",normalizedTitle:"fedora/centos/rhel",charIndex:975}],headersStr:"docker和docker-compose二进制安装 yum安装社区版 Fedora/CentOS/RHEL",content:"# docker和docker-compose二进制安装\n\n> docker-compose下载地址\n> \n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\n> \n> docker下载地址\n> \n> Index of linux/static/stable/x86_64/docker-compose下载地址\n> \n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\n> \n> docker下载地址\n> \n> Index of linux/static/stable/x86_64/\n\ndocker.service启动服务文件\n\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network-online.target firewalld.service containerd.service\nWants=network-online.target\n\n[Service]\nType=notify\nExecStart=/usr/bin/dockerd\nExecReload=/bin/kill -s HUP $MAINPID\nTimeoutSec=0\nRestartSec=2\nRestart=always\nStartLimitBurst=3\nStartLimitInterval=60s\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\nTasksMax=infinity\nDelegate=yes\nKillMode=process\n\n[Install]\nWantedBy=multi-user.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# yum安装社区版\n\n# Fedora/CentOS/RHEL\n\n以下内容根据 官方文档 修改而来。\n\n如果你之前安装过 docker，请先删掉\n\nsudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine\n\n安装一些依赖\n\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n\n根据你的发行版下载repo文件:\n\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n\n把软件仓库地址替换为 TUNA:\n\nsudo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo\n\n最后安装:\n\nsudo yum makecache fast\n\nsudo yum install docker-ce",normalizedContent:"# docker和docker-compose二进制安装\n\n> docker-compose下载地址\n> \n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-linux-x86_64\n> \n> docker下载地址\n> \n> index of linux/static/stable/x86_64/docker-compose下载地址\n> \n> https://github.com/docker/compose/releases/download/1.27.4/docker-compose-linux-x86_64\n> \n> docker下载地址\n> \n> index of linux/static/stable/x86_64/\n\ndocker.service启动服务文件\n\n[unit]\ndescription=docker application container engine\ndocumentation=https://docs.docker.com\nafter=network-online.target firewalld.service containerd.service\nwants=network-online.target\n\n[service]\ntype=notify\nexecstart=/usr/bin/dockerd\nexecreload=/bin/kill -s hup $mainpid\ntimeoutsec=0\nrestartsec=2\nrestart=always\nstartlimitburst=3\nstartlimitinterval=60s\nlimitnofile=infinity\nlimitnproc=infinity\nlimitcore=infinity\ntasksmax=infinity\ndelegate=yes\nkillmode=process\n\n[install]\nwantedby=multi-user.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# yum安装社区版\n\n# fedora/centos/rhel\n\n以下内容根据 官方文档 修改而来。\n\n如果你之前安装过 docker，请先删掉\n\nsudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine\n\n安装一些依赖\n\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n\n根据你的发行版下载repo文件:\n\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n\n把软件仓库地址替换为 tuna:\n\nsudo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo\n\n最后安装:\n\nsudo yum makecache fast\n\nsudo yum install docker-ce",charsets:{cjk:!0}},{title:"mongodb副本集",frontmatter:{title:"mongodb副本集",categories:"mongodb",tags:["mongodb"],date:"2022-12-09T20:50:33.000Z",permalink:"/pages/359db9/",readingShow:"top",description:"MongoDB中的副本集（Replica Set）是一组维护相同数据集的mongod服务。 副本集可提供冗余和高可用性，是所有生产部署的基础。",meta:[{name:"image",content:"https://img-blog.csdnimg.cn/20200918164831238.png"},{name:"twitter:title",content:"mongodb副本集"},{name:"twitter:description",content:"MongoDB中的副本集（Replica Set）是一组维护相同数据集的mongod服务。 副本集可提供冗余和高可用性，是所有生产部署的基础。"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://img-blog.csdnimg.cn/20200918164831238.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/02.mongodb/02.mongodb%E5%89%AF%E6%9C%AC%E9%9B%86.html"},{property:"og:type",content:"article"},{property:"og:title",content:"mongodb副本集"},{property:"og:description",content:"MongoDB中的副本集（Replica Set）是一组维护相同数据集的mongod服务。 副本集可提供冗余和高可用性，是所有生产部署的基础。"},{property:"og:image",content:"https://img-blog.csdnimg.cn/20200918164831238.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/02.mongodb/02.mongodb%E5%89%AF%E6%9C%AC%E9%9B%86.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-09T20:50:33.000Z"},{property:"article:tag",content:"mongodb"},{itemprop:"name",content:"mongodb副本集"},{itemprop:"description",content:"MongoDB中的副本集（Replica Set）是一组维护相同数据集的mongod服务。 副本集可提供冗余和高可用性，是所有生产部署的基础。"},{itemprop:"image",content:"https://img-blog.csdnimg.cn/20200918164831238.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/08.%E6%95%B0%E6%8D%AE%E5%BA%93/02.mongodb/02.mongodb%E5%89%AF%E6%9C%AC%E9%9B%86.html",relativePath:"04.运维/08.数据库/02.mongodb/02.mongodb副本集.md",key:"v-db6593ce",path:"/pages/359db9/",headers:[{level:2,title:"1.1 简介",slug:"_1-1-简介",normalizedTitle:"1.1 简介",charIndex:19},{level:2,title:"1.2 副本集的三个角色",slug:"_1-2-副本集的三个角色",normalizedTitle:"1.2 副本集的三个角色",charIndex:869},{level:2,title:"1.3 副本集架构目标",slug:"_1-3-副本集架构目标",normalizedTitle:"1.3 副本集架构目标",charIndex:1520},{level:2,title:"1.4 副本集的创建",slug:"_1-4-副本集的创建",normalizedTitle:"1.4 副本集的创建",charIndex:1548},{level:3,title:"1.4.1 第一步：创建主节点",slug:"_1-4-1-第一步-创建主节点",normalizedTitle:"1.4.1 第一步：创建主节点",charIndex:1563},{level:3,title:"1.4.2 第二步：创建副本节点",slug:"_1-4-2-第二步-创建副本节点",normalizedTitle:"1.4.2 第二步：创建副本节点",charIndex:3010},{level:3,title:"1.4.3 第三步：创建仲裁节点",slug:"_1-4-3-第三步-创建仲裁节点",normalizedTitle:"1.4.3 第三步：创建仲裁节点",charIndex:4459},{level:3,title:"1.4.4 第四步：初始化配置副本集和主节点",slug:"_1-4-4-第四步-初始化配置副本集和主节点",normalizedTitle:"1.4.4 第四步：初始化配置副本集和主节点",charIndex:5908},{level:3,title:"1.4.5 第五步：查看副本集的配置内容",slug:"_1-4-5-第五步-查看副本集的配置内容",normalizedTitle:"1.4.5 第五步：查看副本集的配置内容",charIndex:6755},{level:3,title:"1.4.6 第六步：查看副本集状态",slug:"_1-4-6-第六步-查看副本集状态",normalizedTitle:"1.4.6 第六步：查看副本集状态",charIndex:9208},{level:3,title:"1.4.7 第七步：添加副本从节点",slug:"_1-4-7-第七步-添加副本从节点",normalizedTitle:"1.4.7 第七步：添加副本从节点",charIndex:11531},{level:3,title:"1.4.8 第八步：添加仲裁从节点",slug:"_1-4-8-第八步-添加仲裁从节点",normalizedTitle:"1.4.8 第八步：添加仲裁从节点",charIndex:15917},{level:2,title:"1.5 副本集的数据读写操作",slug:"_1-5-副本集的数据读写操作",normalizedTitle:"1.5 副本集的数据读写操作",charIndex:20266},{level:2,title:"1.6 主节点的选举原则",slug:"_1-6-主节点的选举原则",normalizedTitle:"1.6 主节点的选举原则",charIndex:24182},{level:2,title:"1.7 故障测试",slug:"_1-7-故障测试",normalizedTitle:"1.7 故障测试",charIndex:26861},{level:3,title:"1.7.1 副本节点故障测试",slug:"_1-7-1-副本节点故障测试",normalizedTitle:"1.7.1 副本节点故障测试",charIndex:26874},{level:3,title:"1.7.2 主节点故障测试",slug:"_1-7-2-主节点故障测试",normalizedTitle:"1.7.2 主节点故障测试",charIndex:27229},{level:3,title:"1.7.3 仲裁节点和主节点故障",slug:"_1-7-3-仲裁节点和主节点故障",normalizedTitle:"1.7.3 仲裁节点和主节点故障",charIndex:27765},{level:3,title:"1.7.4 仲裁节点和从节点故障",slug:"_1-7-4-仲裁节点和从节点故障",normalizedTitle:"1.7.4 仲裁节点和从节点故障",charIndex:28065},{level:2,title:"1.8 Compass 连接副本集",slug:"_1-8-compass-连接副本集",normalizedTitle:"1.8 compass 连接副本集",charIndex:28166},{level:2,title:"1.9 SpringDataMongoDB 连接副本集",slug:"_1-9-springdatamongodb-连接副本集",normalizedTitle:"1.9 springdatamongodb 连接副本集",charIndex:28204},{level:2,title:"2.1 分片概念",slug:"_2-1-分片概念",normalizedTitle:"2.1 分片概念",charIndex:30671},{level:2,title:"2.2 分片集群包含的组件",slug:"_2-2-分片集群包含的组件",normalizedTitle:"2.2 分片集群包含的组件",charIndex:31282},{level:2,title:"2.3 分片集群架构目标",slug:"_2-3-分片集群架构目标",normalizedTitle:"2.3 分片集群架构目标",charIndex:31554},{level:2,title:"2.4 分片（存储）节点副本集的创建",slug:"_2-4-分片-存储-节点副本集的创建",normalizedTitle:"2.4 分片（存储）节点副本集的创建",charIndex:31622},{level:3,title:"2.4.1 第一套副本集",slug:"_2-4-1-第一套副本集",normalizedTitle:"2.4.1 第一套副本集",charIndex:31707},{level:4,title:"2.4.1.1 初始化副本集和创建主节点：",slug:"_2-4-1-1-初始化副本集和创建主节点",normalizedTitle:"2.4.1.1 初始化副本集和创建主节点：",charIndex:37004},{level:4,title:"2.4.1.2 添加副本节点",slug:"_2-4-1-2-添加副本节点",normalizedTitle:"2.4.1.2 添加副本节点",charIndex:37214},{level:4,title:"2.4.1.3 添加仲裁节点",slug:"_2-4-1-3-添加仲裁节点",normalizedTitle:"2.4.1.3 添加仲裁节点",charIndex:37269},{level:3,title:"2.4.2 第二套副本集",slug:"_2-4-2-第二套副本集",normalizedTitle:"2.4.2 第二套副本集",charIndex:38632},{level:4,title:"2.4.2.1 初始化副本集和创建主节点",slug:"_2-4-2-1-初始化副本集和创建主节点",normalizedTitle:"2.4.2.1 初始化副本集和创建主节点",charIndex:43039},{level:4,title:"2.4.2.2 添加副本节点",slug:"_2-4-2-2-添加副本节点",normalizedTitle:"2.4.2.2 添加副本节点",charIndex:43257},{level:4,title:"2.4.2.3 添加仲裁节点",slug:"_2-4-2-3-添加仲裁节点",normalizedTitle:"2.4.2.3 添加仲裁节点",charIndex:43312},{level:2,title:"2.5 配置节点副本集的创建",slug:"_2-5-配置节点副本集的创建",normalizedTitle:"2.5 配置节点副本集的创建",charIndex:47339},{level:3,title:"2.5.1 初始化副本集和创建主节点",slug:"_2-5-1-初始化副本集和创建主节点",normalizedTitle:"2.5.1 初始化副本集和创建主节点",charIndex:51781},{level:3,title:"2.5.2 添加两个副本节点",slug:"_2-5-2-添加两个副本节点",normalizedTitle:"2.5.2 添加两个副本节点",charIndex:51998},{level:2,title:"2.6 路由节点的创建和操作",slug:"_2-6-路由节点的创建和操作",normalizedTitle:"2.6 路由节点的创建和操作",charIndex:52214},{level:3,title:"2.6.1 第一个路由节点的创建和连接",slug:"_2-6-1-第一个路由节点的创建和连接",normalizedTitle:"2.6.1 第一个路由节点的创建和连接",charIndex:52233},{level:3,title:"2.6.2 在路由节点上进行分片配置操作",slug:"_2-6-2-在路由节点上进行分片配置操作",normalizedTitle:"2.6.2 在路由节点上进行分片配置操作",charIndex:54338},{level:4,title:"2.6.2.1 添加分片：",slug:"_2-6-2-1-添加分片",normalizedTitle:"2.6.2.1 添加分片：",charIndex:54373},{level:4,title:"2.6.2.2 开启分片功能",slug:"_2-6-2-2-开启分片功能",normalizedTitle:"2.6.2.2 开启分片功能",charIndex:57268},{level:4,title:"2.6.2.3 集合分片",slug:"_2-6-2-3-集合分片",normalizedTitle:"2.6.2.3 集合分片",charIndex:59167},{level:3,title:"2.6.3 分片后插入数据测试",slug:"_2-6-3-分片后插入数据测试",normalizedTitle:"2.6.3 分片后插入数据测试",charIndex:63680},{level:3,title:"2.6.4 再增加一个路由节点",slug:"_2-6-4-再增加一个路由节点",normalizedTitle:"2.6.4 再增加一个路由节点",charIndex:65781},{level:2,title:"2.7 Compass 连接分片集群",slug:"_2-7-compass-连接分片集群",normalizedTitle:"2.7 compass 连接分片集群",charIndex:67086},{level:2,title:"2.8 SpringDataMongDB 连接分片集群",slug:"_2-8-springdatamongdb-连接分片集群",normalizedTitle:"2.8 springdatamongdb 连接分片集群",charIndex:67171},{level:2,title:"2.9 清除所有的节点数据（备用）",slug:"_2-9-清除所有的节点数据-备用",normalizedTitle:"2.9 清除所有的节点数据（备用）",charIndex:67943},{level:2,title:"3.1 MongoDB 的用户和角色权限简介",slug:"_3-1-mongodb-的用户和角色权限简介",normalizedTitle:"3.1 mongodb 的用户和角色权限简介",charIndex:71471},{level:2,title:"3.2 单实例环境",slug:"_3-2-单实例环境",normalizedTitle:"3.2 单实例环境",charIndex:77873},{level:3,title:"3.2.1 关闭已开启的服务",slug:"_3-2-1-关闭已开启的服务",normalizedTitle:"3.2.1 关闭已开启的服务",charIndex:77942},{level:3,title:"3.2.2 添加用户和权限",slug:"_3-2-2-添加用户和权限",normalizedTitle:"3.2.2 添加用户和权限",charIndex:78562},{level:3,title:"3.2.3 服务端开启认证和客户端连接登录",slug:"_3-2-3-服务端开启认证和客户端连接登录",normalizedTitle:"3.2.3 服务端开启认证和客户端连接登录",charIndex:81158},{level:4,title:"3.2.3.1 关闭已经启动的服务",slug:"_3-2-3-1-关闭已经启动的服务",normalizedTitle:"3.2.3.1 关闭已经启动的服务",charIndex:81183},{level:4,title:"3.2.3.2 以开启认证的方式启动服务",slug:"_3-2-3-2-以开启认证的方式启动服务",normalizedTitle:"3.2.3.2 以开启认证的方式启动服务",charIndex:82110},{level:4,title:"3.2.3.3 开启了认证的情况下的客户端登录",slug:"_3-2-3-3-开启了认证的情况下的客户端登录",normalizedTitle:"3.2.3.3 开启了认证的情况下的客户端登录",charIndex:82480},{level:3,title:"3.2.4 SpringDataMongoDB连接认证",slug:"_3-2-4-springdatamongodb连接认证",normalizedTitle:"3.2.4 springdatamongodb连接认证",charIndex:86330},{level:2,title:"3.3 副本集环境",slug:"_3-3-副本集环境",normalizedTitle:"3.3 副本集环境",charIndex:86970},{level:3,title:"3.3.1 前言",slug:"_3-3-1-前言",normalizedTitle:"3.3.1 前言",charIndex:86984},{level:3,title:"3.3.2 关闭已开启的副本集服务",slug:"_3-3-2-关闭已开启的副本集服务",normalizedTitle:"3.3.2 关闭已开启的副本集服务",charIndex:87454},{level:3,title:"3.3.3 通过主节点添加一个管理员帐号",slug:"_3-3-3-通过主节点添加一个管理员帐号",normalizedTitle:"3.3.3 通过主节点添加一个管理员帐号",charIndex:88526},{level:3,title:"3.3.4 创建副本集认证的key文件",slug:"_3-3-4-创建副本集认证的key文件",normalizedTitle:"3.3.4 创建副本集认证的key文件",charIndex:88886},{level:3,title:"3.3.5 修改配置文件指定keyfile",slug:"_3-3-5-修改配置文件指定keyfile",normalizedTitle:"3.3.5 修改配置文件指定keyfile",charIndex:89658},{level:3,title:"3.3.6 重新启动副本集",slug:"_3-3-6-重新启动副本集",normalizedTitle:"3.3.6 重新启动副本集",charIndex:90279},{level:3,title:"3.3.7 在主节点上添加普通账号",slug:"_3-3-7-在主节点上添加普通账号",normalizedTitle:"3.3.7 在主节点上添加普通账号",charIndex:91122},{level:3,title:"3.3.8 SpringDataMongoDB连接副本集",slug:"_3-3-8-springdatamongodb连接副本集",normalizedTitle:"3.3.8 springdatamongodb连接副本集",charIndex:91361},{level:2,title:"3.4 分片集群环境(扩展)",slug:"_3-4-分片集群环境-扩展",normalizedTitle:"3.4 分片集群环境(扩展)",charIndex:91804},{level:3,title:"3.4.1 关闭已开启的集群服务",slug:"_3-4-1-关闭已开启的集群服务",normalizedTitle:"3.4.1 关闭已开启的集群服务",charIndex:91823},{level:3,title:"3.4.2 创建副本集认证的key文件",slug:"_3-4-2-创建副本集认证的key文件",normalizedTitle:"3.4.2 创建副本集认证的key文件",charIndex:94702},{level:3,title:"3.4.3 修改配置文件指定keyfile",slug:"_3-4-3-修改配置文件指定keyfile",normalizedTitle:"3.4.3 修改配置文件指定keyfile",charIndex:95950},{level:3,title:"3.4.4 重新启动节点",slug:"_3-4-4-重新启动节点",normalizedTitle:"3.4.4 重新启动节点",charIndex:98487},{level:3,title:"3.3.5 创建帐号和认证",slug:"_3-3-5-创建帐号和认证",normalizedTitle:"3.3.5 创建帐号和认证",charIndex:99682},{level:3,title:"3.3.6 SpringDataMongoDB连接认证",slug:"_3-3-6-springdatamongodb连接认证",normalizedTitle:"3.3.6 springdatamongodb连接认证",charIndex:101282}],headersStr:"1.1 简介 1.2 副本集的三个角色 1.3 副本集架构目标 1.4 副本集的创建 1.4.1 第一步：创建主节点 1.4.2 第二步：创建副本节点 1.4.3 第三步：创建仲裁节点 1.4.4 第四步：初始化配置副本集和主节点 1.4.5 第五步：查看副本集的配置内容 1.4.6 第六步：查看副本集状态 1.4.7 第七步：添加副本从节点 1.4.8 第八步：添加仲裁从节点 1.5 副本集的数据读写操作 1.6 主节点的选举原则 1.7 故障测试 1.7.1 副本节点故障测试 1.7.2 主节点故障测试 1.7.3 仲裁节点和主节点故障 1.7.4 仲裁节点和从节点故障 1.8 Compass 连接副本集 1.9 SpringDataMongoDB 连接副本集 2.1 分片概念 2.2 分片集群包含的组件 2.3 分片集群架构目标 2.4 分片（存储）节点副本集的创建 2.4.1 第一套副本集 2.4.1.1 初始化副本集和创建主节点： 2.4.1.2 添加副本节点 2.4.1.3 添加仲裁节点 2.4.2 第二套副本集 2.4.2.1 初始化副本集和创建主节点 2.4.2.2 添加副本节点 2.4.2.3 添加仲裁节点 2.5 配置节点副本集的创建 2.5.1 初始化副本集和创建主节点 2.5.2 添加两个副本节点 2.6 路由节点的创建和操作 2.6.1 第一个路由节点的创建和连接 2.6.2 在路由节点上进行分片配置操作 2.6.2.1 添加分片： 2.6.2.2 开启分片功能 2.6.2.3 集合分片 2.6.3 分片后插入数据测试 2.6.4 再增加一个路由节点 2.7 Compass 连接分片集群 2.8 SpringDataMongDB 连接分片集群 2.9 清除所有的节点数据（备用） 3.1 MongoDB 的用户和角色权限简介 3.2 单实例环境 3.2.1 关闭已开启的服务 3.2.2 添加用户和权限 3.2.3 服务端开启认证和客户端连接登录 3.2.3.1 关闭已经启动的服务 3.2.3.2 以开启认证的方式启动服务 3.2.3.3 开启了认证的情况下的客户端登录 3.2.4 SpringDataMongoDB连接认证 3.3 副本集环境 3.3.1 前言 3.3.2 关闭已开启的副本集服务 3.3.3 通过主节点添加一个管理员帐号 3.3.4 创建副本集认证的key文件 3.3.5 修改配置文件指定keyfile 3.3.6 重新启动副本集 3.3.7 在主节点上添加普通账号 3.3.8 SpringDataMongoDB连接副本集 3.4 分片集群环境(扩展) 3.4.1 关闭已开启的集群服务 3.4.2 创建副本集认证的key文件 3.4.3 修改配置文件指定keyfile 3.4.4 重新启动节点 3.3.5 创建帐号和认证 3.3.6 SpringDataMongoDB连接认证",content:'# Replica Sets\n\n\n# 1.1 简介\n\nMongoDB中的副本集（Replica Set）是一组维护相同数据集的mongod服务。 副本集可提供冗余和高可用性，是所有生产部署的基础。\n\n也可以说，副本集类似于有自动故障恢复功能的主从集群。通俗的讲就是用多台机器进行同一数据的异步同步，从而使多台机器拥有同一数据的多个副本，并且当主库当掉时在不需要用户干预的情况下自动切换其他备份服务器做主库。而且还可以利用副本服务器做只读服务器，实现读写分离，提高负载。\n\n（1）冗余和数据可用性\n\n复制提供冗余并提高数据可用性。 通过在不同数据库服务器上提供多个数据副本，复制可提供一定级别的容错功能，以防止丢失单个数据库服务器。\n\n在某些情况下，复制可以提供增加的读取性能，因为客户端可以将读取操作发送到不同的服务上， 在不同数据中心维护数据副本可以增加分布式应用程序的数据位置和可用性。 您还可以为专用目的维护其他副本，例如灾难恢复，报告或备份。\n\n（2）MongoDB中的复制\n\n副本集是一组维护相同数据集的mongod实例。 副本集包含多个数据承载节点和可选的一个仲裁节点。在承载数据的节点中，一个且仅一个成员被视为主节点，而其他节点被视为次要（从）节点。\n\n主节点接收所有写操作。 副本集只能有一个主要能够确认具有{w：“most”}写入关注的写入; 虽然在某些情况下，另一个mongod实例可能暂时认为自己也是主要的。主要记录其操作日志中的数据集的所有更改，即oplog。\n\n\n\n辅助(副本)节点复制主节点的oplog并将操作应用于其数据集，以使辅助节点的数据集反映主节点的数据集。 如果主要人员不在，则符合条件的中学将举行选举以选出新的主要人员。\n\n（3）主从复制和副本集区别\n\n主从集群和副本集最大的区别就是副本集没有固定的“主节点”；整个集群会选出一个“主节点”，当其挂掉后，又在剩下的从节点中选中其他节点为“主节点”，副本集总有一个活跃点(主、primary)和一个或多个备份节点(从、secondary)。\n\n\n# 1.2 副本集的三个角色\n\n副本集有两种类型三种角色\n\n两种类型：\n\n * 主节点（ Primary）类型：数据操作的主要连接点，可读写。\n * 次要（辅助、从）节点（ Secondaries）类型：数据冗余备份节点，可以读或选举。\n\n三种角色：\n\n * 主要成员（Primary）：主要接收所有写操作。就是主节点。\n * 副本成员（Replicate）：从主节点通过复制操作以维护相同的数据集，即备份数据，不可写操作，但可以读操作（但需要配置）。是默认的一种从节点类型。\n * 仲裁者（ Arbiter）：不保留任何数据的副本，只具有投票选举作用。当然也可以将仲裁服务器维护为副本集的一部分，即副本成员同时也可以是仲裁者。也是一种从节点类型。\n\n\n\n关于仲裁者的额外说明：\n\n您可以将额外的mongod实例添加到副本集作为仲裁者。 仲裁者不维护数据集。 仲裁者的目的是通过响应其他副本集成员的心跳和选举请求来维护副本集中的仲裁。 因为它们不存储数据集，所以仲裁器可以是提供副本集仲裁功能的好方法，其资源成本比具有数据集的全功能副本集成员更便宜。\n\n如果您的副本集具有偶数个成员，请添加仲裁者以获得主要选举中的“大多数”投票。 仲裁者不需要专用硬件。\n\n仲裁者将永远是仲裁者，而主要人员可能会退出并成为次要人员，而次要人员可能成为选举期间的主要人员。\n\n如果你的副本+主节点的个数是偶数，建议加一个仲裁者，形成奇数，容易满足大多数的投票。\n\n如果你的副本+主节点的个数是奇数，可以不加仲裁者。\n\n\n# 1.3 副本集架构目标\n\n一主一副本一仲裁\n\n\n\n\n# 1.4 副本集的创建\n\n\n# 1.4.1 第一步：创建主节点\n\n建立存放数据和日志的目录\n\n#-----------myrs\n#主节点\nmkdir -p /mongodb/replica_sets/myrs_27017/log \\ &\nmkdir -p /mongodb/replica_sets/myrs_27017/data/db\n\n\n1\n2\n3\n4\n\n\n新建或修改配置文件：\n\nvim /mongodb/replica_sets/myrs_27017/mongod.conf\n\n\n1\n\n\nmyrs_27017 ：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/replica_sets/myrs_27017/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/replica_sets/myrs_27017/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/replica_sets/myrs_27017/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #bindIp\n    #绑定的端口\n    port: 27017\nreplication:\n    #副本集的名称\n    replSetName: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动节点服务：\n\n[root@bobohost replica_sets]# /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 54257\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n\n\n\n# 1.4.2 第二步：创建副本节点\n\n建立存放数据和日志的目录\n\n#-----------myrs\n#副本节点\nmkdir -p /mongodb/replica_sets/myrs_27018/log \\ &\nmkdir -p /mongodb/replica_sets/myrs_27018/data/db\n\n\n1\n2\n3\n4\n\n\n新建或修改配置文件：\n\nvim /mongodb/replica_sets/myrs_27018/mongod.conf\n\n\n1\n\n\nmyrs_27018 ：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/replica_sets/myrs_27018/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/replica_sets/myrs_27018/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/replica_sets/myrs_27018/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #bindIp\n    #绑定的端口\n    port: 27018\nreplication:\n    #副本集的名称\n    replSetName: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动节点服务：\n\n[root@bobohost replica_sets]# /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 54361\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n\n\n\n# 1.4.3 第三步：创建仲裁节点\n\n建立存放数据和日志的目录\n\n#-----------myrs\n#仲裁节点\nmkdir -p /mongodb/replica_sets/myrs_27019/log \\ &\nmkdir -p /mongodb/replica_sets/myrs_27019/data/db\n\n\n1\n2\n3\n4\n\n\n新建或修改配置文件：\n\nvim /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n\n\nmyrs_27019 ：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/replica_sets/myrs_27019/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/replica_sets/myrs_27019/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/replica_sets/myrs_27019/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #bindIp\n    #绑定的端口\n    port: 27019\nreplication:\n    #副本集的名称\n    replSetName: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动节点服务：\n\n[root@bobohost replica_sets]# /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 54410\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n\n\n\n# 1.4.4 第四步：初始化配置副本集和主节点\n\n使用客户端命令连接任意一个节点，但这里尽量要连接主节点(27017节点)：\n\n/usr/local/mongodb/bin/mongo --host=180.76.159.126 --port=27017\n\n\n1\n\n\n结果，连接上之后，很多命令无法使用，，比如 show dbs 等，必须初始化副本集才行\n\n准备初始化新的副本集：\n\n语法：\n\nrs.initiate(configuration)\n\n\n1\n\n\n【示例】\n\n使用默认的配置来初始化副本集：\n\nrs.initiate()\n\n\n1\n\n\n执行结果：\n\n> rs.initiate()\n{\n    "info2" : "no configuration specified. Using a default configuration for the set",\n    "me" : "180.76.159.126:27017",\n    "ok" : 1,\n    "operationTime" : Timestamp(1565760476, 1),\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1565760476, 1),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n}\nmyrs:SECONDARY>\nmyrs:PRIMARY>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n提示：\n\n * “ok”的值为1，说明创建成功。\n * 命令行提示符发生变化，变成了一个从节点角色，此时默认不能读写。稍等片刻，回车，变成主节点。\n\n\n# 1.4.5 第五步：查看副本集的配置内容\n\n返回包含当前副本集配置的文档。\n\n语法：\n\nrs.conf(configuration)\n\n\n1\n\n\n提示：\n\nrs.config() 是该方法的别名。\n\nconfiguration：可选，如果没有配置，则使用默认主节点配置。\n\n【示例】\n\n在27017上执行副本集中当前节点的默认节点配置\n\nmyrs:PRIMARY> rs.conf()\n{\n    "_id" : "myrs",\n    "version" : 1,\n    "protocolVersion" : NumberLong(1),\n    "writeConcernMajorityJournalDefault" : true,\n    "members" : [\n        {\n            "_id" : 0,\n            "host" : "180.76.159.126:27017",\n            "arbiterOnly" : false,\n            "buildIndexes" : true,\n            "hidden" : false,\n            "priority" : 1,\n            "tags" : {\n\n            },\n            "slaveDelay" : NumberLong(0),\n            "votes" : 1\n        }\n    ],\n    "settings" : {\n        "chainingAllowed" : true,\n        "heartbeatIntervalMillis" : 2000,\n        "heartbeatTimeoutSecs" : 10,\n        "electionTimeoutMillis" : 10000,\n        "catchUpTimeoutMillis" : -1,\n        "catchUpTakeoverDelayMillis" : 30000,\n        "getLastErrorModes" : {\n\n        },\n        "getLastErrorDefaults" : {\n            "w" : 1,\n            "wtimeout" : 0\n        },\n        "replicaSetId" : ObjectId("5d539bdcd6a308e600d126bb")\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n说明：\n\n * "_id" : "myrs" ：副本集的配置数据存储的主键值，默认就是副本集的名字\n * "members" ：副本集成员数组，此时只有一个： “host” : “180.76.159.126:27017” ，该成员不是仲裁节点：”arbiterOnly” : false ，优先级（权重值）： “priority” : 1,\n * "settings" ：副本集的参数配置。\n\n提示：副本集配置的查看命令，本质是查询的是 system.replset 的表中的数据：\n\nmyrs:PRIMARY> use local\nswitched to db local\nmyrs:PRIMARY> show collections\noplog.rs\nreplset.election\nreplset.minvalid\nreplset.oplogTruncateAfterPoint\nstartup_log\nsystem.replset\nsystem.rollback.id\nmyrs:PRIMARY> db.system.replset.find()\n{ "_id" : "myrs", "version" : 1, "protocolVersion" : NumberLong(1),\n"writeConcernMajorityJournalDefault" : true, "members" : [ { "_id" : 0, "host" :\n"180.76.159.126:27017", "arbiterOnly" : false, "buildIndexes" : true, "hidden" :\nfalse, "priority" : 1, "tags" : { }, "slaveDelay" : NumberLong(0), "votes" : 1\n} ], "settings" : { "chainingAllowed" : true, "heartbeatIntervalMillis" : 2000,\n"heartbeatTimeoutSecs" : 10, "electionTimeoutMillis" : 10000,\n"catchUpTimeoutMillis" : -1, "catchUpTakeoverDelayMillis" : 30000,\n"getLastErrorModes" : { }, "getLastErrorDefaults" : { "w" : 1, "wtimeout" : 0\n}, "replicaSetId" : ObjectId("5d539bdcd6a308e600d126bb") } }\nmyrs:PRIMARY>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 1.4.6 第六步：查看副本集状态\n\n检查副本集状态。\n\n说明：\n\n返回包含状态信息的文档。此输出使用从副本集的其他成员发送的心跳包中获得的数据反映副本集的当前状态。\n\n语法：\n\nrs.status()\n\n\n1\n\n\n【示例】\n\n在27017上查看副本集状态：\n\nmyrs:PRIMARY> rs.status()\n{\n    "set" : "myrs",\n    "date" : ISODate("2019-08-14T05:29:45.161Z"),\n    "myState" : 1,\n    "term" : NumberLong(1),\n    "syncingTo" : "",\n    "syncSourceHost" : "",\n    "syncSourceId" : -1,\n    "heartbeatIntervalMillis" : NumberLong(2000),\n    "optimes" : {\n        "lastCommittedOpTime" : {\n            "ts" : Timestamp(1565760578, 1),\n            "t" : NumberLong(1)\n        },\n        "readConcernMajorityOpTime" : {\n            "ts" : Timestamp(1565760578, 1),\n            "t" : NumberLong(1)\n        },\n        "appliedOpTime" : {\n            "ts" : Timestamp(1565760578, 1),\n            "t" : NumberLong(1)\n        },\n        "durableOpTime" : {\n            "ts" : Timestamp(1565760578, 1),\n            "t" : NumberLong(1)\n        }\n    },\n    "lastStableCheckpointTimestamp" : Timestamp(1565760528, 1),\n    "members" : [\n        {\n        "_id" : 0,\n        "name" : "180.76.159.126:27017",\n        "health" : 1,\n        "state" : 1,\n        "stateStr" : "PRIMARY",\n        "uptime" : 419,\n        "optime" : {\n            "ts" : Timestamp(1565760578, 1),\n            "t" : NumberLong(1)\n        },\n        "optimeDate" : ISODate("2019-08-14T05:29:38Z"),\n        "syncingTo" : "",\n        "syncSourceHost" : "",\n        "syncSourceId" : -1,\n        "infoMessage" : "could not find member to sync from",\n        "electionTime" : Timestamp(1565760476, 2),\n        "electionDate" : ISODate("2019-08-14T05:27:56Z"),\n        "configVersion" : 1,\n        "self" : true,\n        "lastHeartbeatMessage" : ""\n        }\n    ],\n    "ok" : 1,\n    "operationTime" : Timestamp(1565760578, 1),\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1565760578, 1),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n说明：\n\n * "set" : "myrs" ：副本集的名字\n * "myState" : 1：说明状态正常\n * "members" ：副本集成员数组，此时只有一个： "name" : "180.76.159.126:27017" ，该成员的角色是 "stateStr" : "PRIMARY", 该节点是健康的： "health" : 1 。\n\n\n# 1.4.7 第七步：添加副本从节点\n\n在主节点添加从节点，将其他成员加入到副本集\n\n语法：\n\nrs.add(host, arbiterOnly)\n\n\n1\n\n\n选项：\n\nPARAMETER     TYPE                 DESCRIPTION\nhost          string or document   要添加到副本集的新成员。\n                                   指定为字符串或配置文档：1）如果是一个字符串，则需要指定新成员的主机名和可选的端口号；2）如果是一个文档，请指定在members数组中找到的副本集成员配置文档。\n                                   您必须在成员配置文档中指定主机字段。有关文档配置字段的说明，详见下方文档：“主机成员的配置文档”\narbiterOnly   boolean              可选的。 仅在 值为字符串时适用。 如果为true，则添加的主机是仲裁者。\n\n主机成员的配置文档：\n\n{\n    _id: <int>,\n    host: <string>,     // required\n    arbiterOnly: <boolean>,\n    buildIndexes: <boolean>,\n    hidden: <boolean>,\n    priority: <number>,\n    tags: <document>,\n    slaveDelay: <int>,\n    votes: <number>\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n【示例】\n\n将27018的副本节点添加到副本集中：\n\nmyrs:PRIMARY> rs.add("180.76.159.126:27018")\n{\n    "ok" : 1,\n    "operationTime" : Timestamp(1565761757, 1),\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1565761757, 1),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n说明：\n\n * “ok” : 1 ：说明添加成功。\n\n查看副本集状态：\n\nmyrs:PRIMARY> rs.status()\n{\n    "set" : "myrs",\n    "date" : ISODate("2019-08-14T05:50:05.738Z"),\n    "myState" : 1,\n    "term" : NumberLong(1),\n    "syncingTo" : "",\n    "syncSourceHost" : "",\n    "syncSourceId" : -1,\n    "heartbeatIntervalMillis" : NumberLong(2000),\n    "optimes" : {\n        "lastCommittedOpTime" : {\n            "ts" : Timestamp(1565761798, 1),\n            "t" : NumberLong(1)\n        },\n        "readConcernMajorityOpTime" : {\n            "ts" : Timestamp(1565761798, 1),\n            "t" : NumberLong(1)\n        },\n        "appliedOpTime" : {\n            "ts" : Timestamp(1565761798, 1),\n            "t" : NumberLong(1)\n        },\n        "durableOpTime" : {\n            "ts" : Timestamp(1565761798, 1),\n            "t" : NumberLong(1)\n        }\n    },\n    "lastStableCheckpointTimestamp" : Timestamp(1565761798, 1),\n    "members" : [\n        {\n            "_id" : 0,\n            "name" : "180.76.159.126:27017",\n            "health" : 1,\n            "state" : 1,\n            "stateStr" : "PRIMARY",\n            "uptime" : 1639,\n            "optime" : {\n                "ts" : Timestamp(1565761798, 1),\n                "t" : NumberLong(1)\n            },\n            "optimeDate" : ISODate("2019-08-14T05:49:58Z"),\n            "syncingTo" : "",\n            "syncSourceHost" : "",\n            "syncSourceId" : -1,\n            "infoMessage" : "",\n            "electionTime" : Timestamp(1565760476, 2),\n            "electionDate" : ISODate("2019-08-14T05:27:56Z"),\n            "configVersion" : 2,\n            "self" : true,\n            "lastHeartbeatMessage" : ""\n        },\n        {\n            "_id" : 1,\n            "name" : "180.76.159.126:27018",\n            "health" : 1,\n            "state" : 2,\n            "stateStr" : "SECONDARY",\n            "uptime" : 48,\n            "optime" : {\n                "ts" : Timestamp(1565761798, 1),\n                "t" : NumberLong(1)\n            },\n            "optimeDurable" : {\n                "ts" : Timestamp(1565761798, 1),\n                "t" : NumberLong(1)\n            },\n            "optimeDate" : ISODate("2019-08-14T05:49:58Z"),\n            "optimeDurableDate" : ISODate("2019-08-14T05:49:58Z"),\n            "lastHeartbeat" : ISODate("2019-08-14T05:50:05.294Z"),\n            "lastHeartbeatRecv" : ISODate("2019-08-\n            14T05:50:05.476Z"),\n            "pingMs" : NumberLong(0),\n            "lastHeartbeatMessage" : "",\n            "syncingTo" : "180.76.159.126:27017",\n            "syncSourceHost" : "180.76.159.126:27017",\n            "syncSourceId" : 0,\n            "infoMessage" : "",\n            "configVersion" : 2\n        }\n    ],\n    "ok" : 1,\n    "operationTime" : Timestamp(1565761798, 1),\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1565761798, 1),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n\n\n说明：\n\n * "name" : "180.76.159.126:27018" 是第二个节点的名字，其角色是 "stateStr" : "SECONDARY"\n\n\n# 1.4.8 第八步：添加仲裁从节点\n\n添加一个仲裁节点到副本集\n\n语法：\n\nrs.addArb(host)\n\n\n1\n\n\n将27019的仲裁节点添加到副本集中：\n\nmyrs:PRIMARY> rs.addArb("180.76.159.126:27019")\n{\n    "ok" : 1,\n    "operationTime" : Timestamp(1565761959, 1),\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1565761959, 1),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n说明：\n\n * "ok" : 1 ：说明添加成功。\n\n查看副本集状态：\n\nmyrs:PRIMARY> rs.status()\n{\n    "set" : "myrs",\n    "date" : ISODate("2019-08-14T05:53:27.198Z"),\n    "myState" : 1,\n    "term" : NumberLong(1),\n    "syncingTo" : "",\n    "syncSourceHost" : "",\n    "syncSourceId" : -1,\n    "heartbeatIntervalMillis" : NumberLong(2000),\n    "optimes" : {\n        "lastCommittedOpTime" : {\n            "ts" : Timestamp(1565761998, 1),\n            "t" : NumberLong(1)\n        },\n        "readConcernMajorityOpTime" : {\n            "ts" : Timestamp(1565761998, 1),\n            "t" : NumberLong(1)\n        },\n        "appliedOpTime" : {\n            "ts" : Timestamp(1565761998, 1),\n            "t" : NumberLong(1)\n        },\n        "durableOpTime" : {\n            "ts" : Timestamp(1565761998, 1),\n            "t" : NumberLong(1)\n        }\n    },\n    "lastStableCheckpointTimestamp" : Timestamp(1565761978, 1),\n    "members" : [\n        {\n            "_id" : 0,\n            "name" : "180.76.159.126:27017",\n            "health" : 1,\n            "state" : 1,\n            "stateStr" : "PRIMARY",\n            "uptime" : 1841,\n            "optime" : {\n                "ts" : Timestamp(1565761998, 1),\n                "t" : NumberLong(1)\n            },\n            "optimeDate" : ISODate("2019-08-14T05:53:18Z"),\n            "syncingTo" : "",\n            "syncSourceHost" : "",\n            "syncSourceId" : -1,\n            "infoMessage" : "",\n            "electionTime" : Timestamp(1565760476, 2),\n            "electionDate" : ISODate("2019-08-14T05:27:56Z"),\n            "configVersion" : 3,\n            "self" : true,\n            "lastHeartbeatMessage" : ""\n        },\n        {\n            "_id" : 1,\n            "name" : "180.76.159.126:27018",\n            "health" : 1,\n            "state" : 2,\n            "stateStr" : "SECONDARY",\n            "uptime" : 249,\n            "optime" : {\n                "ts" : Timestamp(1565761998, 1),\n                "t" : NumberLong(1)\n            },\n            "optimeDurable" : {\n                "ts" : Timestamp(1565761998, 1),\n                "t" : NumberLong(1)\n            },\n            "optimeDate" : ISODate("2019-08-14T05:53:18Z"),\n            "optimeDurableDate" : ISODate("2019-08-14T05:53:18Z"),\n            "lastHeartbeat" : ISODate("2019-08-14T05:53:25.668Z"),\n            "lastHeartbeatRecv" : ISODate("2019-08-14T05:53:26.702Z"),\n            "pingMs" : NumberLong(0),\n            "lastHeartbeatMessage" : "",\n            "syncingTo" : "180.76.159.126:27017",\n            "syncSourceHost" : "180.76.159.126:27017",\n            "syncSourceId" : 0,\n            "infoMessage" : "",\n            "configVersion" : 3\n        },\n        {\n            "_id" : 2,\n            "name" : "180.76.159.126:27019",\n            "health" : 1,\n            "state" : 7,\n            "stateStr" : "ARBITER",\n            "uptime" : 47,\n            "lastHeartbeat" : ISODate("2019-08-14T05:53:25.668Z"),\n            "lastHeartbeatRecv" : ISODate("2019-08-14T05:53:25.685Z"),\n            "pingMs" : NumberLong(0),\n            "lastHeartbeatMessage" : "",\n            "syncingTo" : "",\n            "syncSourceHost" : "",\n            "syncSourceId" : -1,\n            "infoMessage" : "",\n            "configVersion" : 3\n        }\n    ],\n    "ok" : 1,\n    "operationTime" : Timestamp(1565761998, 1),\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1565761998, 1),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n\n\n说明：\n\n * "name" : "180.76.159.126:27019" 是第二个节点的名字，其角色是 "stateStr" : "ARBITER"\n\n\n# 1.5 副本集的数据读写操作\n\n目标：测试三个不同角色的节点的数据读写情况\n\n登录主节点27017，写入和读取数据：\n\n[root@bobohost ~]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017\nmyrs:PRIMARY> use articledb\nswitched to db articledb\nmyrs:PRIMARY> db\narticledb\nmyrs:PRIMARY> db.comment.insert({"articleid":"100000","content":"今天天气真好，阳光明媚","userid":"1001","nickname":"Rose","createdatetime":new Date()})\nWriteResult({ "nInserted" : 1 })\nmyrs:PRIMARY> db.comment.find()\n{ "_id" : ObjectId("5d4d2ae3068138b4570f53bf"), "articleid" : "100000","content" : "今天天气真好，阳光明媚", "userid" : "1001", "nickname" : "Rose","createdatetime" : ISODate("2019-08-09T08:12:19.427Z") }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n登录从节点27018\n\n[root@bobohost ~]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27018\nmyrs:SECONDARY> show dbs;\n2019-09-10T10:56:51.953+0800 E QUERY  [js] Error: listDatabases failed:{\n    "operationTime" : Timestamp(1568084204, 1),\n    "ok" : 0,\n    "errmsg" : "not master and slaveOk=false",\n    "code" : 13435,\n    "codeName" : "NotMasterNoSlaveOk",\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1568084204, 1),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n       }\n   }\n} :\n_getErrorWithCode@src/mongo/shell/utils.js:25:13\nMongo.prototype.getDBs@src/mongo/shell/mongo.js:139:1\nshellHelper.show@src/mongo/shell/utils.js:882:13\nshellHelper@src/mongo/shell/utils.js:766:15\n@(shellhelp2):1:1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n发现，不能读取集合的数据。当前从节点只是一个备份，不是奴隶节点，无法读取数据，写当然更不行。\n\n因为默认情况下，从节点是没有读写权限的，可以增加读的权限，但需要进行设置\n\n设置读操作权限：\n\n说明：\n\n设置为奴隶节点，允许在从成员上运行读的操作\n\n语法：\n\nrs.slaveOk()\n#或\nrs.slaveOk(true)\n\n\n1\n2\n3\n\n\n提示：\n\n该命令是 db.getMongo().setSlaveOk() 的简化命令。\n\n【示例】\n\n在27018上设置作为奴隶节点权限，具备读权限：\n\nrs:SECONDARY> rs.slaveOk()\n\n\n1\n\n\n此时，在执行查询命令，运行成功！\n\n但仍然不允许插入。\n\nmyrs:SECONDARY> rs.slaveOk()\nmyrs:SECONDARY> show dbs;\nadmin    0.000GB\narticledb  0.000GB\nconfig   0.000GB\nlocal    0.000GB\nmyrs:SECONDARY> use articledb\nswitched to db articledb\nmyrs:SECONDARY> show collections\ncomment\nmyrs:SECONDARY> db.comment.find()\n{ "_id" : ObjectId("5d7710c04cfd7eee2e3cdabe"), "articleid" : "100000","content" : "今天天气真好，阳光明媚", "userid" : "1001", "nickname" : "Rose","createdatetime" : ISODate("2019-09-10T02:56:00.467Z") }\nmyrs:SECONDARY> db.comment.insert({"_id":"1","articleid":"100001","content":"我们不应该把清晨浪费在手机上，健康很重要，k一杯温水幸福你我他。","userid":"1002","nickname":"相忘于江湖","createdatetime":new Date("2019-08-05T22:08:15.522Z"),"likenum":NumberInt(1000),"state":"1"})\nWriteCommandError({\n    "operationTime" : Timestamp(1568084434, 1),\n    "ok" : 0,\n    "errmsg" : "not master",\n    "code" : 10107,\n    "codeName" : "NotMaster",\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1568084434, 1),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n现在可实现了读写分离，让主插入数据，让从来读取数据。\n\n如果要取消作为奴隶节点的读权限：\n\nmyrs:SECONDARY> rs.slaveOk(false)\nmyrs:SECONDARY> db.comment.find()\nError: error: {\n    "operationTime" : Timestamp(1568084459, 1),\n    "ok" : 0,\n    "errmsg" : "not master and slaveOk=false",\n    "code" : 13435,\n    "codeName" : "NotMasterNoSlaveOk",\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1568084459, 1),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n仲裁者节点，不存放任何业务数据的，可以登录查看\n\n[root@bobohost ~]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27019\nmyrs:ARBITER> rs.slaveOk()\nmyrs:ARBITER> show dbs\nlocal  0.000GB\nmyrs:ARBITER> use local\nswitched to db local\nmyrs:ARBITER> show collections\nreplset.minvalid\nreplset.oplogTruncateAfterPoint\nstartup_log\nsystem.replset\nsystem.rollback.id\nmyrs:ARBITER>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n发现，只存放副本集配置等数据。\n\n\n# 1.6 主节点的选举原则\n\nMongoDB在副本集中，会自动进行主节点的选举，主节点选举的触发条件：\n\n * 主节点故障\n * 主节点网络不可达（默认心跳信息为10秒）\n * 人工干预（rs.stepDown(600)）\n\n一旦触发选举，就要根据一定规则来选主节点。\n\n选举规则是根据票数来决定谁获胜：\n\n * 票数最高，且获得了 “大多数”成员的投票支持的节点获胜。\n   \n   “大多数”的定义为：假设复制集内投票成员数量为N，则大多数为 N/2 + 1。例如：3个投票成员，则大多数的值是2。当复制集内存活成员数量不足大多数时，整个复制集将无法选举出Primary，复制集将无法提供写服务，处于只读状态。\n\n * 若票数相同，且都获得了 “大多数”成员的投票支持的，数据新的节点获胜。\n   \n   数据的新旧是通过操作日志oplog来对比的。\n\n在获得票数的时候，优先级（priority）参数影响重大。\n\n可以通过设置优先级（priority）来设置额外票数。优先级即权重，取值为0-1000，相当于可额外增加0-1000的票数，优先级的值越大，就越可能获得多数成员的投票（votes）数。指定较高的值可使成员更有资格成为主要成员，更低的值可使成员更不符合条件。\n\n默认情况下，优先级的值是1\n\nmyrs:PRIMARY> rs.conf()\n{\n    "_id" : "myrs",\n    "version" : 3,\n    "protocolVersion" : NumberLong(1),\n    "writeConcernMajorityJournalDefault" : true,\n    "members" : [\n        {\n            "_id" : 0,\n            "host" : "180.76.159.126:27017",\n            "arbiterOnly" : false,\n            "buildIndexes" : true,\n            "hidden" : false,\n            "priority" : 1,\n            "tags" : {\n\n            },\n            "slaveDelay" : NumberLong(0),\n            "votes" : 1\n        },\n        {\n            "_id" : 1,\n            "host" : "180.76.159.126:27018",\n            "arbiterOnly" : false,\n            "buildIndexes" : true,\n            "hidden" : false,\n            "priority" : 1,\n            "tags" : {\n\n            },\n            "slaveDelay" : NumberLong(0),\n            "votes" : 1\n        },\n        {\n            "_id" : 2,\n            "host" : "180.76.159.126:27019",\n            "arbiterOnly" : true,\n            "buildIndexes" : true,\n            "hidden" : false,\n            "priority" : 0,\n            "tags" : {\n\n            },\n            "slaveDelay" : NumberLong(0),\n            "votes" : 1\n        }\n    ],\n    "settings" : {\n        "chainingAllowed" : true,\n        "heartbeatIntervalMillis" : 2000,\n        "heartbeatTimeoutSecs" : 10,\n        "electionTimeoutMillis" : 10000,\n        "catchUpTimeoutMillis" : -1,\n        "catchUpTakeoverDelayMillis" : 30000,\n        "getLastErrorModes" : {\n\n        },\n        "getLastErrorDefaults" : {\n            "w" : 1,\n            "wtimeout" : 0\n        },\n        "replicaSetId" : ObjectId("5d539bdcd6a308e600d126bb")\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n可以看出，主节点和副本节点的优先级各为 1，即，默认可以认为都已经有了一票。但选举节点，优先级是0，（要注意是，官方说了，选举节点的优先级必须是0，不能是别的值。即不具备选举权，但具有投票权）\n\n【了解】修改优先级\n\n比如，下面提升从节点的优先级：\n\n1）先将配置导入cfg变量\n\nmyrs:SECONDARY> cfg=rs.conf()\n\n\n1\n\n\n2 ）然后修改值（ID号默认从0开始）\n\nmyrs:SECONDARY> cfg.members[1].priority=2\n2\n\n\n1\n2\n\n\n3 ）重新加载配置\n\nmyrs:SECONDARY> rs.reconfig(cfg)\n{ "ok" : 1 }\n\n\n1\n2\n\n\n稍等片刻会重新开始选举。\n\n\n# 1.7 故障测试\n\n\n# 1.7.1 副本节点故障测试\n\n关闭27018副本节点：\n\n发现，主节点和仲裁节点对27018的心跳失败。因为主节点还在，因此，没有触发投票选举。\n\n如果此时，在主节点写入数据。\n\ndb.comment.insert({"_id":"1","articleid":"100001","content":"我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。","userid":"1002","nickname":"相忘于江湖","createdatetime":new Date("2019-08-05T22:08:15.522Z"),"likenum":NumberInt(1000),"state":"1"})\n\n\n1\n\n\n再启动从节点，会发现，主节点写入的数据，会自动同步给从节点。\n\n\n# 1.7.2 主节点故障测试\n\n关闭27017节点\n\n发现，从节点和仲裁节点对27017的心跳失败，当失败超过10秒，此时因为没有主节点了，会自动发起投票。\n\n而副本节点只有27018，因此，候选人只有一个就是27018，开始投票。\n\n27019向27018投了一票，27018本身自带一票，因此共两票，超过了“大多数”\n\n27019是仲裁节点，没有选举权，27018不向其投票，其票数是0.\n\n最终结果，27018成为主节点。具备读写功能。\n\n在27018写入数据查看。\n\ndb.comment.insert({"_id":"2","articleid":"100001","content":"我夏天空腹喝凉开水，冬天喝温开水","userid":"1005","nickname":"伊人憔悴","createdatetime":new Date("2019-08-05T23:58:51.485Z"),"likenum":NumberInt(888),"state":"1"})\n\n\n1\n\n\n再启动 27017节点，发现27017变成了从节点，27018仍保持主节点。\n\n登录27017节点，发现是从节点了，数据自动从27018同步。\n\n从而实现了高可用。\n\n\n# 1.7.3 仲裁节点和主节点故障\n\n先关掉仲裁节点27019，\n\n关掉现在的主节点27018\n\n登录27017后，发现，27017仍然是从节点，副本集中没有主节点了，导致此时，副本集是只读状态，无法写入。\n\n为啥不选举了？因为27017的票数，没有获得大多数，即没有大于等于2，它只有默认的一票（优先级是1）\n\n如果要触发选举，随便加入一个成员即可。\n\n * 如果只加入 27019仲裁节点成员，则主节点一定是27017，因为没得选了，仲裁节点不参与选举，但参与投票。（不演示）\n * 如果只加入 27018节点，会发起选举。因为27017和27018都是两票，则按照谁数据新，谁当主节点。\n\n\n# 1.7.4 仲裁节点和从节点故障\n\n先关掉仲裁节点27019，\n\n关掉现在的副本节点27018\n\n10秒后，27017主节点自动降级为副本节点。（服务降级）\n\n副本集不可写数据了，已经故障了。\n\n\n# 1.8 Compass 连接副本集\n\ncompass连接：\n\n\n\n\n\n\n# 1.9 SpringDataMongoDB 连接副本集\n\n副本集语法：\n\nmongodb://host1,host2,host3/articledb?connect=replicaSet&slaveOk=true&replicaSet=副本集名字\n\n\n1\n\n\n其中：\n\n * slaveOk=true ：开启副本节点读的功能，可实现读写分离。\n * connect=replicaSet ：自动到副本集中选择读写的主机。如果slaveOK是打开的，则实现了读写分离\n\n【示例】\n\n连接 replica set 三台服务器 (端口 27017, 27018, 和27019)，直接连接第一个服务器，无论是replica set一部分或者主服务器或者从服务器，写入操作应用在主服务器 并且分布查询到从服务器。\n\n修改配置文件application.yml\n\nspring:\n    #数据源配置\n    data:\n        mongodb:\n            # 主机地址\n            #   host: 180.76.159.126\n            # 数据库\n            #   database: articledb\n            # 默认端口是27017\n            #   port: 27017\n            #也可以使用uri连接\n            #uri: mongodb://192.168.40.134:27017/articledb\n            # 副本集的连接字符串\n            uri: mongodb://180.76.159.126:27017,180.76.159.126:27018,180.76.159.126:27019/articledb?connect=replicaSet&slaveOk=true&replicaSet=myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n注意：\n\n主机必须是副本集中所有的主机，包括主节点、副本节点、仲裁节点。\n\nSpringDataMongoDB自动实现了读写分离：\n\n写操作时，只打开主节点连接；读操作是，同时打开主节点和从节点连接，但使用从节点获取数据。\n\n完整的连接字符串的参考（了解）：\n\nMongoDB客户端连接语法：\n\nmongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]\n\n\n1\n\n * mongodb:// 这是固定的格式，必须要指定。\n * username:password@ 可选项，如果设置，在连接数据库服务器之后，驱动都会尝试登陆这个数据库\n * host1 必须的指定至少一个host, host1 是这个URI唯一要填写的。它指定了要连接服务器的地址。如果要连接复制集，请指定多个主机地址。\n * portX 可选的指定端口，如果不填，默认为27017\n * /database 如果指定username:password@，连接并验证登陆指定数据库。若不指定，默认打开test 数据库。\n * ?options 是连接选项。如果不使用/database，则前面需要加上/。所有连接选项都是键值对name=value，键值对之间通过&或;（分号）隔开\n\n标准的连接格式包含了多个选项(options)，如下所示：\n\n选项                    描述\nreplicaSet=name       验证replica set的名称。 Impliesconnect=replicaSet.\nslaveOk=true|false    true:在connect=direct模式下，驱动会连接第一台机器，即使这台服务器不是主。在connect=replicaSet模式下，驱动会发送所有的写请求到主并且把读取操作分布在其他从服务器。false:\n                      在connect=direct模式下，驱动会自动找寻主服务器. 在connect=replicaSet\n                      模式下，驱动仅仅连接主服务器，并且所有的读写命令都连接到主服务器。\nsafe=true|false       true: 在执行更新操作之后，驱动都会发送getLastError命令来确保更新成功。(还要参考\n                      wtimeoutMS).false: 在每次更新之后，驱动不会发送getLastError来确保更新成功。\nw=n                   驱动添加 { w : n } 到getLastError命令. 应用于safe=true。\nwtimeoutMS=ms         驱动添加 { wtimeout : ms } 到 getlasterror 命令. 应用于 safe=true.\nfsync=true|false      true: 驱动添加 { fsync : true } 到 getlasterror\n                      命令.应用于safe=true.false: 驱动不会添加到getLastError命令中。\njournal=true|false    如果设置为 true, 同步到 journal (在提交到数据库前写入到实体中).应用于 safe=true\nconnectTimeoutMS=ms   可以打开连接的时间\nsocketTimeoutMS=ms    发送和接受sockets的时间\n\n\n# 2. 分片集群-Sharded Cluster\n\n\n# 2.1 分片概念\n\n分片（sharding）是一种跨多台机器分布数据的方法， MongoDB使用分片来支持具有非常大的数据集和高吞吐量操作的部署。\n\n换句话说：分片(sharding)是指将数据拆分，将其分散存在不同的机器上的过程。有时也用分区(partitioning)来表示这个概念。将数据分散到不同的机器上，不需要功能强大的大型计算机就可以储存更多的数据，处理更多的负载。\n\n具有大型数据集或高吞吐量应用程序的数据库系统可以会挑战单个服务器的容量。例如，高查询率会耗尽服务器的CPU容量。工作集大小大于系统的RAM会强调磁盘驱动器的I / O容量。\n\n有两种解决系统增长的方法：垂直扩展和水平扩展。\n\n垂直扩展意味着增加单个服务器的容量，例如使用更强大的CPU，添加更多RAM或增加存储空间量。可用技术的局限性可能会限制单个机器对于给定工作负载而言足够强大。此外，基于云的提供商基于可用的硬件配置具有硬性上限。结果，垂直缩放有实际的最大值。\n\n水平扩展意味着划分系统数据集并加载多个服务器，添加其他服务器以根据需要增加容量。虽然单个机器的总体速度或容量可能不高，但每台机器处理整个工作负载的子集，可能提供比单个高速大容量服务器更高的效率。扩展部署容量只需要根据需要添加额外的服务器，这可能比单个机器的高端硬件的总体成本更低。权衡是基础架构和部署维护的复杂性增加。\n\nMongoDB支持通过分片进行水平扩展。\n\n\n# 2.2 分片集群包含的组件\n\nMongoDB分片群集包含以下组件：\n\n * 分片（存储）：每个分片包含分片数据的子集。 每个分片都可以部署为副本集。\n * mongos （路由）：mongos充当查询路由器，在客户端应用程序和分片集群之间提供接口。\n * config servers （“调度”的配置）：配置服务器存储群集的元数据和配置设置。 从MongoDB 3.4开始，必须将配置服务器部署为副本集（CSRS）。\n\n下图描述了分片集群中组件的交互：\n\n\n\nMongoDB在集合级别对数据进行分片，将集合数据分布在集群中的分片上。\n\n\n# 2.3 分片集群架构目标\n\n两个分片节点副本集（3+3）+一个配置节点副本集（3）+两个路由节点（2），共11个服务节点。\n\n\n\n\n# 2.4 分片（存储）节点副本集的创建\n\n所有的的配置文件都直接放到 sharded_cluster 的相应的子目录下面，默认配置文件名字：mongod.conf\n\n\n# 2.4.1 第一套副本集\n\n准备存放数据和日志的目录：\n\n#-----------myshardrs01\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27018/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27018/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27118/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27118/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27218/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27218/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n\n\n1\n\n\nmyshardrs01_27018 ：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs01_27018/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/sharded_cluster/myshardrs01_27018/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/sharded_cluster/myshardrs01_27018/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #bindIp\n    #绑定的端口\n    port: 27018\nreplication:\n    #副本集的名称\n    replSetName: myshardrs01\nsharding:\n    #分片角色\n    clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nsharding.clusterRole：\n\nVALUE       DESCRIPTION\nconfigsvr   Start this instance as a config server. The instance starts\n            on port 27019 by default.\nshardsvr    Start this instance as a shard . The instance starts on port\n            27018 by default.\n\n注意：\n\n设置sharding.clusterRole需要mongod实例运行复制。 要将实例部署为副本集成员，请使用replSetName设置并指定副本集的名称。\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n\n\n1\n\n\nmyshardrs01_27118 ：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs01_27118/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/sharded_cluster/myshardrs01_27118/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/sharded_cluster/myshardrs01_27118/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #绑定的端口\n    port: 27118\nreplication:\n    replSetName: myshardrs01\nsharding:\n    clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n\n\n1\n\n\nmyshardrs01_27218 ：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs01_27218/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/sharded_cluster/myshardrs01_27218/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/sharded_cluster/myshardrs01_27218/log/mongod.pid"\nnet:\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #绑定的端口\n    port: 27218\nreplication:\n    replSetName: myshardrs01\nsharding:\n    clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n启动第一套副本集：一主一副本一仲裁\n\n依次启动三个mongod服务：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123223\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123292\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123326\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n查看服务是否启动：\n\n[root@bobohost bin]# ps -ef |grep mongod\npolkitd  61622  61604  0 7月31 ?    00:04:29 mongod --bind_ip_all\nroot   123223    1  1 01:10 ?     00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\nroot   123292    1  4 01:11 ?     00:00:00 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\nroot   123326    1  6 01:11 ?     00:00:00 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n\n\n1\n2\n3\n4\n5\n\n\n# 2.4.1.1 初始化副本集和创建主节点：\n\n使用客户端命令连接任意一个节点，但这里尽量要连接主节点：\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27018\n\n\n1\n\n\n执行初始化副本集命令：\n\nrs.initiate()\n\n\n1\n\n\n查看副本集情况\n\nrs.status()\n\n\n1\n\n\n主节点配置查看\n\nrs.conf()\n\n\n1\n\n\n# 2.4.1.2 添加副本节点\n\nrs.add("180.76.159.126:27118")\n\n\n1\n\n\n# 2.4.1.3 添加仲裁节点\n\nrs.addArb("180.76.159.126:27218")\n\n\n1\n\n\n查看副本集的配置情况\n\nmyshardrs01:PRIMARY> rs.conf()\n{\n    "_id" : "myshardrs01",\n    "version" : 3,\n    "protocolVersion" : NumberLong(1),\n    "writeConcernMajorityJournalDefault" : true,\n    "members" : [\n        {\n            "_id" : 0,\n            "host" : "180.76.159.126:27018",\n            "arbiterOnly" : false,\n            "buildIndexes" : true,\n            "hidden" : false,\n            "priority" : 1,\n            "tags" : {\n\n            },\n            "slaveDelay" : NumberLong(0),\n            "votes" : 1\n        },\n        {\n            "_id" : 1,\n            "host" : "180.76.159.126:27118",\n            "arbiterOnly" : false,\n            "buildIndexes" : true,\n            "hidden" : false,\n            "priority" : 1,\n            "tags" : {\n\n            },\n            "slaveDelay" : NumberLong(0),\n            "votes" : 1\n        },\n        {\n            "_id" : 2,\n            "host" : "180.76.159.126:27218",\n            "arbiterOnly" : true,\n            "buildIndexes" : true,\n            "hidden" : false,\n            "priority" : 0,\n            "tags" : {\n\n            },\n            "slaveDelay" : NumberLong(0),\n            "votes" : 1\n        }\n    ],\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# 2.4.2 第二套副本集\n\n准备存放数据和日志的目录：\n\n#-----------myshardrs02\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27318/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27318/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27418/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27418/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27518/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27518/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n\n\n1\n\n\nmyshardrs02_27318 ：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs02_27318/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/sharded_cluster/myshardrs02_27318/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/sharded_cluster/myshardrs02_27318/log/mongod.pid"\nnet:\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #绑定的端口\n    port: 27318\nreplication:\n    replSetName: myshardrs02\nsharding:\n    clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n\n\n1\n\n\nmyshardrs02_27418 ：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs02_27418/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/sharded_cluster/myshardrs02_27418/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/sharded_cluster/myshardrs02_27418/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #绑定的端口\n    port: 27418\nreplication:\n    replSetName: myshardrs02\nsharding:\n    clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\n\n\n1\n\n\nmyshardrs02_27518 ：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs02_27518/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/sharded_cluster/myshardrs02_27518/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/sharded_cluster/myshardrs02_27518/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #绑定的端口\n    port: 27518\nreplication:\n    replSetName: myshardrs02\nsharding:\n    clusterRole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动第二套副本集：一主一副本一仲裁\n\n依次启动三个mongod服务：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123223\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123292\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123326\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n查看服务是否启动：\n\n[root@bobohost bin]# ps -ef |grep mongod\n\n\n1\n\n\n# 2.4.2.1 初始化副本集和创建主节点\n\n使用客户端命令连接任意一个节点，但这里尽量要连接主节点：\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27318\n\n\n1\n\n\n执行初始化副本集命令：\n\nrs.initiate()\n\n\n1\n\n\n查看副本集情况 (节选内容)：\n\nrs.status()\n\n\n1\n\n\n主节点配置查看：\n\nrs.conf()\n\n\n1\n\n\n# 2.4.2.2 添加副本节点\n\nrs.add("180.76.159.126:27418")\n\n\n1\n\n\n# 2.4.2.3 添加仲裁节点\n\nrs.addArb("180.76.159.126:27518")\n\n\n1\n\n\n查看副本集的配置情况\n\nmyshardrs02:PRIMARY> rs.status()\n{\n    "set" : "myshardrs02",\n    "date" : ISODate("2019-07-31T21:38:22.463Z"),\n    "myState" : 1,\n    "term" : NumberLong(1),\n    "syncingTo" : "",\n    "syncSourceHost" : "",\n    "syncSourceId" : -1,\n    "heartbeatIntervalMillis" : NumberLong(2000),\n    "optimes" : {\n        "lastCommittedOpTime" : {\n            "ts" : Timestamp(1564609094, 1),\n            "t" : NumberLong(1)\n        },\n        "readConcernMajorityOpTime" : {\n            "ts" : Timestamp(1564609094, 1),\n            "t" : NumberLong(1)\n        },\n        "appliedOpTime" : {\n            "ts" : Timestamp(1564609094, 1),\n            "t" : NumberLong(1)\n        },\n        "durableOpTime" : {\n            "ts" : Timestamp(1564609094, 1),\n            "t" : NumberLong(1)\n        }\n    },\n    "lastStableCheckpointTimestamp" : Timestamp(1564609074, 1),\n    "members" : [\n        {\n            "_id" : 0,\n            "name" : "180.76.159.126:27318",\n            "health" : 1,\n            "state" : 1,\n            "stateStr" : "PRIMARY",\n            "uptime" : 5086,\n            "optime" : {\n                "ts" : Timestamp(1564609094, 1),\n                "t" : NumberLong(1)\n            },\n            "optimeDate" : ISODate("2019-07-31T21:38:14Z"),\n            "syncingTo" : "",\n            "syncSourceHost" : "",\n            "syncSourceId" : -1,\n            "infoMessage" : "",\n            "electionTime" : Timestamp(1564604032, 2),\n            "electionDate" : ISODate("2019-07-31T20:13:52Z"),\n            "configVersion" : 3,\n            "self" : true,\n            "lastHeartbeatMessage" : ""\n        },\n        {\n            "_id" : 1,\n            "name" : "180.76.159.126:27418",\n            "health" : 1,\n            "state" : 2,\n            "stateStr" : "SECONDARY",\n            "uptime" : 4452,\n            "optime" : {\n                "ts" : Timestamp(1564609094, 1),\n                "t" : NumberLong(1)\n            },\n            "optimeDurable" : {\n                "ts" : Timestamp(1564609094, 1),\n                "t" : NumberLong(1)\n            },\n            "optimeDate" : ISODate("2019-07-31T21:38:14Z"),\n            "optimeDurableDate" : ISODate("2019-07-31T21:38:14Z"),\n            "lastHeartbeat" : ISODate("2019-07-31T21:38:21.178Z"),\n            "lastHeartbeatRecv" : ISODate("2019-07-31T21:38:20.483Z"),\n            "pingMs" : NumberLong(0),\n            "lastHeartbeatMessage" : "",\n            "syncingTo" : "180.76.159.126:27518",\n            "syncSourceHost" : "180.76.159.126:27518",\n            "syncSourceId" : 2,\n            "infoMessage" : "",\n            "configVersion" : 3\n        },\n        {\n            "_id" : 2,\n            "name" : "180.76.159.126:27518",\n            "health" : 1,\n            "state" : 2,\n            "stateStr" : "SECONDARY",\n            "uptime" : 4448,\n            "optime" : {\n                "ts" : Timestamp(1564609094, 1),\n                "t" : NumberLong(1)\n            },\n            "optimeDurable" : {\n                "ts" : Timestamp(1564609094, 1),\n                "t" : NumberLong(1)\n            },\n            "optimeDate" : ISODate("2019-07-31T21:38:14Z"),\n            "optimeDurableDate" : ISODate("2019-07-31T21:38:14Z"),\n            "lastHeartbeat" : ISODate("2019-07-31T21:38:21.178Z"),\n            "lastHeartbeatRecv" : ISODate("2019-07-31T21:38:22.096Z"),\n            "pingMs" : NumberLong(0),\n            "lastHeartbeatMessage" : "",\n            "syncingTo" : "180.76.159.126:27318",\n            "syncSourceHost" : "180.76.159.126:27318",\n            "syncSourceId" : 0,\n            "infoMessage" : "",\n            "configVersion" : 3\n        }\n    ],\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n\n\n\n# 2.5 配置节点副本集的创建\n\n第一步：准备存放数据和日志的目录：\n\n#-----------configrs\n#建立数据节点data和日志目录\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27019/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27019/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27119/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27119/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27219/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27219/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n\n\n1\n\n\nmyconfigrs_27019 ：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myconfigrs_27019/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/sharded_cluster/myconfigrs_27019/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/sharded_cluster/myconfigrs_27019/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #绑定的端口\n    port: 27019\nreplication:\n    replSetName: myconfigrs\nsharding:\n    clusterRole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n\n\n1\n\n\nmyconfigrs_27119\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myconfigrs_27119/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/sharded_cluster/myconfigrs_27119/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/sharded_cluster/myconfigrs_27119/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #绑定的端口\n    port: 27119\nreplication:\n    replSetName: myconfigrs\nsharding:\n    clusterRole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\n\n\n1\n\n\nmyconfigrs_27219\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myconfigrs_27219/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/sharded_cluster/myconfigrs_27219/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/sharded_cluster/myconfigrs_27219/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #绑定的端口\n    port: 27219\nreplication:\n    replSetName: myconfigrs\nsharding:\n    clusterRole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动配置副本集：一主两副本\n\n依次启动三个mongod服务：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123223\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123292\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123326\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n查看服务是否启动：\n\n[root@bobohost bin]# ps -ef |grep mongod\n\n\n1\n\n\n\n# 2.5.1 初始化副本集和创建主节点\n\n使用客户端命令连接任意一个节点，但这里尽量要连接主节点：\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27019\n\n\n1\n\n\n执行初始化副本集命令：\n\nrs.initiate()\n\n\n1\n\n\n查看副本集情况 (节选内容)：\n\nrs.status()\n\n\n1\n\n\n主节点配置查看：\n\nrs.conf()\n\n\n1\n\n\n\n# 2.5.2 添加两个副本节点\n\nmyshardrs01:PRIMARY> rs.add("180.76.159.126:27119")\nmyshardrs01:PRIMARY> rs.add("180.76.159.126:27219")\n\n\n1\n2\n\n\n查看副本集的配置情况：\n\nmyshardrs01:PRIMARY> rs.conf()\nmyshardrs01:PRIMARY> rs.status()\n\n\n1\n2\n\n\n\n# 2.6 路由节点的创建和操作\n\n\n# 2.6.1 第一个路由节点的创建和连接\n\n第一步：准备存放数据和日志的目录：\n\n#-----------mongos01\nmkdir -p /mongodb/sharded_cluster/mymongos_27017/log\n\n\n1\n2\n\n\nmymongos_27017节点：\n\n新建或修改配置文件：\n\nvi /mongodb/sharded_cluster/mymongos_27017/mongos.conf\n\n\n1\n\n\nmongos.conf：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/mymongos_27017/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: /mongodb/sharded_cluster/mymongos_27017/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #bindIp\n    #绑定的端口\n    port: 27017\nsharding:\n    #指定配置节点副本集\n    configDB: myconfigrs/180.76.159.126:27019,180.76.159.126:27119,180.76.159.126:27219\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n启动mongos：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27017/mongos.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 129874\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n\n\n提示：启动如果失败，可以查看 log目录下的日志，查看失败原因。\n\n客户端登录mongos\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017\n\n\n1\n\n\n此时，写不进去数据，如果写数据会报错：\n\nmongos> use aadb\nswitched to db aadb\nmongos> db.aa.insert({aa:"aa"})\nWriteCommandError({\n    "ok" : 0,\n    "errmsg" : "unable to initialize targeter for write op for collection aa.aa :: caused by :: Database aa not found :: caused by :: No shards found",\n    "code" : 70,\n    "codeName" : "ShardNotFound",\n    "operationTime" : Timestamp(1564600123, 2),\n    "$clusterTime" : {\n    "clusterTime" : Timestamp(1564600123, 2),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n原因：通过路由节点操作，现在只是连接了配置节点，还没有连接分片数据节点，因此无法写入业务数据。\n\n\n# 2.6.2 在路由节点上进行分片配置操作\n\n使用命令添加分片：\n\n# 2.6.2.1 添加分片：\n\n语法：\n\nsh.addShard("IP:Port")\n\n\n1\n\n\n将第一套分片副本集添加进来：\n\nmongos>sh.addShard("myshardrs01/192.168.0.2:27018,180.76.159.126:27118,180.76.159.126:27218")\n{\n    "shardAdded" : "myshardrs01",\n    "ok" : 1,\n    "operationTime" : Timestamp(1564611970, 4),\n    "$clusterTime" : {\n    "clusterTime" : Timestamp(1564611970, 4),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n查看分片状态情况：\n\nmongos> sh.status()\n--- Sharding Status ---\n    sharding version: {\n        "_id" : 1,\n        "minCompatibleVersion" : 5,\n        "currentVersion" : 6,\n        "clusterId" : ObjectId("5d4211b798f3f9a48522c68b")\n    }\n    shards:\n        {  "_id" : "myshardrs01",  "host" : "myshardrs01/180.76.159.126:27018,180.76.159.126:27118",  "state" : 1 }\n    active mongoses:\n        "4.0.10" : 1\n    autosplit:\n        Currently enabled: yes\n    balancer:\n        Currently enabled:  yes\n        Currently running:  no\n        Failed balancer rounds in last 5 attempts:  0\n        Migration Results for the last 24 hours:\n            No recent migrations\n    databases:\n        {  "_id" : "config",  "primary" : "config",  "partitioned" : true }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n继续将第二套分片副本集添加进来：\n\nmongos>sh.addShard("myshardrs02/192.168.0.2:27318,180.76.159.126:27418,180.76.159.126:27518")\n{\n    "shardAdded" : "myshardrs02",\n    "ok" : 1,\n    "operationTime" : Timestamp(1564612147, 5),\n    "$clusterTime" : {\n    "clusterTime" : Timestamp(1564612147, 5),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n查看分片状态：\n\nmongos> sh.status()\n--- Sharding Status ---\n    sharding version: {\n        "_id" : 1,\n        "minCompatibleVersion" : 5,\n        "currentVersion" : 6,\n        "clusterId" : ObjectId("5d4211b798f3f9a48522c68b")\n    }\n    shards:\n        {  "_id" : "myshardrs01",  "host" : "myshardrs01/180.76.159.126:27018,180.76.159.126:27118",  "state" : 1 }\n        {  "_id" : "myshardrs02",  "host" : "myshardrs02/180.76.159.126:27318,180.76.159.126:27418",  "state" : 1 }\n    active mongoses:\n        "4.0.10" : 1\n    autosplit:\n        Currently enabled: yes\n    balancer:\n        Currently enabled:  yes\n        Currently running:  no\n        Failed balancer rounds in last 5 attempts:  0\n        Migration Results for the last 24 hours:\n            No recent migrations\n    databases:\n        {  "_id" : "config",  "primary" : "config",  "partitioned" : true }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n提示：如果添加分片失败，需要先手动移除分片，检查添加分片的信息的正确性后，再次添加分片。\n\n移除分片参考(了解)：\n\nuse admin\ndb.runCommand( { removeShard: "myshardrs02" } )\n\n\n1\n2\n\n\n注意：如果只剩下最后一个 shard，是无法删除的\n\n移除时会自动转移分片数据，需要一个时间过程。\n\n完成后，再次执行删除分片命令才能真正删除。\n\n# 2.6.2.2 开启分片功能\n\nsh.enableSharding(“库名”)、sh.shardCollection(“库名.集合名”,{“key”:1})\n\n在mongos上的articledb数据库配置sharding:\n\nmongos> sh.enableSharding("articledb")\n{\n    "ok" : 1,\n    "operationTime" : Timestamp(1564612296, 5),\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1564612296, 5),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查看分片状态：\n\nmongos> sh.status()\n--- Sharding Status ---\n    sharding version: {\n        "_id" : 1,\n        "minCompatibleVersion" : 5,\n        "currentVersion" : 6,\n        "clusterId" : ObjectId("5d4211b798f3f9a48522c68b")\n    }\n    shards:\n        {  "_id" : "myshardrs01",  "host" : "myshardrs01/180.76.159.126:27018,180.76.159.126:27118",  "state" : 1 }\n        {  "_id" : "myshardrs02",  "host" : "myshardrs02/180.76.159.126:27318,180.76.159.126:27418",  "state" : 1 }\n    active mongoses:\n        "4.0.10" : 1\n    autosplit:\n        Currently enabled: yes\n    balancer:\n        Currently enabled:  yes\n        Currently running:  no\n        Failed balancer rounds in last 5 attempts:  0\n        Migration Results for the last 24 hours:\n            No recent migrations\n    databases:\n        {  "_id" : "articledb",  "primary" : "myshardrs02",  "partitioned" : true, "version" : {  "uuid" : UUID("788c9a3b-bb6a-4cc2-a597-974694772986"), "lastMod" : 1 } }\n        {  "_id" : "config",  "primary" : "config",  "partitioned" : true }\n            config.system.sessions\n                shard key: { "_id" : 1 }\n                unique: false\n                balancing: true\n                chunks:\n                    myshardrs01   1\n                { "_id" : { "$minKey" : 1 } } --\x3e> { "_id" : { "$maxKey" : 1 } } on : myshardrs01 Timestamp(1, 0)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n# 2.6.2.3 集合分片\n\n对集合分片，你必须使用 sh.shardCollection() 方法指定集合和分片键\n\n语法：\n\nsh.shardCollection(namespace, key, unique)\n\n\n1\n\n\n参数：\n\nPARAMETER   TYPE       DESCRIPTION\nnamespace   string     要（分片）共享的目标集合的命名空间，格式： .\nkey         document   用作分片键的索引规范文档。shard键决定MongoDB如何在shard之间分发文档。除非集合为空，否则索引必须在shardcollection命令之前存在。如果集合为空，则MongoDB在对集合进行分片之前创建索引，前提是支持分片键的索引不存在。简单的说：由包含字段和该字段的索引遍历方向的文档组成。\nunique      boolean    当值为true情况下，片键字段上会限制为确保是唯一索引。哈希策略片键不支持唯一索引。默认是false。\n\n对集合进行分片时,你需要选择一个 片键（Shard Key） , shard key 是每条记录都必须包含的,且建立了索引的单个字段或复合字段,MongoDB按照片键将数据划分到不同的 数据块 中,并将 数据块 均衡地分布到所有分片中.为了按照片键划分数据块,MongoDB使用 基于哈希的分片方式（随机平均分配）或者基于范围的分片方式（数值大小分配） 。\n\n用什么字段当片键都可以，如：nickname作为片键，但一定是必填字段。\n\n分片规则一：哈希策略\n\n对于 基于哈希的分片 ,MongoDB计算一个字段的哈希值,并用这个哈希值来创建数据块.\n\n在使用基于哈希分片的系统中,拥有”相近”片键的文档 很可能不会 存储在同一个数据块中,因此数据的分离性更好一些.\n\n使用nickname作为片键，根据其值的哈希值进行数据分片\n\nmongos> sh.shardCollection("articledb.comment",{"nickname":"hashed"})\n{\n    "collectionsharded" : "articledb.comment",\n    "collectionUUID" : UUID("ddea6ed8-ee61-4693-bd16-196acc3a45e8"),\n    "ok" : 1,\n    "operationTime" : Timestamp(1564612840, 28),\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1564612840, 28),\n        "signature" : {\n            "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),\n            "keyId" : NumberLong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n查看分片状态：sh.status()\n\n databases:\n {  "_id" : "articledb",  "primary" : "myshardrs02",  "partitioned" : true,  "version" : {  "uuid" : UUID("251436b7-86c2-4cd8-9a88-70874af29364"), "lastMod" : 1 } }\n     articledb.comment\n         shard key: { "nickname" : "hashed" }\n         unique: false\n         balancing: true\n         chunks:\n             myshardrs01   2\n             myshardrs02   2\n         { "nickname" : { "$minKey" : 1 } } --\x3e> { "nickname" : NumberLong("-4611686018427387902") } on : myshardrs01 Timestamp(1, 0)\n         { "nickname" : NumberLong("-4611686018427387902") } --\x3e> { "nickname" : NumberLong(0) } on : myshardrs01 Timestamp(1, 1)\n         { "nickname" : NumberLong(0) } --\x3e> { "nickname" : NumberLong("4611686018427387902") } on : myshardrs02 Timestamp(1, 2)\n         { "nickname" : NumberLong("4611686018427387902") } --\x3e> { "nickname" : { "$maxKey" : 1 } } on : myshardrs02 Timestamp(1, 3)\n\n{  "_id" : "config",  "primary" : "config",  "partitioned" : true }\n    config.system.sessions\n        shard key: { "_id" : 1 }\n        unique: false\n        balancing: true\n        chunks:\n            myshardrs01   1\n        { "_id" : { "$minKey" : 1 } } --\x3e> { "_id" : { "$maxKey" : 1 } } on : myshardrs01 Timestamp(1, 0)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n分片规则二：范围策略\n\n对于 基于范围的分片 ,MongoDB按照片键的范围把数据分成不同部分.假设有一个数字的片键:想象一个从负无穷到正无穷的直线,每一个片键的值都在直线上画了一个点.MongoDB把这条直线划分为更短的不重叠的片段,并称之为 数据块 ,每个数据块包含了片键在一定范围内的数据.\n\n在使用片键做范围划分的系统中,拥有”相近”片键的文档很可能存储在同一个数据块中,因此也会存储在同一个分片中.\n\n如使用作者年龄字段作为片键，按照点赞数的值进行分片：\n\nmongos> sh.shardCollection("articledb.author",{"age":1})\n{\n    "collectionsharded" : "articledb.author",\n    "collectionUUID" : UUID("9a47bdaa-213a-4039-9c18-e70bfc369df7"),\n    "ok" : 1,\n    "operationTime" : Timestamp(1567512803, 13),\n    "$clusterTime" : {\n        "clusterTime" : Timestamp(1567512803, 13),\n        "signature" : {\n            "hash" : BinData(0,"eE9QT5yE5sL1Tyr7+3U8GRy5+5Q="),\n            "keyId" : NumberLong("6732061237309341726")\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n注意的是：\n\n * 一个集合只能指定一个片键，否则报错。\n * 一旦对一个集合分片，分片键和分片值就不可改变。 如：不能给集合选择不同的分片键、不能更新分片键的值。\n * 根据age索引进行分配数据。\n\n查看分片状态：\n\narticledb.author\n    shard key: { "age" : 1 }\n    unique: false\n    balancing: true\n    chunks:\n        myshardrs01 1\n    { "age" : { "$minKey" : 1 } } --\x3e> { "age" : { "$maxKey" : 1 } } on : myshardrs01 Timestamp(1, 0)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n基于范围的分片方式与基于哈希的分片方式性能对比：\n\n基于范围的分片方式提供了更高效的范围查询,给定一个片键的范围,分发路由可以很简单地确定哪个数据块存储了请求需要的数据,并将请求转发到相应的分片中.\n\n不过,基于范围的分片会导致数据在不同分片上的不均衡,有时候,带来的消极作用会大于查询性能的积极作用.比如,如果片键所在的字段是线性增长的,一定时间内的所有请求都会落到某个固定的数据块中,最终导致分布在同一个分片中.在这种情况下,一小部分分片承载了集群大部分的数据,系统并不能很好地进行扩展.\n\n与此相比,基于哈希的分片方式以范围查询性能的损失为代价,保证了集群中数据的均衡.哈希值的随机性使数据随机分布在每个数据块中,因此也随机分布在不同分片中.但是也正由于随机性,一个范围查询很难确定应该请求哪些分片,通常为了返回需要的结果,需要请求所有分片.\n\n如无特殊情况，一般推荐使用 Hash Sharding。\n\n而使用 _id 作为片键是一个不错的选择，因为它是必有的，你可以使用数据文档 _id 的哈希作为片键。\n\n这个方案能够是的读和写都能够平均分布，并且它能够保证每个文档都有不同的片键所以数据块能够很精细。\n\n似乎还是不够完美，因为这样的话对多个文档的查询必将命中所有的分片。虽说如此，这也是一种比较好的方案了。\n\n理想化的 shard key 可以让 documents 均匀地在集群中分布：\n\n\n\n显示集群的详细信息：\n\nmongos> db.printShardingStatus()\n\n\n1\n\n\n查看均衡器是否工作（需要重新均衡时系统才会自动启动，不用管它）：\n\nmongos> sh.isBalancerRunning()\nfalse\n\n\n1\n2\n\n\n查看当前 Balancer状态：\n\nmongos> sh.getBalancerState()\ntrue\n\n\n1\n2\n\n\n\n# 2.6.3 分片后插入数据测试\n\n测试一（哈希规则）：登录mongs后，向comment循环插入1000条数据做测试：\n\nmongos> use articledb\nswitched to db articledb\nmongos> for(var i=1;i<=1000;i++){db.comment.insert({_id:i+"",nickname:"BoBo"+i})}\nWriteResult({ "nInserted" : 1 })\nmongos> db.comment.count()\n1000\n\n\n1\n2\n3\n4\n5\n6\n\n\n提示： js的语法，因为mongo的shell是一个JavaScript的shell。\n\n注意：从路由上插入的数据，必须包含片键，否则无法插入。\n\n分别登陆两个片的主节点，统计文档数量\n\n第一个分片副本集：\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27018\n\nmyshardrs01:PRIMARY> use articledb\nswitched to db articledb\nmyshardrs01:PRIMARY> db.comment.count()\n507\n\n\n1\n2\n3\n4\n5\n6\n\n\n第二个分片副本集：\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27318\n\nmyshardrs02:PRIMARY> use articledb\nswitched to db articledb\nmyshardrs02:PRIMARY> db.comment.count()\n493\n\n\n1\n2\n3\n4\n5\n6\n\n\n可以看到， 1000条数据近似均匀的分布到了2个shard上。是根据片键的哈希值分配的。\n\n这种分配方式非常易于水平扩展：一旦数据存储需要更大空间，可以直接再增加分片即可，同时提升了性能。\n\n使用db.comment.stats()查看单个集合的完整情况，mongos执行该命令可以查看该集合的数据分片的情况。\n\n使用sh.status()查看本库内所有集合的分片信息。\n\n测试二（范围规则）：登录mongs后，向comment循环插入1000条数据做测试：\n\nmongos> use articledb\nswitched to db articledb\nmongos> for(var i=1;i<=20000;i++){db.author.save({"name":"BoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBo"+i,"age":NumberInt(i%120)})}\nWriteResult({ "nInserted" : 1 })\nmongos> db.comment.count()\n20000\n\n\n1\n2\n3\n4\n5\n6\n\n\n插入成功后，仍然要分别查看两个分片副本集的数据情况。\n\n分片效果：\n\narticledb.author\n    shard key: { "age" : 1 }\n    unique: false\n    balancing: true\n    chunks:\n        myshardrs01 2\n        myshardrs02 1\n    { "age" : { "$minKey" : 1 } } --\x3e> { "age" : 0 } on : myshardrs02 Timestamp(2, 0)\n    { "age" : 0 } --\x3e> { "age" : 112 } on : myshardrs01 Timestamp(2, 1)\n    { "age" : 112 } --\x3e> { "age" : { "$maxKey" : 1 } } on : myshardrs01 Timestamp(1, 3)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n提示：\n\n如果查看状态发现没有分片，则可能是由于以下原因造成了：\n\n * 系统繁忙，正在分片中。\n\n * 数据块（chunk）没有填满，默认的数据块尺寸（chunksize）是64M，填满后才会考虑向其他片的数据块填充数据，因此，为了测试，可以将其改小，这里改为1M，操作如下：\n   \n   use config\n   db.settings.save( { _id:"chunksize", value: 1 } )\n   \n   \n   1\n   2\n   \n   \n   测试完改回来：\n   \n   db.settings.save( { _id:"chunksize", value: 64 } )\n   \n   \n   1\n   \n\n注意：要先改小，再设置分片。为了测试，可以先删除集合，重新建立集合的分片策略，再插入数据测试即可。\n\n\n# 2.6.4 再增加一个路由节点\n\n文件夹：\n\n#-----------mongos02\nmkdir -p /mongodb/sharded_cluster/mymongos_27117/log\n\n\n1\n2\n\n\n新建或修改配置文件：\n\nvi /mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\n\n1\n\n\nmongos.conf：\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/mymongos_27117/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: /mongodb/sharded_cluster/mymongos_27117/log/mongod.pid"\nnet:\n    #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindIpAll: true\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #bindIp\n    #绑定的端口\n    port: 27117\nsharding:\n    configDB: myconfigrs/180.76.159.126:27019,180.76.159.126:27119,180.76.159.126:27219\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n启动mongos2：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27117/mongos.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 129874\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n\n\n使用mongo客户端登录27117，发现，第二个路由无需配置，因为分片配置都保存到了配置服务器中了。\n\n\n# 2.7 Compass 连接分片集群\n\ncompass连接：\n\n\n\n提示：和连接单机 mongod一样。\n\n连接成功后，上方有mongos和分片集群的提示：\n\n\n\n\n# 2.8 SpringDataMongDB 连接分片集群\n\nJava客户端常用的是SpringDataMongoDB，其连接的是mongs路由，配置和单机mongod的配置是一样的。\n\n多个路由的时候的SpringDataMongoDB的客户端配置参考如下：\n\nspring:\n    #数据源配置\n    data:\n        mongodb:\n            # 主机地址\n            #   host: 180.76.159.126\n            # 数据库\n            #   database: articledb\n            # 默认端口是27017\n            #   port: 27017\n            #也可以使用uri连接\n            #   uri: mongodb://192.168.40.134:28017/articledb\n            # 连接副本集字符串\n            #   uri: mongodb://180.76.159.126:27017,180.76.159.126:27018,180.76.159.126:27019/articledb?connect=replicaSet&slaveOk=true&replicaSet=myrs\n            #连接路由字符串\n            uri: mongodb://180.76.159.126:27017,180.76.159.126:27117/articledb\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n通过日志发现，写入数据的时候，会选择一个路由写入\n\n\n# 2.9 清除所有的节点数据（备用）\n\n如果在搭建分片的时候有操作失败或配置有问题，需要重新来过的，可以进行如下操作：\n\n第一步：查询出所有的测试服务节点的进程：\n\n[root@bobohost sharded_cluster]# ps -ef |grep mongo\nroot    10184    1  0 06:04 ?     00:01:25 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\nroot    10219    1  0 06:04 ?     00:01:25 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\nroot    10253    1  0 06:04 ?     00:00:46 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\nroot    10312    1  0 06:04 ?     00:01:23 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\nroot    10346    1  0 06:05 ?     00:01:23 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\nroot    10380    1  0 06:05 ?     00:00:44 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\nroot    10414    1  1 06:05 ?     00:01:36 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\nroot    10453    1  1 06:05 ?     00:01:37 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\nroot    10492    1  1 06:05 ?     00:01:38 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\nroot    11392    1  0 06:15 ?     00:00:24 /usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27017/mongos.conf\nroot    14829    1  0 07:15 ?     00:00:13 /usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n根据上述的进程编号，依次中断进程：\n\nkill -2 进程编号\n\n\n1\n\n\n第二步：清除所有的节点的数据：\n\nrm -rf /mongodb/sharded_cluster/myconfigrs_27019/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myconfigrs_27119/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myconfigrs_27219/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs01_27018/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs01_27118/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs01_27218/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs02_27318/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs02_27418/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs02_27518/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/mymongos_27017/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/mymongos_27117/data/db/*.*\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n第三步：查看或修改有问题的配置\n\n第四步：依次启动所有节点，不包括路由节点：\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n第五步：对两个数据分片副本集和一个配置副本集进行初始化和相关配置\n\n第六步：检查路由mongos的配置，并启动mongos\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/mymongos_27017/mongos.cfg\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/mymongos_27017/mongos.cfg\n\n\n1\n2\n\n\n第七步：mongo登录mongos，在其上进行相关操作。\n\n\n# 3. 安全认证\n\n\n# 3.1 MongoDB 的用户和角色权限简介\n\n默认情况下，MongoDB实例启动运行时是没有启用用户访问权限控制的，也就是说，在实例本机服务器上都可以随意连接到实例进行各种操作，MongoDB不会对连接客户端进行用户验证，这是非常危险的。\n\nmongodb官网上说，为了能保障mongodb的安全可以做以下几个步骤：\n\n * 使用新的端口，默认的27017端口如果一旦知道了ip就能连接上，不太安全。\n * 设置mongodb的网络环境，最好将mongodb部署到公司服务器内网，这样外网是访问不到的。公司内部访问使用vpn等。\n * 开启安全认证。认证要同时设置服务器之间的内部认证方式，同时要设置客户端连接到集群的账号密码认证方式。\n\n为了强制开启用户访问控制(用户验证)，则需要在MongoDB实例启动时使用选项 – auth 或在指定启动配置文件中添加选项 auth=true 。\n\n在开始之前需要了解一下概念\n\n1）启用访问控制：\n\nMongoDB使用的是基于角色的访问控制(Role-Based Access Control,RBAC)来管理用户对实例的访问。通过对用户授予一个或多个角色来控制用户访问数据库资源的权限和数据库操作的权限，在对用户分配角色之前，用户无法访问实例。\n\n在实例启动时添加选项 – auth 或指定启动配置文件中添加选项 auth=true 。\n\n2）角色：\n\n在MongoDB中通过角色对用户授予相应数据库资源的操作权限，每个角色当中的权限可以显式指定，也可以通过继承其他角色的权限，或者两都都存在的权限。\n\n3）权限：\n\n权限由指定的数据库资源(resource)以及允许在指定资源上进行的操作(action)组成。\n\n * 资源(resource)包括：数据库、集合、部分集合和集群；\n * 操作(action)包括：对资源进行的增、删、改、查(CRUD)操作。\n\n在角色定义时可以包含一个或多个已存在的角色，新创建的角色会继承包含的角色所有的权限。在同一个数据库中，新创建角色可以继承其他角色的权限，在 admin 数据库中创建的角色可以继承在其它任意数据库中角色的权限。\n\n关于角色权限的查看，可以通过如下命令查询（了解）：\n\n// 查询所有角色权限(仅用户自定义角色)\n> db.runCommand({ rolesInfo: 1 })\n\n// 查询所有角色权限(包含内置角色)\n> db.runCommand({ rolesInfo: 1, showBuiltinRoles: true })\n\n// 查询当前数据库中的某角色的权限\n> db.runCommand({ rolesInfo: "<rolename>" })\n// 查询其它数据库中指定的角色权限\n> db.runCommand({ rolesInfo: { role: "<rolename>", db: "<database>" } }\n// 查询多个角色权限\n> db.runCommand(\n    {\n        rolesInfo: [\n            "<rolename>",\n            { role: "<rolename>", db: "<database>" },\n            ...\n        ]  \n    }\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n示例：\n\n查看所有内置角色：\n\n> db.runCommand({ rolesInfo: 1, showBuiltinRoles: true })\n{\n    "roles" : [\n        {\n            "role" : "__queryableBackup",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "__system",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "backup",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "clusterAdmin",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "clusterManager",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "clusterMonitor",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "dbAdmin",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "dbAdminAnyDatabase",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "dbOwner",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "enableSharding",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "hostManager",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "read",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "readAnyDatabase",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "readWrite",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "readWriteAnyDatabase",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "restore",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "root",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "userAdmin",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        },\n        {\n            "role" : "userAdminAnyDatabase",\n            "db" : "admin",\n            "isBuiltin" : true,\n            "roles" : [ ],\n            "inheritedRoles" : [ ]\n        }\n    ],\n    "ok" : 1\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n\n\n常用的内置角色：\n\n * 数据库用户角色： read、readWrite;\n * 所有数据库用户角色： readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase\n * 数据库管理角色： dbAdmin、dbOwner、userAdmin；\n * 集群管理角色： clusterAdmin、clusterManager、clusterMonitor、hostManager；\n * 备份恢复角色： backup、restore；\n * 超级用户角色： root\n * 内部角色： system\n\n角色说明：\n\n角色                     权限描述\nread                   可以读取指定数据库中任何数据\nreadWrite              可以读写指定数据库中任何数据，包括创建、重命名、删除集合\nreadAnyDatabase        可以读取所有数据库中任何数据(除了数据库config和local之外)\nreadWriteAnyDatabase   可以读写所有数据库中任何数据(除了数据库config和local之外)\nuserAdminAnyDatabase   可以在指定数据库创建和修改用户(除了数据库config和local之外)\ndbAdminAnyDatabase     可以读取任何数据库以及对数据库进行清理、修改、压缩、获取统计信息、执行检查等操作(除了数据库config和local之外)\ndbAdmin                可以读取指定数据库以及对数据库进行清理、修改、压缩、获取统计信息、执行检查等操作\nuserAdmin              可以在指定数据库创建和修改用户\nclusterAdmin           可以对整个集群或数据库系统进行管理操作\nbackup                 备份MongoDB数据最小的权限\nrestore                从备份文件中还原恢复MongoDB数据(除了system.profile集合)的权限\nroot                   超级账号，超级权限\n\n\n# 3.2 单实例环境\n\n目标：对单实例的MongoDB服务开启安全认证，这里的单实例指的是未开启副本集或分片的MongoDB实例。\n\n\n# 3.2.1 关闭已开启的服务\n\n增加mongod的单实例的安全认证功能，可以在服务搭建的时候直接添加，也可以在之前搭建好的服务上添加。\n\n本文使用之前搭建好的服务，因此，先停止之前的服务\n\n停止服务的方式有两种：快速关闭和标准关闭，下面依次说明：\n\n（1）快速关闭方法（快速，简单，数据可能会出错）\n\n目标：通过系统的kill命令直接杀死进程：\n\n杀完要检查一下，避免有的没有杀掉。\n\n#通过进程编号关闭节点\nkill -2 54410\n\n\n1\n2\n\n\n【补充】\n\n如果一旦是因为数据损坏，则需要进行如下操作（了解）：\n\n1）删除lock文件：\n\nrm -f /mongodb/single/data/db/*.lock\n\n\n1\n\n\n2 ）修复数据：\n\n/usr/local/mongodb/bin/mongod --repair --dbpath=/mongodb/single/data/db\n\n\n1\n\n\n（2）标准的关闭方法（数据不容易出错，但麻烦）：\n\n目标：通过mongo客户端中的shutdownServer命令来关闭服务\n\n主要的操作步骤参考如下：\n\n//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\nmongo --port 27017\n//#切换到admin库\nuse admin\n//关闭服务\ndb.shutdownServer()\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3.2.2 添加用户和权限\n\n（1）先按照普通无授权认证的配置，来配置服务端的配置文件 /mongodb/single/mongod.conf\n\nsystemLog:\n    #MongoDB发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/single/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logAppend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。\n    dbPath: "/mongodb/single/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessManagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID\n    pidFilePath: "/mongodb/single/log/mongod.pid"\nnet:\n    #服务实例绑定的IP\n    bindIp: localhost,192.168.0.2\n    #绑定的端口\n    port: 27017\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n（2）按之前未开启认证的方式（不添加 – auth 参数）来启动MongoDB服务\n\n/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf\n\n\n1\n\n\n提示：\n\n在操作用户时，启动mongod服务时尽量不要开启授权。\n\n（3）使用Mongo客户端登录\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017\n\n\n1\n\n\n（4）创建两个管理员用户，一个是系统的超级管理员 myroot ，一个是admin库的管理用户myadmin\n\n//切换到admin库\n> use admin\n\n//创建系统超级用户 myroot,设置密码123456，设置角色root\n//> db.createUser({user:"myroot",pwd:"123456",roles:[ { "role" : "root", "db" : "admin" } ]})\n//或\n> db.createUser({user:"myroot",pwd:"123456",roles:["root"]})\n\n//创建专门用来管理admin库的账号myadmin，只用来作为用户权限的管理\n> db.createUser({user:"myadmin",pwd:"123456",roles: [{role:"userAdminAnyDatabase",db:"admin"}]})\n\n//查看已经创建了的用户的情况：\n> db.system.users.find()\n\n//删除用户\n> db.dropUser("myadmin")\ntrue\n\n//修改密码\n> db.changeUserPassword("myroot", "123456")\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n提示：\n\n * 本案例创建了两个用户，分别对应超管和专门用来管理用户的角色，事实上，你只需要一个用户即可。如果你对安全要求很高，防止超管泄漏，则不要创建超管用户。\n * 和其它数据库（MySQL）一样，权限的管理都差不多一样，也是将用户和权限信息保存到数据库对应的表中。Mongodb存储所有的用户信息在admin 数据库的集合system.users中，保存用户名、密码和数据库信息。\n * 如果不指定数据库，则创建的指定的权限的用户在所有的数据库上有效，如 {role: “userAdminAnyDatabase”, db:””}\n\n（5）认证测试\n\n测试添加的用户是否正确\n\n//切换到admin\n> use admin\n\n//密码输错\n> db.auth("myroot","12345")\nError: Authentication failed.\n0\n\n//密码正确\n> db.auth("myroot","123456")\n1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n（6）创建普通用户\n\n创建普通用户可以在没有开启认证的时候添加，也可以在开启认证之后添加，但开启认证之后，必须使用有操作admin库的用户登录认证后才能操作。底层都是将用户信息保存在了admin数据库的集合system.users中\n\n//创建(切换)将来要操作的数据库articledb,\n> use articledb\n\n//创建用户，拥有articledb数据库的读写权限readWrite，密码是123456\n> db.createUser({user: "bobo", pwd: "123456", roles: [{ role: "readWrite", db: "articledb" }]})\n//> db.createUser({user: "bobo", pwd: "123456", roles: ["readWrite"]})\n\n//测试是否可用\n> db.auth("bobo","123456")\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n提示：\n\n如果开启了认证后，登录的客户端的用户必须使用admin库的角色，如拥有root角色的myadmin用户，再通过myadmin用户去创建其他角色的用户\n\n\n# 3.2.3 服务端开启认证和客户端连接登录\n\n# 3.2.3.1 关闭已经启动的服务\n\n1）使用linux命令杀死进程：\n\n[root@bobohost single]# ps -ef |grep mongo\nroot    23482    1  0 08:08 ?     00:00:55 /usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf\n[root@bobohost single]# kill -2 23482\n\n\n1\n2\n3\n\n\n2 ）在mongo客户端中使用shutdownServer命令来关闭\n\n> db.shutdownServer()\nshutdown command only works with the admin database; try \'use admin\'\n> use admin\nswitched to db admin\n> db.shutdownServer()\n2019-08-14T11:20:16.450+0800 E QUERY  [js] Error: shutdownServer failed: {\n    "ok" : 0,\n    "errmsg" : "shutdown must run from localhost when running db without auth",\n    "code" : 13,\n    "codeName" : "Unauthorized"\n    } :\n_getErrorWithCode@src/mongo/shell/utils.js:25:13\nDB.prototype.shutdownServer@src/mongo/shell/db.js:453:1\n@(shell):1:1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n需要几个条件：\n\n * 必须是在 admin库下执行该关闭服务命令。\n * 如果没有开启认证，必须是从 localhost登陆的，才能执行关闭服务命令。\n * 非 localhost的、通过远程登录的，必须有登录且必须登录用户有对admin操作权限才可以。\n\n# 3.2.3.2 以开启认证的方式启动服务\n\n有两种方式开启权限认证启动服务：一种是参数方式，一种是配置文件方式\n\n1）参数方式\n\n在启动时指定参数 – auth ，如：\n\n/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf --auth\n\n\n1\n\n\n2）配置文件方式\n\n在mongod.conf配置文件中加入：vim /mongodb/single/mongod.conf\n\nsecurity:\n    #开启授权认证\n    authorization: enabled\n\n\n1\n2\n3\n\n\n启动时可不加 – auth 参数：\n\n/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf\n\n\n1\n\n\n# 3.2.3.3 开启了认证的情况下的客户端登录\n\n有两种认证方式，一种是先登录，在mongo shell中认证；一种是登录时直接认证\n\n1）先连接再认证\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017\nMongoDB shell version v4.0.10\nconnecting to: mongodb://180.76.159.126:27017/?gssapiServiceName=mongodb\nImplicit session: session { "id" : UUID("53fef661-35d6-4d29-b07c-020291d62e1a")}\nMongoDB server version: 4.0.10\n>\n\n\n1\n2\n3\n4\n5\n6\n\n\n提示：\n\n开启认证后再登录，发现打印的日志比较少了。\n\n相关操作需要认证才可以：\n\n查询admin库中的system.users集合的用户：\n\n> use admin\nswitched to db admin\n> db.system.users.find()\nError: error: {\n    "ok" : 0,\n    "errmsg" : "command find requires authentication",\n    "code" : 13,\n    "codeName" : "Unauthorized"\n}\n> db.auth("myroot","123456")\n1\n> db.system.users.find()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查询articledb库中的comment集合的内容：\n\n> use articledb\nswitched to db articledb\n> db.comment.find()\nError: error: {\n    "ok" : 0,\n    "errmsg" : "not authorized on articledb to execute command { find: \\"comment\\", filter: {}, lsid: { id: UUID(\\"53fef661-35d6-4d29-b07c-020291d62e1a\\") }, $db: \\"articledb\\" }",\n    "code" : 13,\n    "codeName" : "Unauthorized"\n}\n> db.auth("bobo","123456")\n1\n> db.comment.find()\nError: error: {\n    "ok" : 0,\n    "errmsg" : "too many users are authenticated",\n    "code" : 13,\n    "codeName" : "Unauthorized"\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n提示：\n\n这里可能出现错误，说是太多的用户正在认证了。因为我们确实连续登录了两个用户了。\n\n解决方案：退出shell，重新进来登录认证\n\n> exit\nbye\n[root@bobohost bin]# ./mongo --host 180.76.159.126 --port 27017\nMongoDB shell version v4.0.10\nconnecting to: mongodb://180.76.159.126:27017/?gssapiServiceName=mongodb\nImplicit session: session { "id" : UUID("329c1897-566d-4231-bcb3-b2acda301863")\n}\nMongoDB server version: 4.0.10\n> db.auth("bobo","123456")\nError: Authentication failed.\n0\n> use articledb\nswitched to db articledb\n> db.auth("bobo","123456")\n1\n> db.comment.find()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n2）连接时直接认证\n\n对admin数据库进行登录认证和相关操作：\n\n[root@bobohost ~]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017 --authenticationDatabase admin -u myroot -p 123456\nMongoDB shell version v4.0.10\nconnecting to: mongodb://180.76.159.126:27017/?\nauthSource=admin&gssapiServiceName=mongodb\nImplicit session: session { "id" : UUID("f959b8d6-6994-44bc-9d35-09fc7cd00ba6")\n}\nMongoDB server version: 4.0.10\nServer has startup warnings:\n2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten] ** WARNING: You are\nrunning this process as the root user, which is not recommended.\n2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten]\n2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten]\n2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten] ** WARNING:\n/sys/kernel/mm/transparent_hugepage/enabled is \'always\'.\n2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten] **    We suggest\nsetting it to \'never\'\n2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten]\n2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten] ** WARNING:\n/sys/kernel/mm/transparent_hugepage/defrag is \'always\'.\n2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten] **    We suggest\nsetting it to \'never\'\n2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten]\n> show dbs;\nadmin    0.000GB\narticledb  0.000GB\nconfig   0.000GB\nlocal    0.000GB\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n对articledb数据库进行登录认证和相关操作：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017 --authenticationDatabase articledb -u bobo -p 123456\nMongoDB shell version v4.0.10\nconnecting to: mongodb://180.76.159.126:27017/?\nauthSource=articledb&gssapiServiceName=mongodb\nImplicit session: session { "id" : UUID("e5d4148f-373b-45b8-9cff-a927ce617100")\n}\nMongoDB server version: 4.0.10\n> use articledb\nswitched to db articledb\n> db.comment.find()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n提示：\n\n * -u ：用户名\n * -p ：密码\n * -- authenticationDatabase ：指定连接到哪个库。当登录是指定用户名密码时，必须指定对应的数据库！\n\n\n# 3.2.4 SpringDataMongoDB连接认证\n\n使用用户名和密码连接到 MongoDB 服务器，你必须使用\'username:password@hostname/dbname\' 格式，’username’为用户名，’password’ 为密码。\n\n目标：使用用户bobo使用密码 123456 连接到MongoDB 服务上。\n\napplication.yml：\n\nspring:\n    #数据源配置\n    data:\n        mongodb:\n            # 主机地址\n            #   host: 180.76.159.126\n            # 数据库\n            #   database: articledb\n            # 默认端口是27017\n            #   port: 27017\n            #帐号\n            #   username: bobo\n            #密码\n            #   password: 123456\n            #单机有认证的情况下，也使用字符串连接\n            uri: mongodb://bobo:123456@180.76.159.126:27017/articledb\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 3.3 副本集环境\n\n\n# 3.3.1 前言\n\n对于搭建好的mongodb副本集，为了安全，启动安全认证，使用账号密码登录。\n\n副本集环境使用之前搭建好的，架构如下：\n\n\n\n对副本集执行访问控制需要配置两个方面 :\n\n * 副本集和共享集群的各个节点成员之间使用内部身份验证，可以使用密钥文件或x.509证书。密钥文件比较简单，本文使用密钥文件，官方推荐如果是测试环境可以使用密钥文件，但是正式环境，官方推荐x.509证书。原理就是，集群中每一个实例彼此连接的时候都检验彼此使用的证书的内容是否相同。只有证书相同的实例彼此才可以访问\n * 使用客户端连接到mongodb集群时，开启访问授权。对于集群外部的访问。如通过可视化客户端，或者通过代码连接的时候，需要开启授权。\n\n在keyfile身份验证中，副本集中的每个mongod实例都使用keyfile的内容作为共享密码，只有具有正确密钥文件的mongod或者mongos实例可以连接到副本集。密钥文件的内容必须在6到1024个字符之间，并且在unix/linux系统中文件所有者必须有对文件至少有读的权限。\n\n\n# 3.3.2 关闭已开启的副本集服务\n\n增加副本集的安全认证和服务鉴权功能，可以在副本集搭建的时候直接添加，也可以在之前搭建好的副本集服务上添加。\n\n本文使用之前搭建好的副本集服务，因此，先停止之前的集群服务\n\n停止服务的方式有两种：快速关闭和标准关闭，下面依次说明：\n\n（1）快速关闭方法（快速，简单，数据可能会出错）\n\n目标：通过系统的kill命令直接杀死进程：\n\n依次杀死仲裁者、副本节点、主节点，直到所有成员都离线。建议主节点最后kill，以避免潜在的回滚。杀完要检查一下，避免有的没有杀掉。\n\n#通过进程编号关闭节点\nkill -2 54410\n\n\n1\n2\n\n\n【补充】\n\n如果一旦是因为数据损坏，则需要进行如下操作（了解）：\n\n1）删除lock文件：\n\nrm -f /mongodb/replica_sets/myrs_27017/data/db/*.lock \\\n/mongodb/replica_sets/myrs_27018/data/db/*.lock \\\n/mongodb/replica_sets/myrs_27019/data/db/mongod.lock \\\n\n\n1\n2\n3\n\n\n2 ）依次修复数据：\n\n/usr/local/mongodb/bin/mongod --repair --dbpath=/mongodb/replica_sets/myrs_27017/data/db\n/usr/local/mongodb/bin/mongod --repair --dbpath=/mongodb/replica_sets/myrs_27018/data/db\n/usr/local/mongodb/bin/mongod --repair --dbpath=/mongodb/replica_sets/myrs_27019/data/db\n\n\n1\n2\n3\n\n\n（2）标准的关闭方法（数据不容易出错，但麻烦）\n\n目标：通过mongo客户端中的shutdownServer命令来依次关闭各个服务\n\n关闭副本集中的服务，建议依次关闭仲裁节点、副本节点、主节点。主要的操作步骤参考如下：\n\n//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\nmongo --port 27017\n//告知副本集说本机要下线\nrs.stepDown()\n//#切换到admin库\nuse admin\n//关闭服务\ndb.shutdownServer()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 3.3.3 通过主节点添加一个管理员帐号\n\n只需要在主节点上添加用户，副本集会自动同步。\n\n开启认证之前，创建超管用户：myroot，密码：123456\n\nmyrs:PRIMARY> use admin\nswitched to db admin\nmyrs:PRIMARY> db.createUser({user:"myroot",pwd:"123456",roles:["root"]})\nSuccessfully added user: { "user" : "myroot", "roles" : [ "root" ] }\n\n\n1\n2\n3\n4\n\n\n该步骤也可以在开启认证之后，但需要通过localhost登录才允许添加用户，用户数据也会自动同步到副本集。\n\n后续再创建其他用户，都可以使用该超管用户创建。\n\n\n# 3.3.4 创建副本集认证的key文件\n\n第一步：生成一个key文件到当前文件夹中。\n\n可以使用任何方法生成密钥文件。例如，以下操作使用openssl生成密码文件，然后使用chmod来更改文件权限，仅为文件所有者提供读取权限\n\n[root@bobohost ~]# openssl rand -base64 90 -out ./mongo.keyfile\n[root@bobohost ~]# chmod 400 ./mongo.keyfile\n[root@bobohost ~]# ll mongo.keyfile\n-r--------. 1 root root 122 8月  14 14:23 mongo.keyfile\n\n\n1\n2\n3\n4\n\n\n提示：\n\n所有副本集节点都必须要用同一份keyfile，一般是在一台机器上生成，然后拷贝到其他机器上，且必须有读的权限，否则将来会报错： permissions on /mongodb/replica_sets/myrs_27017/mongo.keyfile are too open\n\n一定要保证密钥文件一致，文件位置随便。但是为了方便查找，建议每台机器都放到一个固定的位置，都放到和配置文件一起的目录中。\n\n这里将该文件分别拷贝到多个目录中：\n\n[root@bobohost ~]# cp mongo.keyfile /mongodb/replica_sets/myrs_27017\n[root@bobohost ~]# cp mongo.keyfile /mongodb/replica_sets/myrs_27018\n[root@bobohost ~]# cp mongo.keyfile /mongodb/replica_sets/myrs_27019\n\n\n1\n2\n3\n\n\n\n# 3.3.5 修改配置文件指定keyfile\n\n分别编辑几个服务的mongod.conf文件，添加相关内容\n\n/mongodb/replica_sets/myrs_27017/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/replica_sets/myrs_27017/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/replica_sets/myrs_27018/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/replica_sets/myrs_27018/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/replica_sets/myrs_27019/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/replica_sets/myrs_27019/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n\n# 3.3.6 重新启动副本集\n\n如果副本集是开启状态，则先分别关闭关闭复本集中的每个mongod，从次节点开始。直到副本集的所有成员都离线，包括任何仲裁者。主节点必须是最后一个成员关闭以避免潜在的回滚。\n\n#通过进程编号关闭三个节点\nkill -2 54410 54361 54257\n\n\n1\n2\n\n\n分别启动副本集节点：\n\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n2\n3\n\n\n查看进程情况：\n\n[root@bobohost replica_sets]# ps -ef |grep mongod\nroot    62425    1  5 14:43 ?     00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.conf\nroot    62495    1  7 14:43 ?     00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.conf\nroot    62567    1 11 14:43 ?     00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n2\n3\n4\n\n\n\n# 3.3.7 在主节点上添加普通账号\n\n#先用管理员账号登录\n#切换到admin库\nuse admin\n#管理员账号认证\ndb.auth("myroot","123456")\n#切换到要认证的库\nuse articledb\n#添加普通用户\ndb.createUser({user: "bobo", pwd: "123456", roles: ["readWrite"]})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n重新连接，使用普通用户 bobo重新登录，查看数据。\n\n\n# 3.3.8 SpringDataMongoDB连接副本集\n\n使用用户名和密码连接到 MongoDB 服务器，你必须使用\'username:password@hostname/dbname\' 格式，’username’为用户名，’password’ 为密码。\n\n目标：使用用户bobo使用密码 123456 连接到MongoDB 服务上。\n\napplication.yml：\n\nspring:\n    #数据源配置\n    data:\n        mongodb:\n            #副本集有认证的情况下，字符串连接\n            uri: mongodb://bobo:123456@180.76.159.126:27017,180.76.159.126:27018,180.76.159.126:27019/articledb?connect=replicaSet&slaveOk=true&replicaSet=myrs\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3.4 分片集群环境(扩展)\n\n\n# 3.4.1 关闭已开启的集群服务\n\n分片集群环境下的安全认证和副本集环境下基本上一样。\n\n但分片集群的服务器环境和架构较为复杂，建议在搭建分片集群的时候，直接加入安全认证和服务器间的鉴权，如果之前有数据，可先将之前的数据备份出来，再还原回去。\n\n本文使用之前搭建好的集群服务，因此，先停止之前的集群服务\n\n停止服务的方式有两种：快速关闭和标准关闭，下面依次说明：\n\n（1）快速关闭方法（快速，简单，数据可能会出错）\n\n目标：通过系统的kill命令直接杀死进程：\n\n依次杀死 mongos路由、配置副本集服务，分片副本集服务，从次节点开始。直到所有成员都离线。副本集杀的时候，建议先杀仲裁者，再杀副本节点，最后是主节点，以避免潜在的回滚。杀完要检查一下，避免有的没有杀掉。\n\n#通过进程编号关闭节点\nkill -2 54410\n\n\n1\n2\n\n\n【补充】\n\n如果一旦是因为数据损坏，则需要进行如下操作（了解）：\n\n1）删除lock文件：\n\nrm -f /mongodb/sharded_cluster/myshardrs01_27018/data/db/*.lock \\\n/mongodb/sharded_cluster/myshardrs01_27118/data/db/*.lock \\\n/mongodb/sharded_cluster/myshardrs01_27218/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myshardrs02_27318/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myshardrs02_27418/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myshardrs02_27518/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myconfigrs_27019/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myconfigrs_27119/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myconfigrs_27219/data/db/mongod.lock\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n2 ）依次修复数据：\n\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs01_27018/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs01_27118/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs01_27218/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs02_27318/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs02_27418/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs02_27518/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myconfigrs_27019/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myconfigrs_27119/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myconfigrs_27219/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/mymongos_27017/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/mymongos_27117/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n（2）标准的关闭方法（数据不容易出错，但麻烦）：\n\n目标：通过mongo客户端中的shutdownServer命令来依次关闭各个服务\n\n关闭分片服务器副本集中的服务，建议依次关闭仲裁节点、副本节点、主节点。主要的操作步骤参考如下：\n\n//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\nmongo --port 27018\n//告知副本集说本机要下线\nrs.stepDown()\n//#切换到admin库\nuse admin\n//关闭服务\ndb.shutdownServer()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n关闭配置服务器副本集的服务，建议依次关闭副本节点、主节点。主要的操作步骤参考如下：\n\n//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\nmongo --port 27019\n//告知副本集说本机要下线\nrs.stepDown()\n//#切换到admin库\nuse admin\n//关闭服务\ndb.shutdownServer()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n关闭路由服务器的服务，建议依次关闭两个路由节点。主要的操作步骤参考如下：\n\n//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\nmongo --port 27017\n//告知副本集说本机要下线\nrs.stepDown()\n//#切换到admin库\nuse admin\n//关闭服务\ndb.shutdownServer()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 3.4.2 创建副本集认证的key文件\n\n第一步：生成一个key文件到当前文件夹中。\n\n可以使用任何方法生成密钥文件。例如，以下操作使用openssl生成密码文件，然后使用chmod来更改文件权限，仅为文件所有者提供读取权限\n\n[root@bobohost ~]# openssl rand -base64 90 -out ./mongo.keyfile\n[root@bobohost ~]# chmod 400 ./mongo.keyfile\n[root@bobohost ~]# ll mongo.keyfile\n-r--------. 1 root root 122 8月  14 14:23 mongo.keyfile\n\n\n1\n2\n3\n4\n\n\n提示：\n\n所有副本集节点都必须要用同一份keyfile，一般是在一台机器上生成，然后拷贝到其他机器上，且必须 有读的权限，否则将来会报错： permissions on /mongodb/replica_sets/myrs_27017/mongo.keyfile are too open\n\n一定要保证密钥文件一致，文件位置随便。但是为了方便查找，建议每台机器都放到一个固定的位置，都放到和配置文件一起的目录中。\n\n这里将该文件分别拷贝到多个目录中：\n\necho \'/mongodb/sharded_cluster/myshardrs01_27018/mongo.keyfile\n/mongodb/sharded_cluster/myshardrs01_27118/mongo.keyfile\n/mongodb/sharded_cluster/myshardrs01_27218/mongo.keyfile\n/mongodb/sharded_cluster/myshardrs02_27318/mongo.keyfile\n/mongodb/sharded_cluster/myshardrs02_27418/mongo.keyfile\n/mongodb/sharded_cluster/myshardrs02_27518/mongo.keyfile\n/mongodb/sharded_cluster/myconfigrs_27019/mongo.keyfile\n/mongodb/sharded_cluster/myconfigrs_27119/mongo.keyfile\n/mongodb/sharded_cluster/myconfigrs_27219/mongo.keyfile\n/mongodb/sharded_cluster/mymongos_27017/mongo.keyfile\n/mongodb/sharded_cluster/mymongos_27117/mongo.keyfile\' | xargs -n 1 cp -v /root/mongo.keyfile\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 3.4.3 修改配置文件指定keyfile\n\n分别编辑几个服务的mongod.conf文件，添加相关内容：\n\n/mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/sharded_cluster/myshardrs01_27018/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/sharded_cluster/myshardrs01_27118/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/sharded_cluster/myshardrs01_27218/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/sharded_cluster/myshardrs02_27318/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/sharded_cluster/myshardrs02_27418/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/sharded_cluster/myshardrs02_27518/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/sharded_cluster/myconfigrs_27019/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/sharded_cluster/myconfigrs_27119/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/sharded_cluster/myconfigrs_27219/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/mymongos_27017/mongos.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/sharded_cluster/mymongos_27017/mongo.keyfile\n\n\n1\n2\n3\n\n\n/mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\nsecurity:\n    #KeyFile鉴权文件\n    keyFile: /mongodb/sharded_cluster/mymongos_27117/mongo.keyfile\n\n\n1\n2\n3\n\n\nmongos 比mongod少了authorization：enabled的配置。原因是，副本集加分片的安全认证需要配置两方面的，副本集各个节点之间使用内部身份验证，用于内部各个mongo实例的通信，只有相同keyfile才能相互访问。所以都要开启 keyFile: /mongodb/sharded_cluster/mymongos_27117/mongo.keyfile 。\n\n然而对于所有的mongod，才是真正的保存数据的分片。mongos只做路由，不保存数据。所以所有的mongod开启访问数据的授权authorization:enabled。这样用户只有账号密码正确才能访问到数据。\n\n\n# 3.4.4 重新启动节点\n\n必须依次启动配置节点、分片节点、路由节点：\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\n/usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27017/mongos.conf\n/usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n注意：\n\n这里有个非常特别的情况，就是启动顺序。先启动配置节点，再启动分片节点，最后启动路由节点。\n\n如果先启动分片节点，会卡住，提示：\n\nabout to fork child process, waiting until server is ready for connections\n\n\n1\n\n\n这也许是个 bug。原因未知。\n\n\n# 3.3.5 创建帐号和认证\n\n客户端mongo，通过localhost登录任意一个mongos路由，\n\n[root@bobohost db]# /usr/local/mongodb/bin/mongo --port 27017\n\n\n1\n\n\n提示：相当于一个后门，只能在 admin下添加用户。\n\n创建一个管理员帐号：\n\nmongos> use admin\nswitched to db admin\nmongos>  db.createUser({user:"myroot",pwd:"123456",roles:["root"]})\nSuccessfully added user: { "user" : "myroot", "roles" : [ "root" ] }\n\n\n1\n2\n3\n4\n\n\n提示：如果在开启认证之前已经创建了管理员账号，这里可以忽略\n\n创建一个普通权限帐号：\n\nmongos> use admin\nswitched to db admin\nmongos> db.auth("myroot","123456")\n1\nmongos> use articledb\nswitched to db articledb\nmongos> db.createUser({user: "bobo", pwd: "123456", roles: [{ role: "readWrite",db: "articledb" }]})\nSuccessfully added user: {\n    "user" : "bobo",\n    "roles" : [\n        {\n          "role" : "readWrite",\n          "db" : "articledb"\n        }\n    ]\n}\nmongos> db.auth("bobo","123456")\n1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n提示：\n\n通过mongos添加的账号信息，只会保存到配置节点的服务中，具体的数据节点不保存账号信息，因此，分片中的账号信息不涉及到同步问题。\n\nmongo客户端登录mongos路由，用管理员帐号登录可查看分片情况：\n\nmongos> use admin\nswitched to db admin\nmongos>  db.auth("myroot","123456")\n1\nmongos> sh.status()\n\n\n1\n2\n3\n4\n5\n\n\n退出连接，重新连接服务，使用普通权限帐号访问数据：\n\n[root@bobohost db]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017\nMongoDB shell version v4.0.10\nconnecting to: mongodb://180.76.159.126:27017/?gssapiServiceName=mongodb\nImplicit session: session { "id" : UUID("6f84fa91-2414-407e-b3ab-c0b7eedde825")\n}\nMongoDB server version: 4.0.10\nmongos> use articledb\nswitched to db articledb\nmongos> db.auth("bobo","123456")\n1\nmongos> show collections\ncomment\ncomment2\nmongos> db.comment.count()\n10001\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 3.3.6 SpringDataMongoDB连接认证\n\n使用用户名和密码连接到 MongoDB 服务器，你必须使用\'username:password@hostname/dbname\' 格式，’username’为用户名，’password’ 为密码。\n\n目标：使用用户bobo使用密码 123456 连接到MongoDB 服务上。\n\napplication.yml：\n\nspring:\n    #数据源配置\n    data:\n        mongodb:\n        # 分片集群有认证的情况下，字符串连接\n            uri: mongodb://bobo:123456@180.76.159.126:27017,180.76.159.126:27117/articledb\n\n\n1\n2\n3\n4\n5\n6\n\n\n原文链接：https://wgy1993.gitee.io/archives/afe7047c.html',normalizedContent:'# replica sets\n\n\n# 1.1 简介\n\nmongodb中的副本集（replica set）是一组维护相同数据集的mongod服务。 副本集可提供冗余和高可用性，是所有生产部署的基础。\n\n也可以说，副本集类似于有自动故障恢复功能的主从集群。通俗的讲就是用多台机器进行同一数据的异步同步，从而使多台机器拥有同一数据的多个副本，并且当主库当掉时在不需要用户干预的情况下自动切换其他备份服务器做主库。而且还可以利用副本服务器做只读服务器，实现读写分离，提高负载。\n\n（1）冗余和数据可用性\n\n复制提供冗余并提高数据可用性。 通过在不同数据库服务器上提供多个数据副本，复制可提供一定级别的容错功能，以防止丢失单个数据库服务器。\n\n在某些情况下，复制可以提供增加的读取性能，因为客户端可以将读取操作发送到不同的服务上， 在不同数据中心维护数据副本可以增加分布式应用程序的数据位置和可用性。 您还可以为专用目的维护其他副本，例如灾难恢复，报告或备份。\n\n（2）mongodb中的复制\n\n副本集是一组维护相同数据集的mongod实例。 副本集包含多个数据承载节点和可选的一个仲裁节点。在承载数据的节点中，一个且仅一个成员被视为主节点，而其他节点被视为次要（从）节点。\n\n主节点接收所有写操作。 副本集只能有一个主要能够确认具有{w：“most”}写入关注的写入; 虽然在某些情况下，另一个mongod实例可能暂时认为自己也是主要的。主要记录其操作日志中的数据集的所有更改，即oplog。\n\n\n\n辅助(副本)节点复制主节点的oplog并将操作应用于其数据集，以使辅助节点的数据集反映主节点的数据集。 如果主要人员不在，则符合条件的中学将举行选举以选出新的主要人员。\n\n（3）主从复制和副本集区别\n\n主从集群和副本集最大的区别就是副本集没有固定的“主节点”；整个集群会选出一个“主节点”，当其挂掉后，又在剩下的从节点中选中其他节点为“主节点”，副本集总有一个活跃点(主、primary)和一个或多个备份节点(从、secondary)。\n\n\n# 1.2 副本集的三个角色\n\n副本集有两种类型三种角色\n\n两种类型：\n\n * 主节点（ primary）类型：数据操作的主要连接点，可读写。\n * 次要（辅助、从）节点（ secondaries）类型：数据冗余备份节点，可以读或选举。\n\n三种角色：\n\n * 主要成员（primary）：主要接收所有写操作。就是主节点。\n * 副本成员（replicate）：从主节点通过复制操作以维护相同的数据集，即备份数据，不可写操作，但可以读操作（但需要配置）。是默认的一种从节点类型。\n * 仲裁者（ arbiter）：不保留任何数据的副本，只具有投票选举作用。当然也可以将仲裁服务器维护为副本集的一部分，即副本成员同时也可以是仲裁者。也是一种从节点类型。\n\n\n\n关于仲裁者的额外说明：\n\n您可以将额外的mongod实例添加到副本集作为仲裁者。 仲裁者不维护数据集。 仲裁者的目的是通过响应其他副本集成员的心跳和选举请求来维护副本集中的仲裁。 因为它们不存储数据集，所以仲裁器可以是提供副本集仲裁功能的好方法，其资源成本比具有数据集的全功能副本集成员更便宜。\n\n如果您的副本集具有偶数个成员，请添加仲裁者以获得主要选举中的“大多数”投票。 仲裁者不需要专用硬件。\n\n仲裁者将永远是仲裁者，而主要人员可能会退出并成为次要人员，而次要人员可能成为选举期间的主要人员。\n\n如果你的副本+主节点的个数是偶数，建议加一个仲裁者，形成奇数，容易满足大多数的投票。\n\n如果你的副本+主节点的个数是奇数，可以不加仲裁者。\n\n\n# 1.3 副本集架构目标\n\n一主一副本一仲裁\n\n\n\n\n# 1.4 副本集的创建\n\n\n# 1.4.1 第一步：创建主节点\n\n建立存放数据和日志的目录\n\n#-----------myrs\n#主节点\nmkdir -p /mongodb/replica_sets/myrs_27017/log \\ &\nmkdir -p /mongodb/replica_sets/myrs_27017/data/db\n\n\n1\n2\n3\n4\n\n\n新建或修改配置文件：\n\nvim /mongodb/replica_sets/myrs_27017/mongod.conf\n\n\n1\n\n\nmyrs_27017 ：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/replica_sets/myrs_27017/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/replica_sets/myrs_27017/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/replica_sets/myrs_27017/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #bindip\n    #绑定的端口\n    port: 27017\nreplication:\n    #副本集的名称\n    replsetname: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动节点服务：\n\n[root@bobohost replica_sets]# /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 54257\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n\n\n\n# 1.4.2 第二步：创建副本节点\n\n建立存放数据和日志的目录\n\n#-----------myrs\n#副本节点\nmkdir -p /mongodb/replica_sets/myrs_27018/log \\ &\nmkdir -p /mongodb/replica_sets/myrs_27018/data/db\n\n\n1\n2\n3\n4\n\n\n新建或修改配置文件：\n\nvim /mongodb/replica_sets/myrs_27018/mongod.conf\n\n\n1\n\n\nmyrs_27018 ：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/replica_sets/myrs_27018/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/replica_sets/myrs_27018/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/replica_sets/myrs_27018/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #bindip\n    #绑定的端口\n    port: 27018\nreplication:\n    #副本集的名称\n    replsetname: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动节点服务：\n\n[root@bobohost replica_sets]# /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 54361\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n\n\n\n# 1.4.3 第三步：创建仲裁节点\n\n建立存放数据和日志的目录\n\n#-----------myrs\n#仲裁节点\nmkdir -p /mongodb/replica_sets/myrs_27019/log \\ &\nmkdir -p /mongodb/replica_sets/myrs_27019/data/db\n\n\n1\n2\n3\n4\n\n\n新建或修改配置文件：\n\nvim /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n\n\nmyrs_27019 ：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/replica_sets/myrs_27019/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/replica_sets/myrs_27019/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/replica_sets/myrs_27019/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #bindip\n    #绑定的端口\n    port: 27019\nreplication:\n    #副本集的名称\n    replsetname: myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动节点服务：\n\n[root@bobohost replica_sets]# /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 54410\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n\n\n\n# 1.4.4 第四步：初始化配置副本集和主节点\n\n使用客户端命令连接任意一个节点，但这里尽量要连接主节点(27017节点)：\n\n/usr/local/mongodb/bin/mongo --host=180.76.159.126 --port=27017\n\n\n1\n\n\n结果，连接上之后，很多命令无法使用，，比如 show dbs 等，必须初始化副本集才行\n\n准备初始化新的副本集：\n\n语法：\n\nrs.initiate(configuration)\n\n\n1\n\n\n【示例】\n\n使用默认的配置来初始化副本集：\n\nrs.initiate()\n\n\n1\n\n\n执行结果：\n\n> rs.initiate()\n{\n    "info2" : "no configuration specified. using a default configuration for the set",\n    "me" : "180.76.159.126:27017",\n    "ok" : 1,\n    "operationtime" : timestamp(1565760476, 1),\n    "$clustertime" : {\n        "clustertime" : timestamp(1565760476, 1),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n}\nmyrs:secondary>\nmyrs:primary>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n提示：\n\n * “ok”的值为1，说明创建成功。\n * 命令行提示符发生变化，变成了一个从节点角色，此时默认不能读写。稍等片刻，回车，变成主节点。\n\n\n# 1.4.5 第五步：查看副本集的配置内容\n\n返回包含当前副本集配置的文档。\n\n语法：\n\nrs.conf(configuration)\n\n\n1\n\n\n提示：\n\nrs.config() 是该方法的别名。\n\nconfiguration：可选，如果没有配置，则使用默认主节点配置。\n\n【示例】\n\n在27017上执行副本集中当前节点的默认节点配置\n\nmyrs:primary> rs.conf()\n{\n    "_id" : "myrs",\n    "version" : 1,\n    "protocolversion" : numberlong(1),\n    "writeconcernmajorityjournaldefault" : true,\n    "members" : [\n        {\n            "_id" : 0,\n            "host" : "180.76.159.126:27017",\n            "arbiteronly" : false,\n            "buildindexes" : true,\n            "hidden" : false,\n            "priority" : 1,\n            "tags" : {\n\n            },\n            "slavedelay" : numberlong(0),\n            "votes" : 1\n        }\n    ],\n    "settings" : {\n        "chainingallowed" : true,\n        "heartbeatintervalmillis" : 2000,\n        "heartbeattimeoutsecs" : 10,\n        "electiontimeoutmillis" : 10000,\n        "catchuptimeoutmillis" : -1,\n        "catchuptakeoverdelaymillis" : 30000,\n        "getlasterrormodes" : {\n\n        },\n        "getlasterrordefaults" : {\n            "w" : 1,\n            "wtimeout" : 0\n        },\n        "replicasetid" : objectid("5d539bdcd6a308e600d126bb")\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n说明：\n\n * "_id" : "myrs" ：副本集的配置数据存储的主键值，默认就是副本集的名字\n * "members" ：副本集成员数组，此时只有一个： “host” : “180.76.159.126:27017” ，该成员不是仲裁节点：”arbiteronly” : false ，优先级（权重值）： “priority” : 1,\n * "settings" ：副本集的参数配置。\n\n提示：副本集配置的查看命令，本质是查询的是 system.replset 的表中的数据：\n\nmyrs:primary> use local\nswitched to db local\nmyrs:primary> show collections\noplog.rs\nreplset.election\nreplset.minvalid\nreplset.oplogtruncateafterpoint\nstartup_log\nsystem.replset\nsystem.rollback.id\nmyrs:primary> db.system.replset.find()\n{ "_id" : "myrs", "version" : 1, "protocolversion" : numberlong(1),\n"writeconcernmajorityjournaldefault" : true, "members" : [ { "_id" : 0, "host" :\n"180.76.159.126:27017", "arbiteronly" : false, "buildindexes" : true, "hidden" :\nfalse, "priority" : 1, "tags" : { }, "slavedelay" : numberlong(0), "votes" : 1\n} ], "settings" : { "chainingallowed" : true, "heartbeatintervalmillis" : 2000,\n"heartbeattimeoutsecs" : 10, "electiontimeoutmillis" : 10000,\n"catchuptimeoutmillis" : -1, "catchuptakeoverdelaymillis" : 30000,\n"getlasterrormodes" : { }, "getlasterrordefaults" : { "w" : 1, "wtimeout" : 0\n}, "replicasetid" : objectid("5d539bdcd6a308e600d126bb") } }\nmyrs:primary>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 1.4.6 第六步：查看副本集状态\n\n检查副本集状态。\n\n说明：\n\n返回包含状态信息的文档。此输出使用从副本集的其他成员发送的心跳包中获得的数据反映副本集的当前状态。\n\n语法：\n\nrs.status()\n\n\n1\n\n\n【示例】\n\n在27017上查看副本集状态：\n\nmyrs:primary> rs.status()\n{\n    "set" : "myrs",\n    "date" : isodate("2019-08-14t05:29:45.161z"),\n    "mystate" : 1,\n    "term" : numberlong(1),\n    "syncingto" : "",\n    "syncsourcehost" : "",\n    "syncsourceid" : -1,\n    "heartbeatintervalmillis" : numberlong(2000),\n    "optimes" : {\n        "lastcommittedoptime" : {\n            "ts" : timestamp(1565760578, 1),\n            "t" : numberlong(1)\n        },\n        "readconcernmajorityoptime" : {\n            "ts" : timestamp(1565760578, 1),\n            "t" : numberlong(1)\n        },\n        "appliedoptime" : {\n            "ts" : timestamp(1565760578, 1),\n            "t" : numberlong(1)\n        },\n        "durableoptime" : {\n            "ts" : timestamp(1565760578, 1),\n            "t" : numberlong(1)\n        }\n    },\n    "laststablecheckpointtimestamp" : timestamp(1565760528, 1),\n    "members" : [\n        {\n        "_id" : 0,\n        "name" : "180.76.159.126:27017",\n        "health" : 1,\n        "state" : 1,\n        "statestr" : "primary",\n        "uptime" : 419,\n        "optime" : {\n            "ts" : timestamp(1565760578, 1),\n            "t" : numberlong(1)\n        },\n        "optimedate" : isodate("2019-08-14t05:29:38z"),\n        "syncingto" : "",\n        "syncsourcehost" : "",\n        "syncsourceid" : -1,\n        "infomessage" : "could not find member to sync from",\n        "electiontime" : timestamp(1565760476, 2),\n        "electiondate" : isodate("2019-08-14t05:27:56z"),\n        "configversion" : 1,\n        "self" : true,\n        "lastheartbeatmessage" : ""\n        }\n    ],\n    "ok" : 1,\n    "operationtime" : timestamp(1565760578, 1),\n    "$clustertime" : {\n        "clustertime" : timestamp(1565760578, 1),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n说明：\n\n * "set" : "myrs" ：副本集的名字\n * "mystate" : 1：说明状态正常\n * "members" ：副本集成员数组，此时只有一个： "name" : "180.76.159.126:27017" ，该成员的角色是 "statestr" : "primary", 该节点是健康的： "health" : 1 。\n\n\n# 1.4.7 第七步：添加副本从节点\n\n在主节点添加从节点，将其他成员加入到副本集\n\n语法：\n\nrs.add(host, arbiteronly)\n\n\n1\n\n\n选项：\n\nparameter     type                 description\nhost          string or document   要添加到副本集的新成员。\n                                   指定为字符串或配置文档：1）如果是一个字符串，则需要指定新成员的主机名和可选的端口号；2）如果是一个文档，请指定在members数组中找到的副本集成员配置文档。\n                                   您必须在成员配置文档中指定主机字段。有关文档配置字段的说明，详见下方文档：“主机成员的配置文档”\narbiteronly   boolean              可选的。 仅在 值为字符串时适用。 如果为true，则添加的主机是仲裁者。\n\n主机成员的配置文档：\n\n{\n    _id: <int>,\n    host: <string>,     // required\n    arbiteronly: <boolean>,\n    buildindexes: <boolean>,\n    hidden: <boolean>,\n    priority: <number>,\n    tags: <document>,\n    slavedelay: <int>,\n    votes: <number>\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n【示例】\n\n将27018的副本节点添加到副本集中：\n\nmyrs:primary> rs.add("180.76.159.126:27018")\n{\n    "ok" : 1,\n    "operationtime" : timestamp(1565761757, 1),\n    "$clustertime" : {\n        "clustertime" : timestamp(1565761757, 1),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n说明：\n\n * “ok” : 1 ：说明添加成功。\n\n查看副本集状态：\n\nmyrs:primary> rs.status()\n{\n    "set" : "myrs",\n    "date" : isodate("2019-08-14t05:50:05.738z"),\n    "mystate" : 1,\n    "term" : numberlong(1),\n    "syncingto" : "",\n    "syncsourcehost" : "",\n    "syncsourceid" : -1,\n    "heartbeatintervalmillis" : numberlong(2000),\n    "optimes" : {\n        "lastcommittedoptime" : {\n            "ts" : timestamp(1565761798, 1),\n            "t" : numberlong(1)\n        },\n        "readconcernmajorityoptime" : {\n            "ts" : timestamp(1565761798, 1),\n            "t" : numberlong(1)\n        },\n        "appliedoptime" : {\n            "ts" : timestamp(1565761798, 1),\n            "t" : numberlong(1)\n        },\n        "durableoptime" : {\n            "ts" : timestamp(1565761798, 1),\n            "t" : numberlong(1)\n        }\n    },\n    "laststablecheckpointtimestamp" : timestamp(1565761798, 1),\n    "members" : [\n        {\n            "_id" : 0,\n            "name" : "180.76.159.126:27017",\n            "health" : 1,\n            "state" : 1,\n            "statestr" : "primary",\n            "uptime" : 1639,\n            "optime" : {\n                "ts" : timestamp(1565761798, 1),\n                "t" : numberlong(1)\n            },\n            "optimedate" : isodate("2019-08-14t05:49:58z"),\n            "syncingto" : "",\n            "syncsourcehost" : "",\n            "syncsourceid" : -1,\n            "infomessage" : "",\n            "electiontime" : timestamp(1565760476, 2),\n            "electiondate" : isodate("2019-08-14t05:27:56z"),\n            "configversion" : 2,\n            "self" : true,\n            "lastheartbeatmessage" : ""\n        },\n        {\n            "_id" : 1,\n            "name" : "180.76.159.126:27018",\n            "health" : 1,\n            "state" : 2,\n            "statestr" : "secondary",\n            "uptime" : 48,\n            "optime" : {\n                "ts" : timestamp(1565761798, 1),\n                "t" : numberlong(1)\n            },\n            "optimedurable" : {\n                "ts" : timestamp(1565761798, 1),\n                "t" : numberlong(1)\n            },\n            "optimedate" : isodate("2019-08-14t05:49:58z"),\n            "optimedurabledate" : isodate("2019-08-14t05:49:58z"),\n            "lastheartbeat" : isodate("2019-08-14t05:50:05.294z"),\n            "lastheartbeatrecv" : isodate("2019-08-\n            14t05:50:05.476z"),\n            "pingms" : numberlong(0),\n            "lastheartbeatmessage" : "",\n            "syncingto" : "180.76.159.126:27017",\n            "syncsourcehost" : "180.76.159.126:27017",\n            "syncsourceid" : 0,\n            "infomessage" : "",\n            "configversion" : 2\n        }\n    ],\n    "ok" : 1,\n    "operationtime" : timestamp(1565761798, 1),\n    "$clustertime" : {\n        "clustertime" : timestamp(1565761798, 1),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n\n\n说明：\n\n * "name" : "180.76.159.126:27018" 是第二个节点的名字，其角色是 "statestr" : "secondary"\n\n\n# 1.4.8 第八步：添加仲裁从节点\n\n添加一个仲裁节点到副本集\n\n语法：\n\nrs.addarb(host)\n\n\n1\n\n\n将27019的仲裁节点添加到副本集中：\n\nmyrs:primary> rs.addarb("180.76.159.126:27019")\n{\n    "ok" : 1,\n    "operationtime" : timestamp(1565761959, 1),\n    "$clustertime" : {\n        "clustertime" : timestamp(1565761959, 1),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n说明：\n\n * "ok" : 1 ：说明添加成功。\n\n查看副本集状态：\n\nmyrs:primary> rs.status()\n{\n    "set" : "myrs",\n    "date" : isodate("2019-08-14t05:53:27.198z"),\n    "mystate" : 1,\n    "term" : numberlong(1),\n    "syncingto" : "",\n    "syncsourcehost" : "",\n    "syncsourceid" : -1,\n    "heartbeatintervalmillis" : numberlong(2000),\n    "optimes" : {\n        "lastcommittedoptime" : {\n            "ts" : timestamp(1565761998, 1),\n            "t" : numberlong(1)\n        },\n        "readconcernmajorityoptime" : {\n            "ts" : timestamp(1565761998, 1),\n            "t" : numberlong(1)\n        },\n        "appliedoptime" : {\n            "ts" : timestamp(1565761998, 1),\n            "t" : numberlong(1)\n        },\n        "durableoptime" : {\n            "ts" : timestamp(1565761998, 1),\n            "t" : numberlong(1)\n        }\n    },\n    "laststablecheckpointtimestamp" : timestamp(1565761978, 1),\n    "members" : [\n        {\n            "_id" : 0,\n            "name" : "180.76.159.126:27017",\n            "health" : 1,\n            "state" : 1,\n            "statestr" : "primary",\n            "uptime" : 1841,\n            "optime" : {\n                "ts" : timestamp(1565761998, 1),\n                "t" : numberlong(1)\n            },\n            "optimedate" : isodate("2019-08-14t05:53:18z"),\n            "syncingto" : "",\n            "syncsourcehost" : "",\n            "syncsourceid" : -1,\n            "infomessage" : "",\n            "electiontime" : timestamp(1565760476, 2),\n            "electiondate" : isodate("2019-08-14t05:27:56z"),\n            "configversion" : 3,\n            "self" : true,\n            "lastheartbeatmessage" : ""\n        },\n        {\n            "_id" : 1,\n            "name" : "180.76.159.126:27018",\n            "health" : 1,\n            "state" : 2,\n            "statestr" : "secondary",\n            "uptime" : 249,\n            "optime" : {\n                "ts" : timestamp(1565761998, 1),\n                "t" : numberlong(1)\n            },\n            "optimedurable" : {\n                "ts" : timestamp(1565761998, 1),\n                "t" : numberlong(1)\n            },\n            "optimedate" : isodate("2019-08-14t05:53:18z"),\n            "optimedurabledate" : isodate("2019-08-14t05:53:18z"),\n            "lastheartbeat" : isodate("2019-08-14t05:53:25.668z"),\n            "lastheartbeatrecv" : isodate("2019-08-14t05:53:26.702z"),\n            "pingms" : numberlong(0),\n            "lastheartbeatmessage" : "",\n            "syncingto" : "180.76.159.126:27017",\n            "syncsourcehost" : "180.76.159.126:27017",\n            "syncsourceid" : 0,\n            "infomessage" : "",\n            "configversion" : 3\n        },\n        {\n            "_id" : 2,\n            "name" : "180.76.159.126:27019",\n            "health" : 1,\n            "state" : 7,\n            "statestr" : "arbiter",\n            "uptime" : 47,\n            "lastheartbeat" : isodate("2019-08-14t05:53:25.668z"),\n            "lastheartbeatrecv" : isodate("2019-08-14t05:53:25.685z"),\n            "pingms" : numberlong(0),\n            "lastheartbeatmessage" : "",\n            "syncingto" : "",\n            "syncsourcehost" : "",\n            "syncsourceid" : -1,\n            "infomessage" : "",\n            "configversion" : 3\n        }\n    ],\n    "ok" : 1,\n    "operationtime" : timestamp(1565761998, 1),\n    "$clustertime" : {\n        "clustertime" : timestamp(1565761998, 1),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n\n\n说明：\n\n * "name" : "180.76.159.126:27019" 是第二个节点的名字，其角色是 "statestr" : "arbiter"\n\n\n# 1.5 副本集的数据读写操作\n\n目标：测试三个不同角色的节点的数据读写情况\n\n登录主节点27017，写入和读取数据：\n\n[root@bobohost ~]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017\nmyrs:primary> use articledb\nswitched to db articledb\nmyrs:primary> db\narticledb\nmyrs:primary> db.comment.insert({"articleid":"100000","content":"今天天气真好，阳光明媚","userid":"1001","nickname":"rose","createdatetime":new date()})\nwriteresult({ "ninserted" : 1 })\nmyrs:primary> db.comment.find()\n{ "_id" : objectid("5d4d2ae3068138b4570f53bf"), "articleid" : "100000","content" : "今天天气真好，阳光明媚", "userid" : "1001", "nickname" : "rose","createdatetime" : isodate("2019-08-09t08:12:19.427z") }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n登录从节点27018\n\n[root@bobohost ~]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27018\nmyrs:secondary> show dbs;\n2019-09-10t10:56:51.953+0800 e query  [js] error: listdatabases failed:{\n    "operationtime" : timestamp(1568084204, 1),\n    "ok" : 0,\n    "errmsg" : "not master and slaveok=false",\n    "code" : 13435,\n    "codename" : "notmasternoslaveok",\n    "$clustertime" : {\n        "clustertime" : timestamp(1568084204, 1),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n       }\n   }\n} :\n_geterrorwithcode@src/mongo/shell/utils.js:25:13\nmongo.prototype.getdbs@src/mongo/shell/mongo.js:139:1\nshellhelper.show@src/mongo/shell/utils.js:882:13\nshellhelper@src/mongo/shell/utils.js:766:15\n@(shellhelp2):1:1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n发现，不能读取集合的数据。当前从节点只是一个备份，不是奴隶节点，无法读取数据，写当然更不行。\n\n因为默认情况下，从节点是没有读写权限的，可以增加读的权限，但需要进行设置\n\n设置读操作权限：\n\n说明：\n\n设置为奴隶节点，允许在从成员上运行读的操作\n\n语法：\n\nrs.slaveok()\n#或\nrs.slaveok(true)\n\n\n1\n2\n3\n\n\n提示：\n\n该命令是 db.getmongo().setslaveok() 的简化命令。\n\n【示例】\n\n在27018上设置作为奴隶节点权限，具备读权限：\n\nrs:secondary> rs.slaveok()\n\n\n1\n\n\n此时，在执行查询命令，运行成功！\n\n但仍然不允许插入。\n\nmyrs:secondary> rs.slaveok()\nmyrs:secondary> show dbs;\nadmin    0.000gb\narticledb  0.000gb\nconfig   0.000gb\nlocal    0.000gb\nmyrs:secondary> use articledb\nswitched to db articledb\nmyrs:secondary> show collections\ncomment\nmyrs:secondary> db.comment.find()\n{ "_id" : objectid("5d7710c04cfd7eee2e3cdabe"), "articleid" : "100000","content" : "今天天气真好，阳光明媚", "userid" : "1001", "nickname" : "rose","createdatetime" : isodate("2019-09-10t02:56:00.467z") }\nmyrs:secondary> db.comment.insert({"_id":"1","articleid":"100001","content":"我们不应该把清晨浪费在手机上，健康很重要，k一杯温水幸福你我他。","userid":"1002","nickname":"相忘于江湖","createdatetime":new date("2019-08-05t22:08:15.522z"),"likenum":numberint(1000),"state":"1"})\nwritecommanderror({\n    "operationtime" : timestamp(1568084434, 1),\n    "ok" : 0,\n    "errmsg" : "not master",\n    "code" : 10107,\n    "codename" : "notmaster",\n    "$clustertime" : {\n        "clustertime" : timestamp(1568084434, 1),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n现在可实现了读写分离，让主插入数据，让从来读取数据。\n\n如果要取消作为奴隶节点的读权限：\n\nmyrs:secondary> rs.slaveok(false)\nmyrs:secondary> db.comment.find()\nerror: error: {\n    "operationtime" : timestamp(1568084459, 1),\n    "ok" : 0,\n    "errmsg" : "not master and slaveok=false",\n    "code" : 13435,\n    "codename" : "notmasternoslaveok",\n    "$clustertime" : {\n        "clustertime" : timestamp(1568084459, 1),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n仲裁者节点，不存放任何业务数据的，可以登录查看\n\n[root@bobohost ~]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27019\nmyrs:arbiter> rs.slaveok()\nmyrs:arbiter> show dbs\nlocal  0.000gb\nmyrs:arbiter> use local\nswitched to db local\nmyrs:arbiter> show collections\nreplset.minvalid\nreplset.oplogtruncateafterpoint\nstartup_log\nsystem.replset\nsystem.rollback.id\nmyrs:arbiter>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n发现，只存放副本集配置等数据。\n\n\n# 1.6 主节点的选举原则\n\nmongodb在副本集中，会自动进行主节点的选举，主节点选举的触发条件：\n\n * 主节点故障\n * 主节点网络不可达（默认心跳信息为10秒）\n * 人工干预（rs.stepdown(600)）\n\n一旦触发选举，就要根据一定规则来选主节点。\n\n选举规则是根据票数来决定谁获胜：\n\n * 票数最高，且获得了 “大多数”成员的投票支持的节点获胜。\n   \n   “大多数”的定义为：假设复制集内投票成员数量为n，则大多数为 n/2 + 1。例如：3个投票成员，则大多数的值是2。当复制集内存活成员数量不足大多数时，整个复制集将无法选举出primary，复制集将无法提供写服务，处于只读状态。\n\n * 若票数相同，且都获得了 “大多数”成员的投票支持的，数据新的节点获胜。\n   \n   数据的新旧是通过操作日志oplog来对比的。\n\n在获得票数的时候，优先级（priority）参数影响重大。\n\n可以通过设置优先级（priority）来设置额外票数。优先级即权重，取值为0-1000，相当于可额外增加0-1000的票数，优先级的值越大，就越可能获得多数成员的投票（votes）数。指定较高的值可使成员更有资格成为主要成员，更低的值可使成员更不符合条件。\n\n默认情况下，优先级的值是1\n\nmyrs:primary> rs.conf()\n{\n    "_id" : "myrs",\n    "version" : 3,\n    "protocolversion" : numberlong(1),\n    "writeconcernmajorityjournaldefault" : true,\n    "members" : [\n        {\n            "_id" : 0,\n            "host" : "180.76.159.126:27017",\n            "arbiteronly" : false,\n            "buildindexes" : true,\n            "hidden" : false,\n            "priority" : 1,\n            "tags" : {\n\n            },\n            "slavedelay" : numberlong(0),\n            "votes" : 1\n        },\n        {\n            "_id" : 1,\n            "host" : "180.76.159.126:27018",\n            "arbiteronly" : false,\n            "buildindexes" : true,\n            "hidden" : false,\n            "priority" : 1,\n            "tags" : {\n\n            },\n            "slavedelay" : numberlong(0),\n            "votes" : 1\n        },\n        {\n            "_id" : 2,\n            "host" : "180.76.159.126:27019",\n            "arbiteronly" : true,\n            "buildindexes" : true,\n            "hidden" : false,\n            "priority" : 0,\n            "tags" : {\n\n            },\n            "slavedelay" : numberlong(0),\n            "votes" : 1\n        }\n    ],\n    "settings" : {\n        "chainingallowed" : true,\n        "heartbeatintervalmillis" : 2000,\n        "heartbeattimeoutsecs" : 10,\n        "electiontimeoutmillis" : 10000,\n        "catchuptimeoutmillis" : -1,\n        "catchuptakeoverdelaymillis" : 30000,\n        "getlasterrormodes" : {\n\n        },\n        "getlasterrordefaults" : {\n            "w" : 1,\n            "wtimeout" : 0\n        },\n        "replicasetid" : objectid("5d539bdcd6a308e600d126bb")\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n可以看出，主节点和副本节点的优先级各为 1，即，默认可以认为都已经有了一票。但选举节点，优先级是0，（要注意是，官方说了，选举节点的优先级必须是0，不能是别的值。即不具备选举权，但具有投票权）\n\n【了解】修改优先级\n\n比如，下面提升从节点的优先级：\n\n1）先将配置导入cfg变量\n\nmyrs:secondary> cfg=rs.conf()\n\n\n1\n\n\n2 ）然后修改值（id号默认从0开始）\n\nmyrs:secondary> cfg.members[1].priority=2\n2\n\n\n1\n2\n\n\n3 ）重新加载配置\n\nmyrs:secondary> rs.reconfig(cfg)\n{ "ok" : 1 }\n\n\n1\n2\n\n\n稍等片刻会重新开始选举。\n\n\n# 1.7 故障测试\n\n\n# 1.7.1 副本节点故障测试\n\n关闭27018副本节点：\n\n发现，主节点和仲裁节点对27018的心跳失败。因为主节点还在，因此，没有触发投票选举。\n\n如果此时，在主节点写入数据。\n\ndb.comment.insert({"_id":"1","articleid":"100001","content":"我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。","userid":"1002","nickname":"相忘于江湖","createdatetime":new date("2019-08-05t22:08:15.522z"),"likenum":numberint(1000),"state":"1"})\n\n\n1\n\n\n再启动从节点，会发现，主节点写入的数据，会自动同步给从节点。\n\n\n# 1.7.2 主节点故障测试\n\n关闭27017节点\n\n发现，从节点和仲裁节点对27017的心跳失败，当失败超过10秒，此时因为没有主节点了，会自动发起投票。\n\n而副本节点只有27018，因此，候选人只有一个就是27018，开始投票。\n\n27019向27018投了一票，27018本身自带一票，因此共两票，超过了“大多数”\n\n27019是仲裁节点，没有选举权，27018不向其投票，其票数是0.\n\n最终结果，27018成为主节点。具备读写功能。\n\n在27018写入数据查看。\n\ndb.comment.insert({"_id":"2","articleid":"100001","content":"我夏天空腹喝凉开水，冬天喝温开水","userid":"1005","nickname":"伊人憔悴","createdatetime":new date("2019-08-05t23:58:51.485z"),"likenum":numberint(888),"state":"1"})\n\n\n1\n\n\n再启动 27017节点，发现27017变成了从节点，27018仍保持主节点。\n\n登录27017节点，发现是从节点了，数据自动从27018同步。\n\n从而实现了高可用。\n\n\n# 1.7.3 仲裁节点和主节点故障\n\n先关掉仲裁节点27019，\n\n关掉现在的主节点27018\n\n登录27017后，发现，27017仍然是从节点，副本集中没有主节点了，导致此时，副本集是只读状态，无法写入。\n\n为啥不选举了？因为27017的票数，没有获得大多数，即没有大于等于2，它只有默认的一票（优先级是1）\n\n如果要触发选举，随便加入一个成员即可。\n\n * 如果只加入 27019仲裁节点成员，则主节点一定是27017，因为没得选了，仲裁节点不参与选举，但参与投票。（不演示）\n * 如果只加入 27018节点，会发起选举。因为27017和27018都是两票，则按照谁数据新，谁当主节点。\n\n\n# 1.7.4 仲裁节点和从节点故障\n\n先关掉仲裁节点27019，\n\n关掉现在的副本节点27018\n\n10秒后，27017主节点自动降级为副本节点。（服务降级）\n\n副本集不可写数据了，已经故障了。\n\n\n# 1.8 compass 连接副本集\n\ncompass连接：\n\n\n\n\n\n\n# 1.9 springdatamongodb 连接副本集\n\n副本集语法：\n\nmongodb://host1,host2,host3/articledb?connect=replicaset&slaveok=true&replicaset=副本集名字\n\n\n1\n\n\n其中：\n\n * slaveok=true ：开启副本节点读的功能，可实现读写分离。\n * connect=replicaset ：自动到副本集中选择读写的主机。如果slaveok是打开的，则实现了读写分离\n\n【示例】\n\n连接 replica set 三台服务器 (端口 27017, 27018, 和27019)，直接连接第一个服务器，无论是replica set一部分或者主服务器或者从服务器，写入操作应用在主服务器 并且分布查询到从服务器。\n\n修改配置文件application.yml\n\nspring:\n    #数据源配置\n    data:\n        mongodb:\n            # 主机地址\n            #   host: 180.76.159.126\n            # 数据库\n            #   database: articledb\n            # 默认端口是27017\n            #   port: 27017\n            #也可以使用uri连接\n            #uri: mongodb://192.168.40.134:27017/articledb\n            # 副本集的连接字符串\n            uri: mongodb://180.76.159.126:27017,180.76.159.126:27018,180.76.159.126:27019/articledb?connect=replicaset&slaveok=true&replicaset=myrs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n注意：\n\n主机必须是副本集中所有的主机，包括主节点、副本节点、仲裁节点。\n\nspringdatamongodb自动实现了读写分离：\n\n写操作时，只打开主节点连接；读操作是，同时打开主节点和从节点连接，但使用从节点获取数据。\n\n完整的连接字符串的参考（了解）：\n\nmongodb客户端连接语法：\n\nmongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostn[:portn]]][/[database][?options]]\n\n\n1\n\n * mongodb:// 这是固定的格式，必须要指定。\n * username:password@ 可选项，如果设置，在连接数据库服务器之后，驱动都会尝试登陆这个数据库\n * host1 必须的指定至少一个host, host1 是这个uri唯一要填写的。它指定了要连接服务器的地址。如果要连接复制集，请指定多个主机地址。\n * portx 可选的指定端口，如果不填，默认为27017\n * /database 如果指定username:password@，连接并验证登陆指定数据库。若不指定，默认打开test 数据库。\n * ?options 是连接选项。如果不使用/database，则前面需要加上/。所有连接选项都是键值对name=value，键值对之间通过&或;（分号）隔开\n\n标准的连接格式包含了多个选项(options)，如下所示：\n\n选项                    描述\nreplicaset=name       验证replica set的名称。 impliesconnect=replicaset.\nslaveok=true|false    true:在connect=direct模式下，驱动会连接第一台机器，即使这台服务器不是主。在connect=replicaset模式下，驱动会发送所有的写请求到主并且把读取操作分布在其他从服务器。false:\n                      在connect=direct模式下，驱动会自动找寻主服务器. 在connect=replicaset\n                      模式下，驱动仅仅连接主服务器，并且所有的读写命令都连接到主服务器。\nsafe=true|false       true: 在执行更新操作之后，驱动都会发送getlasterror命令来确保更新成功。(还要参考\n                      wtimeoutms).false: 在每次更新之后，驱动不会发送getlasterror来确保更新成功。\nw=n                   驱动添加 { w : n } 到getlasterror命令. 应用于safe=true。\nwtimeoutms=ms         驱动添加 { wtimeout : ms } 到 getlasterror 命令. 应用于 safe=true.\nfsync=true|false      true: 驱动添加 { fsync : true } 到 getlasterror\n                      命令.应用于safe=true.false: 驱动不会添加到getlasterror命令中。\njournal=true|false    如果设置为 true, 同步到 journal (在提交到数据库前写入到实体中).应用于 safe=true\nconnecttimeoutms=ms   可以打开连接的时间\nsockettimeoutms=ms    发送和接受sockets的时间\n\n\n# 2. 分片集群-sharded cluster\n\n\n# 2.1 分片概念\n\n分片（sharding）是一种跨多台机器分布数据的方法， mongodb使用分片来支持具有非常大的数据集和高吞吐量操作的部署。\n\n换句话说：分片(sharding)是指将数据拆分，将其分散存在不同的机器上的过程。有时也用分区(partitioning)来表示这个概念。将数据分散到不同的机器上，不需要功能强大的大型计算机就可以储存更多的数据，处理更多的负载。\n\n具有大型数据集或高吞吐量应用程序的数据库系统可以会挑战单个服务器的容量。例如，高查询率会耗尽服务器的cpu容量。工作集大小大于系统的ram会强调磁盘驱动器的i / o容量。\n\n有两种解决系统增长的方法：垂直扩展和水平扩展。\n\n垂直扩展意味着增加单个服务器的容量，例如使用更强大的cpu，添加更多ram或增加存储空间量。可用技术的局限性可能会限制单个机器对于给定工作负载而言足够强大。此外，基于云的提供商基于可用的硬件配置具有硬性上限。结果，垂直缩放有实际的最大值。\n\n水平扩展意味着划分系统数据集并加载多个服务器，添加其他服务器以根据需要增加容量。虽然单个机器的总体速度或容量可能不高，但每台机器处理整个工作负载的子集，可能提供比单个高速大容量服务器更高的效率。扩展部署容量只需要根据需要添加额外的服务器，这可能比单个机器的高端硬件的总体成本更低。权衡是基础架构和部署维护的复杂性增加。\n\nmongodb支持通过分片进行水平扩展。\n\n\n# 2.2 分片集群包含的组件\n\nmongodb分片群集包含以下组件：\n\n * 分片（存储）：每个分片包含分片数据的子集。 每个分片都可以部署为副本集。\n * mongos （路由）：mongos充当查询路由器，在客户端应用程序和分片集群之间提供接口。\n * config servers （“调度”的配置）：配置服务器存储群集的元数据和配置设置。 从mongodb 3.4开始，必须将配置服务器部署为副本集（csrs）。\n\n下图描述了分片集群中组件的交互：\n\n\n\nmongodb在集合级别对数据进行分片，将集合数据分布在集群中的分片上。\n\n\n# 2.3 分片集群架构目标\n\n两个分片节点副本集（3+3）+一个配置节点副本集（3）+两个路由节点（2），共11个服务节点。\n\n\n\n\n# 2.4 分片（存储）节点副本集的创建\n\n所有的的配置文件都直接放到 sharded_cluster 的相应的子目录下面，默认配置文件名字：mongod.conf\n\n\n# 2.4.1 第一套副本集\n\n准备存放数据和日志的目录：\n\n#-----------myshardrs01\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27018/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27018/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27118/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27118/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27218/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs01_27218/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n\n\n1\n\n\nmyshardrs01_27018 ：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs01_27018/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/sharded_cluster/myshardrs01_27018/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/sharded_cluster/myshardrs01_27018/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #bindip\n    #绑定的端口\n    port: 27018\nreplication:\n    #副本集的名称\n    replsetname: myshardrs01\nsharding:\n    #分片角色\n    clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nsharding.clusterrole：\n\nvalue       description\nconfigsvr   start this instance as a config server. the instance starts\n            on port 27019 by default.\nshardsvr    start this instance as a shard . the instance starts on port\n            27018 by default.\n\n注意：\n\n设置sharding.clusterrole需要mongod实例运行复制。 要将实例部署为副本集成员，请使用replsetname设置并指定副本集的名称。\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n\n\n1\n\n\nmyshardrs01_27118 ：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs01_27118/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/sharded_cluster/myshardrs01_27118/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/sharded_cluster/myshardrs01_27118/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #绑定的端口\n    port: 27118\nreplication:\n    replsetname: myshardrs01\nsharding:\n    clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n\n\n1\n\n\nmyshardrs01_27218 ：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs01_27218/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/sharded_cluster/myshardrs01_27218/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/sharded_cluster/myshardrs01_27218/log/mongod.pid"\nnet:\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #绑定的端口\n    port: 27218\nreplication:\n    replsetname: myshardrs01\nsharding:\n    clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n启动第一套副本集：一主一副本一仲裁\n\n依次启动三个mongod服务：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123223\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123292\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123326\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n查看服务是否启动：\n\n[root@bobohost bin]# ps -ef |grep mongod\npolkitd  61622  61604  0 7月31 ?    00:04:29 mongod --bind_ip_all\nroot   123223    1  1 01:10 ?     00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\nroot   123292    1  4 01:11 ?     00:00:00 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\nroot   123326    1  6 01:11 ?     00:00:00 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n\n\n1\n2\n3\n4\n5\n\n\n# 2.4.1.1 初始化副本集和创建主节点：\n\n使用客户端命令连接任意一个节点，但这里尽量要连接主节点：\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27018\n\n\n1\n\n\n执行初始化副本集命令：\n\nrs.initiate()\n\n\n1\n\n\n查看副本集情况\n\nrs.status()\n\n\n1\n\n\n主节点配置查看\n\nrs.conf()\n\n\n1\n\n\n# 2.4.1.2 添加副本节点\n\nrs.add("180.76.159.126:27118")\n\n\n1\n\n\n# 2.4.1.3 添加仲裁节点\n\nrs.addarb("180.76.159.126:27218")\n\n\n1\n\n\n查看副本集的配置情况\n\nmyshardrs01:primary> rs.conf()\n{\n    "_id" : "myshardrs01",\n    "version" : 3,\n    "protocolversion" : numberlong(1),\n    "writeconcernmajorityjournaldefault" : true,\n    "members" : [\n        {\n            "_id" : 0,\n            "host" : "180.76.159.126:27018",\n            "arbiteronly" : false,\n            "buildindexes" : true,\n            "hidden" : false,\n            "priority" : 1,\n            "tags" : {\n\n            },\n            "slavedelay" : numberlong(0),\n            "votes" : 1\n        },\n        {\n            "_id" : 1,\n            "host" : "180.76.159.126:27118",\n            "arbiteronly" : false,\n            "buildindexes" : true,\n            "hidden" : false,\n            "priority" : 1,\n            "tags" : {\n\n            },\n            "slavedelay" : numberlong(0),\n            "votes" : 1\n        },\n        {\n            "_id" : 2,\n            "host" : "180.76.159.126:27218",\n            "arbiteronly" : true,\n            "buildindexes" : true,\n            "hidden" : false,\n            "priority" : 0,\n            "tags" : {\n\n            },\n            "slavedelay" : numberlong(0),\n            "votes" : 1\n        }\n    ],\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# 2.4.2 第二套副本集\n\n准备存放数据和日志的目录：\n\n#-----------myshardrs02\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27318/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27318/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27418/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27418/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27518/log \\ &\nmkdir -p /mongodb/sharded_cluster/myshardrs02_27518/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n\n\n1\n\n\nmyshardrs02_27318 ：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs02_27318/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/sharded_cluster/myshardrs02_27318/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/sharded_cluster/myshardrs02_27318/log/mongod.pid"\nnet:\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #绑定的端口\n    port: 27318\nreplication:\n    replsetname: myshardrs02\nsharding:\n    clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n\n\n1\n\n\nmyshardrs02_27418 ：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs02_27418/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/sharded_cluster/myshardrs02_27418/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/sharded_cluster/myshardrs02_27418/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #绑定的端口\n    port: 27418\nreplication:\n    replsetname: myshardrs02\nsharding:\n    clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\n\n\n1\n\n\nmyshardrs02_27518 ：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myshardrs02_27518/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/sharded_cluster/myshardrs02_27518/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/sharded_cluster/myshardrs02_27518/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #绑定的端口\n    port: 27518\nreplication:\n    replsetname: myshardrs02\nsharding:\n    clusterrole: shardsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动第二套副本集：一主一副本一仲裁\n\n依次启动三个mongod服务：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123223\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123292\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123326\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n查看服务是否启动：\n\n[root@bobohost bin]# ps -ef |grep mongod\n\n\n1\n\n\n# 2.4.2.1 初始化副本集和创建主节点\n\n使用客户端命令连接任意一个节点，但这里尽量要连接主节点：\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27318\n\n\n1\n\n\n执行初始化副本集命令：\n\nrs.initiate()\n\n\n1\n\n\n查看副本集情况 (节选内容)：\n\nrs.status()\n\n\n1\n\n\n主节点配置查看：\n\nrs.conf()\n\n\n1\n\n\n# 2.4.2.2 添加副本节点\n\nrs.add("180.76.159.126:27418")\n\n\n1\n\n\n# 2.4.2.3 添加仲裁节点\n\nrs.addarb("180.76.159.126:27518")\n\n\n1\n\n\n查看副本集的配置情况\n\nmyshardrs02:primary> rs.status()\n{\n    "set" : "myshardrs02",\n    "date" : isodate("2019-07-31t21:38:22.463z"),\n    "mystate" : 1,\n    "term" : numberlong(1),\n    "syncingto" : "",\n    "syncsourcehost" : "",\n    "syncsourceid" : -1,\n    "heartbeatintervalmillis" : numberlong(2000),\n    "optimes" : {\n        "lastcommittedoptime" : {\n            "ts" : timestamp(1564609094, 1),\n            "t" : numberlong(1)\n        },\n        "readconcernmajorityoptime" : {\n            "ts" : timestamp(1564609094, 1),\n            "t" : numberlong(1)\n        },\n        "appliedoptime" : {\n            "ts" : timestamp(1564609094, 1),\n            "t" : numberlong(1)\n        },\n        "durableoptime" : {\n            "ts" : timestamp(1564609094, 1),\n            "t" : numberlong(1)\n        }\n    },\n    "laststablecheckpointtimestamp" : timestamp(1564609074, 1),\n    "members" : [\n        {\n            "_id" : 0,\n            "name" : "180.76.159.126:27318",\n            "health" : 1,\n            "state" : 1,\n            "statestr" : "primary",\n            "uptime" : 5086,\n            "optime" : {\n                "ts" : timestamp(1564609094, 1),\n                "t" : numberlong(1)\n            },\n            "optimedate" : isodate("2019-07-31t21:38:14z"),\n            "syncingto" : "",\n            "syncsourcehost" : "",\n            "syncsourceid" : -1,\n            "infomessage" : "",\n            "electiontime" : timestamp(1564604032, 2),\n            "electiondate" : isodate("2019-07-31t20:13:52z"),\n            "configversion" : 3,\n            "self" : true,\n            "lastheartbeatmessage" : ""\n        },\n        {\n            "_id" : 1,\n            "name" : "180.76.159.126:27418",\n            "health" : 1,\n            "state" : 2,\n            "statestr" : "secondary",\n            "uptime" : 4452,\n            "optime" : {\n                "ts" : timestamp(1564609094, 1),\n                "t" : numberlong(1)\n            },\n            "optimedurable" : {\n                "ts" : timestamp(1564609094, 1),\n                "t" : numberlong(1)\n            },\n            "optimedate" : isodate("2019-07-31t21:38:14z"),\n            "optimedurabledate" : isodate("2019-07-31t21:38:14z"),\n            "lastheartbeat" : isodate("2019-07-31t21:38:21.178z"),\n            "lastheartbeatrecv" : isodate("2019-07-31t21:38:20.483z"),\n            "pingms" : numberlong(0),\n            "lastheartbeatmessage" : "",\n            "syncingto" : "180.76.159.126:27518",\n            "syncsourcehost" : "180.76.159.126:27518",\n            "syncsourceid" : 2,\n            "infomessage" : "",\n            "configversion" : 3\n        },\n        {\n            "_id" : 2,\n            "name" : "180.76.159.126:27518",\n            "health" : 1,\n            "state" : 2,\n            "statestr" : "secondary",\n            "uptime" : 4448,\n            "optime" : {\n                "ts" : timestamp(1564609094, 1),\n                "t" : numberlong(1)\n            },\n            "optimedurable" : {\n                "ts" : timestamp(1564609094, 1),\n                "t" : numberlong(1)\n            },\n            "optimedate" : isodate("2019-07-31t21:38:14z"),\n            "optimedurabledate" : isodate("2019-07-31t21:38:14z"),\n            "lastheartbeat" : isodate("2019-07-31t21:38:21.178z"),\n            "lastheartbeatrecv" : isodate("2019-07-31t21:38:22.096z"),\n            "pingms" : numberlong(0),\n            "lastheartbeatmessage" : "",\n            "syncingto" : "180.76.159.126:27318",\n            "syncsourcehost" : "180.76.159.126:27318",\n            "syncsourceid" : 0,\n            "infomessage" : "",\n            "configversion" : 3\n        }\n    ],\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n\n\n\n# 2.5 配置节点副本集的创建\n\n第一步：准备存放数据和日志的目录：\n\n#-----------configrs\n#建立数据节点data和日志目录\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27019/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27019/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27119/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27119/data/db \\ &\n\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27219/log \\ &\nmkdir -p /mongodb/sharded_cluster/myconfigrs_27219/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n\n\n1\n\n\nmyconfigrs_27019 ：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myconfigrs_27019/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/sharded_cluster/myconfigrs_27019/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/sharded_cluster/myconfigrs_27019/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #绑定的端口\n    port: 27019\nreplication:\n    replsetname: myconfigrs\nsharding:\n    clusterrole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n\n\n1\n\n\nmyconfigrs_27119\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myconfigrs_27119/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/sharded_cluster/myconfigrs_27119/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/sharded_cluster/myconfigrs_27119/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #绑定的端口\n    port: 27119\nreplication:\n    replsetname: myconfigrs\nsharding:\n    clusterrole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n新建或修改配置文件：\n\nvim /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\n\n\n1\n\n\nmyconfigrs_27219\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/myconfigrs_27219/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/sharded_cluster/myconfigrs_27219/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/sharded_cluster/myconfigrs_27219/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #绑定的端口\n    port: 27219\nreplication:\n    replsetname: myconfigrs\nsharding:\n    clusterrole: configsvr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n启动配置副本集：一主两副本\n\n依次启动三个mongod服务：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123223\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123292\nchild process started successfully, parent exiting\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 123326\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n查看服务是否启动：\n\n[root@bobohost bin]# ps -ef |grep mongod\n\n\n1\n\n\n\n# 2.5.1 初始化副本集和创建主节点\n\n使用客户端命令连接任意一个节点，但这里尽量要连接主节点：\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27019\n\n\n1\n\n\n执行初始化副本集命令：\n\nrs.initiate()\n\n\n1\n\n\n查看副本集情况 (节选内容)：\n\nrs.status()\n\n\n1\n\n\n主节点配置查看：\n\nrs.conf()\n\n\n1\n\n\n\n# 2.5.2 添加两个副本节点\n\nmyshardrs01:primary> rs.add("180.76.159.126:27119")\nmyshardrs01:primary> rs.add("180.76.159.126:27219")\n\n\n1\n2\n\n\n查看副本集的配置情况：\n\nmyshardrs01:primary> rs.conf()\nmyshardrs01:primary> rs.status()\n\n\n1\n2\n\n\n\n# 2.6 路由节点的创建和操作\n\n\n# 2.6.1 第一个路由节点的创建和连接\n\n第一步：准备存放数据和日志的目录：\n\n#-----------mongos01\nmkdir -p /mongodb/sharded_cluster/mymongos_27017/log\n\n\n1\n2\n\n\nmymongos_27017节点：\n\n新建或修改配置文件：\n\nvi /mongodb/sharded_cluster/mymongos_27017/mongos.conf\n\n\n1\n\n\nmongos.conf：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/mymongos_27017/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: /mongodb/sharded_cluster/mymongos_27017/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #bindip\n    #绑定的端口\n    port: 27017\nsharding:\n    #指定配置节点副本集\n    configdb: myconfigrs/180.76.159.126:27019,180.76.159.126:27119,180.76.159.126:27219\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n启动mongos：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27017/mongos.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 129874\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n\n\n提示：启动如果失败，可以查看 log目录下的日志，查看失败原因。\n\n客户端登录mongos\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017\n\n\n1\n\n\n此时，写不进去数据，如果写数据会报错：\n\nmongos> use aadb\nswitched to db aadb\nmongos> db.aa.insert({aa:"aa"})\nwritecommanderror({\n    "ok" : 0,\n    "errmsg" : "unable to initialize targeter for write op for collection aa.aa :: caused by :: database aa not found :: caused by :: no shards found",\n    "code" : 70,\n    "codename" : "shardnotfound",\n    "operationtime" : timestamp(1564600123, 2),\n    "$clustertime" : {\n    "clustertime" : timestamp(1564600123, 2),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n原因：通过路由节点操作，现在只是连接了配置节点，还没有连接分片数据节点，因此无法写入业务数据。\n\n\n# 2.6.2 在路由节点上进行分片配置操作\n\n使用命令添加分片：\n\n# 2.6.2.1 添加分片：\n\n语法：\n\nsh.addshard("ip:port")\n\n\n1\n\n\n将第一套分片副本集添加进来：\n\nmongos>sh.addshard("myshardrs01/192.168.0.2:27018,180.76.159.126:27118,180.76.159.126:27218")\n{\n    "shardadded" : "myshardrs01",\n    "ok" : 1,\n    "operationtime" : timestamp(1564611970, 4),\n    "$clustertime" : {\n    "clustertime" : timestamp(1564611970, 4),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n查看分片状态情况：\n\nmongos> sh.status()\n--- sharding status ---\n    sharding version: {\n        "_id" : 1,\n        "mincompatibleversion" : 5,\n        "currentversion" : 6,\n        "clusterid" : objectid("5d4211b798f3f9a48522c68b")\n    }\n    shards:\n        {  "_id" : "myshardrs01",  "host" : "myshardrs01/180.76.159.126:27018,180.76.159.126:27118",  "state" : 1 }\n    active mongoses:\n        "4.0.10" : 1\n    autosplit:\n        currently enabled: yes\n    balancer:\n        currently enabled:  yes\n        currently running:  no\n        failed balancer rounds in last 5 attempts:  0\n        migration results for the last 24 hours:\n            no recent migrations\n    databases:\n        {  "_id" : "config",  "primary" : "config",  "partitioned" : true }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n继续将第二套分片副本集添加进来：\n\nmongos>sh.addshard("myshardrs02/192.168.0.2:27318,180.76.159.126:27418,180.76.159.126:27518")\n{\n    "shardadded" : "myshardrs02",\n    "ok" : 1,\n    "operationtime" : timestamp(1564612147, 5),\n    "$clustertime" : {\n    "clustertime" : timestamp(1564612147, 5),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n查看分片状态：\n\nmongos> sh.status()\n--- sharding status ---\n    sharding version: {\n        "_id" : 1,\n        "mincompatibleversion" : 5,\n        "currentversion" : 6,\n        "clusterid" : objectid("5d4211b798f3f9a48522c68b")\n    }\n    shards:\n        {  "_id" : "myshardrs01",  "host" : "myshardrs01/180.76.159.126:27018,180.76.159.126:27118",  "state" : 1 }\n        {  "_id" : "myshardrs02",  "host" : "myshardrs02/180.76.159.126:27318,180.76.159.126:27418",  "state" : 1 }\n    active mongoses:\n        "4.0.10" : 1\n    autosplit:\n        currently enabled: yes\n    balancer:\n        currently enabled:  yes\n        currently running:  no\n        failed balancer rounds in last 5 attempts:  0\n        migration results for the last 24 hours:\n            no recent migrations\n    databases:\n        {  "_id" : "config",  "primary" : "config",  "partitioned" : true }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n提示：如果添加分片失败，需要先手动移除分片，检查添加分片的信息的正确性后，再次添加分片。\n\n移除分片参考(了解)：\n\nuse admin\ndb.runcommand( { removeshard: "myshardrs02" } )\n\n\n1\n2\n\n\n注意：如果只剩下最后一个 shard，是无法删除的\n\n移除时会自动转移分片数据，需要一个时间过程。\n\n完成后，再次执行删除分片命令才能真正删除。\n\n# 2.6.2.2 开启分片功能\n\nsh.enablesharding(“库名”)、sh.shardcollection(“库名.集合名”,{“key”:1})\n\n在mongos上的articledb数据库配置sharding:\n\nmongos> sh.enablesharding("articledb")\n{\n    "ok" : 1,\n    "operationtime" : timestamp(1564612296, 5),\n    "$clustertime" : {\n        "clustertime" : timestamp(1564612296, 5),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查看分片状态：\n\nmongos> sh.status()\n--- sharding status ---\n    sharding version: {\n        "_id" : 1,\n        "mincompatibleversion" : 5,\n        "currentversion" : 6,\n        "clusterid" : objectid("5d4211b798f3f9a48522c68b")\n    }\n    shards:\n        {  "_id" : "myshardrs01",  "host" : "myshardrs01/180.76.159.126:27018,180.76.159.126:27118",  "state" : 1 }\n        {  "_id" : "myshardrs02",  "host" : "myshardrs02/180.76.159.126:27318,180.76.159.126:27418",  "state" : 1 }\n    active mongoses:\n        "4.0.10" : 1\n    autosplit:\n        currently enabled: yes\n    balancer:\n        currently enabled:  yes\n        currently running:  no\n        failed balancer rounds in last 5 attempts:  0\n        migration results for the last 24 hours:\n            no recent migrations\n    databases:\n        {  "_id" : "articledb",  "primary" : "myshardrs02",  "partitioned" : true, "version" : {  "uuid" : uuid("788c9a3b-bb6a-4cc2-a597-974694772986"), "lastmod" : 1 } }\n        {  "_id" : "config",  "primary" : "config",  "partitioned" : true }\n            config.system.sessions\n                shard key: { "_id" : 1 }\n                unique: false\n                balancing: true\n                chunks:\n                    myshardrs01   1\n                { "_id" : { "$minkey" : 1 } } --\x3e> { "_id" : { "$maxkey" : 1 } } on : myshardrs01 timestamp(1, 0)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n# 2.6.2.3 集合分片\n\n对集合分片，你必须使用 sh.shardcollection() 方法指定集合和分片键\n\n语法：\n\nsh.shardcollection(namespace, key, unique)\n\n\n1\n\n\n参数：\n\nparameter   type       description\nnamespace   string     要（分片）共享的目标集合的命名空间，格式： .\nkey         document   用作分片键的索引规范文档。shard键决定mongodb如何在shard之间分发文档。除非集合为空，否则索引必须在shardcollection命令之前存在。如果集合为空，则mongodb在对集合进行分片之前创建索引，前提是支持分片键的索引不存在。简单的说：由包含字段和该字段的索引遍历方向的文档组成。\nunique      boolean    当值为true情况下，片键字段上会限制为确保是唯一索引。哈希策略片键不支持唯一索引。默认是false。\n\n对集合进行分片时,你需要选择一个 片键（shard key） , shard key 是每条记录都必须包含的,且建立了索引的单个字段或复合字段,mongodb按照片键将数据划分到不同的 数据块 中,并将 数据块 均衡地分布到所有分片中.为了按照片键划分数据块,mongodb使用 基于哈希的分片方式（随机平均分配）或者基于范围的分片方式（数值大小分配） 。\n\n用什么字段当片键都可以，如：nickname作为片键，但一定是必填字段。\n\n分片规则一：哈希策略\n\n对于 基于哈希的分片 ,mongodb计算一个字段的哈希值,并用这个哈希值来创建数据块.\n\n在使用基于哈希分片的系统中,拥有”相近”片键的文档 很可能不会 存储在同一个数据块中,因此数据的分离性更好一些.\n\n使用nickname作为片键，根据其值的哈希值进行数据分片\n\nmongos> sh.shardcollection("articledb.comment",{"nickname":"hashed"})\n{\n    "collectionsharded" : "articledb.comment",\n    "collectionuuid" : uuid("ddea6ed8-ee61-4693-bd16-196acc3a45e8"),\n    "ok" : 1,\n    "operationtime" : timestamp(1564612840, 28),\n    "$clustertime" : {\n        "clustertime" : timestamp(1564612840, 28),\n        "signature" : {\n            "hash" : bindata(0,"aaaaaaaaaaaaaaaaaaaaaaaaaaa="),\n            "keyid" : numberlong(0)\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n查看分片状态：sh.status()\n\n databases:\n {  "_id" : "articledb",  "primary" : "myshardrs02",  "partitioned" : true,  "version" : {  "uuid" : uuid("251436b7-86c2-4cd8-9a88-70874af29364"), "lastmod" : 1 } }\n     articledb.comment\n         shard key: { "nickname" : "hashed" }\n         unique: false\n         balancing: true\n         chunks:\n             myshardrs01   2\n             myshardrs02   2\n         { "nickname" : { "$minkey" : 1 } } --\x3e> { "nickname" : numberlong("-4611686018427387902") } on : myshardrs01 timestamp(1, 0)\n         { "nickname" : numberlong("-4611686018427387902") } --\x3e> { "nickname" : numberlong(0) } on : myshardrs01 timestamp(1, 1)\n         { "nickname" : numberlong(0) } --\x3e> { "nickname" : numberlong("4611686018427387902") } on : myshardrs02 timestamp(1, 2)\n         { "nickname" : numberlong("4611686018427387902") } --\x3e> { "nickname" : { "$maxkey" : 1 } } on : myshardrs02 timestamp(1, 3)\n\n{  "_id" : "config",  "primary" : "config",  "partitioned" : true }\n    config.system.sessions\n        shard key: { "_id" : 1 }\n        unique: false\n        balancing: true\n        chunks:\n            myshardrs01   1\n        { "_id" : { "$minkey" : 1 } } --\x3e> { "_id" : { "$maxkey" : 1 } } on : myshardrs01 timestamp(1, 0)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n分片规则二：范围策略\n\n对于 基于范围的分片 ,mongodb按照片键的范围把数据分成不同部分.假设有一个数字的片键:想象一个从负无穷到正无穷的直线,每一个片键的值都在直线上画了一个点.mongodb把这条直线划分为更短的不重叠的片段,并称之为 数据块 ,每个数据块包含了片键在一定范围内的数据.\n\n在使用片键做范围划分的系统中,拥有”相近”片键的文档很可能存储在同一个数据块中,因此也会存储在同一个分片中.\n\n如使用作者年龄字段作为片键，按照点赞数的值进行分片：\n\nmongos> sh.shardcollection("articledb.author",{"age":1})\n{\n    "collectionsharded" : "articledb.author",\n    "collectionuuid" : uuid("9a47bdaa-213a-4039-9c18-e70bfc369df7"),\n    "ok" : 1,\n    "operationtime" : timestamp(1567512803, 13),\n    "$clustertime" : {\n        "clustertime" : timestamp(1567512803, 13),\n        "signature" : {\n            "hash" : bindata(0,"ee9qt5ye5sl1tyr7+3u8gry5+5q="),\n            "keyid" : numberlong("6732061237309341726")\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n注意的是：\n\n * 一个集合只能指定一个片键，否则报错。\n * 一旦对一个集合分片，分片键和分片值就不可改变。 如：不能给集合选择不同的分片键、不能更新分片键的值。\n * 根据age索引进行分配数据。\n\n查看分片状态：\n\narticledb.author\n    shard key: { "age" : 1 }\n    unique: false\n    balancing: true\n    chunks:\n        myshardrs01 1\n    { "age" : { "$minkey" : 1 } } --\x3e> { "age" : { "$maxkey" : 1 } } on : myshardrs01 timestamp(1, 0)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n基于范围的分片方式与基于哈希的分片方式性能对比：\n\n基于范围的分片方式提供了更高效的范围查询,给定一个片键的范围,分发路由可以很简单地确定哪个数据块存储了请求需要的数据,并将请求转发到相应的分片中.\n\n不过,基于范围的分片会导致数据在不同分片上的不均衡,有时候,带来的消极作用会大于查询性能的积极作用.比如,如果片键所在的字段是线性增长的,一定时间内的所有请求都会落到某个固定的数据块中,最终导致分布在同一个分片中.在这种情况下,一小部分分片承载了集群大部分的数据,系统并不能很好地进行扩展.\n\n与此相比,基于哈希的分片方式以范围查询性能的损失为代价,保证了集群中数据的均衡.哈希值的随机性使数据随机分布在每个数据块中,因此也随机分布在不同分片中.但是也正由于随机性,一个范围查询很难确定应该请求哪些分片,通常为了返回需要的结果,需要请求所有分片.\n\n如无特殊情况，一般推荐使用 hash sharding。\n\n而使用 _id 作为片键是一个不错的选择，因为它是必有的，你可以使用数据文档 _id 的哈希作为片键。\n\n这个方案能够是的读和写都能够平均分布，并且它能够保证每个文档都有不同的片键所以数据块能够很精细。\n\n似乎还是不够完美，因为这样的话对多个文档的查询必将命中所有的分片。虽说如此，这也是一种比较好的方案了。\n\n理想化的 shard key 可以让 documents 均匀地在集群中分布：\n\n\n\n显示集群的详细信息：\n\nmongos> db.printshardingstatus()\n\n\n1\n\n\n查看均衡器是否工作（需要重新均衡时系统才会自动启动，不用管它）：\n\nmongos> sh.isbalancerrunning()\nfalse\n\n\n1\n2\n\n\n查看当前 balancer状态：\n\nmongos> sh.getbalancerstate()\ntrue\n\n\n1\n2\n\n\n\n# 2.6.3 分片后插入数据测试\n\n测试一（哈希规则）：登录mongs后，向comment循环插入1000条数据做测试：\n\nmongos> use articledb\nswitched to db articledb\nmongos> for(var i=1;i<=1000;i++){db.comment.insert({_id:i+"",nickname:"bobo"+i})}\nwriteresult({ "ninserted" : 1 })\nmongos> db.comment.count()\n1000\n\n\n1\n2\n3\n4\n5\n6\n\n\n提示： js的语法，因为mongo的shell是一个javascript的shell。\n\n注意：从路由上插入的数据，必须包含片键，否则无法插入。\n\n分别登陆两个片的主节点，统计文档数量\n\n第一个分片副本集：\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27018\n\nmyshardrs01:primary> use articledb\nswitched to db articledb\nmyshardrs01:primary> db.comment.count()\n507\n\n\n1\n2\n3\n4\n5\n6\n\n\n第二个分片副本集：\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27318\n\nmyshardrs02:primary> use articledb\nswitched to db articledb\nmyshardrs02:primary> db.comment.count()\n493\n\n\n1\n2\n3\n4\n5\n6\n\n\n可以看到， 1000条数据近似均匀的分布到了2个shard上。是根据片键的哈希值分配的。\n\n这种分配方式非常易于水平扩展：一旦数据存储需要更大空间，可以直接再增加分片即可，同时提升了性能。\n\n使用db.comment.stats()查看单个集合的完整情况，mongos执行该命令可以查看该集合的数据分片的情况。\n\n使用sh.status()查看本库内所有集合的分片信息。\n\n测试二（范围规则）：登录mongs后，向comment循环插入1000条数据做测试：\n\nmongos> use articledb\nswitched to db articledb\nmongos> for(var i=1;i<=20000;i++){db.author.save({"name":"bobobobobobobobobobobobobobobobobobobobobobobobobobobobobobobobobobobobo"+i,"age":numberint(i%120)})}\nwriteresult({ "ninserted" : 1 })\nmongos> db.comment.count()\n20000\n\n\n1\n2\n3\n4\n5\n6\n\n\n插入成功后，仍然要分别查看两个分片副本集的数据情况。\n\n分片效果：\n\narticledb.author\n    shard key: { "age" : 1 }\n    unique: false\n    balancing: true\n    chunks:\n        myshardrs01 2\n        myshardrs02 1\n    { "age" : { "$minkey" : 1 } } --\x3e> { "age" : 0 } on : myshardrs02 timestamp(2, 0)\n    { "age" : 0 } --\x3e> { "age" : 112 } on : myshardrs01 timestamp(2, 1)\n    { "age" : 112 } --\x3e> { "age" : { "$maxkey" : 1 } } on : myshardrs01 timestamp(1, 3)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n提示：\n\n如果查看状态发现没有分片，则可能是由于以下原因造成了：\n\n * 系统繁忙，正在分片中。\n\n * 数据块（chunk）没有填满，默认的数据块尺寸（chunksize）是64m，填满后才会考虑向其他片的数据块填充数据，因此，为了测试，可以将其改小，这里改为1m，操作如下：\n   \n   use config\n   db.settings.save( { _id:"chunksize", value: 1 } )\n   \n   \n   1\n   2\n   \n   \n   测试完改回来：\n   \n   db.settings.save( { _id:"chunksize", value: 64 } )\n   \n   \n   1\n   \n\n注意：要先改小，再设置分片。为了测试，可以先删除集合，重新建立集合的分片策略，再插入数据测试即可。\n\n\n# 2.6.4 再增加一个路由节点\n\n文件夹：\n\n#-----------mongos02\nmkdir -p /mongodb/sharded_cluster/mymongos_27117/log\n\n\n1\n2\n\n\n新建或修改配置文件：\n\nvi /mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\n\n1\n\n\nmongos.conf：\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/sharded_cluster/mymongos_27117/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: /mongodb/sharded_cluster/mymongos_27117/log/mongod.pid"\nnet:\n    #服务实例绑定所有ip，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip\n    #bindipall: true\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #bindip\n    #绑定的端口\n    port: 27117\nsharding:\n    configdb: myconfigrs/180.76.159.126:27019,180.76.159.126:27119,180.76.159.126:27219\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n启动mongos2：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27117/mongos.conf\nabout to fork child process, waiting until server is ready for connections.\nforked process: 129874\nchild process started successfully, parent exiting\n\n\n1\n2\n3\n4\n\n\n使用mongo客户端登录27117，发现，第二个路由无需配置，因为分片配置都保存到了配置服务器中了。\n\n\n# 2.7 compass 连接分片集群\n\ncompass连接：\n\n\n\n提示：和连接单机 mongod一样。\n\n连接成功后，上方有mongos和分片集群的提示：\n\n\n\n\n# 2.8 springdatamongdb 连接分片集群\n\njava客户端常用的是springdatamongodb，其连接的是mongs路由，配置和单机mongod的配置是一样的。\n\n多个路由的时候的springdatamongodb的客户端配置参考如下：\n\nspring:\n    #数据源配置\n    data:\n        mongodb:\n            # 主机地址\n            #   host: 180.76.159.126\n            # 数据库\n            #   database: articledb\n            # 默认端口是27017\n            #   port: 27017\n            #也可以使用uri连接\n            #   uri: mongodb://192.168.40.134:28017/articledb\n            # 连接副本集字符串\n            #   uri: mongodb://180.76.159.126:27017,180.76.159.126:27018,180.76.159.126:27019/articledb?connect=replicaset&slaveok=true&replicaset=myrs\n            #连接路由字符串\n            uri: mongodb://180.76.159.126:27017,180.76.159.126:27117/articledb\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n通过日志发现，写入数据的时候，会选择一个路由写入\n\n\n# 2.9 清除所有的节点数据（备用）\n\n如果在搭建分片的时候有操作失败或配置有问题，需要重新来过的，可以进行如下操作：\n\n第一步：查询出所有的测试服务节点的进程：\n\n[root@bobohost sharded_cluster]# ps -ef |grep mongo\nroot    10184    1  0 06:04 ?     00:01:25 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\nroot    10219    1  0 06:04 ?     00:01:25 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\nroot    10253    1  0 06:04 ?     00:00:46 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\nroot    10312    1  0 06:04 ?     00:01:23 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\nroot    10346    1  0 06:05 ?     00:01:23 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\nroot    10380    1  0 06:05 ?     00:00:44 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\nroot    10414    1  1 06:05 ?     00:01:36 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\nroot    10453    1  1 06:05 ?     00:01:37 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\nroot    10492    1  1 06:05 ?     00:01:38 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\nroot    11392    1  0 06:15 ?     00:00:24 /usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27017/mongos.conf\nroot    14829    1  0 07:15 ?     00:00:13 /usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n根据上述的进程编号，依次中断进程：\n\nkill -2 进程编号\n\n\n1\n\n\n第二步：清除所有的节点的数据：\n\nrm -rf /mongodb/sharded_cluster/myconfigrs_27019/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myconfigrs_27119/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myconfigrs_27219/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs01_27018/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs01_27118/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs01_27218/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs02_27318/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs02_27418/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/myshardrs02_27518/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/mymongos_27017/data/db/*.* \\ &\nrm -rf /mongodb/sharded_cluster/mymongos_27117/data/db/*.*\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n第三步：查看或修改有问题的配置\n\n第四步：依次启动所有节点，不包括路由节点：\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n第五步：对两个数据分片副本集和一个配置副本集进行初始化和相关配置\n\n第六步：检查路由mongos的配置，并启动mongos\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/mymongos_27017/mongos.cfg\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/mymongos_27017/mongos.cfg\n\n\n1\n2\n\n\n第七步：mongo登录mongos，在其上进行相关操作。\n\n\n# 3. 安全认证\n\n\n# 3.1 mongodb 的用户和角色权限简介\n\n默认情况下，mongodb实例启动运行时是没有启用用户访问权限控制的，也就是说，在实例本机服务器上都可以随意连接到实例进行各种操作，mongodb不会对连接客户端进行用户验证，这是非常危险的。\n\nmongodb官网上说，为了能保障mongodb的安全可以做以下几个步骤：\n\n * 使用新的端口，默认的27017端口如果一旦知道了ip就能连接上，不太安全。\n * 设置mongodb的网络环境，最好将mongodb部署到公司服务器内网，这样外网是访问不到的。公司内部访问使用vpn等。\n * 开启安全认证。认证要同时设置服务器之间的内部认证方式，同时要设置客户端连接到集群的账号密码认证方式。\n\n为了强制开启用户访问控制(用户验证)，则需要在mongodb实例启动时使用选项 – auth 或在指定启动配置文件中添加选项 auth=true 。\n\n在开始之前需要了解一下概念\n\n1）启用访问控制：\n\nmongodb使用的是基于角色的访问控制(role-based access control,rbac)来管理用户对实例的访问。通过对用户授予一个或多个角色来控制用户访问数据库资源的权限和数据库操作的权限，在对用户分配角色之前，用户无法访问实例。\n\n在实例启动时添加选项 – auth 或指定启动配置文件中添加选项 auth=true 。\n\n2）角色：\n\n在mongodb中通过角色对用户授予相应数据库资源的操作权限，每个角色当中的权限可以显式指定，也可以通过继承其他角色的权限，或者两都都存在的权限。\n\n3）权限：\n\n权限由指定的数据库资源(resource)以及允许在指定资源上进行的操作(action)组成。\n\n * 资源(resource)包括：数据库、集合、部分集合和集群；\n * 操作(action)包括：对资源进行的增、删、改、查(crud)操作。\n\n在角色定义时可以包含一个或多个已存在的角色，新创建的角色会继承包含的角色所有的权限。在同一个数据库中，新创建角色可以继承其他角色的权限，在 admin 数据库中创建的角色可以继承在其它任意数据库中角色的权限。\n\n关于角色权限的查看，可以通过如下命令查询（了解）：\n\n// 查询所有角色权限(仅用户自定义角色)\n> db.runcommand({ rolesinfo: 1 })\n\n// 查询所有角色权限(包含内置角色)\n> db.runcommand({ rolesinfo: 1, showbuiltinroles: true })\n\n// 查询当前数据库中的某角色的权限\n> db.runcommand({ rolesinfo: "<rolename>" })\n// 查询其它数据库中指定的角色权限\n> db.runcommand({ rolesinfo: { role: "<rolename>", db: "<database>" } }\n// 查询多个角色权限\n> db.runcommand(\n    {\n        rolesinfo: [\n            "<rolename>",\n            { role: "<rolename>", db: "<database>" },\n            ...\n        ]  \n    }\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n示例：\n\n查看所有内置角色：\n\n> db.runcommand({ rolesinfo: 1, showbuiltinroles: true })\n{\n    "roles" : [\n        {\n            "role" : "__queryablebackup",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "__system",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "backup",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "clusteradmin",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "clustermanager",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "clustermonitor",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "dbadmin",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "dbadminanydatabase",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "dbowner",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "enablesharding",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "hostmanager",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "read",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "readanydatabase",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "readwrite",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "readwriteanydatabase",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "restore",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "root",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "useradmin",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        },\n        {\n            "role" : "useradminanydatabase",\n            "db" : "admin",\n            "isbuiltin" : true,\n            "roles" : [ ],\n            "inheritedroles" : [ ]\n        }\n    ],\n    "ok" : 1\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n\n\n常用的内置角色：\n\n * 数据库用户角色： read、readwrite;\n * 所有数据库用户角色： readanydatabase、readwriteanydatabase、useradminanydatabase、dbadminanydatabase\n * 数据库管理角色： dbadmin、dbowner、useradmin；\n * 集群管理角色： clusteradmin、clustermanager、clustermonitor、hostmanager；\n * 备份恢复角色： backup、restore；\n * 超级用户角色： root\n * 内部角色： system\n\n角色说明：\n\n角色                     权限描述\nread                   可以读取指定数据库中任何数据\nreadwrite              可以读写指定数据库中任何数据，包括创建、重命名、删除集合\nreadanydatabase        可以读取所有数据库中任何数据(除了数据库config和local之外)\nreadwriteanydatabase   可以读写所有数据库中任何数据(除了数据库config和local之外)\nuseradminanydatabase   可以在指定数据库创建和修改用户(除了数据库config和local之外)\ndbadminanydatabase     可以读取任何数据库以及对数据库进行清理、修改、压缩、获取统计信息、执行检查等操作(除了数据库config和local之外)\ndbadmin                可以读取指定数据库以及对数据库进行清理、修改、压缩、获取统计信息、执行检查等操作\nuseradmin              可以在指定数据库创建和修改用户\nclusteradmin           可以对整个集群或数据库系统进行管理操作\nbackup                 备份mongodb数据最小的权限\nrestore                从备份文件中还原恢复mongodb数据(除了system.profile集合)的权限\nroot                   超级账号，超级权限\n\n\n# 3.2 单实例环境\n\n目标：对单实例的mongodb服务开启安全认证，这里的单实例指的是未开启副本集或分片的mongodb实例。\n\n\n# 3.2.1 关闭已开启的服务\n\n增加mongod的单实例的安全认证功能，可以在服务搭建的时候直接添加，也可以在之前搭建好的服务上添加。\n\n本文使用之前搭建好的服务，因此，先停止之前的服务\n\n停止服务的方式有两种：快速关闭和标准关闭，下面依次说明：\n\n（1）快速关闭方法（快速，简单，数据可能会出错）\n\n目标：通过系统的kill命令直接杀死进程：\n\n杀完要检查一下，避免有的没有杀掉。\n\n#通过进程编号关闭节点\nkill -2 54410\n\n\n1\n2\n\n\n【补充】\n\n如果一旦是因为数据损坏，则需要进行如下操作（了解）：\n\n1）删除lock文件：\n\nrm -f /mongodb/single/data/db/*.lock\n\n\n1\n\n\n2 ）修复数据：\n\n/usr/local/mongodb/bin/mongod --repair --dbpath=/mongodb/single/data/db\n\n\n1\n\n\n（2）标准的关闭方法（数据不容易出错，但麻烦）：\n\n目标：通过mongo客户端中的shutdownserver命令来关闭服务\n\n主要的操作步骤参考如下：\n\n//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\nmongo --port 27017\n//#切换到admin库\nuse admin\n//关闭服务\ndb.shutdownserver()\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3.2.2 添加用户和权限\n\n（1）先按照普通无授权认证的配置，来配置服务端的配置文件 /mongodb/single/mongod.conf\n\nsystemlog:\n    #mongodb发送所有日志输出的目标指定为文件\n    destination: file\n    #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径\n    path: "/mongodb/single/log/mongod.log"\n    #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\n    logappend: true\nstorage:\n    #mongod实例存储其数据的目录。storage.dbpath设置仅适用于mongod。\n    dbpath: "/mongodb/single/data/db"\n    journal:\n        #启用或禁用持久性日志以确保数据文件保持有效和可恢复。\n        enabled: true\nprocessmanagement:\n    #启用在后台运行mongos或mongod进程的守护进程模式。\n    fork: true\n    #指定用于保存mongos或mongod进程的进程id的文件位置，其中mongos或mongod将写入其pid\n    pidfilepath: "/mongodb/single/log/mongod.pid"\nnet:\n    #服务实例绑定的ip\n    bindip: localhost,192.168.0.2\n    #绑定的端口\n    port: 27017\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n（2）按之前未开启认证的方式（不添加 – auth 参数）来启动mongodb服务\n\n/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf\n\n\n1\n\n\n提示：\n\n在操作用户时，启动mongod服务时尽量不要开启授权。\n\n（3）使用mongo客户端登录\n\n/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017\n\n\n1\n\n\n（4）创建两个管理员用户，一个是系统的超级管理员 myroot ，一个是admin库的管理用户myadmin\n\n//切换到admin库\n> use admin\n\n//创建系统超级用户 myroot,设置密码123456，设置角色root\n//> db.createuser({user:"myroot",pwd:"123456",roles:[ { "role" : "root", "db" : "admin" } ]})\n//或\n> db.createuser({user:"myroot",pwd:"123456",roles:["root"]})\n\n//创建专门用来管理admin库的账号myadmin，只用来作为用户权限的管理\n> db.createuser({user:"myadmin",pwd:"123456",roles: [{role:"useradminanydatabase",db:"admin"}]})\n\n//查看已经创建了的用户的情况：\n> db.system.users.find()\n\n//删除用户\n> db.dropuser("myadmin")\ntrue\n\n//修改密码\n> db.changeuserpassword("myroot", "123456")\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n提示：\n\n * 本案例创建了两个用户，分别对应超管和专门用来管理用户的角色，事实上，你只需要一个用户即可。如果你对安全要求很高，防止超管泄漏，则不要创建超管用户。\n * 和其它数据库（mysql）一样，权限的管理都差不多一样，也是将用户和权限信息保存到数据库对应的表中。mongodb存储所有的用户信息在admin 数据库的集合system.users中，保存用户名、密码和数据库信息。\n * 如果不指定数据库，则创建的指定的权限的用户在所有的数据库上有效，如 {role: “useradminanydatabase”, db:””}\n\n（5）认证测试\n\n测试添加的用户是否正确\n\n//切换到admin\n> use admin\n\n//密码输错\n> db.auth("myroot","12345")\nerror: authentication failed.\n0\n\n//密码正确\n> db.auth("myroot","123456")\n1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n（6）创建普通用户\n\n创建普通用户可以在没有开启认证的时候添加，也可以在开启认证之后添加，但开启认证之后，必须使用有操作admin库的用户登录认证后才能操作。底层都是将用户信息保存在了admin数据库的集合system.users中\n\n//创建(切换)将来要操作的数据库articledb,\n> use articledb\n\n//创建用户，拥有articledb数据库的读写权限readwrite，密码是123456\n> db.createuser({user: "bobo", pwd: "123456", roles: [{ role: "readwrite", db: "articledb" }]})\n//> db.createuser({user: "bobo", pwd: "123456", roles: ["readwrite"]})\n\n//测试是否可用\n> db.auth("bobo","123456")\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n提示：\n\n如果开启了认证后，登录的客户端的用户必须使用admin库的角色，如拥有root角色的myadmin用户，再通过myadmin用户去创建其他角色的用户\n\n\n# 3.2.3 服务端开启认证和客户端连接登录\n\n# 3.2.3.1 关闭已经启动的服务\n\n1）使用linux命令杀死进程：\n\n[root@bobohost single]# ps -ef |grep mongo\nroot    23482    1  0 08:08 ?     00:00:55 /usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf\n[root@bobohost single]# kill -2 23482\n\n\n1\n2\n3\n\n\n2 ）在mongo客户端中使用shutdownserver命令来关闭\n\n> db.shutdownserver()\nshutdown command only works with the admin database; try \'use admin\'\n> use admin\nswitched to db admin\n> db.shutdownserver()\n2019-08-14t11:20:16.450+0800 e query  [js] error: shutdownserver failed: {\n    "ok" : 0,\n    "errmsg" : "shutdown must run from localhost when running db without auth",\n    "code" : 13,\n    "codename" : "unauthorized"\n    } :\n_geterrorwithcode@src/mongo/shell/utils.js:25:13\ndb.prototype.shutdownserver@src/mongo/shell/db.js:453:1\n@(shell):1:1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n需要几个条件：\n\n * 必须是在 admin库下执行该关闭服务命令。\n * 如果没有开启认证，必须是从 localhost登陆的，才能执行关闭服务命令。\n * 非 localhost的、通过远程登录的，必须有登录且必须登录用户有对admin操作权限才可以。\n\n# 3.2.3.2 以开启认证的方式启动服务\n\n有两种方式开启权限认证启动服务：一种是参数方式，一种是配置文件方式\n\n1）参数方式\n\n在启动时指定参数 – auth ，如：\n\n/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf --auth\n\n\n1\n\n\n2）配置文件方式\n\n在mongod.conf配置文件中加入：vim /mongodb/single/mongod.conf\n\nsecurity:\n    #开启授权认证\n    authorization: enabled\n\n\n1\n2\n3\n\n\n启动时可不加 – auth 参数：\n\n/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf\n\n\n1\n\n\n# 3.2.3.3 开启了认证的情况下的客户端登录\n\n有两种认证方式，一种是先登录，在mongo shell中认证；一种是登录时直接认证\n\n1）先连接再认证\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017\nmongodb shell version v4.0.10\nconnecting to: mongodb://180.76.159.126:27017/?gssapiservicename=mongodb\nimplicit session: session { "id" : uuid("53fef661-35d6-4d29-b07c-020291d62e1a")}\nmongodb server version: 4.0.10\n>\n\n\n1\n2\n3\n4\n5\n6\n\n\n提示：\n\n开启认证后再登录，发现打印的日志比较少了。\n\n相关操作需要认证才可以：\n\n查询admin库中的system.users集合的用户：\n\n> use admin\nswitched to db admin\n> db.system.users.find()\nerror: error: {\n    "ok" : 0,\n    "errmsg" : "command find requires authentication",\n    "code" : 13,\n    "codename" : "unauthorized"\n}\n> db.auth("myroot","123456")\n1\n> db.system.users.find()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查询articledb库中的comment集合的内容：\n\n> use articledb\nswitched to db articledb\n> db.comment.find()\nerror: error: {\n    "ok" : 0,\n    "errmsg" : "not authorized on articledb to execute command { find: \\"comment\\", filter: {}, lsid: { id: uuid(\\"53fef661-35d6-4d29-b07c-020291d62e1a\\") }, $db: \\"articledb\\" }",\n    "code" : 13,\n    "codename" : "unauthorized"\n}\n> db.auth("bobo","123456")\n1\n> db.comment.find()\nerror: error: {\n    "ok" : 0,\n    "errmsg" : "too many users are authenticated",\n    "code" : 13,\n    "codename" : "unauthorized"\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n提示：\n\n这里可能出现错误，说是太多的用户正在认证了。因为我们确实连续登录了两个用户了。\n\n解决方案：退出shell，重新进来登录认证\n\n> exit\nbye\n[root@bobohost bin]# ./mongo --host 180.76.159.126 --port 27017\nmongodb shell version v4.0.10\nconnecting to: mongodb://180.76.159.126:27017/?gssapiservicename=mongodb\nimplicit session: session { "id" : uuid("329c1897-566d-4231-bcb3-b2acda301863")\n}\nmongodb server version: 4.0.10\n> db.auth("bobo","123456")\nerror: authentication failed.\n0\n> use articledb\nswitched to db articledb\n> db.auth("bobo","123456")\n1\n> db.comment.find()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n2）连接时直接认证\n\n对admin数据库进行登录认证和相关操作：\n\n[root@bobohost ~]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017 --authenticationdatabase admin -u myroot -p 123456\nmongodb shell version v4.0.10\nconnecting to: mongodb://180.76.159.126:27017/?\nauthsource=admin&gssapiservicename=mongodb\nimplicit session: session { "id" : uuid("f959b8d6-6994-44bc-9d35-09fc7cd00ba6")\n}\nmongodb server version: 4.0.10\nserver has startup warnings:\n2019-09-10t15:23:40.102+0800 i control [initandlisten] ** warning: you are\nrunning this process as the root user, which is not recommended.\n2019-09-10t15:23:40.102+0800 i control [initandlisten]\n2019-09-10t15:23:40.102+0800 i control [initandlisten]\n2019-09-10t15:23:40.102+0800 i control [initandlisten] ** warning:\n/sys/kernel/mm/transparent_hugepage/enabled is \'always\'.\n2019-09-10t15:23:40.102+0800 i control [initandlisten] **    we suggest\nsetting it to \'never\'\n2019-09-10t15:23:40.102+0800 i control [initandlisten]\n2019-09-10t15:23:40.102+0800 i control [initandlisten] ** warning:\n/sys/kernel/mm/transparent_hugepage/defrag is \'always\'.\n2019-09-10t15:23:40.102+0800 i control [initandlisten] **    we suggest\nsetting it to \'never\'\n2019-09-10t15:23:40.102+0800 i control [initandlisten]\n> show dbs;\nadmin    0.000gb\narticledb  0.000gb\nconfig   0.000gb\nlocal    0.000gb\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n对articledb数据库进行登录认证和相关操作：\n\n[root@bobohost bin]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017 --authenticationdatabase articledb -u bobo -p 123456\nmongodb shell version v4.0.10\nconnecting to: mongodb://180.76.159.126:27017/?\nauthsource=articledb&gssapiservicename=mongodb\nimplicit session: session { "id" : uuid("e5d4148f-373b-45b8-9cff-a927ce617100")\n}\nmongodb server version: 4.0.10\n> use articledb\nswitched to db articledb\n> db.comment.find()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n提示：\n\n * -u ：用户名\n * -p ：密码\n * -- authenticationdatabase ：指定连接到哪个库。当登录是指定用户名密码时，必须指定对应的数据库！\n\n\n# 3.2.4 springdatamongodb连接认证\n\n使用用户名和密码连接到 mongodb 服务器，你必须使用\'username:password@hostname/dbname\' 格式，’username’为用户名，’password’ 为密码。\n\n目标：使用用户bobo使用密码 123456 连接到mongodb 服务上。\n\napplication.yml：\n\nspring:\n    #数据源配置\n    data:\n        mongodb:\n            # 主机地址\n            #   host: 180.76.159.126\n            # 数据库\n            #   database: articledb\n            # 默认端口是27017\n            #   port: 27017\n            #帐号\n            #   username: bobo\n            #密码\n            #   password: 123456\n            #单机有认证的情况下，也使用字符串连接\n            uri: mongodb://bobo:123456@180.76.159.126:27017/articledb\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 3.3 副本集环境\n\n\n# 3.3.1 前言\n\n对于搭建好的mongodb副本集，为了安全，启动安全认证，使用账号密码登录。\n\n副本集环境使用之前搭建好的，架构如下：\n\n\n\n对副本集执行访问控制需要配置两个方面 :\n\n * 副本集和共享集群的各个节点成员之间使用内部身份验证，可以使用密钥文件或x.509证书。密钥文件比较简单，本文使用密钥文件，官方推荐如果是测试环境可以使用密钥文件，但是正式环境，官方推荐x.509证书。原理就是，集群中每一个实例彼此连接的时候都检验彼此使用的证书的内容是否相同。只有证书相同的实例彼此才可以访问\n * 使用客户端连接到mongodb集群时，开启访问授权。对于集群外部的访问。如通过可视化客户端，或者通过代码连接的时候，需要开启授权。\n\n在keyfile身份验证中，副本集中的每个mongod实例都使用keyfile的内容作为共享密码，只有具有正确密钥文件的mongod或者mongos实例可以连接到副本集。密钥文件的内容必须在6到1024个字符之间，并且在unix/linux系统中文件所有者必须有对文件至少有读的权限。\n\n\n# 3.3.2 关闭已开启的副本集服务\n\n增加副本集的安全认证和服务鉴权功能，可以在副本集搭建的时候直接添加，也可以在之前搭建好的副本集服务上添加。\n\n本文使用之前搭建好的副本集服务，因此，先停止之前的集群服务\n\n停止服务的方式有两种：快速关闭和标准关闭，下面依次说明：\n\n（1）快速关闭方法（快速，简单，数据可能会出错）\n\n目标：通过系统的kill命令直接杀死进程：\n\n依次杀死仲裁者、副本节点、主节点，直到所有成员都离线。建议主节点最后kill，以避免潜在的回滚。杀完要检查一下，避免有的没有杀掉。\n\n#通过进程编号关闭节点\nkill -2 54410\n\n\n1\n2\n\n\n【补充】\n\n如果一旦是因为数据损坏，则需要进行如下操作（了解）：\n\n1）删除lock文件：\n\nrm -f /mongodb/replica_sets/myrs_27017/data/db/*.lock \\\n/mongodb/replica_sets/myrs_27018/data/db/*.lock \\\n/mongodb/replica_sets/myrs_27019/data/db/mongod.lock \\\n\n\n1\n2\n3\n\n\n2 ）依次修复数据：\n\n/usr/local/mongodb/bin/mongod --repair --dbpath=/mongodb/replica_sets/myrs_27017/data/db\n/usr/local/mongodb/bin/mongod --repair --dbpath=/mongodb/replica_sets/myrs_27018/data/db\n/usr/local/mongodb/bin/mongod --repair --dbpath=/mongodb/replica_sets/myrs_27019/data/db\n\n\n1\n2\n3\n\n\n（2）标准的关闭方法（数据不容易出错，但麻烦）\n\n目标：通过mongo客户端中的shutdownserver命令来依次关闭各个服务\n\n关闭副本集中的服务，建议依次关闭仲裁节点、副本节点、主节点。主要的操作步骤参考如下：\n\n//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\nmongo --port 27017\n//告知副本集说本机要下线\nrs.stepdown()\n//#切换到admin库\nuse admin\n//关闭服务\ndb.shutdownserver()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 3.3.3 通过主节点添加一个管理员帐号\n\n只需要在主节点上添加用户，副本集会自动同步。\n\n开启认证之前，创建超管用户：myroot，密码：123456\n\nmyrs:primary> use admin\nswitched to db admin\nmyrs:primary> db.createuser({user:"myroot",pwd:"123456",roles:["root"]})\nsuccessfully added user: { "user" : "myroot", "roles" : [ "root" ] }\n\n\n1\n2\n3\n4\n\n\n该步骤也可以在开启认证之后，但需要通过localhost登录才允许添加用户，用户数据也会自动同步到副本集。\n\n后续再创建其他用户，都可以使用该超管用户创建。\n\n\n# 3.3.4 创建副本集认证的key文件\n\n第一步：生成一个key文件到当前文件夹中。\n\n可以使用任何方法生成密钥文件。例如，以下操作使用openssl生成密码文件，然后使用chmod来更改文件权限，仅为文件所有者提供读取权限\n\n[root@bobohost ~]# openssl rand -base64 90 -out ./mongo.keyfile\n[root@bobohost ~]# chmod 400 ./mongo.keyfile\n[root@bobohost ~]# ll mongo.keyfile\n-r--------. 1 root root 122 8月  14 14:23 mongo.keyfile\n\n\n1\n2\n3\n4\n\n\n提示：\n\n所有副本集节点都必须要用同一份keyfile，一般是在一台机器上生成，然后拷贝到其他机器上，且必须有读的权限，否则将来会报错： permissions on /mongodb/replica_sets/myrs_27017/mongo.keyfile are too open\n\n一定要保证密钥文件一致，文件位置随便。但是为了方便查找，建议每台机器都放到一个固定的位置，都放到和配置文件一起的目录中。\n\n这里将该文件分别拷贝到多个目录中：\n\n[root@bobohost ~]# cp mongo.keyfile /mongodb/replica_sets/myrs_27017\n[root@bobohost ~]# cp mongo.keyfile /mongodb/replica_sets/myrs_27018\n[root@bobohost ~]# cp mongo.keyfile /mongodb/replica_sets/myrs_27019\n\n\n1\n2\n3\n\n\n\n# 3.3.5 修改配置文件指定keyfile\n\n分别编辑几个服务的mongod.conf文件，添加相关内容\n\n/mongodb/replica_sets/myrs_27017/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/replica_sets/myrs_27017/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/replica_sets/myrs_27018/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/replica_sets/myrs_27018/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/replica_sets/myrs_27019/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/replica_sets/myrs_27019/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n\n# 3.3.6 重新启动副本集\n\n如果副本集是开启状态，则先分别关闭关闭复本集中的每个mongod，从次节点开始。直到副本集的所有成员都离线，包括任何仲裁者。主节点必须是最后一个成员关闭以避免潜在的回滚。\n\n#通过进程编号关闭三个节点\nkill -2 54410 54361 54257\n\n\n1\n2\n\n\n分别启动副本集节点：\n\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n2\n3\n\n\n查看进程情况：\n\n[root@bobohost replica_sets]# ps -ef |grep mongod\nroot    62425    1  5 14:43 ?     00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.conf\nroot    62495    1  7 14:43 ?     00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.conf\nroot    62567    1 11 14:43 ?     00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf\n\n\n1\n2\n3\n4\n\n\n\n# 3.3.7 在主节点上添加普通账号\n\n#先用管理员账号登录\n#切换到admin库\nuse admin\n#管理员账号认证\ndb.auth("myroot","123456")\n#切换到要认证的库\nuse articledb\n#添加普通用户\ndb.createuser({user: "bobo", pwd: "123456", roles: ["readwrite"]})\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n重新连接，使用普通用户 bobo重新登录，查看数据。\n\n\n# 3.3.8 springdatamongodb连接副本集\n\n使用用户名和密码连接到 mongodb 服务器，你必须使用\'username:password@hostname/dbname\' 格式，’username’为用户名，’password’ 为密码。\n\n目标：使用用户bobo使用密码 123456 连接到mongodb 服务上。\n\napplication.yml：\n\nspring:\n    #数据源配置\n    data:\n        mongodb:\n            #副本集有认证的情况下，字符串连接\n            uri: mongodb://bobo:123456@180.76.159.126:27017,180.76.159.126:27018,180.76.159.126:27019/articledb?connect=replicaset&slaveok=true&replicaset=myrs\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3.4 分片集群环境(扩展)\n\n\n# 3.4.1 关闭已开启的集群服务\n\n分片集群环境下的安全认证和副本集环境下基本上一样。\n\n但分片集群的服务器环境和架构较为复杂，建议在搭建分片集群的时候，直接加入安全认证和服务器间的鉴权，如果之前有数据，可先将之前的数据备份出来，再还原回去。\n\n本文使用之前搭建好的集群服务，因此，先停止之前的集群服务\n\n停止服务的方式有两种：快速关闭和标准关闭，下面依次说明：\n\n（1）快速关闭方法（快速，简单，数据可能会出错）\n\n目标：通过系统的kill命令直接杀死进程：\n\n依次杀死 mongos路由、配置副本集服务，分片副本集服务，从次节点开始。直到所有成员都离线。副本集杀的时候，建议先杀仲裁者，再杀副本节点，最后是主节点，以避免潜在的回滚。杀完要检查一下，避免有的没有杀掉。\n\n#通过进程编号关闭节点\nkill -2 54410\n\n\n1\n2\n\n\n【补充】\n\n如果一旦是因为数据损坏，则需要进行如下操作（了解）：\n\n1）删除lock文件：\n\nrm -f /mongodb/sharded_cluster/myshardrs01_27018/data/db/*.lock \\\n/mongodb/sharded_cluster/myshardrs01_27118/data/db/*.lock \\\n/mongodb/sharded_cluster/myshardrs01_27218/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myshardrs02_27318/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myshardrs02_27418/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myshardrs02_27518/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myconfigrs_27019/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myconfigrs_27119/data/db/mongod.lock \\\n/mongodb/sharded_cluster/myconfigrs_27219/data/db/mongod.lock\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n2 ）依次修复数据：\n\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs01_27018/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs01_27118/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs01_27218/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs02_27318/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs02_27418/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myshardrs02_27518/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myconfigrs_27019/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myconfigrs_27119/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/myconfigrs_27219/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/mymongos_27017/data/db\n/usr/local/mongodb/bin/mongod --repair --\ndbpath=/mongodb/sharded_cluster/mymongos_27117/data/db\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n（2）标准的关闭方法（数据不容易出错，但麻烦）：\n\n目标：通过mongo客户端中的shutdownserver命令来依次关闭各个服务\n\n关闭分片服务器副本集中的服务，建议依次关闭仲裁节点、副本节点、主节点。主要的操作步骤参考如下：\n\n//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\nmongo --port 27018\n//告知副本集说本机要下线\nrs.stepdown()\n//#切换到admin库\nuse admin\n//关闭服务\ndb.shutdownserver()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n关闭配置服务器副本集的服务，建议依次关闭副本节点、主节点。主要的操作步骤参考如下：\n\n//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\nmongo --port 27019\n//告知副本集说本机要下线\nrs.stepdown()\n//#切换到admin库\nuse admin\n//关闭服务\ndb.shutdownserver()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n关闭路由服务器的服务，建议依次关闭两个路由节点。主要的操作步骤参考如下：\n\n//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。\nmongo --port 27017\n//告知副本集说本机要下线\nrs.stepdown()\n//#切换到admin库\nuse admin\n//关闭服务\ndb.shutdownserver()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 3.4.2 创建副本集认证的key文件\n\n第一步：生成一个key文件到当前文件夹中。\n\n可以使用任何方法生成密钥文件。例如，以下操作使用openssl生成密码文件，然后使用chmod来更改文件权限，仅为文件所有者提供读取权限\n\n[root@bobohost ~]# openssl rand -base64 90 -out ./mongo.keyfile\n[root@bobohost ~]# chmod 400 ./mongo.keyfile\n[root@bobohost ~]# ll mongo.keyfile\n-r--------. 1 root root 122 8月  14 14:23 mongo.keyfile\n\n\n1\n2\n3\n4\n\n\n提示：\n\n所有副本集节点都必须要用同一份keyfile，一般是在一台机器上生成，然后拷贝到其他机器上，且必须 有读的权限，否则将来会报错： permissions on /mongodb/replica_sets/myrs_27017/mongo.keyfile are too open\n\n一定要保证密钥文件一致，文件位置随便。但是为了方便查找，建议每台机器都放到一个固定的位置，都放到和配置文件一起的目录中。\n\n这里将该文件分别拷贝到多个目录中：\n\necho \'/mongodb/sharded_cluster/myshardrs01_27018/mongo.keyfile\n/mongodb/sharded_cluster/myshardrs01_27118/mongo.keyfile\n/mongodb/sharded_cluster/myshardrs01_27218/mongo.keyfile\n/mongodb/sharded_cluster/myshardrs02_27318/mongo.keyfile\n/mongodb/sharded_cluster/myshardrs02_27418/mongo.keyfile\n/mongodb/sharded_cluster/myshardrs02_27518/mongo.keyfile\n/mongodb/sharded_cluster/myconfigrs_27019/mongo.keyfile\n/mongodb/sharded_cluster/myconfigrs_27119/mongo.keyfile\n/mongodb/sharded_cluster/myconfigrs_27219/mongo.keyfile\n/mongodb/sharded_cluster/mymongos_27017/mongo.keyfile\n/mongodb/sharded_cluster/mymongos_27117/mongo.keyfile\' | xargs -n 1 cp -v /root/mongo.keyfile\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 3.4.3 修改配置文件指定keyfile\n\n分别编辑几个服务的mongod.conf文件，添加相关内容：\n\n/mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/sharded_cluster/myshardrs01_27018/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/sharded_cluster/myshardrs01_27118/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/sharded_cluster/myshardrs01_27218/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/sharded_cluster/myshardrs02_27318/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/sharded_cluster/myshardrs02_27418/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/sharded_cluster/myshardrs02_27518/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/sharded_cluster/myconfigrs_27019/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/sharded_cluster/myconfigrs_27119/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/sharded_cluster/myconfigrs_27219/mongo.keyfile\n    #开启认证方式运行\n    authorization: enabled\n\n\n1\n2\n3\n4\n5\n\n\n/mongodb/sharded_cluster/mymongos_27017/mongos.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/sharded_cluster/mymongos_27017/mongo.keyfile\n\n\n1\n2\n3\n\n\n/mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\nsecurity:\n    #keyfile鉴权文件\n    keyfile: /mongodb/sharded_cluster/mymongos_27117/mongo.keyfile\n\n\n1\n2\n3\n\n\nmongos 比mongod少了authorization：enabled的配置。原因是，副本集加分片的安全认证需要配置两方面的，副本集各个节点之间使用内部身份验证，用于内部各个mongo实例的通信，只有相同keyfile才能相互访问。所以都要开启 keyfile: /mongodb/sharded_cluster/mymongos_27117/mongo.keyfile 。\n\n然而对于所有的mongod，才是真正的保存数据的分片。mongos只做路由，不保存数据。所以所有的mongod开启访问数据的授权authorization:enabled。这样用户只有账号密码正确才能访问到数据。\n\n\n# 3.4.4 重新启动节点\n\n必须依次启动配置节点、分片节点、路由节点：\n\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf\n/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf\n/usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27017/mongos.conf\n/usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27117/mongos.conf\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n注意：\n\n这里有个非常特别的情况，就是启动顺序。先启动配置节点，再启动分片节点，最后启动路由节点。\n\n如果先启动分片节点，会卡住，提示：\n\nabout to fork child process, waiting until server is ready for connections\n\n\n1\n\n\n这也许是个 bug。原因未知。\n\n\n# 3.3.5 创建帐号和认证\n\n客户端mongo，通过localhost登录任意一个mongos路由，\n\n[root@bobohost db]# /usr/local/mongodb/bin/mongo --port 27017\n\n\n1\n\n\n提示：相当于一个后门，只能在 admin下添加用户。\n\n创建一个管理员帐号：\n\nmongos> use admin\nswitched to db admin\nmongos>  db.createuser({user:"myroot",pwd:"123456",roles:["root"]})\nsuccessfully added user: { "user" : "myroot", "roles" : [ "root" ] }\n\n\n1\n2\n3\n4\n\n\n提示：如果在开启认证之前已经创建了管理员账号，这里可以忽略\n\n创建一个普通权限帐号：\n\nmongos> use admin\nswitched to db admin\nmongos> db.auth("myroot","123456")\n1\nmongos> use articledb\nswitched to db articledb\nmongos> db.createuser({user: "bobo", pwd: "123456", roles: [{ role: "readwrite",db: "articledb" }]})\nsuccessfully added user: {\n    "user" : "bobo",\n    "roles" : [\n        {\n          "role" : "readwrite",\n          "db" : "articledb"\n        }\n    ]\n}\nmongos> db.auth("bobo","123456")\n1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n提示：\n\n通过mongos添加的账号信息，只会保存到配置节点的服务中，具体的数据节点不保存账号信息，因此，分片中的账号信息不涉及到同步问题。\n\nmongo客户端登录mongos路由，用管理员帐号登录可查看分片情况：\n\nmongos> use admin\nswitched to db admin\nmongos>  db.auth("myroot","123456")\n1\nmongos> sh.status()\n\n\n1\n2\n3\n4\n5\n\n\n退出连接，重新连接服务，使用普通权限帐号访问数据：\n\n[root@bobohost db]# /usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017\nmongodb shell version v4.0.10\nconnecting to: mongodb://180.76.159.126:27017/?gssapiservicename=mongodb\nimplicit session: session { "id" : uuid("6f84fa91-2414-407e-b3ab-c0b7eedde825")\n}\nmongodb server version: 4.0.10\nmongos> use articledb\nswitched to db articledb\nmongos> db.auth("bobo","123456")\n1\nmongos> show collections\ncomment\ncomment2\nmongos> db.comment.count()\n10001\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 3.3.6 springdatamongodb连接认证\n\n使用用户名和密码连接到 mongodb 服务器，你必须使用\'username:password@hostname/dbname\' 格式，’username’为用户名，’password’ 为密码。\n\n目标：使用用户bobo使用密码 123456 连接到mongodb 服务上。\n\napplication.yml：\n\nspring:\n    #数据源配置\n    data:\n        mongodb:\n        # 分片集群有认证的情况下，字符串连接\n            uri: mongodb://bobo:123456@180.76.159.126:27017,180.76.159.126:27117/articledb\n\n\n1\n2\n3\n4\n5\n6\n\n\n原文链接：https://wgy1993.gitee.io/archives/afe7047c.html',charsets:{cjk:!0}},{title:"Docker构建镜像",frontmatter:{title:"Docker构建镜像",date:"2023-02-24T16:15:35.000Z",permalink:"/pages/83a393/",categories:["运维","docker"],tags:[null],readingShow:"top",description:"什么是 Dockerfile？",meta:[{name:"twitter:title",content:"Docker构建镜像"},{name:"twitter:description",content:"什么是 Dockerfile？"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/10.docker/01.Docker%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F.html"},{property:"og:type",content:"article"},{property:"og:title",content:"Docker构建镜像"},{property:"og:description",content:"什么是 Dockerfile？"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/10.docker/01.Docker%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-24T16:15:35.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"Docker构建镜像"},{itemprop:"description",content:"什么是 Dockerfile？"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/10.docker/01.Docker%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F.html",relativePath:"04.运维/10.docker/01.Docker构建镜像.md",key:"v-3ed70d81",path:"/pages/83a393/",headers:[{level:2,title:"一、Dockerfile 简介",slug:"一、dockerfile-简介",normalizedTitle:"一、dockerfile 简介",charIndex:2},{level:2,title:"二、Dockerfile 指令",slug:"二、dockerfile-指令",normalizedTitle:"二、dockerfile 指令",charIndex:499},{level:3,title:"1、常用指令",slug:"_1、常用指令",normalizedTitle:"1、常用指令",charIndex:519},{level:3,title:"2、指令详解",slug:"_2、指令详解",normalizedTitle:"2、指令详解",charIndex:1790},{level:4,title:"1、FROM",slug:"_1、from",normalizedTitle:"1、from",charIndex:1800},{level:4,title:"2、COPY",slug:"_2、copy",normalizedTitle:"2、copy",charIndex:2071},{level:4,title:"3、ADD",slug:"_3、add",normalizedTitle:"3、add",charIndex:2462},{level:4,title:"4、RUN",slug:"_4、run",normalizedTitle:"4、run",charIndex:2830},{level:4,title:"5、CMD",slug:"_5、cmd",normalizedTitle:"5、cmd",charIndex:3554},{level:4,title:"6、ENTRYPOINT",slug:"_6、entrypoint",normalizedTitle:"6、entrypoint",charIndex:4375},{level:4,title:"7、ENV",slug:"_7、env",normalizedTitle:"7、env",charIndex:5710},{level:4,title:"8、ARG",slug:"_8、arg",normalizedTitle:"8、arg",charIndex:6283},{level:4,title:"9、VOLUME",slug:"_9、volume",normalizedTitle:"9、volume",charIndex:6472},{level:4,title:"10、EXPOSE",slug:"_10、expose",normalizedTitle:"10、expose",charIndex:6655},{level:4,title:"11、WORKDIR",slug:"_11、workdir",normalizedTitle:"11、workdir",charIndex:6805},{level:4,title:"12、USER",slug:"_12、user",normalizedTitle:"12、user",charIndex:6921},{level:4,title:"13、HEALTHCHECK",slug:"_13、healthcheck",normalizedTitle:"13、healthcheck",charIndex:7013},{level:4,title:"14、ONBUILD",slug:"_14、onbuild",normalizedTitle:"14、onbuild",charIndex:8162},{level:4,title:"15、LABEL",slug:"_15、label",normalizedTitle:"15、label",charIndex:8787},{level:4,title:"16、.dockerignore 文件",slug:"_16、-dockerignore-文件",normalizedTitle:"16、.dockerignore 文件",charIndex:9261},{level:2,title:"三、镜像优化",slug:"三、镜像优化",normalizedTitle:"三、镜像优化",charIndex:9894},{level:3,title:"1、基础镜像选择",slug:"_1、基础镜像选择",normalizedTitle:"1、基础镜像选择",charIndex:9905},{level:3,title:"2、配置国内软件源",slug:"_2、配置国内软件源",normalizedTitle:"2、配置国内软件源",charIndex:11345},{level:2,title:"四、构建镜像",slug:"四、构建镜像",normalizedTitle:"四、构建镜像",charIndex:12270},{level:3,title:"1、命名",slug:"_1、命名",normalizedTitle:"1、命名",charIndex:12281},{level:3,title:"2、基于镜像部署服务",slug:"_2、基于镜像部署服务",normalizedTitle:"2、基于镜像部署服务",charIndex:12524},{level:3,title:"3、使用Makefile操作Dockerfile",slug:"_3、使用makefile操作dockerfile",normalizedTitle:"3、使用makefile操作dockerfile",charIndex:14093},{level:2,title:"五、多阶段构建镜像",slug:"五、多阶段构建镜像",normalizedTitle:"五、多阶段构建镜像",charIndex:14792},{level:2,title:"六、Dockerfile 传参",slug:"六、dockerfile-传参",normalizedTitle:"六、dockerfile 传参",charIndex:15935},{level:2,title:"七、Dockerfile 优化",slug:"七、dockerfile-优化",normalizedTitle:"七、dockerfile 优化",charIndex:16573},{level:3,title:"1、尽量不使用root用户",slug:"_1、尽量不使用root用户",normalizedTitle:"1、尽量不使用root用户",charIndex:16593},{level:3,title:"2、移除所有缓存等不必要信息",slug:"_2、移除所有缓存等不必要信息",normalizedTitle:"2、移除所有缓存等不必要信息",charIndex:16812},{level:3,title:"3、使用合理的ENTRYPOINT脚本",slug:"_3、使用合理的entrypoint脚本",normalizedTitle:"3、使用合理的entrypoint脚本",charIndex:16975},{level:3,title:"4、设置时区",slug:"_4、设置时区",normalizedTitle:"4、设置时区",charIndex:17245},{level:4,title:"1、基于 Alpine 镜像",slug:"_1、基于-alpine-镜像",normalizedTitle:"1、基于 alpine 镜像",charIndex:17464},{level:4,title:"2、基于 Centos7 镜像",slug:"_2、基于-centos7-镜像",normalizedTitle:"2、基于 centos7 镜像",charIndex:17764},{level:4,title:"3、基于 Debian 镜像",slug:"_3、基于-debian-镜像",normalizedTitle:"3、基于 debian 镜像",charIndex:17927},{level:4,title:"4、基于 Ubuntu 镜像",slug:"_4、基于-ubuntu-镜像",normalizedTitle:"4、基于 ubuntu 镜像",charIndex:18041},{level:3,title:"5、设置系统语言",slug:"_5、设置系统语言",normalizedTitle:"5、设置系统语言",charIndex:18291},{level:4,title:"1、基于 Alpine 镜像",slug:"_1、基于-alpine-镜像-2",normalizedTitle:"1、基于 alpine 镜像",charIndex:17464},{level:4,title:"2、基于 Centos7 镜像",slug:"_2、基于-centos7-镜像-2",normalizedTitle:"2、基于 centos7 镜像",charIndex:17764},{level:4,title:"3、基于 Debian 镜像",slug:"_3、基于-debian-镜像-2",normalizedTitle:"3、基于 debian 镜像",charIndex:17927},{level:4,title:"4、基于 Ubuntu 镜像",slug:"_4、基于-ubuntu-镜像-2",normalizedTitle:"4、基于 ubuntu 镜像",charIndex:18041},{level:3,title:"6、使用Label标注作者、软件版本等元信息",slug:"_6、使用label标注作者、软件版本等元信息",normalizedTitle:"6、使用label标注作者、软件版本等元信息",charIndex:19711},{level:3,title:"7、指定工作区",slug:"_7、指定工作区",normalizedTitle:"7、指定工作区",charIndex:19890},{level:3,title:"8、RUN指令显示优化",slug:"_8、run指令显示优化",normalizedTitle:"8、run指令显示优化",charIndex:19926},{level:3,title:"9、使用 URL 添加源码",slug:"_9、使用-url-添加源码",normalizedTitle:"9、使用 url 添加源码",charIndex:19978},{level:3,title:"10、使用虚拟编译环境",slug:"_10、使用虚拟编译环境",normalizedTitle:"10、使用虚拟编译环境",charIndex:20231},{level:3,title:"11、最小化层数",slug:"_11、最小化层数",normalizedTitle:"11、最小化层数",charIndex:20608}],headersStr:"一、Dockerfile 简介 二、Dockerfile 指令 1、常用指令 2、指令详解 1、FROM 2、COPY 3、ADD 4、RUN 5、CMD 6、ENTRYPOINT 7、ENV 8、ARG 9、VOLUME 10、EXPOSE 11、WORKDIR 12、USER 13、HEALTHCHECK 14、ONBUILD 15、LABEL 16、.dockerignore 文件 三、镜像优化 1、基础镜像选择 2、配置国内软件源 四、构建镜像 1、命名 2、基于镜像部署服务 3、使用Makefile操作Dockerfile 五、多阶段构建镜像 六、Dockerfile 传参 七、Dockerfile 优化 1、尽量不使用root用户 2、移除所有缓存等不必要信息 3、使用合理的ENTRYPOINT脚本 4、设置时区 1、基于 Alpine 镜像 2、基于 Centos7 镜像 3、基于 Debian 镜像 4、基于 Ubuntu 镜像 5、设置系统语言 1、基于 Alpine 镜像 2、基于 Centos7 镜像 3、基于 Debian 镜像 4、基于 Ubuntu 镜像 6、使用Label标注作者、软件版本等元信息 7、指定工作区 8、RUN指令显示优化 9、使用 URL 添加源码 10、使用虚拟编译环境 11、最小化层数",content:'# 一、Dockerfile 简介\n\n什么是 Dockerfile？\n\nDockerfile 是一个用来构建镜像的文本文件，文本内容包含了构建镜像所需的指令和说明。\n\n在构建时 docker Engine 会在当前目录中查找名为 “Dockerfile” 的文件，注意这个文件名必须为 “Dockerfile” 首字母大写。当您运行 docker build 命令的时候，Dockerfile 文件以及与 Dockerfile 文件处于同级路径下的所有文件都将一并提交至 docker Engine 来处理，docker Engine 将逐行解析 Dockerfile 文件，并执行相关操作。Dockerfile 的解析是由上而下的顺序进行。\n\n注意您放置 Dockerfile 的目录最好是一个空目录，或者只放置与本次构建相关的文件即可，因为在执行 build 时会将 Dockerfile 以及 Dockerfile 路径下的所有文件提交至 docker Engine 处理，因此您不应该在 Dockerfile 目录中放置一些无关的文件，这是无意义的并且这将会严重拖慢整个构建过程。\n\n\n# 二、Dockerfile 指令\n\n\n# 1、常用指令\n\nDocker 镜像可以通过 Docker hub 或者阿里云等仓库中获取，这些镜像是由官方或者社区人员提供的，对于 Docker 用户来说并不能满足我们的需求，但是从无开始构建镜像成本大。常用的数据库、中间件、应用软件等都有现成的 Docker 官方镜像或社区创建的镜像，我们只需要稍作配置就可以直接使用。\n\n使用现成镜像的好处除了省去自己做镜像的工作量外，更重要的是可以利用前人的经验。特别是使用那些官方镜像，因为 Docker 的工程师知道如何更好的在容器中运行软件。\n\n当然，某些情况下我们也不得不自己构建镜像，比如找不到现成的镜像，比如自己开发的应用程序，需要在镜像中加入特定的功能。\n\n在编写 Dockerfile 时，您需要用到 Dockerfile 指令来定义构建过程中所要执行的操作，这里列举几个常用的指令：\n\n常用指令          说明\nFROM          FROM 指令是定义本阶段构建所要使用的基础镜像。\nMAINTAINER    镜像维护者姓名或邮箱地址\nRUN           构建镜像时运行的 shell 命令\nCMD           运行容器时执行的 shell 命令\nEXPOSE        仅仅只是声明端口，声明容器内应用所使用的端口号\nENV           设置容器环境变量，当然在您使用该镜像运行容器时，ENV 所定义的变量仍然对容器有效\nADD           拷贝文件或目录到镜像，如果是 URL 或压缩包会自动下载或自动解压\nCOPY          拷贝文件或目录到镜像容器内，跟 ADD 类似，但不具备自动下载或解压功能\nENTRYPOINT    运行容器时执行的 shell 命令\nVOLUME        指定容器挂载点到宿主机自动生成的目录或其他容器\nUSER          为 RUN、CMD、和 ENTRYPOINT 执行命令指定运行用户\nWORKDIR       为 RUN、CMD、ENTRYPOINT、COPY 和 ADD 设置工作目录，意思为切换目录\nHEALTHCHECK   健康检查\nARG           构建时指定的一些参数\n\nRUN、CMD 与 ENTRYPOINT 的区别\n\n * RUN：执行命令并创建新的镜像层，常用于安装软件包，在 docker build 过程中执行；\n * CMD：设置容器启动后默认执行的命令及其参数，在 docker run 时运行，但 docker run 后跟参数时会替换（忽略） CMD，CMD 可以被覆盖，如果有 ENTRYPIOINT 的话，CMD 就是 ENTRYPIOINT 的参数。；\n * ENTRYPOINT：配置容器启动时运行的命令，不管 docker run … 后是否运行有其他命令，ENTRYPOINT 指令后的命令一定会被执行， 后面如果再接命令，会报错多余的参数。\n * CMD 和 ENTRYPIOINT 必须要有一个\n\n\n# 2、指令详解\n\n# 1、FROM\n\nFROM 指令是定义本阶段构建所要使用的基础镜像。例如您需要将您的 java 开发的项目构建为一个镜像，那么您可能需要 tomcat 镜像，而制作 tomcat 镜像就可能需要 jdk 镜像，那么 jdk 镜像就需要一个基本的操作系统镜像，这可能是 utuntu、debian、alpine 等。\n\n根据以上，我们应该可以知道由于容器镜像是可以增量叠加的，那么 tomcat 镜像中就应该包含了 jdk 以及一个基本的操作系统。\n\n您可以从 hub.docker.com 网站上来挑选一个合适的镜像来作为您的基础镜像。\n\n# 2、COPY\n\n复制指令，从上下文目录中复制文件或者目录到容器里指定路径\n\n（上下文目录是指放置 Dockerfile 文件的目录）\n\n格式如下：\n\nCOPY [--chown=<user>:<group>] <源路径1>... <目标路径>\nCOPY [--chown=<user>:<group>] ["<源路径1>",... "<目标路径>"]\n\n\n1\n2\n\n\n注意 [--chown=<user>:<group>]： 可选参数，用户改变复制到容器内文件的拥有者和属组。\n\n<源路径>：源文件或者源目录，这里可以是通配符表达式，其通配符规则要满足 Go 的 filepath.Match 规则。例如：\n\nCOPY hom* /mydir/\n\nCOPY hom?.txt /mydir/\n\n<目标路径>：容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。\n\n# 3、ADD\n\nADD 指令和 COPY 的使用格式一致（同样需求下，官方推荐使用 COPY）。功能也类似，不同之处如下：\n\nADD 的优点：在执行 <源文件> 为 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，会自动复制并解压到 < 目标路径 >。\n\nADD 的缺点：在不解压的前提下，无法复制 tar 压缩文件。会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。具体是否使用，可以根据是否需要自动解压来决定。\n\n对于使用 ADD 指令下载远程服务器上的 tar 包并解压，建议使用以下方式代替\n\nRUN curl -s http://192.168.1.7/repository/tools/jdk-8u241-linux-x64.tar.gz | tar -xC /opt/\n\n\n1\n\n\n# 4、RUN\n\n当我们在构建镜像过程中，需要执行一些配置，例如使用 mkdir 命令新建一个目录，用 sed 命令来替换一些文本内容等，当然可能在实际过程中需要更复杂的命令，那么此时您可以使用 RUN 指令来定义将要运行的命令。\n\nRUN 指令在 Dockerfile 中可以出现多次，docker Engine 在构建过程中会读取 Dockerfile 然后由上而下依次执行 RUN 指令所标记的命令。\n\nRUN 后面跟着的命令行命令。有以下俩种格式：\n\nShell 格式\n\nRUN <命令行命令>\n# <命令行命令> 等同于，在终端操作的 shell 命令。\n\n\n1\n2\n\n\nExec 格式：\n\nRUN ["可执行文件", "参数1", "参数2"]\n# 例如：\n# RUN ["./test.php", "dev", "offline"] 等价于 RUN ./test.php dev offline\n\n\n1\n2\n3\n\n\n注意在构建过程中 docker Engine 会为 Dockerfile 中的每个 RUN 指令创建一个镜像层（layer）来记录这种改变。因此为了减少不必要的镜像层数，通常的做法是将多个命令定义在一个 RUN 指令中，如下所示：\n\nRUN mkdir demo \\\n    && cd demo \\\n    && wget https://www.demo.com/download/demo.tar.gz \\\n    && tar -xf demo.tar.gz \\\n    && rm -rf demo.tar.gz\n\n\n1\n2\n3\n4\n5\n\n\n如上，以 && 符号连接命令，这样执行后，只会创建 1 层镜像\n\n# 5、CMD\n\n类似于 RUN 指令，用于运行程序，但二者运行的时间点不同:\n\nCMD 在 docker run 时运行。\n\nRUN 是在 docker build 过程中执行。\n\n作用：为启动的容器指定默认要运行的程序，程序运行结束，容器也就结束。CMD 指令指定的程序可被 docker run 命令行参数中指定要运行的程序所覆盖。\n\n> 注意\n> \n> 如果 Dockerfile 中如果存在多个 CMD 指令，仅最后一个生效。\n\n格式：\n\nCMD <shell 命令> \nCMD ["<可执行文件或命令>","<param1>","<param2>",...] \nCMD ["<param1>","<param2>",...]  # 该写法是为 ENTRYPOINT 指令指定的程序提供默认参数\n\n\n1\n2\n3\n\n\n推荐使用第二种格式，执行过程比较明确。第一种格式实际上在运行的过程中也会自动转换成第二种格式运行，并且默认可执行文件是 sh。\n\n例如您有一个 nodejs 程序，启动该程序的命令是 node server.js，然后您希望在使用该镜像运行容器时也以这种方式运行，那么您可以在 Dockerfile 中使用 CMD 来标记，如下：\n\nCMD ["node","server.js"]\n\n\n1\n\n\n通常 CMD 指令出现在 Dockerfile 末尾处。当您将以上定义写入 Dockerfile 中并打包成镜像（demo:v1），然后您使用该镜像运行时，那么该容器将使用 Dockerfile 中 CMD 定义的命令来运行\n\ndocker  run  -d  --name=demo  deom:v1\n\n\n1\n\n\n当然您可以在运行时覆盖 CMD 指令中的命令，例如这里改成 npm start，命令如下\n\ndocker  run  -d  --name=demo  deom:v1  npm  start\n\n\n1\n\n\n# 6、ENTRYPOINT\n\n类似于 CMD 指令，但其不会被 docker run 的命令行参数指定的指令所覆盖，而且这些命令行参数会被当作参数送给 ENTRYPOINT 指令指定的程序。\n\n但是，如果运行 docker run 时使用了 --entrypoint 选项，将覆盖 CMD 指令指定的程序。\n\n优点：在执行 docker run 的时候可以指定 ENTRYPOINT 运行所需的参数。\n\n注意：如果 Dockerfile 中如果存在多个 ENTRYPOINT 指令，仅最后一个生效。\n\n格式：\n\nENTRYPOINT ["<executeable>","<param1>","<param2>",...]\n\n\n1\n\n\n可以搭配 CMD 命令使用：一般是变参才会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参，以下示例会提到。\n\n示例：\n\n假设已通过 Dockerfile 构建了 nginx:test 镜像\n\nFROM nginx\nENTRYPOINT ["nginx", "-c"] # 定参\nCMD ["/etc/nginx/nginx.conf"] # 变参\n\n\n1\n2\n3\n\n\n不传参运行\n\ndocker run nginx:test\n\n\n1\n\n\n容器内会默认运行以下命令，启动主进程。\n\nnginx -c /etc/nginx/nginx.conf\n\n\n1\n\n\n传参运行\n\ndocker run nginx:test -c /etc/nginx/new.conf\n\n\n1\n\n\n容器内会默认运行以下命令，启动主进程 (/etc/nginx/new.conf: 假设容器内已有此文件)\n\nnginx -c /etc/nginx/new.conf\n\n\n1\n\n\n通常更好的做法是使用 ENTRYPOINT 来定义一个 docker-entrypoint.sh 脚本，然后在该脚本中定义一些预处理及条件判断，来应对更多未知情况。例如 cups 镜像的 Dockerfile\n\nARG ARCH=amd64\nFROM $ARCH/debian:buster-slim\n# environment\nENV ADMIN_PASSWORD=admin\n# ……内容太多，中间内容已省略\nENTRYPOINT [ "docker-entrypoint.sh" ]\n# default command\nCMD ["cupsd", "-f"]\n# volumes\nVOLUME ["/etc/cups"]\n# ports\nEXPOSE 631\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n该镜像的 docker-entrypoint.sh 文件内容如下：\n\n#!/bin/bash -e\necho -e "${ADMIN_PASSWORD}\\n${ADMIN_PASSWORD}" | passwd admin\nif [ ! -f /etc/cups/cupsd.conf ]; then\n  cp -rpn /etc/cups-skel/* /etc/cups/\nfi\nexec "$@"\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 7、ENV\n\n设置环境变量，定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。当然在您使用该镜像运行容器时，ENV 所定义的变量仍然对容器有效，例如 MySQL 镜像的 Dockerfile 中就包含 root 用户的初始密码，使用 ENV 来指定。\n\n格式：\n\nENV <key> <value>\nENV <key1>=<value1> <key2>=<value2>...\n\n\n1\n2\n\n\n以下示例设置 NODE_VERSION = 16.19.0 ， 在后续的指令中可以通过 $NODE_VERSION 引用：\n\nENV NODE_VERSION 16.19.0\nRUN curl -SLO "https://nodejs.org/download/release/latest-v16.x/node-v$NODE_VERSION-linux-x64.tar.gz" \n\n\n1\n2\n\n\nDockerfile 中 ENV 指令像 RUN 指令一样，每一个都会创建一个临时层。\n\nENV JAVA_HOME=/opt/jdk1.8.0_241 \\\n    CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib \nENV PATH=$PATH:$JAVA_HOME/bin\n\n\n1\n2\n3\n\n\n# 8、ARG\n\n构建参数，与 ENV 作用一至。不过作用域不一样。ARG 设置的环境变量仅对 Dockerfile 内有效，也就是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量。\n\n构建命令 docker build 中可以用 --build-arg <参数名>=< 值 > 来覆盖。\n\n格式\n\nARG <参数名>[=<默认值>]\n\n\n1\n\n\n# 9、VOLUME\n\n定义匿名数据卷。在启动容器时忘记挂载数据卷，会自动挂载到匿名卷。\n\n作用：\n\n避免重要的数据，因容器重启而丢失，这是非常致命的。\n\n避免容器不断变大。\n\n格式：\n\nVOLUME ["<路径1>", "<路径2>"...]\nVOLUME <路径\n\n\n1\n2\n\n\n在启动容器 docker run 的时候，我们可以通过 -v 参数修改挂载点。\n\n# 10、EXPOSE\n\n仅仅只是声明端口。\n\n作用：\n\n帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射。\n\n在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。\n\n格式：\n\nEXPOSE <端口1> [<端口2>...]\n\n\n1\n\n\n# 11、WORKDIR\n\n指定工作目录。用 WORKDIR 指定的工作目录，会在构建镜像的每一层中都存在。（WORKDIR 指定的工作目录，如果目录不存在则会自动创建）。\n\n格式：\n\nWORKDIR <工作目录路径>\n\n\n1\n\n\n# 12、USER\n\n用于指定执行后续命令的用户和用户组，这边只是切换后续命令执行的用户（用户和用户组必须提前已经存在）。\n\n格式\n\nUSER <用户名>[:<用户组>]\n\n\n1\n\n\n# 13、HEALTHCHECK\n\n用于指定某个程序或者指令来监控 docker 容器服务的运行状态。\n\n格式：\n\nHEALTHCHECK [选项] CMD <命令>：设置检查容器健康状况的命令\nHEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令\nHEALTHCHECK [选项] CMD <命令> : 这边 CMD 后面跟随的命令使用，可以参考 CMD 的用法。\nHEALTHCHECK 支持下列选项：\n--interval=<间隔>：两次健康检查的间隔，默认为 30 秒；\n--timeout=<时长>：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒；\n--retries=<次数>：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n和 CMD, ENTRYPOINT 一样，HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。\n\n在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：0：成功；1：失败；2：保留，不要使用这个值。\n\n假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl 来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写：\n\nFROM nginx\nRUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*\nHEALTHCHECK --interval=5s --timeout=3s \\\n  CMD curl -fs http://localhost/ || exit 1\n\n\n1\n2\n3\n4\n\n\n当然您可以在 docker run 命令中，直接指明 healthcheck 相关策略，如下：\n\ndocker run --rm -d \\\n    --name=elasticsearch \\\n    --health-cmd="curl --silent --fail localhost:9200/_cluster/health || exit 1" \\\n    --health-interval=5s \\\n    --health-retries=12 \\\n    --health-timeout=2s \\\n    elasticsearch:7\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 14、ONBUILD\n\n用于延迟构建命令的执行。简单的说，就是 Dockerfile 里用 ONBUILD 指定的命令，在本次构建镜像的过程中不会执行（假设镜像为 test-build）。当有新的 Dockerfile 使用了之前构建的镜像 FROM test-build ，这是执行新镜像的 Dockerfile 构建时候，会执行 test-build 的 Dockerfile 里的 ONBUILD 指定的命令。\n\n格式：\n\nONBUILD <其它指令>\n\n\n1\n\n\n假设我们要制作 Node.js 所写的应用的镜像。我们都知道 Node.js 使用 npm 进行包管理，所有依赖、配置、启动信息等会放到 package.json 文件里。在拿到程序代码后，需要先进行 npm install 才可以获得所有需要的依赖。将项目相关的指令加上 ONBUILD，这样在构建基础镜像的时候，这三行并不会被执行。基础镜像变化后，各个项目都用这个 Dockerfile 重新构建镜像，会继承基础镜像的更新。\n\nFROM node:slim\nRUN mkdir /app\nWORKDIR /app\nONBUILD COPY ./package.json /app\nONBUILD RUN [ "npm", "install" ]\nONBUILD COPY . /app/\nCMD [ "npm", "start" ]\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 15、LABEL\n\nLABEL 指令用来给镜像以键值对的形式添加一些元数据（metadata）\n\n格式：\n\nLABEL <key>=<value> <key>=<value> <key>=<value> ...\n\n\n1\n\n\n我们还可以用一些标签来申明镜像的作者、文档地址等，例如：\n\nLABEL org.opencontainers.image.authors="deamon"\nLABEL org.opencontainers.image.documentation="https://daemon.gitbooks.io"\n或者\nLABEL vendor=ACME\\ Incorporated \\\n      com.example.is-beta= \\\n      com.example.is-production="" \\\n      com.example.version="0.0.1-beta" \\\n      com.example.release-date="2015-02-12"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 16、.dockerignore 文件\n\n执行 docker build 命令时，当前的工作目录被称为构建上下文。默认情况下，Dockerfile 就位于该路径下。也可以通过 -f 参数来指定 dockerfile ，但 docker 客户端会将当前工作目录下的所有文件发送到 docker 守护进程进行构建。\n\n所以来说，当执行 docker build 进行构建镜像时，当前目录一定要 干净 ，切记不要在家里录下创建一个 Dockerfile 紧接着 docker build 一把梭。\n\n正确做法是为项目建立一个文件夹，把构建镜像时所需要的资源放在这个文件夹下。比如这样：\n\nmkdir project\ncd !$\nvi Dockerfile\n# 编写 Dockerfile\n\n\n1\n2\n3\n4\n\n\n也可以通过 .dockerignore 文件来忽略不需要的文件发送到 docker 守护进程\n\n在 docker CLI 将上下文发送到 docker 守护程序之前，它会在上下文的根目录中查找一个名为.dockerignore 的文件。如果此文件存在，CLI 将修改上下文以排除与其中模式匹配的文件和目录。这有助于避免不必要地将大型或敏感文件和目录发送到守护程序，并可能使用或将它们添加到镜像中。\n\n例如：\n\n# comment\n*/temp*\n*/*/temp*\ntemp?\n*.md\nREADME-secret.md\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 三、镜像优化\n\n\n# 1、基础镜像选择\n\n使用体积较小的基础镜像，比如 alpine 或者 debian:buster-slim，像 openjdk 可以选用 openjdk:xxx-slim，由于 openjdk 是基于 debian 的基础镜像构建的，所以向 debian 基础镜像一样，后面带个 slim 就是基于 debian:xxx-slim 镜像构建的。\n\n目前 Docker 官方已开始推荐使用 Alpine 替代之前的 Ubuntu 做为基础镜像环境。这样会带来多个好处。包括镜像下载速度加快，镜像安全性提高，主机之间的切换更方便，占用更少磁盘空间等。\n\nREPOSITORY                  TAG                 IMAGE ID            CREATED             SIZE\ndebian                      buster-slim         e1af56d072b8        4 days ago          69.2MB\nalpine                      latest              cc0abc535e36        8 days ago          5.59MB\n\n\n1\n2\n3\n\n\n不过需要注意的是，alpine 的 c 库是 musl libc ，而不是正统的 glibc，另外对于一些依赖 glibc 的大型项目，像 openjdk 、tomcat、rabbitmq 等都不建议使用 alpine 基础镜像，因为 musl libc 可能会导致 JVM 一些奇怪的问题。这也是为什么 tomcat 官方没有给出基础镜像是 alpine 的 Dockerfile 的原因。\n\n制作前端镜像时一定不要使用centos、Ubuntu等系统镜像，我们可以直接使用官方提供的nginx镜像来作为基础镜像，这样我们只需把制作好的web静态文件拷贝一下就可以了。\n\n您可以在hub.docker.com网站上搜索更小的镜像或者说更符合您要求的镜像，然后作为基础镜像来完成构建。\n\n基础镜像      优点                  缺点                         备注\nAlpine    占用空间小               基于musl libc和busybox        官方推荐\nbusybox   占用空间小，极度轻量版         工具太少                       不推荐，组件不全\nscratch   空镜像，镜像大小约等于执行文件大小   没有sh或bash，无法进入容器内进行交互式调试   适合go语言\n\ndocker镜像常见的参数\n\n参数               说明\nbuster           适用与 debian 10\nstretch          适用于 debian 9\njessie           适用于 debian 8\nslim             表示最小安装包，仅包含需要运行指定容器的特定工具集\nAlphine/alpine   专门为容器构建的操作系统，比其他的操作系统更小，但是其上会缺少很多软件包并且使用的 glibc 等都是阉割版\nbullseye         开发版本，处于未稳定状态\n\n\n# 2、配置国内软件源\n\n使用默认的软件源安装构建时所需的依赖，对于绝大多数基础镜像来说，可以通过修改软件源的方式更换为国内的软件源镜像站。目前国内稳定可靠的镜像站主要有，华为云、阿里云、腾讯云、163 等。\n\n对于 alpine 基础镜像修改软件源\n\nRUN sed -i \'s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\' /etc/apk/repositories\n\n\n1\n\n\ndebian 基础镜像修改默认软件源\n\nRUN sed -i s@/archive.ubuntu.com/@/mirrors.aliyun.com/@g /etc/apt/sources.list\nRUN sed -i "s@http://deb.debian.org@http://mirrors.aliyun.com@g" /etc/apt/sources.list && \\\n    rm -Rf /var/lib/apt/lists/* && \\\n\n\n1\n2\n3\n\n\nUbuntu 基础镜像修改默认软件源\n\nRUN sed -i \'s/archive.ubuntu.com/mirrors.aliyun.com/g\' /etc/apt/sources.list && \\\n    sed -i \'s/security.ubuntu.com/mirrors.aliyun.com/g\' /etc/apt/sources.list\n\n\n1\n2\n\n\n对于 CentOS ???\n\n你确定要用 230MB 大小的基础镜像？\n\nREPOSITORY                               TAG           IMAGE ID       CREATED          SIZE\ncentos                                   latest        5d0da3dc9764   16 months ago    231MB\n\n\n1\n2\n\n\n建议这些命令就放在 RUN 指令的第一条，update 以下软件源，之后再 install 相应的依赖。\n\n\n# 四、构建镜像\n\n\n# 1、命名\n\n原则是见名知意。可使用三段式\n\n> 镜像仓库地址/类型库/镜像名:版本号\n\n * registry/runtime/Java:8.1.2\n * registry/runtime/php-fpm-nginx:7.3-1.14\n * registry/cicd/kubctl-helm:1.17-3.0\n * registry/cicd/git-compose-docker:v1\n * registry/applications/demo:git_commit_id\n\n\n# 2、基于镜像部署服务\n\n那么现在我们就可以利用以上指令，将我们在宿主机上执行的操作，写在一个Dockerfile中，示例如下：\n\nFROM centos:7\nMAINTAINER 759600963@qq.com\n#执行下面命令，安装基础环境\nRUN yum install -y gcc gcc-c++ make \\\n    openssl-devel pcre-devel gd-devel \\\n    iproute net-tools telnet wget curl && \\\n    yum clean all && \\\n    rm -rf /var/cache/yum/*\nRUN wget http://nginx.org/download/nginx-1.20.1.tar.gz\nRUN tar -zxf nginx-1.20.1.tar.gz -C /usr/src\nRUN useradd -M -s /sbin/nologin nginx\nWORKDIR /usr/src/nginx-1.20.1\nRUN ./configure --prefix=/usr/local/nginx --user=nginx --group=nginx && make && make install\nRUN ln -s /usr/local/nginx/sbin/* /usr/local/sbin/\nRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\nWORKDIR /usr/local/nginx\nEXPOSE 80\nCMD ["nginx", "-g", "daemon off;"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n如上所示，FROM定义了基于centos7镜像来构建，这里使用了yum方式来安装依赖，并通过源码编译的方式进行安装。然后使用WORKDIR定义了进程的工作目录，使用EXPOSE声明了应用要使用的端口，最后使用CMD指定了容器启动时默认命令。\n\n# docker pull centos:7\n\n\n1\n\n\n那么此时我们可以创建一个空目录例如nginx，然后将Dockerfile文件放置于该目录中，然后执行构建命令\n\n# docker build -t web:centos .\n\n\n1\n\n\n注意\n\n这里的-t参数是为这个构建的镜像取一个名字，设置一个标签，在自动生成镜像的命令指定镜像后，一定不要忘记写新生成镜像的存放路径，也就是空格后的 一 个“.”代表当前路径，否则会报错。\n\n当我们执行build后，docker会将build文件夹连同Dockerfile一起所有的文件（如果有其他文件）提交至docker engine来处理。docker Engine将逐行解析Dockerfile文件，并执行相关操作。Dockerfile的解析是由上而下的顺序进行。\n\n注意您放置Dockerfile的目录最好是一个空目录，或者只放置与本次构建相关的文件即可，因为在执行build时会将Dockerfile以及Dockerfile路径下的所有文件提交至docker Engine处理，因此您不应该在Dockerfile目录中放置一些无关的文件，这是无意义的并且这将会严重拖慢整个构建过程。\n\n使用新的镜像运行容器\n\n# docker run -itd --name testweb -p 80:80 web:centos\n73277ceaa7a9a72d7a729029360413cfc0364effb4062b3df25aa960c67b894a\n\n\n1\n2\n\n\n\n# 3、使用Makefile操作Dockerfile\n\nIMAGE_BASE = registry/runtime\nIMAGE_NAME = php-fpm\nIMAGE_VERSION = 7.3\nIMAGE_TAGVERSION = $(GIT_COMMIT)\n\nall: build tag push\n\nbuild:\n  docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} .\n\ntag:\n  docker tag ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION}\n\npush:\n  docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION}\n\n# 构建并推送\nmake \n# 仅构建\nmake build \n# 仅打tag\nmake tag\n# 仅推送\nmake push\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nmakefile中的命令必须以tab作为开头(分隔符),不能用扩展的tab即用空格代替的tab。(如果是vim编辑的话,执行 set noexpandtab)。否则会报如下错误：Makefile:10: *** multiple target patterns. Stop.\n\n\n# 五、多阶段构建镜像\n\n在编写Dockerfile构建docker镜像时，常遇到以下问题：\n\n 1. RUN命令会让镜像新增layer，导致镜像变大，虽然通过&&连接多个命令能缓解此问题，但如果命令之间用到docker指令例如COPY、WORKDIR等，依然会导致多个layer；\n 2. 有些工具在构建过程中会用到，但是最终的镜像是不需要的（例如用npm编译构建前端工程），这要求Dockerfile的编写者花更多精力来清理这些工具，清理的过程又可能导致新的layer；\n\n为了解决上述问题，从17.05版本开始Docker在构建镜像时增加了新特性：多阶段构建(multi-stage builds)，将构建过程分为多个阶段，每个阶段都可以指定一个基础镜像，这样在一个Dockerfile就能将多个镜像的特性同时用到\n\n我们可以在一个Dockerfile中使用多个FROM指令并将一次构建分成多个阶段来完成\n\n# build stage\nFROM node:14.18.2-stretch-slim AS build-env\n# 工作目录\nWORKDIR /app\n# 将git仓库下所有文件拷贝到工作目录\nCOPY . .\nRUN npm install\nRUN npm audit fix\nRUN npm run build:production\n\n# production stage\n# 生产环境基础nginx镜像（上面的镜像已经打包为了静态文件）\nFROM nginx:alpine\nADD  prod.conf /etc/nginx/conf.d/\n# 使用--from把上面产生的静态文件复制到nginx的运行目录\nCOPY --from=build-env /app/dist /usr/share/nginx/html\nRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && \\\n    echo \'Asia/Shanghai\' >/etc/timezone\n# nginx容器内部暴露的端口\nEXPOSE 80\n# 运行的命令\nCMD ["/bin/sh", "-c", "nginx -g \\"daemon off;\\"" ]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n在第一个FROM指令后使用as build-env 意思是为该阶段取一个标记名为build-env，然后在第二个FROM之后使用了COPY --from=build-env，意思是从上一个名为builder的阶段中拷贝文件至本阶段中。\n\n\n# 六、Dockerfile 传参\n\n使用 ARG 和 build-arg 传入动态变量：\n\n# base image \nFROM centos:7 \n# MAINTAINER dot # deprecated \n\nLABEL maintainer="dot" version="demo" \nLABEL multiple="true" \n\nARG USERNAME \nARG DIR="defaultValue" \n\nRUN useradd -m $USERNAME -u 1001 && mkdir $DIR \n\n# docker build --build-arg USERNAME="test_arg" -t test:arg . \n# docker run -ti --rm test:arg bash \n# ls \nbin             dev  home  lib64       media  opt   root  selinux  sys  usr \ndefaultValue  etc  lib   lost+found  mnt    proc  sbin  srv      tmp  var\n# tail -1 /etc/passwd \ntest_arg:x:1001:1001::/home/test_arg:/bin/bash\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 七、Dockerfile 优化\n\n\n# 1、尽量不使用root用户\n\n在做基础运行时镜像时，创建运行时普通用户和用户组，并做工作区与权限限制，启动服务时尽量使用普通用户。\n\ngosu\n\nFROM alpine:3.11.5\nRUN sed -i "s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g" /etc/apk/repositories \\\n    && apk add --no-cache gosu\n\n\n1\n2\n3\n\n\n\n# 2、移除所有缓存等不必要信息\n\n * 删除解压后的源压缩包（参考第二章第二节）\n * 清理包管理器下载安装软件时的缓存\n   * 使用Alipine镜像中APK命令安装包时记得加上--no-cache\n   * 使用Ubuntu镜像中的APT命令安装软件后记得 rm -rf /var/lib/apt/lists/*\n\n\n# 3、使用合理的ENTRYPOINT脚本\n\n示例：\n\n#!/bin/bash\nset -e\n\nif [ "$1" = \'postgres\' ]; then\n    chown -R postgres "$PGDATA"\n\n    if [ -z "$(ls -A "$PGDATA")" ]; then\n        gosu postgres initdb\n    fi\n\n    exec gosu postgres "$@"\nfi\n\nexec "$@"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 4、设置时区\n\n由于绝大多数基础镜像都是默认采用 UTC 的时区，与北京时间相差 8 个小时，这将会导致容器内的时间与北京时间不一致，因而会对一些应用造成一些影响，还会影响容器内日志和监控的数据。\n\n因此对于东八区的用户，最好在构建镜像的时候设定一下容器内的时区，以免以后因为时区遇到一些 bug。\n\n可以通过环境变量设置容器内的时区。在启动的时候可以通过设置环境变量 -e TZ=Asia/Shanghai 来设定容器内的时区。\n\n# 1、基于 Alpine 镜像\n\nFROM alpine:3.15\nENV TZ=Asia/Shanghai\nRUN sed -i \'s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\' /etc/apk/repositories \\\n    && apk add --no-cache tzdata \\\n    && ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  \\\n    && echo "Asia/Shanghai" > /etc/timezone\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 2、基于 Centos7 镜像\n\nFROM centos:7\n#定义时区参数\nENV TZ=Asia/Shanghai\n#设置时区\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo \'$TZ\' > /etc/timezone\n\n\n1\n2\n3\n4\n5\n\n\n# 3、基于 Debian 镜像\n\n# 由于 Debian 镜像中已经包含了tzdata，所以只需添加环境变量TZ即可。\nFROM debian:latest\n\nENV TZ=Asia/Shanghai\n\n\n1\n2\n3\n4\n\n\n# 4、基于 Ubuntu 镜像\n\nFROM ubuntu:bionic\n\nENV TZ=Asia/Shanghai\nRUN echo "${TZ}" > /etc/timezone\n    && ln -sf /usr/share/zoneinfo/${TZ} /etc/localtime\n    && apt update\n    && apt install -y tzdata\n    && rm -rf /var/lib/apt/lists/*\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 5、设置系统语言\n\n# 1、基于 Alpine 镜像\n\nFROM alpine:3.15\nENV LANG=en_US.UTF-8 \\\n    LANGUAGE=en_US.UTF-8\n\nRUN apk --no-cache add ca-certificates wget \\ \n    && wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \\ \n    && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.33-r0/glibc-2.33-r0.apk \\ \n    && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.33-r0/glibc-bin-2.33-r0.apk \\\n    && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.33-r0/glibc-i18n-2.33-r0.apk \\\n    && apk add glibc-bin-2.33-r0.apk glibc-i18n-2.33-r0.apk glibc-2.33-r0.apk \\\n    && rm -rf /usr/lib/jvm glibc-2.29-r0.apk glibc-bin-2.29-r0.apk  glibc-i18n-2.29-r0.apk \\\n    && /usr/glibc-compat/bin/localedef --force --inputfile POSIX --charmap UTF-8 "$LANG" || true \\\n    && echo "export LANG=$LANG" > /etc/profile.d/locale.sh \\\n    && apk del glibc-i18n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 2、基于 Centos7 镜像\n\nFROM centos:7\n#安装必要应用\nRUN yum -y install kde-l10n-Chinese glibc-common\n#设置编码\nRUN localedef -c -f UTF-8 -i zh_CN zh_CN.utf8\n#设置环境变量\nENV LC_ALL zh_CN.utf8\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 3、基于 Debian 镜像\n\n# 由于 Debian 镜像中已经包含了tzdata，所以只需添加环境变量TZ即可。\nFROM debian:latest\n\nENV LANG C.UTF-8\nRUN apt-get update; \\\n    apt-get install -y --no-install-recommends fontconfig;\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 4、基于 Ubuntu 镜像\n\nFROM ubuntu:bionic\n\nENV LANG C.UTF-8\n\n\n1\n2\n3\n\n\n\n# 6、使用Label标注作者、软件版本等元信息\n\nFROM alpine:3.11.5\nLABEL Author=Curiouser \\\n      Mail=****@163.com \\\n      PHP=7.3 \\\n      Tools=“git、vim、curl” \\\n      Update="添加用户组"\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 7、指定工作区\n\nWORKDIR /var/wwww\n\n\n1\n\n\n\n# 8、RUN指令显示优化\n\nRUN set -eux ; \\\n    ls -al\n\n\n1\n2\n\n\n\n# 9、使用 URL 添加源码\n\n如果不采用分阶段构建，对于一些需要在容器内进行编译的项目，最好通过 git 或者 wegt 的方式将源码打入到镜像内，而非采用 ADD 或者 COPY ，因为源码编译完成之后，源码就不需要可以删掉了，而通过 ADD 或者 COPY 添加进去的源码已经用在下一层镜像中了，是删不掉滴啦。\n\n也就是说 git & wget source 然后 build，最后 rm -rf source/ 这三部放在一条 RUN 指令中，这样就能避免源码添加到镜像中而增大镜像体积啦。\n\n\n# 10、使用虚拟编译环境\n\n对于只在编译过程中使用到的依赖，我们可以将这些依赖安装在虚拟环境中，编译完成之后可以一并删除这些依赖，比如 alpine 中可以使用 apk add --no-cache --virtual .build-deps，后面加上需要安装的相关依赖。\n\napk add --no-cache --virtual .build-deps gcc libc-dev make perl-dev openssl-dev pcre-dev zlib-dev git\n\n\n1\n\n\n构建完成之后可以使用 apk del .build-deps 命令，一并将这些编译依赖全部删除。\n\n需要注意的是，.build-deps 后面接的是编译时以来的软件包，并不是所有的编译依赖都可以删除，不要把运行时的依赖包接在后面，最好单独 add 一下。\n\n\n# 11、最小化层数\n\ndocker 在 1.10 以后，只有 RUN、COPY 和 ADD 指令会创建层，其他指令会创建临时的中间镜像，但是不会直接增加构建的镜像大小了。\n\n前文提到了建议使用 git 或者 wget 的方式来将文件打入到镜像当中，但如果我们必须要使用 COPY 或者 ADD 指令呢？\n\n还是拿 FastDFS 为例：\n\n# centos 7\nFROM centos:7\n# 添加配置文件\n# add profiles\nADD conf/client.conf /etc/fdfs/\nADD conf/http.conf /etc/fdfs/\nADD conf/mime.types /etc/fdfs/\nADD conf/storage.conf /etc/fdfs/\nADD conf/tracker.conf /etc/fdfs/\nADD fastdfs.sh /home\nADD conf/nginx.conf /etc/fdfs/\nADD conf/mod_fastdfs.conf /etc/fdfs\n\n# 添加源文件\n# add source code\nADD source/libfastcommon.tar.gz /usr/local/src/\nADD source/fastdfs.tar.gz /usr/local/src/\nADD source/fastdfs-nginx-module.tar.gz /usr/local/src/\nADD source/nginx-1.15.4.tar.gz /usr/local/src/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n多个文件需要添加到容器中不同的路径，每个文件使用一条 ADD 指令的话就会增加一层镜像，这样戏曲就多了 12 层镜像 。\n\n其实大可不必，我们可以将这些文件全部打包为一个文件为 src.tar.gz 然后通过 ADD 的方式把文件添加到当中去，然后在 RUN 指令后使用 mv 命令把文件移动到指定的位置。这样仅仅一条 ADD 和 RUN 指令取代掉了 12 个 ADD 指令。\n\nFROM alpine:3.10\nCOPY src.tar.gz /usr/local/src.tar.gz\nRUN set -xe \\\n    && apk add --no-cache --virtual .build-deps gcc libc-dev make perl-dev openssl-dev pcre-dev zlib-dev tzdata \\\n    && cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\\n    && tar -xvf /usr/local/src.tar.gz -C /usr/local \\\n    && mv /usr/local/src/conf/fastdfs.sh /home/fastdfs/ \\\n    && mv /usr/local/src/conf/* /etc/fdfs \\\n    && chmod +x /home/fastdfs/fastdfs.sh \\\n    && rm -rf /usr/local/src/* /var/cache/apk/* /tmp/* /var/tmp/* $HOME/.cache\nVOLUME /var/fdfs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n其他最小化层数无非就是把构建项目的整个步骤弄成一条 RUN 指令，不过多条命令合并可以使用 && 或者 ; 这两者都可以，不过据我在 docker hub 上的所见所闻，使用 ; 的居多，尤其是官方的 Dockerfile。',normalizedContent:'# 一、dockerfile 简介\n\n什么是 dockerfile？\n\ndockerfile 是一个用来构建镜像的文本文件，文本内容包含了构建镜像所需的指令和说明。\n\n在构建时 docker engine 会在当前目录中查找名为 “dockerfile” 的文件，注意这个文件名必须为 “dockerfile” 首字母大写。当您运行 docker build 命令的时候，dockerfile 文件以及与 dockerfile 文件处于同级路径下的所有文件都将一并提交至 docker engine 来处理，docker engine 将逐行解析 dockerfile 文件，并执行相关操作。dockerfile 的解析是由上而下的顺序进行。\n\n注意您放置 dockerfile 的目录最好是一个空目录，或者只放置与本次构建相关的文件即可，因为在执行 build 时会将 dockerfile 以及 dockerfile 路径下的所有文件提交至 docker engine 处理，因此您不应该在 dockerfile 目录中放置一些无关的文件，这是无意义的并且这将会严重拖慢整个构建过程。\n\n\n# 二、dockerfile 指令\n\n\n# 1、常用指令\n\ndocker 镜像可以通过 docker hub 或者阿里云等仓库中获取，这些镜像是由官方或者社区人员提供的，对于 docker 用户来说并不能满足我们的需求，但是从无开始构建镜像成本大。常用的数据库、中间件、应用软件等都有现成的 docker 官方镜像或社区创建的镜像，我们只需要稍作配置就可以直接使用。\n\n使用现成镜像的好处除了省去自己做镜像的工作量外，更重要的是可以利用前人的经验。特别是使用那些官方镜像，因为 docker 的工程师知道如何更好的在容器中运行软件。\n\n当然，某些情况下我们也不得不自己构建镜像，比如找不到现成的镜像，比如自己开发的应用程序，需要在镜像中加入特定的功能。\n\n在编写 dockerfile 时，您需要用到 dockerfile 指令来定义构建过程中所要执行的操作，这里列举几个常用的指令：\n\n常用指令          说明\nfrom          from 指令是定义本阶段构建所要使用的基础镜像。\nmaintainer    镜像维护者姓名或邮箱地址\nrun           构建镜像时运行的 shell 命令\ncmd           运行容器时执行的 shell 命令\nexpose        仅仅只是声明端口，声明容器内应用所使用的端口号\nenv           设置容器环境变量，当然在您使用该镜像运行容器时，env 所定义的变量仍然对容器有效\nadd           拷贝文件或目录到镜像，如果是 url 或压缩包会自动下载或自动解压\ncopy          拷贝文件或目录到镜像容器内，跟 add 类似，但不具备自动下载或解压功能\nentrypoint    运行容器时执行的 shell 命令\nvolume        指定容器挂载点到宿主机自动生成的目录或其他容器\nuser          为 run、cmd、和 entrypoint 执行命令指定运行用户\nworkdir       为 run、cmd、entrypoint、copy 和 add 设置工作目录，意思为切换目录\nhealthcheck   健康检查\narg           构建时指定的一些参数\n\nrun、cmd 与 entrypoint 的区别\n\n * run：执行命令并创建新的镜像层，常用于安装软件包，在 docker build 过程中执行；\n * cmd：设置容器启动后默认执行的命令及其参数，在 docker run 时运行，但 docker run 后跟参数时会替换（忽略） cmd，cmd 可以被覆盖，如果有 entrypioint 的话，cmd 就是 entrypioint 的参数。；\n * entrypoint：配置容器启动时运行的命令，不管 docker run … 后是否运行有其他命令，entrypoint 指令后的命令一定会被执行， 后面如果再接命令，会报错多余的参数。\n * cmd 和 entrypioint 必须要有一个\n\n\n# 2、指令详解\n\n# 1、from\n\nfrom 指令是定义本阶段构建所要使用的基础镜像。例如您需要将您的 java 开发的项目构建为一个镜像，那么您可能需要 tomcat 镜像，而制作 tomcat 镜像就可能需要 jdk 镜像，那么 jdk 镜像就需要一个基本的操作系统镜像，这可能是 utuntu、debian、alpine 等。\n\n根据以上，我们应该可以知道由于容器镜像是可以增量叠加的，那么 tomcat 镜像中就应该包含了 jdk 以及一个基本的操作系统。\n\n您可以从 hub.docker.com 网站上来挑选一个合适的镜像来作为您的基础镜像。\n\n# 2、copy\n\n复制指令，从上下文目录中复制文件或者目录到容器里指定路径\n\n（上下文目录是指放置 dockerfile 文件的目录）\n\n格式如下：\n\ncopy [--chown=<user>:<group>] <源路径1>... <目标路径>\ncopy [--chown=<user>:<group>] ["<源路径1>",... "<目标路径>"]\n\n\n1\n2\n\n\n注意 [--chown=<user>:<group>]： 可选参数，用户改变复制到容器内文件的拥有者和属组。\n\n<源路径>：源文件或者源目录，这里可以是通配符表达式，其通配符规则要满足 go 的 filepath.match 规则。例如：\n\ncopy hom* /mydir/\n\ncopy hom?.txt /mydir/\n\n<目标路径>：容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。\n\n# 3、add\n\nadd 指令和 copy 的使用格式一致（同样需求下，官方推荐使用 copy）。功能也类似，不同之处如下：\n\nadd 的优点：在执行 <源文件> 为 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，会自动复制并解压到 < 目标路径 >。\n\nadd 的缺点：在不解压的前提下，无法复制 tar 压缩文件。会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。具体是否使用，可以根据是否需要自动解压来决定。\n\n对于使用 add 指令下载远程服务器上的 tar 包并解压，建议使用以下方式代替\n\nrun curl -s http://192.168.1.7/repository/tools/jdk-8u241-linux-x64.tar.gz | tar -xc /opt/\n\n\n1\n\n\n# 4、run\n\n当我们在构建镜像过程中，需要执行一些配置，例如使用 mkdir 命令新建一个目录，用 sed 命令来替换一些文本内容等，当然可能在实际过程中需要更复杂的命令，那么此时您可以使用 run 指令来定义将要运行的命令。\n\nrun 指令在 dockerfile 中可以出现多次，docker engine 在构建过程中会读取 dockerfile 然后由上而下依次执行 run 指令所标记的命令。\n\nrun 后面跟着的命令行命令。有以下俩种格式：\n\nshell 格式\n\nrun <命令行命令>\n# <命令行命令> 等同于，在终端操作的 shell 命令。\n\n\n1\n2\n\n\nexec 格式：\n\nrun ["可执行文件", "参数1", "参数2"]\n# 例如：\n# run ["./test.php", "dev", "offline"] 等价于 run ./test.php dev offline\n\n\n1\n2\n3\n\n\n注意在构建过程中 docker engine 会为 dockerfile 中的每个 run 指令创建一个镜像层（layer）来记录这种改变。因此为了减少不必要的镜像层数，通常的做法是将多个命令定义在一个 run 指令中，如下所示：\n\nrun mkdir demo \\\n    && cd demo \\\n    && wget https://www.demo.com/download/demo.tar.gz \\\n    && tar -xf demo.tar.gz \\\n    && rm -rf demo.tar.gz\n\n\n1\n2\n3\n4\n5\n\n\n如上，以 && 符号连接命令，这样执行后，只会创建 1 层镜像\n\n# 5、cmd\n\n类似于 run 指令，用于运行程序，但二者运行的时间点不同:\n\ncmd 在 docker run 时运行。\n\nrun 是在 docker build 过程中执行。\n\n作用：为启动的容器指定默认要运行的程序，程序运行结束，容器也就结束。cmd 指令指定的程序可被 docker run 命令行参数中指定要运行的程序所覆盖。\n\n> 注意\n> \n> 如果 dockerfile 中如果存在多个 cmd 指令，仅最后一个生效。\n\n格式：\n\ncmd <shell 命令> \ncmd ["<可执行文件或命令>","<param1>","<param2>",...] \ncmd ["<param1>","<param2>",...]  # 该写法是为 entrypoint 指令指定的程序提供默认参数\n\n\n1\n2\n3\n\n\n推荐使用第二种格式，执行过程比较明确。第一种格式实际上在运行的过程中也会自动转换成第二种格式运行，并且默认可执行文件是 sh。\n\n例如您有一个 nodejs 程序，启动该程序的命令是 node server.js，然后您希望在使用该镜像运行容器时也以这种方式运行，那么您可以在 dockerfile 中使用 cmd 来标记，如下：\n\ncmd ["node","server.js"]\n\n\n1\n\n\n通常 cmd 指令出现在 dockerfile 末尾处。当您将以上定义写入 dockerfile 中并打包成镜像（demo:v1），然后您使用该镜像运行时，那么该容器将使用 dockerfile 中 cmd 定义的命令来运行\n\ndocker  run  -d  --name=demo  deom:v1\n\n\n1\n\n\n当然您可以在运行时覆盖 cmd 指令中的命令，例如这里改成 npm start，命令如下\n\ndocker  run  -d  --name=demo  deom:v1  npm  start\n\n\n1\n\n\n# 6、entrypoint\n\n类似于 cmd 指令，但其不会被 docker run 的命令行参数指定的指令所覆盖，而且这些命令行参数会被当作参数送给 entrypoint 指令指定的程序。\n\n但是，如果运行 docker run 时使用了 --entrypoint 选项，将覆盖 cmd 指令指定的程序。\n\n优点：在执行 docker run 的时候可以指定 entrypoint 运行所需的参数。\n\n注意：如果 dockerfile 中如果存在多个 entrypoint 指令，仅最后一个生效。\n\n格式：\n\nentrypoint ["<executeable>","<param1>","<param2>",...]\n\n\n1\n\n\n可以搭配 cmd 命令使用：一般是变参才会使用 cmd ，这里的 cmd 等于是在给 entrypoint 传参，以下示例会提到。\n\n示例：\n\n假设已通过 dockerfile 构建了 nginx:test 镜像\n\nfrom nginx\nentrypoint ["nginx", "-c"] # 定参\ncmd ["/etc/nginx/nginx.conf"] # 变参\n\n\n1\n2\n3\n\n\n不传参运行\n\ndocker run nginx:test\n\n\n1\n\n\n容器内会默认运行以下命令，启动主进程。\n\nnginx -c /etc/nginx/nginx.conf\n\n\n1\n\n\n传参运行\n\ndocker run nginx:test -c /etc/nginx/new.conf\n\n\n1\n\n\n容器内会默认运行以下命令，启动主进程 (/etc/nginx/new.conf: 假设容器内已有此文件)\n\nnginx -c /etc/nginx/new.conf\n\n\n1\n\n\n通常更好的做法是使用 entrypoint 来定义一个 docker-entrypoint.sh 脚本，然后在该脚本中定义一些预处理及条件判断，来应对更多未知情况。例如 cups 镜像的 dockerfile\n\narg arch=amd64\nfrom $arch/debian:buster-slim\n# environment\nenv admin_password=admin\n# ……内容太多，中间内容已省略\nentrypoint [ "docker-entrypoint.sh" ]\n# default command\ncmd ["cupsd", "-f"]\n# volumes\nvolume ["/etc/cups"]\n# ports\nexpose 631\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n该镜像的 docker-entrypoint.sh 文件内容如下：\n\n#!/bin/bash -e\necho -e "${admin_password}\\n${admin_password}" | passwd admin\nif [ ! -f /etc/cups/cupsd.conf ]; then\n  cp -rpn /etc/cups-skel/* /etc/cups/\nfi\nexec "$@"\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 7、env\n\n设置环境变量，定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。当然在您使用该镜像运行容器时，env 所定义的变量仍然对容器有效，例如 mysql 镜像的 dockerfile 中就包含 root 用户的初始密码，使用 env 来指定。\n\n格式：\n\nenv <key> <value>\nenv <key1>=<value1> <key2>=<value2>...\n\n\n1\n2\n\n\n以下示例设置 node_version = 16.19.0 ， 在后续的指令中可以通过 $node_version 引用：\n\nenv node_version 16.19.0\nrun curl -slo "https://nodejs.org/download/release/latest-v16.x/node-v$node_version-linux-x64.tar.gz" \n\n\n1\n2\n\n\ndockerfile 中 env 指令像 run 指令一样，每一个都会创建一个临时层。\n\nenv java_home=/opt/jdk1.8.0_241 \\\n    classpath=.:$java_home/lib:$java_home/jre/lib \nenv path=$path:$java_home/bin\n\n\n1\n2\n3\n\n\n# 8、arg\n\n构建参数，与 env 作用一至。不过作用域不一样。arg 设置的环境变量仅对 dockerfile 内有效，也就是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量。\n\n构建命令 docker build 中可以用 --build-arg <参数名>=< 值 > 来覆盖。\n\n格式\n\narg <参数名>[=<默认值>]\n\n\n1\n\n\n# 9、volume\n\n定义匿名数据卷。在启动容器时忘记挂载数据卷，会自动挂载到匿名卷。\n\n作用：\n\n避免重要的数据，因容器重启而丢失，这是非常致命的。\n\n避免容器不断变大。\n\n格式：\n\nvolume ["<路径1>", "<路径2>"...]\nvolume <路径\n\n\n1\n2\n\n\n在启动容器 docker run 的时候，我们可以通过 -v 参数修改挂载点。\n\n# 10、expose\n\n仅仅只是声明端口。\n\n作用：\n\n帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射。\n\n在运行时使用随机端口映射时，也就是 docker run -p 时，会自动随机映射 expose 的端口。\n\n格式：\n\nexpose <端口1> [<端口2>...]\n\n\n1\n\n\n# 11、workdir\n\n指定工作目录。用 workdir 指定的工作目录，会在构建镜像的每一层中都存在。（workdir 指定的工作目录，如果目录不存在则会自动创建）。\n\n格式：\n\nworkdir <工作目录路径>\n\n\n1\n\n\n# 12、user\n\n用于指定执行后续命令的用户和用户组，这边只是切换后续命令执行的用户（用户和用户组必须提前已经存在）。\n\n格式\n\nuser <用户名>[:<用户组>]\n\n\n1\n\n\n# 13、healthcheck\n\n用于指定某个程序或者指令来监控 docker 容器服务的运行状态。\n\n格式：\n\nhealthcheck [选项] cmd <命令>：设置检查容器健康状况的命令\nhealthcheck none：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令\nhealthcheck [选项] cmd <命令> : 这边 cmd 后面跟随的命令使用，可以参考 cmd 的用法。\nhealthcheck 支持下列选项：\n--interval=<间隔>：两次健康检查的间隔，默认为 30 秒；\n--timeout=<时长>：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒；\n--retries=<次数>：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n和 cmd, entrypoint 一样，healthcheck 只可以出现一次，如果写了多个，只有最后一个生效。\n\n在 healthcheck [选项] cmd 后面的命令，格式和 entrypoint 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：0：成功；1：失败；2：保留，不要使用这个值。\n\n假设我们有个镜像是个最简单的 web 服务，我们希望增加健康检查来判断其 web 服务是否在正常工作，我们可以用 curl 来帮助判断，其 dockerfile 的 healthcheck 可以这么写：\n\nfrom nginx\nrun apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*\nhealthcheck --interval=5s --timeout=3s \\\n  cmd curl -fs http://localhost/ || exit 1\n\n\n1\n2\n3\n4\n\n\n当然您可以在 docker run 命令中，直接指明 healthcheck 相关策略，如下：\n\ndocker run --rm -d \\\n    --name=elasticsearch \\\n    --health-cmd="curl --silent --fail localhost:9200/_cluster/health || exit 1" \\\n    --health-interval=5s \\\n    --health-retries=12 \\\n    --health-timeout=2s \\\n    elasticsearch:7\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 14、onbuild\n\n用于延迟构建命令的执行。简单的说，就是 dockerfile 里用 onbuild 指定的命令，在本次构建镜像的过程中不会执行（假设镜像为 test-build）。当有新的 dockerfile 使用了之前构建的镜像 from test-build ，这是执行新镜像的 dockerfile 构建时候，会执行 test-build 的 dockerfile 里的 onbuild 指定的命令。\n\n格式：\n\nonbuild <其它指令>\n\n\n1\n\n\n假设我们要制作 node.js 所写的应用的镜像。我们都知道 node.js 使用 npm 进行包管理，所有依赖、配置、启动信息等会放到 package.json 文件里。在拿到程序代码后，需要先进行 npm install 才可以获得所有需要的依赖。将项目相关的指令加上 onbuild，这样在构建基础镜像的时候，这三行并不会被执行。基础镜像变化后，各个项目都用这个 dockerfile 重新构建镜像，会继承基础镜像的更新。\n\nfrom node:slim\nrun mkdir /app\nworkdir /app\nonbuild copy ./package.json /app\nonbuild run [ "npm", "install" ]\nonbuild copy . /app/\ncmd [ "npm", "start" ]\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 15、label\n\nlabel 指令用来给镜像以键值对的形式添加一些元数据（metadata）\n\n格式：\n\nlabel <key>=<value> <key>=<value> <key>=<value> ...\n\n\n1\n\n\n我们还可以用一些标签来申明镜像的作者、文档地址等，例如：\n\nlabel org.opencontainers.image.authors="deamon"\nlabel org.opencontainers.image.documentation="https://daemon.gitbooks.io"\n或者\nlabel vendor=acme\\ incorporated \\\n      com.example.is-beta= \\\n      com.example.is-production="" \\\n      com.example.version="0.0.1-beta" \\\n      com.example.release-date="2015-02-12"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 16、.dockerignore 文件\n\n执行 docker build 命令时，当前的工作目录被称为构建上下文。默认情况下，dockerfile 就位于该路径下。也可以通过 -f 参数来指定 dockerfile ，但 docker 客户端会将当前工作目录下的所有文件发送到 docker 守护进程进行构建。\n\n所以来说，当执行 docker build 进行构建镜像时，当前目录一定要 干净 ，切记不要在家里录下创建一个 dockerfile 紧接着 docker build 一把梭。\n\n正确做法是为项目建立一个文件夹，把构建镜像时所需要的资源放在这个文件夹下。比如这样：\n\nmkdir project\ncd !$\nvi dockerfile\n# 编写 dockerfile\n\n\n1\n2\n3\n4\n\n\n也可以通过 .dockerignore 文件来忽略不需要的文件发送到 docker 守护进程\n\n在 docker cli 将上下文发送到 docker 守护程序之前，它会在上下文的根目录中查找一个名为.dockerignore 的文件。如果此文件存在，cli 将修改上下文以排除与其中模式匹配的文件和目录。这有助于避免不必要地将大型或敏感文件和目录发送到守护程序，并可能使用或将它们添加到镜像中。\n\n例如：\n\n# comment\n*/temp*\n*/*/temp*\ntemp?\n*.md\nreadme-secret.md\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 三、镜像优化\n\n\n# 1、基础镜像选择\n\n使用体积较小的基础镜像，比如 alpine 或者 debian:buster-slim，像 openjdk 可以选用 openjdk:xxx-slim，由于 openjdk 是基于 debian 的基础镜像构建的，所以向 debian 基础镜像一样，后面带个 slim 就是基于 debian:xxx-slim 镜像构建的。\n\n目前 docker 官方已开始推荐使用 alpine 替代之前的 ubuntu 做为基础镜像环境。这样会带来多个好处。包括镜像下载速度加快，镜像安全性提高，主机之间的切换更方便，占用更少磁盘空间等。\n\nrepository                  tag                 image id            created             size\ndebian                      buster-slim         e1af56d072b8        4 days ago          69.2mb\nalpine                      latest              cc0abc535e36        8 days ago          5.59mb\n\n\n1\n2\n3\n\n\n不过需要注意的是，alpine 的 c 库是 musl libc ，而不是正统的 glibc，另外对于一些依赖 glibc 的大型项目，像 openjdk 、tomcat、rabbitmq 等都不建议使用 alpine 基础镜像，因为 musl libc 可能会导致 jvm 一些奇怪的问题。这也是为什么 tomcat 官方没有给出基础镜像是 alpine 的 dockerfile 的原因。\n\n制作前端镜像时一定不要使用centos、ubuntu等系统镜像，我们可以直接使用官方提供的nginx镜像来作为基础镜像，这样我们只需把制作好的web静态文件拷贝一下就可以了。\n\n您可以在hub.docker.com网站上搜索更小的镜像或者说更符合您要求的镜像，然后作为基础镜像来完成构建。\n\n基础镜像      优点                  缺点                         备注\nalpine    占用空间小               基于musl libc和busybox        官方推荐\nbusybox   占用空间小，极度轻量版         工具太少                       不推荐，组件不全\nscratch   空镜像，镜像大小约等于执行文件大小   没有sh或bash，无法进入容器内进行交互式调试   适合go语言\n\ndocker镜像常见的参数\n\n参数               说明\nbuster           适用与 debian 10\nstretch          适用于 debian 9\njessie           适用于 debian 8\nslim             表示最小安装包，仅包含需要运行指定容器的特定工具集\nalphine/alpine   专门为容器构建的操作系统，比其他的操作系统更小，但是其上会缺少很多软件包并且使用的 glibc 等都是阉割版\nbullseye         开发版本，处于未稳定状态\n\n\n# 2、配置国内软件源\n\n使用默认的软件源安装构建时所需的依赖，对于绝大多数基础镜像来说，可以通过修改软件源的方式更换为国内的软件源镜像站。目前国内稳定可靠的镜像站主要有，华为云、阿里云、腾讯云、163 等。\n\n对于 alpine 基础镜像修改软件源\n\nrun sed -i \'s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\' /etc/apk/repositories\n\n\n1\n\n\ndebian 基础镜像修改默认软件源\n\nrun sed -i s@/archive.ubuntu.com/@/mirrors.aliyun.com/@g /etc/apt/sources.list\nrun sed -i "s@http://deb.debian.org@http://mirrors.aliyun.com@g" /etc/apt/sources.list && \\\n    rm -rf /var/lib/apt/lists/* && \\\n\n\n1\n2\n3\n\n\nubuntu 基础镜像修改默认软件源\n\nrun sed -i \'s/archive.ubuntu.com/mirrors.aliyun.com/g\' /etc/apt/sources.list && \\\n    sed -i \'s/security.ubuntu.com/mirrors.aliyun.com/g\' /etc/apt/sources.list\n\n\n1\n2\n\n\n对于 centos ???\n\n你确定要用 230mb 大小的基础镜像？\n\nrepository                               tag           image id       created          size\ncentos                                   latest        5d0da3dc9764   16 months ago    231mb\n\n\n1\n2\n\n\n建议这些命令就放在 run 指令的第一条，update 以下软件源，之后再 install 相应的依赖。\n\n\n# 四、构建镜像\n\n\n# 1、命名\n\n原则是见名知意。可使用三段式\n\n> 镜像仓库地址/类型库/镜像名:版本号\n\n * registry/runtime/java:8.1.2\n * registry/runtime/php-fpm-nginx:7.3-1.14\n * registry/cicd/kubctl-helm:1.17-3.0\n * registry/cicd/git-compose-docker:v1\n * registry/applications/demo:git_commit_id\n\n\n# 2、基于镜像部署服务\n\n那么现在我们就可以利用以上指令，将我们在宿主机上执行的操作，写在一个dockerfile中，示例如下：\n\nfrom centos:7\nmaintainer 759600963@qq.com\n#执行下面命令，安装基础环境\nrun yum install -y gcc gcc-c++ make \\\n    openssl-devel pcre-devel gd-devel \\\n    iproute net-tools telnet wget curl && \\\n    yum clean all && \\\n    rm -rf /var/cache/yum/*\nrun wget http://nginx.org/download/nginx-1.20.1.tar.gz\nrun tar -zxf nginx-1.20.1.tar.gz -c /usr/src\nrun useradd -m -s /sbin/nologin nginx\nworkdir /usr/src/nginx-1.20.1\nrun ./configure --prefix=/usr/local/nginx --user=nginx --group=nginx && make && make install\nrun ln -s /usr/local/nginx/sbin/* /usr/local/sbin/\nrun ln -sf /usr/share/zoneinfo/asia/shanghai /etc/localtime\nworkdir /usr/local/nginx\nexpose 80\ncmd ["nginx", "-g", "daemon off;"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n如上所示，from定义了基于centos7镜像来构建，这里使用了yum方式来安装依赖，并通过源码编译的方式进行安装。然后使用workdir定义了进程的工作目录，使用expose声明了应用要使用的端口，最后使用cmd指定了容器启动时默认命令。\n\n# docker pull centos:7\n\n\n1\n\n\n那么此时我们可以创建一个空目录例如nginx，然后将dockerfile文件放置于该目录中，然后执行构建命令\n\n# docker build -t web:centos .\n\n\n1\n\n\n注意\n\n这里的-t参数是为这个构建的镜像取一个名字，设置一个标签，在自动生成镜像的命令指定镜像后，一定不要忘记写新生成镜像的存放路径，也就是空格后的 一 个“.”代表当前路径，否则会报错。\n\n当我们执行build后，docker会将build文件夹连同dockerfile一起所有的文件（如果有其他文件）提交至docker engine来处理。docker engine将逐行解析dockerfile文件，并执行相关操作。dockerfile的解析是由上而下的顺序进行。\n\n注意您放置dockerfile的目录最好是一个空目录，或者只放置与本次构建相关的文件即可，因为在执行build时会将dockerfile以及dockerfile路径下的所有文件提交至docker engine处理，因此您不应该在dockerfile目录中放置一些无关的文件，这是无意义的并且这将会严重拖慢整个构建过程。\n\n使用新的镜像运行容器\n\n# docker run -itd --name testweb -p 80:80 web:centos\n73277ceaa7a9a72d7a729029360413cfc0364effb4062b3df25aa960c67b894a\n\n\n1\n2\n\n\n\n# 3、使用makefile操作dockerfile\n\nimage_base = registry/runtime\nimage_name = php-fpm\nimage_version = 7.3\nimage_tagversion = $(git_commit)\n\nall: build tag push\n\nbuild:\n  docker build --rm -f dockerfile -t ${image_base}/${image_name}:${image_version} .\n\ntag:\n  docker tag ${image_base}/${image_name}:${image_version} ${image_base}/${image_name}:${image_tagversion}\n\npush:\n  docker push ${image_base}/${image_name}:${image_tagversion}\n\n# 构建并推送\nmake \n# 仅构建\nmake build \n# 仅打tag\nmake tag\n# 仅推送\nmake push\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nmakefile中的命令必须以tab作为开头(分隔符),不能用扩展的tab即用空格代替的tab。(如果是vim编辑的话,执行 set noexpandtab)。否则会报如下错误：makefile:10: *** multiple target patterns. stop.\n\n\n# 五、多阶段构建镜像\n\n在编写dockerfile构建docker镜像时，常遇到以下问题：\n\n 1. run命令会让镜像新增layer，导致镜像变大，虽然通过&&连接多个命令能缓解此问题，但如果命令之间用到docker指令例如copy、workdir等，依然会导致多个layer；\n 2. 有些工具在构建过程中会用到，但是最终的镜像是不需要的（例如用npm编译构建前端工程），这要求dockerfile的编写者花更多精力来清理这些工具，清理的过程又可能导致新的layer；\n\n为了解决上述问题，从17.05版本开始docker在构建镜像时增加了新特性：多阶段构建(multi-stage builds)，将构建过程分为多个阶段，每个阶段都可以指定一个基础镜像，这样在一个dockerfile就能将多个镜像的特性同时用到\n\n我们可以在一个dockerfile中使用多个from指令并将一次构建分成多个阶段来完成\n\n# build stage\nfrom node:14.18.2-stretch-slim as build-env\n# 工作目录\nworkdir /app\n# 将git仓库下所有文件拷贝到工作目录\ncopy . .\nrun npm install\nrun npm audit fix\nrun npm run build:production\n\n# production stage\n# 生产环境基础nginx镜像（上面的镜像已经打包为了静态文件）\nfrom nginx:alpine\nadd  prod.conf /etc/nginx/conf.d/\n# 使用--from把上面产生的静态文件复制到nginx的运行目录\ncopy --from=build-env /app/dist /usr/share/nginx/html\nrun ln -sf /usr/share/zoneinfo/asia/shanghai /etc/localtime && \\\n    echo \'asia/shanghai\' >/etc/timezone\n# nginx容器内部暴露的端口\nexpose 80\n# 运行的命令\ncmd ["/bin/sh", "-c", "nginx -g \\"daemon off;\\"" ]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n在第一个from指令后使用as build-env 意思是为该阶段取一个标记名为build-env，然后在第二个from之后使用了copy --from=build-env，意思是从上一个名为builder的阶段中拷贝文件至本阶段中。\n\n\n# 六、dockerfile 传参\n\n使用 arg 和 build-arg 传入动态变量：\n\n# base image \nfrom centos:7 \n# maintainer dot # deprecated \n\nlabel maintainer="dot" version="demo" \nlabel multiple="true" \n\narg username \narg dir="defaultvalue" \n\nrun useradd -m $username -u 1001 && mkdir $dir \n\n# docker build --build-arg username="test_arg" -t test:arg . \n# docker run -ti --rm test:arg bash \n# ls \nbin             dev  home  lib64       media  opt   root  selinux  sys  usr \ndefaultvalue  etc  lib   lost+found  mnt    proc  sbin  srv      tmp  var\n# tail -1 /etc/passwd \ntest_arg:x:1001:1001::/home/test_arg:/bin/bash\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 七、dockerfile 优化\n\n\n# 1、尽量不使用root用户\n\n在做基础运行时镜像时，创建运行时普通用户和用户组，并做工作区与权限限制，启动服务时尽量使用普通用户。\n\ngosu\n\nfrom alpine:3.11.5\nrun sed -i "s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g" /etc/apk/repositories \\\n    && apk add --no-cache gosu\n\n\n1\n2\n3\n\n\n\n# 2、移除所有缓存等不必要信息\n\n * 删除解压后的源压缩包（参考第二章第二节）\n * 清理包管理器下载安装软件时的缓存\n   * 使用alipine镜像中apk命令安装包时记得加上--no-cache\n   * 使用ubuntu镜像中的apt命令安装软件后记得 rm -rf /var/lib/apt/lists/*\n\n\n# 3、使用合理的entrypoint脚本\n\n示例：\n\n#!/bin/bash\nset -e\n\nif [ "$1" = \'postgres\' ]; then\n    chown -r postgres "$pgdata"\n\n    if [ -z "$(ls -a "$pgdata")" ]; then\n        gosu postgres initdb\n    fi\n\n    exec gosu postgres "$@"\nfi\n\nexec "$@"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 4、设置时区\n\n由于绝大多数基础镜像都是默认采用 utc 的时区，与北京时间相差 8 个小时，这将会导致容器内的时间与北京时间不一致，因而会对一些应用造成一些影响，还会影响容器内日志和监控的数据。\n\n因此对于东八区的用户，最好在构建镜像的时候设定一下容器内的时区，以免以后因为时区遇到一些 bug。\n\n可以通过环境变量设置容器内的时区。在启动的时候可以通过设置环境变量 -e tz=asia/shanghai 来设定容器内的时区。\n\n# 1、基于 alpine 镜像\n\nfrom alpine:3.15\nenv tz=asia/shanghai\nrun sed -i \'s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\' /etc/apk/repositories \\\n    && apk add --no-cache tzdata \\\n    && ln -sf /usr/share/zoneinfo/asia/shanghai /etc/localtime  \\\n    && echo "asia/shanghai" > /etc/timezone\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 2、基于 centos7 镜像\n\nfrom centos:7\n#定义时区参数\nenv tz=asia/shanghai\n#设置时区\nrun ln -snf /usr/share/zoneinfo/$tz /etc/localtime && echo \'$tz\' > /etc/timezone\n\n\n1\n2\n3\n4\n5\n\n\n# 3、基于 debian 镜像\n\n# 由于 debian 镜像中已经包含了tzdata，所以只需添加环境变量tz即可。\nfrom debian:latest\n\nenv tz=asia/shanghai\n\n\n1\n2\n3\n4\n\n\n# 4、基于 ubuntu 镜像\n\nfrom ubuntu:bionic\n\nenv tz=asia/shanghai\nrun echo "${tz}" > /etc/timezone\n    && ln -sf /usr/share/zoneinfo/${tz} /etc/localtime\n    && apt update\n    && apt install -y tzdata\n    && rm -rf /var/lib/apt/lists/*\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 5、设置系统语言\n\n# 1、基于 alpine 镜像\n\nfrom alpine:3.15\nenv lang=en_us.utf-8 \\\n    language=en_us.utf-8\n\nrun apk --no-cache add ca-certificates wget \\ \n    && wget -q -o /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \\ \n    && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.33-r0/glibc-2.33-r0.apk \\ \n    && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.33-r0/glibc-bin-2.33-r0.apk \\\n    && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.33-r0/glibc-i18n-2.33-r0.apk \\\n    && apk add glibc-bin-2.33-r0.apk glibc-i18n-2.33-r0.apk glibc-2.33-r0.apk \\\n    && rm -rf /usr/lib/jvm glibc-2.29-r0.apk glibc-bin-2.29-r0.apk  glibc-i18n-2.29-r0.apk \\\n    && /usr/glibc-compat/bin/localedef --force --inputfile posix --charmap utf-8 "$lang" || true \\\n    && echo "export lang=$lang" > /etc/profile.d/locale.sh \\\n    && apk del glibc-i18n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 2、基于 centos7 镜像\n\nfrom centos:7\n#安装必要应用\nrun yum -y install kde-l10n-chinese glibc-common\n#设置编码\nrun localedef -c -f utf-8 -i zh_cn zh_cn.utf8\n#设置环境变量\nenv lc_all zh_cn.utf8\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 3、基于 debian 镜像\n\n# 由于 debian 镜像中已经包含了tzdata，所以只需添加环境变量tz即可。\nfrom debian:latest\n\nenv lang c.utf-8\nrun apt-get update; \\\n    apt-get install -y --no-install-recommends fontconfig;\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 4、基于 ubuntu 镜像\n\nfrom ubuntu:bionic\n\nenv lang c.utf-8\n\n\n1\n2\n3\n\n\n\n# 6、使用label标注作者、软件版本等元信息\n\nfrom alpine:3.11.5\nlabel author=curiouser \\\n      mail=****@163.com \\\n      php=7.3 \\\n      tools=“git、vim、curl” \\\n      update="添加用户组"\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 7、指定工作区\n\nworkdir /var/wwww\n\n\n1\n\n\n\n# 8、run指令显示优化\n\nrun set -eux ; \\\n    ls -al\n\n\n1\n2\n\n\n\n# 9、使用 url 添加源码\n\n如果不采用分阶段构建，对于一些需要在容器内进行编译的项目，最好通过 git 或者 wegt 的方式将源码打入到镜像内，而非采用 add 或者 copy ，因为源码编译完成之后，源码就不需要可以删掉了，而通过 add 或者 copy 添加进去的源码已经用在下一层镜像中了，是删不掉滴啦。\n\n也就是说 git & wget source 然后 build，最后 rm -rf source/ 这三部放在一条 run 指令中，这样就能避免源码添加到镜像中而增大镜像体积啦。\n\n\n# 10、使用虚拟编译环境\n\n对于只在编译过程中使用到的依赖，我们可以将这些依赖安装在虚拟环境中，编译完成之后可以一并删除这些依赖，比如 alpine 中可以使用 apk add --no-cache --virtual .build-deps，后面加上需要安装的相关依赖。\n\napk add --no-cache --virtual .build-deps gcc libc-dev make perl-dev openssl-dev pcre-dev zlib-dev git\n\n\n1\n\n\n构建完成之后可以使用 apk del .build-deps 命令，一并将这些编译依赖全部删除。\n\n需要注意的是，.build-deps 后面接的是编译时以来的软件包，并不是所有的编译依赖都可以删除，不要把运行时的依赖包接在后面，最好单独 add 一下。\n\n\n# 11、最小化层数\n\ndocker 在 1.10 以后，只有 run、copy 和 add 指令会创建层，其他指令会创建临时的中间镜像，但是不会直接增加构建的镜像大小了。\n\n前文提到了建议使用 git 或者 wget 的方式来将文件打入到镜像当中，但如果我们必须要使用 copy 或者 add 指令呢？\n\n还是拿 fastdfs 为例：\n\n# centos 7\nfrom centos:7\n# 添加配置文件\n# add profiles\nadd conf/client.conf /etc/fdfs/\nadd conf/http.conf /etc/fdfs/\nadd conf/mime.types /etc/fdfs/\nadd conf/storage.conf /etc/fdfs/\nadd conf/tracker.conf /etc/fdfs/\nadd fastdfs.sh /home\nadd conf/nginx.conf /etc/fdfs/\nadd conf/mod_fastdfs.conf /etc/fdfs\n\n# 添加源文件\n# add source code\nadd source/libfastcommon.tar.gz /usr/local/src/\nadd source/fastdfs.tar.gz /usr/local/src/\nadd source/fastdfs-nginx-module.tar.gz /usr/local/src/\nadd source/nginx-1.15.4.tar.gz /usr/local/src/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n多个文件需要添加到容器中不同的路径，每个文件使用一条 add 指令的话就会增加一层镜像，这样戏曲就多了 12 层镜像 。\n\n其实大可不必，我们可以将这些文件全部打包为一个文件为 src.tar.gz 然后通过 add 的方式把文件添加到当中去，然后在 run 指令后使用 mv 命令把文件移动到指定的位置。这样仅仅一条 add 和 run 指令取代掉了 12 个 add 指令。\n\nfrom alpine:3.10\ncopy src.tar.gz /usr/local/src.tar.gz\nrun set -xe \\\n    && apk add --no-cache --virtual .build-deps gcc libc-dev make perl-dev openssl-dev pcre-dev zlib-dev tzdata \\\n    && cp /usr/share/zoneinfo/asia/shanghai /etc/localtime \\\n    && tar -xvf /usr/local/src.tar.gz -c /usr/local \\\n    && mv /usr/local/src/conf/fastdfs.sh /home/fastdfs/ \\\n    && mv /usr/local/src/conf/* /etc/fdfs \\\n    && chmod +x /home/fastdfs/fastdfs.sh \\\n    && rm -rf /usr/local/src/* /var/cache/apk/* /tmp/* /var/tmp/* $home/.cache\nvolume /var/fdfs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n其他最小化层数无非就是把构建项目的整个步骤弄成一条 run 指令，不过多条命令合并可以使用 && 或者 ; 这两者都可以，不过据我在 docker hub 上的所见所闻，使用 ; 的居多，尤其是官方的 dockerfile。',charsets:{cjk:!0}},{title:"如何选择docker基础镜像",frontmatter:{title:"如何选择docker基础镜像",date:"2023-01-30T16:28:23.000Z",permalink:"/pages/8884ac/",categories:["运维","docker"],tags:[null],readingShow:"top",description:"因为Docker镜像是基于基础镜像来构建的，因此选择的基础镜像越高级，我们要做的底层工作就越少。",meta:[{name:"twitter:title",content:"如何选择docker基础镜像"},{name:"twitter:description",content:"因为Docker镜像是基于基础镜像来构建的，因此选择的基础镜像越高级，我们要做的底层工作就越少。"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/10.docker/03.%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9docker%E5%9F%BA%E7%A1%80%E9%95%9C%E5%83%8F.html"},{property:"og:type",content:"article"},{property:"og:title",content:"如何选择docker基础镜像"},{property:"og:description",content:"因为Docker镜像是基于基础镜像来构建的，因此选择的基础镜像越高级，我们要做的底层工作就越少。"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/10.docker/03.%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9docker%E5%9F%BA%E7%A1%80%E9%95%9C%E5%83%8F.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-30T16:28:23.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"如何选择docker基础镜像"},{itemprop:"description",content:"因为Docker镜像是基于基础镜像来构建的，因此选择的基础镜像越高级，我们要做的底层工作就越少。"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/10.docker/03.%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9docker%E5%9F%BA%E7%A1%80%E9%95%9C%E5%83%8F.html",relativePath:"04.运维/10.docker/03.如何选择docker基础镜像.md",key:"v-573ab138",path:"/pages/8884ac/",headers:[{level:3,title:"镜像官网",slug:"镜像官网",normalizedTitle:"镜像官网",charIndex:137},{level:3,title:"操作系统基础镜像",slug:"操作系统基础镜像",normalizedTitle:"操作系统基础镜像",charIndex:350},{level:3,title:"busybox",slug:"busybox",normalizedTitle:"busybox",charIndex:425},{level:3,title:"Alpine",slug:"alpine",normalizedTitle:"alpine",charIndex:781},{level:3,title:"CentOS",slug:"centos",normalizedTitle:"centos",charIndex:511},{level:3,title:"Ubuntu",slug:"ubuntu",normalizedTitle:"ubuntu",charIndex:1157},{level:3,title:"Debian",slug:"debian",normalizedTitle:"debian",charIndex:1274},{level:3,title:"编程语言基础镜像",slug:"编程语言基础镜像",normalizedTitle:"编程语言基础镜像",charIndex:1392},{level:4,title:"java基础镜像",slug:"java基础镜像",normalizedTitle:"java基础镜像",charIndex:1404},{level:4,title:"python基础镜像",slug:"python基础镜像",normalizedTitle:"python基础镜像",charIndex:1454},{level:4,title:"nodejs基础镜像",slug:"nodejs基础镜像",normalizedTitle:"nodejs基础镜像",charIndex:1505},{level:3,title:"应用基础镜像",slug:"应用基础镜像",normalizedTitle:"应用基础镜像",charIndex:1555},{level:4,title:"nginx基础镜像",slug:"nginx基础镜像",normalizedTitle:"nginx基础镜像",charIndex:1565},{level:4,title:"tomcat基础镜像",slug:"tomcat基础镜像",normalizedTitle:"tomcat基础镜像",charIndex:1614},{level:4,title:"jetty基础镜像",slug:"jetty基础镜像",normalizedTitle:"jetty基础镜像",charIndex:1665},{level:3,title:"其它基础镜像例子",slug:"其它基础镜像例子",normalizedTitle:"其它基础镜像例子",charIndex:1715},{level:4,title:"maven基础镜像",slug:"maven基础镜像",normalizedTitle:"maven基础镜像",charIndex:1727},{level:4,title:"jenkins基础镜像",slug:"jenkins基础镜像",normalizedTitle:"jenkins基础镜像",charIndex:1776},{level:4,title:"gitlab基础镜像",slug:"gitlab基础镜像",normalizedTitle:"gitlab基础镜像",charIndex:1837}],headersStr:"镜像官网 操作系统基础镜像 busybox Alpine CentOS Ubuntu Debian 编程语言基础镜像 java基础镜像 python基础镜像 nodejs基础镜像 应用基础镜像 nginx基础镜像 tomcat基础镜像 jetty基础镜像 其它基础镜像例子 maven基础镜像 jenkins基础镜像 gitlab基础镜像",content:"# 如何选择docker基础镜像\n\n因为Docker镜像是基于基础镜像来构建的，因此选择的基础镜像越高级，我们要做的底层工作就越少。\n\n比如，如果构建一个Java应用的镜像，选择一个openjdk的镜像作为基础镜像比选择一个alpine镜像作为基础镜像要简单地多。\n\n\n# 镜像官网\n\n * Docker镜像官网（Docker Hub）: https://hub.docker.com\n\n * 阿里云容器Hub：https://dev.aliyun.com\n\n * Google镜像（gcr.io）：https://console.cloud.google.com/gcr/images/google-containers/GLOBAL （需要科学上网，主要为Kubernetes相关镜像）\n\n\n# 操作系统基础镜像\n\n比如你要从Linux操作系统基础镜像开始构建，可以参考下表来选择合适的基础镜像：\n\n镜像名称      大小      使用场景\nbusybox   1.15M   临时使用\nalpine    4.41M   主要用于测试，也可用于生产环境\ncentos    200M    主要用于生产环境，支持CentOS/Red Hat，常用于追求稳定性的企业应用\nutuntu    81.1M   主要用于生产环境，常用于人工智能计算和企业应用\ndebian    101M    主要用于生产环境\n\n\n# busybox\n\n描述：可以将busybox理解为一个超级简化版嵌入式Linux系统。\n\n官网：https://www.busybox.net/\n\n镜像：https://hub.docker.com/_/busybox/\n\n包管理命令：apk, lbu\n\n包管理文档：https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management\n\n\n# Alpine\n\n描述：Alpine是一个面向安全的、轻量级的Linux系统，基于musl libc和busybox。\n\n官网：https://www.alpinelinux.org/\n\n镜像：https://hub.docker.com/_/alpine/\n\n包管理命令：apk, lbu\n\n包管理文档：https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management\n\n\n# CentOS\n\n描述：可以理解CentOS是RedHat的社区版\n\n官网：https://www.centos.org/\n\n镜像：https://hub.docker.com/_/centos/\n\n包管理命令：yum, rpm\n\n\n# Ubuntu\n\n描述：另一个非常出色的Linux发行版\n\n官网：http://www.ubuntu.com/\n\n镜像：https://hub.docker.com/_/ubuntu/\n\n包管理命令：apt-get, dpkg\n\n\n# Debian\n\n描述：另一个非常出色的Linux发行版\n\n官网：https://www.debian.org/\n\n镜像：https://hub.docker.com/_/debian/\n\n包管理命令：apt-get, dpkg\n\n\n# 编程语言基础镜像\n\n# java基础镜像\n\n * https://hub.docker.com/_/openjdk/\n\n# python基础镜像\n\n * https://hub.docker.com/_/python/\n\n# nodejs基础镜像\n\n * https://hub.docker.com/_/node/\n\n\n# 应用基础镜像\n\n# nginx基础镜像\n\n * https://hub.docker.com/_/nginx/\n\n# tomcat基础镜像\n\n * https://hub.docker.com/_/tomcat/\n\n# jetty基础镜像\n\n * https://hub.docker.com/_/jetty/\n\n\n# 其它基础镜像例子\n\n# maven基础镜像\n\n * https://hub.docker.com/_/maven/\n\n# jenkins基础镜像\n\n * https://hub.docker.com/r/jenkins/jenkins/\n\n# gitlab基础镜像\n\n * https://hub.docker.com/r/gitlab/gitlab-ce/",normalizedContent:"# 如何选择docker基础镜像\n\n因为docker镜像是基于基础镜像来构建的，因此选择的基础镜像越高级，我们要做的底层工作就越少。\n\n比如，如果构建一个java应用的镜像，选择一个openjdk的镜像作为基础镜像比选择一个alpine镜像作为基础镜像要简单地多。\n\n\n# 镜像官网\n\n * docker镜像官网（docker hub）: https://hub.docker.com\n\n * 阿里云容器hub：https://dev.aliyun.com\n\n * google镜像（gcr.io）：https://console.cloud.google.com/gcr/images/google-containers/global （需要科学上网，主要为kubernetes相关镜像）\n\n\n# 操作系统基础镜像\n\n比如你要从linux操作系统基础镜像开始构建，可以参考下表来选择合适的基础镜像：\n\n镜像名称      大小      使用场景\nbusybox   1.15m   临时使用\nalpine    4.41m   主要用于测试，也可用于生产环境\ncentos    200m    主要用于生产环境，支持centos/red hat，常用于追求稳定性的企业应用\nutuntu    81.1m   主要用于生产环境，常用于人工智能计算和企业应用\ndebian    101m    主要用于生产环境\n\n\n# busybox\n\n描述：可以将busybox理解为一个超级简化版嵌入式linux系统。\n\n官网：https://www.busybox.net/\n\n镜像：https://hub.docker.com/_/busybox/\n\n包管理命令：apk, lbu\n\n包管理文档：https://wiki.alpinelinux.org/wiki/alpine_linux_package_management\n\n\n# alpine\n\n描述：alpine是一个面向安全的、轻量级的linux系统，基于musl libc和busybox。\n\n官网：https://www.alpinelinux.org/\n\n镜像：https://hub.docker.com/_/alpine/\n\n包管理命令：apk, lbu\n\n包管理文档：https://wiki.alpinelinux.org/wiki/alpine_linux_package_management\n\n\n# centos\n\n描述：可以理解centos是redhat的社区版\n\n官网：https://www.centos.org/\n\n镜像：https://hub.docker.com/_/centos/\n\n包管理命令：yum, rpm\n\n\n# ubuntu\n\n描述：另一个非常出色的linux发行版\n\n官网：http://www.ubuntu.com/\n\n镜像：https://hub.docker.com/_/ubuntu/\n\n包管理命令：apt-get, dpkg\n\n\n# debian\n\n描述：另一个非常出色的linux发行版\n\n官网：https://www.debian.org/\n\n镜像：https://hub.docker.com/_/debian/\n\n包管理命令：apt-get, dpkg\n\n\n# 编程语言基础镜像\n\n# java基础镜像\n\n * https://hub.docker.com/_/openjdk/\n\n# python基础镜像\n\n * https://hub.docker.com/_/python/\n\n# nodejs基础镜像\n\n * https://hub.docker.com/_/node/\n\n\n# 应用基础镜像\n\n# nginx基础镜像\n\n * https://hub.docker.com/_/nginx/\n\n# tomcat基础镜像\n\n * https://hub.docker.com/_/tomcat/\n\n# jetty基础镜像\n\n * https://hub.docker.com/_/jetty/\n\n\n# 其它基础镜像例子\n\n# maven基础镜像\n\n * https://hub.docker.com/_/maven/\n\n# jenkins基础镜像\n\n * https://hub.docker.com/r/jenkins/jenkins/\n\n# gitlab基础镜像\n\n * https://hub.docker.com/r/gitlab/gitlab-ce/",charsets:{cjk:!0}},{title:"常见的dockerfile汇总",frontmatter:{title:"常见的dockerfile汇总",date:"2023-02-01T12:38:37.000Z",permalink:"/pages/382a6c/",categories:["运维","docker"],tags:[null],readingShow:"top",description:"python",meta:[{name:"twitter:title",content:"常见的dockerfile汇总"},{name:"twitter:description",content:"python"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/10.docker/04.%E5%B8%B8%E8%A7%81%E7%9A%84dockerfile%E6%B1%87%E6%80%BB.html"},{property:"og:type",content:"article"},{property:"og:title",content:"常见的dockerfile汇总"},{property:"og:description",content:"python"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/10.docker/04.%E5%B8%B8%E8%A7%81%E7%9A%84dockerfile%E6%B1%87%E6%80%BB.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-02-01T12:38:37.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"常见的dockerfile汇总"},{itemprop:"description",content:"python"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/10.docker/04.%E5%B8%B8%E8%A7%81%E7%9A%84dockerfile%E6%B1%87%E6%80%BB.html",relativePath:"04.运维/10.docker/04.常见的dockerfile汇总.md",key:"v-20b39df5",path:"/pages/382a6c/",headersStr:null,content:'python\n\nFROM python:3.7-alpine\nWORKDIR /code\nENV FLASK_APP=app.py\nENV FLASK_RUN_HOST=0.0.0.0\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\nEXPOSE 5000\nCOPY . .\nCMD ["flask", "run"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nphp\n\nFROM php:7.2.34-fpm-alpine\nWORKDIR /app\nENV TZ "Asia/Shanghai"\nRUN sed -i "s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g" /etc/apk/repositories \\\n&& apk add --no-cache autoconf g++ libtool make curl-dev libxml2-dev linux-headers \\\n&& docker-php-ext-install -j 2 zip \\\n&& apk add --no-cache freetype-dev libjpeg-turbo-dev libpng-dev \\\n&& docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\\n&& docker-php-ext-install -j 2 gd \\\n&& docker-php-ext-install -j$(nproc) bcmath \\\n&& docker-php-ext-install -j 2 mysqli \\\n&& docker-php-ext-install pdo_mysql\ncopy . .\nRUN chown -R www-data:www-data /app\nEXPOSE 9000\nENTRYPOINT ["php-fpm"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\ngo\n\nFROM golang:1.16.7-alpine3.13 AS builder\n\nLABEL stage=gobuilder\n\nENV CGO_ENABLED 0\nENV GOOS linux\nENV GOPROXY https://goproxy.cn,direct\nARG VERSION\nARG GIT_COMMIT\nARG GIT_TREE_STATE\nARG BUILD_DATE\n\nWORKDIR /build/zero\n\nADD go.mod .\nADD go.sum .\nRUN go mod download\nCOPY . .\nRUN go build -ldflags="-s -w -X trade/common/version.GitVersion=${VERSION} -X trade/common/version.GitCommit=${GIT_COMMIT} -X trade/common/version.GitTreeState=${GIT_TREE_STATE} -X trade/common/version.BuildDate=${BUILD_DATE}" -o /app/backend api/backend/backend.go\n\n\nFROM alpine:3.13\nRUN sed -i \'s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\' /etc/apk/repositories\nRUN apk update --no-cache && apk add --no-cache ca-certificates tzdata\nENV TZ Asia/Shanghai\n\nWORKDIR /app\nCOPY --from=builder /app/backend /app/backend\nCOPY --from=builder /build/zero/api/backend/etc /app/etc\n\nEXPOSE 8888\n\nCMD ["./backend"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\nnode\n\nFROM dh.1quant.me/library/node:8.15.0-alpine-i18n AS builder\n\nLABEL stage=h5builder\n\nWORKDIR /app\n\nCOPY . .\n\nRUN npm cache verify && npm i && npm run build\n\nFROM dh.1quant.me/library/nginx:1.14.2-alpine\n\nCOPY --from=builder /app/dist /usr/share/nginx/html\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nc++\n\nFROM debian:buster\n\nENV TZ Asia/Shanghai\n\nWORKDIR /app\nENV LD_LIBRARY_PATH=/app/lib\n\nCOPY . .\nRUN ln -s libssl.so.1.0.2k ./lib/libssl.so.10 && ln -s libcrypto.so.1.0.2k ./lib/libcrypto.so.10\n\nEXPOSE 9101\n\nCMD ["./bin/trade_service"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\njenkins\n\nFROM jenkins/jenkins:2.332.3\n\nUSER root\nRUN echo \'\' > /etc/apt/sources.list \\\n&& echo "deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free" > /etc/apt/sources.list \\\n&& echo "deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free" >> /etc/apt/sources.list \\\n&& echo "deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free" >> /etc/apt/sources.list \\\n&& echo "deb http://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free" >> /etc/apt/sources.list \\\n&& apt update \\\n&& apt install sudo -y \\\n&& apt install vim -y \\\n&& apt install sshpass -y \\\n&& apt install python -y \\\n&& sed -i \'27a\\jenkins ALL=(ALL)       NOPASSWD:ALL\' /etc/sudoers \\\n&& apt install ansible -y\nUSER jenkins\nENV LANG C.UTF-8\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n',normalizedContent:'python\n\nfrom python:3.7-alpine\nworkdir /code\nenv flask_app=app.py\nenv flask_run_host=0.0.0.0\ncopy requirements.txt requirements.txt\nrun pip install -r requirements.txt\nexpose 5000\ncopy . .\ncmd ["flask", "run"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nphp\n\nfrom php:7.2.34-fpm-alpine\nworkdir /app\nenv tz "asia/shanghai"\nrun sed -i "s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g" /etc/apk/repositories \\\n&& apk add --no-cache autoconf g++ libtool make curl-dev libxml2-dev linux-headers \\\n&& docker-php-ext-install -j 2 zip \\\n&& apk add --no-cache freetype-dev libjpeg-turbo-dev libpng-dev \\\n&& docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\\n&& docker-php-ext-install -j 2 gd \\\n&& docker-php-ext-install -j$(nproc) bcmath \\\n&& docker-php-ext-install -j 2 mysqli \\\n&& docker-php-ext-install pdo_mysql\ncopy . .\nrun chown -r www-data:www-data /app\nexpose 9000\nentrypoint ["php-fpm"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\ngo\n\nfrom golang:1.16.7-alpine3.13 as builder\n\nlabel stage=gobuilder\n\nenv cgo_enabled 0\nenv goos linux\nenv goproxy https://goproxy.cn,direct\narg version\narg git_commit\narg git_tree_state\narg build_date\n\nworkdir /build/zero\n\nadd go.mod .\nadd go.sum .\nrun go mod download\ncopy . .\nrun go build -ldflags="-s -w -x trade/common/version.gitversion=${version} -x trade/common/version.gitcommit=${git_commit} -x trade/common/version.gittreestate=${git_tree_state} -x trade/common/version.builddate=${build_date}" -o /app/backend api/backend/backend.go\n\n\nfrom alpine:3.13\nrun sed -i \'s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\' /etc/apk/repositories\nrun apk update --no-cache && apk add --no-cache ca-certificates tzdata\nenv tz asia/shanghai\n\nworkdir /app\ncopy --from=builder /app/backend /app/backend\ncopy --from=builder /build/zero/api/backend/etc /app/etc\n\nexpose 8888\n\ncmd ["./backend"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\nnode\n\nfrom dh.1quant.me/library/node:8.15.0-alpine-i18n as builder\n\nlabel stage=h5builder\n\nworkdir /app\n\ncopy . .\n\nrun npm cache verify && npm i && npm run build\n\nfrom dh.1quant.me/library/nginx:1.14.2-alpine\n\ncopy --from=builder /app/dist /usr/share/nginx/html\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nc++\n\nfrom debian:buster\n\nenv tz asia/shanghai\n\nworkdir /app\nenv ld_library_path=/app/lib\n\ncopy . .\nrun ln -s libssl.so.1.0.2k ./lib/libssl.so.10 && ln -s libcrypto.so.1.0.2k ./lib/libcrypto.so.10\n\nexpose 9101\n\ncmd ["./bin/trade_service"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\njenkins\n\nfrom jenkins/jenkins:2.332.3\n\nuser root\nrun echo \'\' > /etc/apt/sources.list \\\n&& echo "deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free" > /etc/apt/sources.list \\\n&& echo "deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free" >> /etc/apt/sources.list \\\n&& echo "deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free" >> /etc/apt/sources.list \\\n&& echo "deb http://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free" >> /etc/apt/sources.list \\\n&& apt update \\\n&& apt install sudo -y \\\n&& apt install vim -y \\\n&& apt install sshpass -y \\\n&& apt install python -y \\\n&& sed -i \'27a\\jenkins all=(all)       nopasswd:all\' /etc/sudoers \\\n&& apt install ansible -y\nuser jenkins\nenv lang c.utf-8\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n',charsets:{}},{title:"基于官方php7.2.34镜像构建生产可用镜像",frontmatter:{title:"基于官方php7.2.34镜像构建生产可用镜像",date:"2023-01-30T20:21:15.000Z",permalink:"/pages/6e3cb9/",categories:["docker"],tags:[null],readingShow:"top",description:"Dockerhub 上 PHP 官方基础镜像主要分为三个分支：",meta:[{name:"twitter:title",content:"基于官方php7.2.34镜像构建生产可用镜像"},{name:"twitter:description",content:"Dockerhub 上 PHP 官方基础镜像主要分为三个分支："},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/10.docker/05.%E5%9F%BA%E4%BA%8E%E5%AE%98%E6%96%B9php7-2-34%E9%95%9C%E5%83%8F%E6%9E%84%E5%BB%BA%E7%94%9F%E4%BA%A7%E5%8F%AF%E7%94%A8%E9%95%9C%E5%83%8F.html"},{property:"og:type",content:"article"},{property:"og:title",content:"基于官方php7.2.34镜像构建生产可用镜像"},{property:"og:description",content:"Dockerhub 上 PHP 官方基础镜像主要分为三个分支："},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/10.docker/05.%E5%9F%BA%E4%BA%8E%E5%AE%98%E6%96%B9php7-2-34%E9%95%9C%E5%83%8F%E6%9E%84%E5%BB%BA%E7%94%9F%E4%BA%A7%E5%8F%AF%E7%94%A8%E9%95%9C%E5%83%8F.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-30T20:21:15.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"基于官方php7.2.34镜像构建生产可用镜像"},{itemprop:"description",content:"Dockerhub 上 PHP 官方基础镜像主要分为三个分支："}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/10.docker/05.%E5%9F%BA%E4%BA%8E%E5%AE%98%E6%96%B9php7-2-34%E9%95%9C%E5%83%8F%E6%9E%84%E5%BB%BA%E7%94%9F%E4%BA%A7%E5%8F%AF%E7%94%A8%E9%95%9C%E5%83%8F.html",relativePath:"04.运维/10.docker/05.基于官方php7-2-34镜像构建生产可用镜像.md",key:"v-0f41bfdd",path:"/pages/6e3cb9/",headers:[{level:2,title:"1 了解官方php镜像",slug:"_1-了解官方php镜像",normalizedTitle:"1 了解官方php镜像",charIndex:2},{level:2,title:"2 需求分析和镜像选择",slug:"_2-需求分析和镜像选择",normalizedTitle:"2 需求分析和镜像选择",charIndex:511},{level:2,title:"3 构建准备",slug:"_3-构建准备",normalizedTitle:"3 构建准备",charIndex:1562},{level:2,title:"4 创建Dockerfile",slug:"_4-创建dockerfile",normalizedTitle:"4 创建dockerfile",charIndex:2616},{level:2,title:"5 推送到Harbor镜像仓库",slug:"_5-推送到harbor镜像仓库",normalizedTitle:"5 推送到harbor镜像仓库",charIndex:5560},{level:2,title:"6 参考资料",slug:"_6-参考资料",normalizedTitle:"6 参考资料",charIndex:6998}],headersStr:"1 了解官方php镜像 2 需求分析和镜像选择 3 构建准备 4 创建Dockerfile 5 推送到Harbor镜像仓库 6 参考资料",content:'# 1 了解官方php镜像\n\nDockerhub 上 PHP 官方基础镜像主要分为三个分支：\n\n 1. cli: 没有开启 CGI 也就是说不能运行fpm。只可以运行命令行。\n\n 2. fpm: 开启了CGI，可以用来运行web服务也可以用来运行cli命令。\n\n 3. zts: 开启了线程安全的版本。\n\n选择什么分支的镜像？\n\n一般在生成环境会使用到 php-fpm。先了解一下什么是php-fpm？\n\n * 由于nginx本身不能处理PHP，它只是个web服务器，当接收到请求后，如果是php请求，则发给php解释器处理，并把结果返回给客户端；\n\n * nginx 一般是把请求发fastcgi管理进程处理，fascgi管理进程选择 CGI 子进程处理结果并返回被 nginx ；\n\n * PHP-FPM 是一个PHP FastCGI管理器，旨在将FastCGI进程管理整合进PHP包中；\n\n * PHP-FPM提供了更好的PHP进程管理方式，可以有效控制内存和进程、可以平滑重载PHP配置，比spawn-fcgi具有更多优点，所以被PHP官方收录了。\n\n所以，LNMP架构的一般会选择 fpm 分支的镜像。\n\n\n# 2 需求分析和镜像选择\n\n镜像选择\n\n首先是基础镜像版本的选择，我这里选择的是 php:7.2.34-fpm-alpine 镜像。php 版本为 7.2.34 （php版本需要结合具体业务代码进行选择），这个官方镜像基于alpine 镜像进行构建，alpine镜像体积非常小 ，大小只有5m左右。\n\n需求分析\n\n对于php应用来说，都需要自己额外安装一些扩展。官方基础镜像的扩展一般满足不我们的需求，所以需要看看官方镜像有哪些扩展，然后再按需安装我们要的扩展。\n\n查看官方基础镜像有什么扩展：\n\nshell\n\n[root@harbor harbor]# docker run php:7.2.34-fpm-alpine php -m\n\n[PHP Modules]\nCore\nctype\ncurl\ndate\ndom\nfileinfo\nfilter\nftp\nhash\niconv\njson\nlibxml\nmbstring\nmysqlnd\nopenssl\npcre\nPDO\npdo_sqlite\nPhar\nposix\nreadline\nReflection\nsession\nSimpleXML\nsodium\nSPL\nsqlite3\nstandard\ntokenizer\nxml\nxmlreader\nxmlwriter\nzlib\n[Zend Modules]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n安装经验，一般需要额外安装的扩展：\n\n * redis：redis是目前主流的Nosql数据库，常用。\n * zip：文件压缩扩展。\n * gd：图片处理扩展，一些图形验证码的生成不能没有这个扩展。\n * bcmath：没有这个库的话可能一些框架或者类库的composer依赖校验会无法通过。\n * pdo_mysql：连接数据库扩展。\n * opcache： 是 PHP 中的 Zend 扩展，可以大大提升 PHP 的性能。\n * swoole：一个PHP高级Web开发框架，可按需添加。\n\n除了扩展之外，还可以安装一个php的包管理工具 composer。composer 是PHP的包管理、包依赖关系管理工具，有了它，我们就很轻松一个命令就可以把他人优秀的代码用到我们的项目中来，而且很容易管理依赖关系，更新删除等操作也很轻易的实现。\n\n\n# 3 构建准备\n\n 1. 创建工程目录\n\n[root@harbor ~]# mkdir lnmp/php -p\n[root@harbor ~]# cd lnmp/php/\n\n\n1\n2\n\n 2. 准备 composer 包，放在工程目录下，下载链接：Releases · composer/composer · GitHub\n\n[root@harbor php]# wget https://github.com/composer/composer/releases/download/2.0.13/composer.phar\n\n\n1\n\n 3. 准备 date.ini 文件，设置PHP默认时区为东八区：\n\n[root@harbor php]# mkdir conf.d\n[root@harbor php]# echo "date.timezone = Asia/Shanghai" > conf.d/date.ini\n\n\n1\n2\n\n 4. 准备 opcode.ini 文件，用于设置opcode默认的参数，并且设置环境变量 OPCODE 以控制其是否被开启。当环境变量OPCODE的值被设置为1的时候表示开启opcode，0则关闭。\n\n# 这里加 \\EOF 是防止${OPCODE}被转义\n[root@harbor php]# cat >> conf.d/opcode.ini << \\EOF\n> opcache.enable=${OPCODE}\n> enable_clopcache.enable_cli=1\n> opcache.revalidate_freq=60\n> opcache.max_accelerated_files=100000\n> opcache.validate_timestamps=1\n> EOF\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 5. 准备 php-fpm.conf 文件\n\n[root@harbor php]# cat www.conf \n[www]\nuser = www\ngroup = www\nlisten = 0.0.0.0:9000\npm = dynamic\npm.max_children = 100\npm.start_servers = 30\npm.min_spare_servers = 20\npm.max_spare_servers = 50\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n准备工作完成，接下来创建Dockerfile进行构建。\n\n\n# 4 创建Dockerfile\n\n[root@harbor php]# touch Dockerfile\n\n\n1\n\n\nDockerfile文件内容如下：\n\nFROM php:7.2.34-fpm-alpine\nLABEL MAINTAINER="syushin moliyoyoyo@163.com"\nENV TZ "Asia/Shanghai"\nENV TERM xterm\n# 默认关闭opcode\nENV OPCODE 0\n\nCOPY ./conf.d/ $PHP_INI_DIR/conf.d/\nCOPY composer.phar /usr/local/bin/composer\nCOPY www.conf /usr/local/etc/php-fpm.d/www.conf\n# 创建www用户\nRUN addgroup -g 1000 -S www && adduser -s /sbin/nologin -S -D -u 1000 -G www www\n# 配置阿里云镜像源，加快构建速度\nRUN sed -i "s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g" /etc/apk/repositories\n\n# PHPIZE_DEPS 包含 gcc g++ 等编译辅助类库，完成编译后删除\nRUN apk add --no-cache $PHPIZE_DEPS \\\n    && apk add --no-cache libstdc++ libzip-dev vim\\\n    && apk update \\\n    && pecl install redis-5.3.4 \\\n    && pecl install zip \\\n    && pecl install swoole \\\n    && docker-php-ext-enable redis zip swoole\\\n    && apk del $PHPIZE_DEPS\n# docker-php-ext-install 指令已经包含编译辅助类库的删除逻辑\nRUN apk add --no-cache freetype libpng libjpeg-turbo freetype-dev libpng-dev libjpeg-turbo-dev \\\n    && apk update \\\n    && docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ --with-png-dir=/usr/include/ \\\n    && docker-php-ext-install -j$(nproc) gd \\\n    && docker-php-ext-install -j$(nproc) pdo_mysql \\\n    && docker-php-ext-install -j$(nproc) opcache \\\n    && docker-php-ext-install -j$(nproc) bcmath \\\n    && docker-php-ext-install -j$(nproc) mysqli \\\n    && chmod +x /usr/local/bin/composer\n\nRUN mv "$PHP_INI_DIR/php.ini-production" "$PHP_INI_DIR/php.ini"\nEXPOSE 9000\nENTRYPOINT ["php-fpm"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n说明：\n\n * PHP_INI_DIR : 这个环境变量的定义在php基础镜像的Dockerfile有定义，变量值是 /usr/local/etc/php\n * PHPIZE_DEPS ：这个也是定义在基础镜像Dockerfile中，包含了扩展编译安装时需要但是php运行不需要的linux软件库。我们需要把它们挑选出来，在编译完扩展之后删除。变量值： autoconf dpkg-dev dpkg\n * php-fpm.conf 配置文件在 /usr/local/etc/ 目录下\n * www.conf 配置文件在 /usr/local/etc/php-fpm.d 目录下\n\n可以编写一个脚本进行构建，好处是以后可以通过查看脚本知道哪个标签的镜像是此次构建的。\n\n[root@harbor php]# cat build-command.sh \n#!/bin/bash\ndocker build -t php-7.2.34-fpm-alpine:v1 .\n\n\n1\n2\n3\n\n\n执行构建\n\n[root@harbor php]# sh build-command.sh \n\n\n1\n\n\n稍等时间即可看到构建完成\n\n# 编译完成后，镜像体积是160m\n[root@harbor php]# docker images\nREPOSITORY                    TAG                 IMAGE ID            CREATED             SIZE\nphp-7.2.34-fpm-alpine         v1                  fb3d05761dfa        2 hours ago         160MB\n\n# 检查扩展\n[root@harbor php]# docker run php-7.2.34-fpm-alpine:v1 php -m\n[PHP Modules]\nbcmath\nCore\nctype\ncurl\ndate\ndom\nfileinfo\nfilter\nftp\ngd\nhash\niconv\njson\nlibxml\nmbstring\nmysqlnd\nopenssl\npcre\nPDO\npdo_mysql\npdo_sqlite\nPhar\nposix\nreadline\nredis\nReflection\nsession\nSimpleXML\nsodium\nSPL\nsqlite3\nstandard\nswoole\ntokenizer\nxml\nxmlreader\nxmlwriter\nZend OPcache\nzip\nzlib\n\n[Zend Modules]\nZend OPcache\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n这样，一个镜像就构建完成了。\n\n\n# 5 推送到Harbor镜像仓库\n\n一般在企业，自己构建好镜像后，需要推送到自己的私有仓库。我这里本地有个harbor的私有仓库，仓库地址：http://192.168.18.100\n\n并且仓库上面有个项目叫 lnmp，存放相关镜像。\n\n将上面构建的镜像推送到harbor仓库：\n\n# Docker登录仓库\n[root@harbor php]# docker login 192.168.18.100:80\nAuthenticating with existing credentials...\nWARNING! Your password will be stored unencrypted in /root/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nLogin Succeeded\n\n# 给镜像打标签\n[root@harbor php]# docker tag php-7.2.34-fpm-alpine:v1 192.168.18.100:80/lnmp/myphp:7.2\n\n# 上传镜像\n[root@harbor php]# docker push 192.168.18.100:80/lnmp/myphp:7.2\nThe push refers to repository [192.168.18.100:80/lnmp/myphp]\n7dfdf785728d: Pushed \n3ec4258bee2c: Pushed \nf01174c9f645: Pushing [================>                                  ]  24.29MB/73.92MB\nc21c6905870c: Pushed \n752870d17619: Pushed \n0e7d6edc15aa: Pushed \nc9685eb5cbc9: Pushed \ne07b1aa3ce21: Pushed \n69e56c02a5f3: Pushed \n5ce5d9de209b: Pushed \nee81ef73796d: Pushed \n738a430a6dab: Pushing [==============>                                    ]  15.51MB/53.39MB\n7d12c6e1d8f1: Pushing [==================================================>]  4.096kB\nbe80e727dd27: Waiting \n24e52497c24f: Waiting \n86d905c1f58e: Waiting \n22573737ba76: Waiting \n777b2c648970: Waiting \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n等待推送完成，即可看到仓库上已经有该镜像了。\n\n\n# 6 参考资料\n\n * 制作一个生产环境可用的PHP基础镜像 泛黄的日历\n * Dockerhub 官方php镜像\n * Alpine 系统简介\n\n原文链接： 基于官方php:7.2.34-fpm-alpine镜像构建生产可用镜像 - syushin - 博客园',normalizedContent:'# 1 了解官方php镜像\n\ndockerhub 上 php 官方基础镜像主要分为三个分支：\n\n 1. cli: 没有开启 cgi 也就是说不能运行fpm。只可以运行命令行。\n\n 2. fpm: 开启了cgi，可以用来运行web服务也可以用来运行cli命令。\n\n 3. zts: 开启了线程安全的版本。\n\n选择什么分支的镜像？\n\n一般在生成环境会使用到 php-fpm。先了解一下什么是php-fpm？\n\n * 由于nginx本身不能处理php，它只是个web服务器，当接收到请求后，如果是php请求，则发给php解释器处理，并把结果返回给客户端；\n\n * nginx 一般是把请求发fastcgi管理进程处理，fascgi管理进程选择 cgi 子进程处理结果并返回被 nginx ；\n\n * php-fpm 是一个php fastcgi管理器，旨在将fastcgi进程管理整合进php包中；\n\n * php-fpm提供了更好的php进程管理方式，可以有效控制内存和进程、可以平滑重载php配置，比spawn-fcgi具有更多优点，所以被php官方收录了。\n\n所以，lnmp架构的一般会选择 fpm 分支的镜像。\n\n\n# 2 需求分析和镜像选择\n\n镜像选择\n\n首先是基础镜像版本的选择，我这里选择的是 php:7.2.34-fpm-alpine 镜像。php 版本为 7.2.34 （php版本需要结合具体业务代码进行选择），这个官方镜像基于alpine 镜像进行构建，alpine镜像体积非常小 ，大小只有5m左右。\n\n需求分析\n\n对于php应用来说，都需要自己额外安装一些扩展。官方基础镜像的扩展一般满足不我们的需求，所以需要看看官方镜像有哪些扩展，然后再按需安装我们要的扩展。\n\n查看官方基础镜像有什么扩展：\n\nshell\n\n[root@harbor harbor]# docker run php:7.2.34-fpm-alpine php -m\n\n[php modules]\ncore\nctype\ncurl\ndate\ndom\nfileinfo\nfilter\nftp\nhash\niconv\njson\nlibxml\nmbstring\nmysqlnd\nopenssl\npcre\npdo\npdo_sqlite\nphar\nposix\nreadline\nreflection\nsession\nsimplexml\nsodium\nspl\nsqlite3\nstandard\ntokenizer\nxml\nxmlreader\nxmlwriter\nzlib\n[zend modules]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n安装经验，一般需要额外安装的扩展：\n\n * redis：redis是目前主流的nosql数据库，常用。\n * zip：文件压缩扩展。\n * gd：图片处理扩展，一些图形验证码的生成不能没有这个扩展。\n * bcmath：没有这个库的话可能一些框架或者类库的composer依赖校验会无法通过。\n * pdo_mysql：连接数据库扩展。\n * opcache： 是 php 中的 zend 扩展，可以大大提升 php 的性能。\n * swoole：一个php高级web开发框架，可按需添加。\n\n除了扩展之外，还可以安装一个php的包管理工具 composer。composer 是php的包管理、包依赖关系管理工具，有了它，我们就很轻松一个命令就可以把他人优秀的代码用到我们的项目中来，而且很容易管理依赖关系，更新删除等操作也很轻易的实现。\n\n\n# 3 构建准备\n\n 1. 创建工程目录\n\n[root@harbor ~]# mkdir lnmp/php -p\n[root@harbor ~]# cd lnmp/php/\n\n\n1\n2\n\n 2. 准备 composer 包，放在工程目录下，下载链接：releases · composer/composer · github\n\n[root@harbor php]# wget https://github.com/composer/composer/releases/download/2.0.13/composer.phar\n\n\n1\n\n 3. 准备 date.ini 文件，设置php默认时区为东八区：\n\n[root@harbor php]# mkdir conf.d\n[root@harbor php]# echo "date.timezone = asia/shanghai" > conf.d/date.ini\n\n\n1\n2\n\n 4. 准备 opcode.ini 文件，用于设置opcode默认的参数，并且设置环境变量 opcode 以控制其是否被开启。当环境变量opcode的值被设置为1的时候表示开启opcode，0则关闭。\n\n# 这里加 \\eof 是防止${opcode}被转义\n[root@harbor php]# cat >> conf.d/opcode.ini << \\eof\n> opcache.enable=${opcode}\n> enable_clopcache.enable_cli=1\n> opcache.revalidate_freq=60\n> opcache.max_accelerated_files=100000\n> opcache.validate_timestamps=1\n> eof\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n 5. 准备 php-fpm.conf 文件\n\n[root@harbor php]# cat www.conf \n[www]\nuser = www\ngroup = www\nlisten = 0.0.0.0:9000\npm = dynamic\npm.max_children = 100\npm.start_servers = 30\npm.min_spare_servers = 20\npm.max_spare_servers = 50\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n准备工作完成，接下来创建dockerfile进行构建。\n\n\n# 4 创建dockerfile\n\n[root@harbor php]# touch dockerfile\n\n\n1\n\n\ndockerfile文件内容如下：\n\nfrom php:7.2.34-fpm-alpine\nlabel maintainer="syushin moliyoyoyo@163.com"\nenv tz "asia/shanghai"\nenv term xterm\n# 默认关闭opcode\nenv opcode 0\n\ncopy ./conf.d/ $php_ini_dir/conf.d/\ncopy composer.phar /usr/local/bin/composer\ncopy www.conf /usr/local/etc/php-fpm.d/www.conf\n# 创建www用户\nrun addgroup -g 1000 -s www && adduser -s /sbin/nologin -s -d -u 1000 -g www www\n# 配置阿里云镜像源，加快构建速度\nrun sed -i "s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g" /etc/apk/repositories\n\n# phpize_deps 包含 gcc g++ 等编译辅助类库，完成编译后删除\nrun apk add --no-cache $phpize_deps \\\n    && apk add --no-cache libstdc++ libzip-dev vim\\\n    && apk update \\\n    && pecl install redis-5.3.4 \\\n    && pecl install zip \\\n    && pecl install swoole \\\n    && docker-php-ext-enable redis zip swoole\\\n    && apk del $phpize_deps\n# docker-php-ext-install 指令已经包含编译辅助类库的删除逻辑\nrun apk add --no-cache freetype libpng libjpeg-turbo freetype-dev libpng-dev libjpeg-turbo-dev \\\n    && apk update \\\n    && docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ --with-png-dir=/usr/include/ \\\n    && docker-php-ext-install -j$(nproc) gd \\\n    && docker-php-ext-install -j$(nproc) pdo_mysql \\\n    && docker-php-ext-install -j$(nproc) opcache \\\n    && docker-php-ext-install -j$(nproc) bcmath \\\n    && docker-php-ext-install -j$(nproc) mysqli \\\n    && chmod +x /usr/local/bin/composer\n\nrun mv "$php_ini_dir/php.ini-production" "$php_ini_dir/php.ini"\nexpose 9000\nentrypoint ["php-fpm"]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n说明：\n\n * php_ini_dir : 这个环境变量的定义在php基础镜像的dockerfile有定义，变量值是 /usr/local/etc/php\n * phpize_deps ：这个也是定义在基础镜像dockerfile中，包含了扩展编译安装时需要但是php运行不需要的linux软件库。我们需要把它们挑选出来，在编译完扩展之后删除。变量值： autoconf dpkg-dev dpkg\n * php-fpm.conf 配置文件在 /usr/local/etc/ 目录下\n * www.conf 配置文件在 /usr/local/etc/php-fpm.d 目录下\n\n可以编写一个脚本进行构建，好处是以后可以通过查看脚本知道哪个标签的镜像是此次构建的。\n\n[root@harbor php]# cat build-command.sh \n#!/bin/bash\ndocker build -t php-7.2.34-fpm-alpine:v1 .\n\n\n1\n2\n3\n\n\n执行构建\n\n[root@harbor php]# sh build-command.sh \n\n\n1\n\n\n稍等时间即可看到构建完成\n\n# 编译完成后，镜像体积是160m\n[root@harbor php]# docker images\nrepository                    tag                 image id            created             size\nphp-7.2.34-fpm-alpine         v1                  fb3d05761dfa        2 hours ago         160mb\n\n# 检查扩展\n[root@harbor php]# docker run php-7.2.34-fpm-alpine:v1 php -m\n[php modules]\nbcmath\ncore\nctype\ncurl\ndate\ndom\nfileinfo\nfilter\nftp\ngd\nhash\niconv\njson\nlibxml\nmbstring\nmysqlnd\nopenssl\npcre\npdo\npdo_mysql\npdo_sqlite\nphar\nposix\nreadline\nredis\nreflection\nsession\nsimplexml\nsodium\nspl\nsqlite3\nstandard\nswoole\ntokenizer\nxml\nxmlreader\nxmlwriter\nzend opcache\nzip\nzlib\n\n[zend modules]\nzend opcache\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n这样，一个镜像就构建完成了。\n\n\n# 5 推送到harbor镜像仓库\n\n一般在企业，自己构建好镜像后，需要推送到自己的私有仓库。我这里本地有个harbor的私有仓库，仓库地址：http://192.168.18.100\n\n并且仓库上面有个项目叫 lnmp，存放相关镜像。\n\n将上面构建的镜像推送到harbor仓库：\n\n# docker登录仓库\n[root@harbor php]# docker login 192.168.18.100:80\nauthenticating with existing credentials...\nwarning! your password will be stored unencrypted in /root/.docker/config.json.\nconfigure a credential helper to remove this warning. see\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nlogin succeeded\n\n# 给镜像打标签\n[root@harbor php]# docker tag php-7.2.34-fpm-alpine:v1 192.168.18.100:80/lnmp/myphp:7.2\n\n# 上传镜像\n[root@harbor php]# docker push 192.168.18.100:80/lnmp/myphp:7.2\nthe push refers to repository [192.168.18.100:80/lnmp/myphp]\n7dfdf785728d: pushed \n3ec4258bee2c: pushed \nf01174c9f645: pushing [================>                                  ]  24.29mb/73.92mb\nc21c6905870c: pushed \n752870d17619: pushed \n0e7d6edc15aa: pushed \nc9685eb5cbc9: pushed \ne07b1aa3ce21: pushed \n69e56c02a5f3: pushed \n5ce5d9de209b: pushed \nee81ef73796d: pushed \n738a430a6dab: pushing [==============>                                    ]  15.51mb/53.39mb\n7d12c6e1d8f1: pushing [==================================================>]  4.096kb\nbe80e727dd27: waiting \n24e52497c24f: waiting \n86d905c1f58e: waiting \n22573737ba76: waiting \n777b2c648970: waiting \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n等待推送完成，即可看到仓库上已经有该镜像了。\n\n\n# 6 参考资料\n\n * 制作一个生产环境可用的php基础镜像 泛黄的日历\n * dockerhub 官方php镜像\n * alpine 系统简介\n\n原文链接： 基于官方php:7.2.34-fpm-alpine镜像构建生产可用镜像 - syushin - 博客园',charsets:{cjk:!0}},{title:"debian中科大替换教程",frontmatter:{title:"debian中科大替换教程",date:"2022-12-15T18:58:50.000Z",permalink:"/pages/767641/",categories:["运维","镜像源"],tags:[null],readingShow:"top",description:"警告",meta:[{name:"twitter:title",content:"debian中科大替换教程"},{name:"twitter:description",content:"警告"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/11.other/01.debian%E4%B8%AD%E7%A7%91%E5%A4%A7%E6%9B%BF%E6%8D%A2%E6%95%99%E7%A8%8B.html"},{property:"og:type",content:"article"},{property:"og:title",content:"debian中科大替换教程"},{property:"og:description",content:"警告"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/11.other/01.debian%E4%B8%AD%E7%A7%91%E5%A4%A7%E6%9B%BF%E6%8D%A2%E6%95%99%E7%A8%8B.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T18:58:50.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"debian中科大替换教程"},{itemprop:"description",content:"警告"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/11.other/01.debian%E4%B8%AD%E7%A7%91%E5%A4%A7%E6%9B%BF%E6%8D%A2%E6%95%99%E7%A8%8B.html",relativePath:"04.运维/11.other/01.debian中科大替换教程.md",key:"v-28471c42",path:"/pages/767641/",headers:[{level:2,title:"命令安装",slug:"命令安装",normalizedTitle:"命令安装",charIndex:1031}],headersStr:"命令安装",content:"警告\n\n操作前请做好相应备份\n\n一般情况下，将 /etc/apt/sources.list 文件中 Debian 默认的源地址 http://deb.debian.org/ 替换为 http://mirrors.ustc.edu.cn 即可。\n\n可以使用如下命令：\n\nsudo sed -i 's/deb.debian.org/mirrors.ustc.edu.cn/g' /etc/apt/sources.list\n\n当然也可以直接编辑 /etc/apt/sources.list 文件（需要使用 sudo）。以下是 Debian Stable 参考配置内容：\n\ndeb http://mirrors.ustc.edu.cn/debian stable main contrib non-free # deb-src http://mirrors.ustc.edu.cn/debian stable main contrib non-free deb http://mirrors.ustc.edu.cn/debian stable-updates main contrib non-free # deb-src http://mirrors.ustc.edu.cn/debian stable-updates main contrib non-free # deb http://mirrors.ustc.edu.cn/debian stable-proposed-updates main contrib non-free # deb-src http://mirrors.ustc.edu.cn/debian stable-proposed-updates main contrib non-free\n\n同时你也可能需要更改 Debian Security 源，请参考 Debian Security 源使用帮助\n\nsed -i 's/security.debian.org/mirrors.ustc.edu.cn/g' /etc/apt/sources.list\n\n更改完 sources.list 文件后请运行 sudo apt-get update 更新索引以生效。\n\n小技巧\n\n使用 HTTPS 可以有效避免国内运营商的缓存劫持，但需要事先安装 apt-transport-https (Debian Buster 及以上版本不需要)。\n\n\n# 命令安装\n\nubuntu 容器安装ping ifconfig ip命令\n进入容器测试ifconfig  ping 没有--------\x3e>很尴尬\napt-get install net-tools\n###   ifconfig\napt-get install iputils-ping\n###  ping\napt-get install iproute2\n####  ip\ncentos安装ping\nyum install -y iputils\n\n安装ps命令\napt-get update\napt-get  install  procps\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n",normalizedContent:"警告\n\n操作前请做好相应备份\n\n一般情况下，将 /etc/apt/sources.list 文件中 debian 默认的源地址 http://deb.debian.org/ 替换为 http://mirrors.ustc.edu.cn 即可。\n\n可以使用如下命令：\n\nsudo sed -i 's/deb.debian.org/mirrors.ustc.edu.cn/g' /etc/apt/sources.list\n\n当然也可以直接编辑 /etc/apt/sources.list 文件（需要使用 sudo）。以下是 debian stable 参考配置内容：\n\ndeb http://mirrors.ustc.edu.cn/debian stable main contrib non-free # deb-src http://mirrors.ustc.edu.cn/debian stable main contrib non-free deb http://mirrors.ustc.edu.cn/debian stable-updates main contrib non-free # deb-src http://mirrors.ustc.edu.cn/debian stable-updates main contrib non-free # deb http://mirrors.ustc.edu.cn/debian stable-proposed-updates main contrib non-free # deb-src http://mirrors.ustc.edu.cn/debian stable-proposed-updates main contrib non-free\n\n同时你也可能需要更改 debian security 源，请参考 debian security 源使用帮助\n\nsed -i 's/security.debian.org/mirrors.ustc.edu.cn/g' /etc/apt/sources.list\n\n更改完 sources.list 文件后请运行 sudo apt-get update 更新索引以生效。\n\n小技巧\n\n使用 https 可以有效避免国内运营商的缓存劫持，但需要事先安装 apt-transport-https (debian buster 及以上版本不需要)。\n\n\n# 命令安装\n\nubuntu 容器安装ping ifconfig ip命令\n进入容器测试ifconfig  ping 没有--------\x3e>很尴尬\napt-get install net-tools\n###   ifconfig\napt-get install iputils-ping\n###  ping\napt-get install iproute2\n####  ip\ncentos安装ping\nyum install -y iputils\n\n安装ps命令\napt-get update\napt-get  install  procps\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n",charsets:{cjk:!0}},{title:"Alpine安装php各种扩展",frontmatter:{title:"Alpine安装php各种扩展",date:"2023-01-30T20:43:25.000Z",permalink:"/pages/aa9eee/",categories:["运维","中间件"],tags:[null],readingShow:"top",description:"详细如下",meta:[{name:"twitter:title",content:"Alpine安装php各种扩展"},{name:"twitter:description",content:"详细如下"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/11.other/02.Alpine%E5%AE%89%E8%A3%85php%E5%90%84%E7%A7%8D%E6%89%A9%E5%B1%95.html"},{property:"og:type",content:"article"},{property:"og:title",content:"Alpine安装php各种扩展"},{property:"og:description",content:"详细如下"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/11.other/02.Alpine%E5%AE%89%E8%A3%85php%E5%90%84%E7%A7%8D%E6%89%A9%E5%B1%95.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-30T20:43:25.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"Alpine安装php各种扩展"},{itemprop:"description",content:"详细如下"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/11.other/02.Alpine%E5%AE%89%E8%A3%85php%E5%90%84%E7%A7%8D%E6%89%A9%E5%B1%95.html",relativePath:"04.运维/11.other/02.Alpine安装php各种扩展.md",key:"v-2e1ddaa3",path:"/pages/aa9eee/",headersStr:null,content:'详细如下\n\n#!/bin/sh\n\nsed -i "s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g" /etc/apk/repositories\n\napk add --no-cache autoconf g++ libtool make curl-dev libxml2-dev linux-headers\n\necho "---------- Install mcrypt ----------"\napk add --no-cache libmcrypt-dev\ndocker-php-ext-install mcrypt\n\nech "---------- Install pdo_mysql ----------"\ndocker-php-ext-install -j 2 pdo_mysql\n\necho "---------- Install zip ----------"\ndocker-php-ext-install -j 2 zip\n\necho "---------- Install pcntl ----------"\ndocker-php-ext-install -j 2 pcntl\n\n\necho "---------- Install mysqli ----------"\ndocker-php-ext-install -j 2 mysqli\n\n\necho "---------- Install mbstring ----------"\ndocker-php-ext-install -j 2 mbstring\n\necho "---------- Install exif ----------"\ndocker-php-ext-install -j 2 exif\n\necho "---------- Install calendar ----------"\ndocker-php-ext-install -j 2 calendar\n\necho "---------- Install sockets ----------"\ndocker-php-ext-install -j 2 sockets\n\necho "---------- Install gettext ----------"\napk add --no-cache gettext-dev\ndocker-php-ext-install -j 2 gettext\n\necho "---------- Install shmop ----------"\ndocker-php-ext-install -j 2 shmop\n\necho "---------- Install bz2 ----------"\napk add --no-cache bzip2-dev\ndocker-php-ext-install -j 2 bz2\n\necho "---------- Install xsl ----------"\napk add --no-cache libxslt-dev\ndocker-php-ext-install -j 2 xsl\n\necho "---------- Install wddx ----------"\napk add --no-cache libxslt-dev\nocker-php-ext-install -j 2 wddx\n\necho "---------- Install curl ----------"\ndocker-php-ext-install -j 2 curl\n\necho "---------- Install mysql ----------"\ndocker-php-ext-install -j 2 mysql\n\necho "---------- Install wddx ----------"\ndocker-php-ext-install -j 2 wddx\n\necho "---------- Install readline ----------"\napk add --no-cache readline-dev\napk add --no-cache libedit-dev\ndocker-php-ext-install -j 2 readline\n\necho "---------- Install gmp ----------"\napk add --no-cache gmp-dev\ndocker-php-ext-install -j 2 gmp\n\necho "---------- Install ldap ----------"\napk add --no-cache ldb-dev\napk add --no-cache openldap-dev\ndocker-php-ext-install -j 2 ldap\n\necho "---------- Install redis ----------"\nmkdir redis \\\n&& tar -xf redis-4.1.1.tgz -C redis --strip-components=1 \\\n&& ( cd redis && phpize && ./configure && make && make install ) \\\n&& docker-php-ext-enable redis\n\necho "---------- Install gd ----------"\napk add --no-cache freetype-dev libjpeg-turbo-dev libpng-dev \\\n&& docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\\n&& docker-php-ext-install -j 2 gd\n\necho "---------- Install mhash ----------"\nmkdir mhash \\\n&& tar -xf mhash-0.9.9.9.tar.gz -C mhash --strip-components=1 \\\n&& ( cd mhash  && ./configure && make && make install )\\\n&& docker-php-source extract \\\n&& ( cd /usr/src/php && ./configure --with-mcrypt --with-mhash=/usr/local/include && make && make install )\\\n&& docker-php-source delete\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n\n\n原文链接：Alpine安装php各种扩展_打卤的博客-CSDN博客_alpine 安装php8 xml',normalizedContent:'详细如下\n\n#!/bin/sh\n\nsed -i "s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g" /etc/apk/repositories\n\napk add --no-cache autoconf g++ libtool make curl-dev libxml2-dev linux-headers\n\necho "---------- install mcrypt ----------"\napk add --no-cache libmcrypt-dev\ndocker-php-ext-install mcrypt\n\nech "---------- install pdo_mysql ----------"\ndocker-php-ext-install -j 2 pdo_mysql\n\necho "---------- install zip ----------"\ndocker-php-ext-install -j 2 zip\n\necho "---------- install pcntl ----------"\ndocker-php-ext-install -j 2 pcntl\n\n\necho "---------- install mysqli ----------"\ndocker-php-ext-install -j 2 mysqli\n\n\necho "---------- install mbstring ----------"\ndocker-php-ext-install -j 2 mbstring\n\necho "---------- install exif ----------"\ndocker-php-ext-install -j 2 exif\n\necho "---------- install calendar ----------"\ndocker-php-ext-install -j 2 calendar\n\necho "---------- install sockets ----------"\ndocker-php-ext-install -j 2 sockets\n\necho "---------- install gettext ----------"\napk add --no-cache gettext-dev\ndocker-php-ext-install -j 2 gettext\n\necho "---------- install shmop ----------"\ndocker-php-ext-install -j 2 shmop\n\necho "---------- install bz2 ----------"\napk add --no-cache bzip2-dev\ndocker-php-ext-install -j 2 bz2\n\necho "---------- install xsl ----------"\napk add --no-cache libxslt-dev\ndocker-php-ext-install -j 2 xsl\n\necho "---------- install wddx ----------"\napk add --no-cache libxslt-dev\nocker-php-ext-install -j 2 wddx\n\necho "---------- install curl ----------"\ndocker-php-ext-install -j 2 curl\n\necho "---------- install mysql ----------"\ndocker-php-ext-install -j 2 mysql\n\necho "---------- install wddx ----------"\ndocker-php-ext-install -j 2 wddx\n\necho "---------- install readline ----------"\napk add --no-cache readline-dev\napk add --no-cache libedit-dev\ndocker-php-ext-install -j 2 readline\n\necho "---------- install gmp ----------"\napk add --no-cache gmp-dev\ndocker-php-ext-install -j 2 gmp\n\necho "---------- install ldap ----------"\napk add --no-cache ldb-dev\napk add --no-cache openldap-dev\ndocker-php-ext-install -j 2 ldap\n\necho "---------- install redis ----------"\nmkdir redis \\\n&& tar -xf redis-4.1.1.tgz -c redis --strip-components=1 \\\n&& ( cd redis && phpize && ./configure && make && make install ) \\\n&& docker-php-ext-enable redis\n\necho "---------- install gd ----------"\napk add --no-cache freetype-dev libjpeg-turbo-dev libpng-dev \\\n&& docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\\n&& docker-php-ext-install -j 2 gd\n\necho "---------- install mhash ----------"\nmkdir mhash \\\n&& tar -xf mhash-0.9.9.9.tar.gz -c mhash --strip-components=1 \\\n&& ( cd mhash  && ./configure && make && make install )\\\n&& docker-php-source extract \\\n&& ( cd /usr/src/php && ./configure --with-mcrypt --with-mhash=/usr/local/include && make && make install )\\\n&& docker-php-source delete\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n\n\n原文链接：alpine安装php各种扩展_打卤的博客-csdn博客_alpine 安装php8 xml',charsets:{cjk:!0}},{title:"docker文件安装zabbix5",frontmatter:{title:"docker文件安装zabbix5",date:"2022-12-20T16:08:39.000Z",permalink:"/pages/e0085e/",categories:["专题","zabbix"],tags:[null],readingShow:"top",description:"version: '2'\nservices:\n zabbix-server:\n  image: zabbix/zabbix-server-mysql:centos-5.0-latest\n  networks:\n   zabb:\n    ipv4_address: 172.19.0.2\n  ports:\n   \"10051:10051\"\n  volumes:\n   /etc/localtime:/etc/localtime\n   /etc/timezone:/etc/timezone\n   ./zbx_env/usr/lib/zabbix/alertscripts:/usr/lib/zabbix/alertscripts\n   #- ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts\n   #- ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules\n   #- ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc\n   #- ./zbxenv/var/lib/zabbix/sshkeys:/var/lib/zabbix/ssh_keys\n   #- ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs\n  privileged: true\n  volumes_from:\n   zabbix-snmptraps:rw",meta:[{name:"twitter:title",content:"docker文件安装zabbix5"},{name:"twitter:description",content:"version: '2'\nservices:\n zabbix-server:\n  image: zabbix/zabbix-server-mysql:centos-5.0-latest\n  networks:\n   zabb:\n    ipv4_address: 172.19.0.2\n  ports:\n   \"10051:10051\"\n  volumes:\n   /etc/localtime:/etc/localtime\n   /etc/timezone:/etc/timezone\n   ./zbx_env/usr/lib/zabbix/alertscripts:/usr/lib/zabbix/alertscripts\n   #- ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts\n   #- ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules\n   #- ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc\n   #- ./zbxenv/var/lib/zabbix/sshkeys:/var/lib/zabbix/ssh_keys\n   #- ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs\n  privileged: true\n  volumes_from:\n   zabbix-snmptraps:rw"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/03.docker%E6%96%87%E4%BB%B6%E5%AE%89%E8%A3%85zabbix5.html"},{property:"og:type",content:"article"},{property:"og:title",content:"docker文件安装zabbix5"},{property:"og:description",content:"version: '2'\nservices:\n zabbix-server:\n  image: zabbix/zabbix-server-mysql:centos-5.0-latest\n  networks:\n   zabb:\n    ipv4_address: 172.19.0.2\n  ports:\n   \"10051:10051\"\n  volumes:\n   /etc/localtime:/etc/localtime\n   /etc/timezone:/etc/timezone\n   ./zbx_env/usr/lib/zabbix/alertscripts:/usr/lib/zabbix/alertscripts\n   #- ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts\n   #- ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules\n   #- ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc\n   #- ./zbxenv/var/lib/zabbix/sshkeys:/var/lib/zabbix/ssh_keys\n   #- ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs\n  privileged: true\n  volumes_from:\n   zabbix-snmptraps:rw"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/03.docker%E6%96%87%E4%BB%B6%E5%AE%89%E8%A3%85zabbix5.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-20T16:08:39.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"docker文件安装zabbix5"},{itemprop:"description",content:"version: '2'\nservices:\n zabbix-server:\n  image: zabbix/zabbix-server-mysql:centos-5.0-latest\n  networks:\n   zabb:\n    ipv4_address: 172.19.0.2\n  ports:\n   \"10051:10051\"\n  volumes:\n   /etc/localtime:/etc/localtime\n   /etc/timezone:/etc/timezone\n   ./zbx_env/usr/lib/zabbix/alertscripts:/usr/lib/zabbix/alertscripts\n   #- ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts\n   #- ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules\n   #- ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc\n   #- ./zbxenv/var/lib/zabbix/sshkeys:/var/lib/zabbix/ssh_keys\n   #- ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs\n  privileged: true\n  volumes_from:\n   zabbix-snmptraps:rw"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/03.docker%E6%96%87%E4%BB%B6%E5%AE%89%E8%A3%85zabbix5.html",relativePath:"04.运维/12.监控/01.zabbix/03.docker文件安装zabbix5.md",key:"v-10b832a2",path:"/pages/e0085e/",headers:[{level:4,title:"docker-compose.yaml文件",slug:"docker-compose-yaml文件",normalizedTitle:"docker-compose.yaml文件",charIndex:2},{level:4,title:"字体乱码修改",slug:"字体乱码修改",normalizedTitle:"字体乱码修改",charIndex:3147},{level:4,title:"docker-compose 5.4安装文件",slug:"docker-compose-5-4安装文件",normalizedTitle:"docker-compose 5.4安装文件",charIndex:3390}],headersStr:"docker-compose.yaml文件 字体乱码修改 docker-compose 5.4安装文件",content:'# docker-compose.yaml文件\n\nversion: \'2\'\nservices:\n zabbix-server:\n  image: zabbix/zabbix-server-mysql:centos-5.0-latest\n  networks:\n   zabb:\n    ipv4_address: 172.19.0.2\n  ports:\n   - "10051:10051"\n  volumes:\n   - /etc/localtime:/etc/localtime\n   - /etc/timezone:/etc/timezone \n   - ./zbx_env/usr/lib/zabbix/alertscripts:/usr/lib/zabbix/alertscripts\n   #- ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts\n   #- ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules\n   #- ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc\n   #- ./zbx_env/var/lib/zabbix/ssh_keys:/var/lib/zabbix/ssh_keys\n   #- ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs\n  privileged: true\n  volumes_from:\n   - zabbix-snmptraps:rw\n\n  links:\n   - mysql-server:mysql-server\n   - zabbix-java-gateway:zabbix-java-gateway\n  ulimits:\n   nproc: 65535\n   nofile:\n    soft: 20000\n    hard: 40000\n  mem_limit: 1024m\n  env_file:\n   - .env_db_mysql\n   - .env_srv\n\n zabbix-web-nginx-mysql:\n  image: zabbix/zabbix-web-nginx-mysql:centos-5.0-latest\n  networks:\n   zabb:\n    ipv4_address: 172.19.0.3\n  ports:\n   - 82:8080\n   - 9010:9000\n   #- "8443:443"\n  links:\n   - mysql-server:mysql-server\n   - zabbix-server:zabbix-server\n  mem_limit: 512m\n  volumes:\n   - /etc/localtime:/etc/localtime\n   - /etc/timezone:/etc/timezone\n   - /usr/share/zabbix:/usr/share/zabbix\n   #- ./zbx_env/etc/php-fpm.d:/etc/php-fpm.d\n   #- ./zbx_env/phpsock/:/tmp/proxy\n   #- ./zbx_env/etc/php-fpm.conf:/etc/php-fpm.conf\n   #- ./zbx_env/etc/php.ini:/etc/php.ini\n   #- ./zbx_env/etc/nginx/conf.d:/etc/nginx/conf.d\n  env_file:\n   - .env_db_mysql\n   - .env_web\n zabbix-java-gateway:\n   image: zabbix/zabbix-java-gateway:centos-5.0-latest\n   networks:\n    zabb:\n     ipv4_address: 172.19.0.5\n   ports:\n    - "10052:10052"\n   env_file:\n    - .env_java\n zabbix-snmptraps:\n   image: zabbix/zabbix-snmptraps:centos-5.0-latest\n   networks:\n    zabb:\n     ipv4_address: 172.19.0.6\n   ports:\n    - "162:162/udp"\n   privileged: true\n   #volumes:\n    #- ./zbx_env/var/lib/zabbix/snmptraps:/var/lib/zabbix/snmptraps:rw\n\n mysql-server:\n  image: mysql:5.7\n  networks:\n   zabb:\n    ipv4_address: 172.19.0.7\n  ports:\n   - "3306:3306"\n  volumes:\n   - /data/zabbix_mysql/mysql:/var/lib/mysql\n   #- /data/zabbix_mysql/zabbix_mysqld.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf\n  environment:\n   MYSQL_ROOT_PASSWORD: "mysql_pwd"\n  command: [mysqld, --character-set-server=utf8, --collation-server=utf8_bin]\n  #volumes_from:\n    #- db_data_mysql\n  #volume_driver: local\n  #env_file:\n   #- .env_db_mysql\n\n #db_data_mysql:\n    #image: busybox\n    #networks:\n     #zabb:\n      #ipv4_address: 172.19.0.8\n    #volumes:\n    #- ./zbx_env/var/lib/mysql:/var/lib/mysql:rw\nnetworks:\n zabb:\n  driver: bridge\n  ipam:\n   config:\n    - subnet: 172.19.0.0/24\n      gateway: 172.19.0.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n\n\n# 字体乱码修改\n\n将中文字体放到这个目录下\n\n/opt/apps/zabbix/zbx_env/usr/share/zabbix/assets/fonts\n\n修改php配置\n\n/opt/apps/zabbix/zbx_env/usr/share/zabbix/include/defines.inc.php\n\ndefine(\'ZBX_GRAPH_FONT_NAME\', \'simhei\');\n\ndefine(\'ZBX_FONT_NAME\', \'simhei\');\n\n重启web服务\n\n# docker-compose 5.4安装文件\n\nversion: "3"\nservices:\n  mysql-server:\n    container_name: mysql-server\n    image: mysql:8.0\n    ports:\n      - "3306:3306"\n    volumes:\n      - /etc/localtime:/etc/localtime\n      - /data/mysql:/var/lib/mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: root_pwd\n      MYSQL_DATABASE: zabbix\n      MYSQL_USER: zabbix\n      MYSQL_PASSWORD: zabbix_pwd\n      TZ: Asia/Shanghai\n    command: \n      --character-set-server=utf8\n      --collation-server=utf8_bin\n    restart: always\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.2\n  zabbix-java-gateway:\n    container_name: zabbix-java-gateway\n    image: zabbix/zabbix-java-gateway:alpine-5.4-latest\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.3\n  zabbix-server:\n    container_name: zabbix-server\n    image: zabbix/zabbix-server-mysql:alpine-5.4-latest\n    ports:\n      - "10051:10051"\n    volumes:\n      - ./zbx_env/usr/lib/zabbix/alertscripts:/usr/lib/zabbix/alertscripts\n      - ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts\n      - ./zbx_env/var/lib/zabbix/export:/var/lib/zabbix/export\n      - ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules\n      - ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc\n      - ./zbx_env/var/lib/zabbix/ssh_keys:/var/lib/zabbix/ssh_keys\n      - ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs\n      - ./zbx_env/var/lib/zabbix/snmptraps:/var/lib/zabbix/snmptraps\n      - ./zbx_env/etc/zabbix/zabbix_server.conf:/etc/zabbix/zabbix_server.conf\n    environment:\n      DB_SERVER_HOST: mysql-server\n      DB_SERVER_PORT: 3306\n      MYSQL_DATABASE: zabbix\n      MYSQL_USER: zabbix\n      MYSQL_PASSWORD: zabbix_pwd\n      ZBX_JAVAGATEWAY: "zabbix-java-gateway"\n      TZ: Asia/Shanghai\n    depends_on:\n      - mysql-server\n      - zabbix-java-gateway\n      - zabbix-web-service\n    restart: always\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.4\n  zabbix-web:\n    container_name: zabbix-web\n    image: zabbix/zabbix-web-nginx-mysql:alpine-5.4-latest\n    ports:\n      - "80:8080"\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /etc/timezone:/etc/timezone:ro\n      - ./zbx_env/usr/share/zabbix:/usr/share/zabbix\n      - ./zbx_env/etc/php-fpm.d:/etc/php7/php-fpm.d\n      - ./zbx_env/etc/php-fpm.conf:/etc/php7/php-fpm.conf\n      - ./zbx_env/etc/php.ini:/etc/php7/php.ini\n    environment:\n      DB_SERVER_HOST: mysql-server\n      DB_SERVER_PORT: 3306\n      MYSQL_DATABASE: zabbix\n      MYSQL_USER: zabbix\n      MYSQL_PASSWORD: zabbix_pwd\n      MYSQL_ROOT_PASSWORD: root_pwd\n      TZ: Asia/Shanghai\n      ZBX_SERVER_HOST: zabbix-server\n    depends_on:\n      - mysql-server\n      - zabbix-server\n    restart: always\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.5\n  zabbix-web-service:\n    container_name: zabbix-web-service\n    image: zabbix/zabbix-web-service:alpine-5.4-latest\n#    profiles:\n#      - full\n#      - all\n    ports:\n      - "10053:10053"\n#    volumes:\n#      - ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc:ro\n    cap_add:\n      - SYS_ADMIN\n    extra_hosts:\n      - "zabbix-server:172.21.0.4"\n    deploy:\n      resources:\n        limits:\n          cpus: \'0.5\'\n          memory: 512M\n        reservations:\n          cpus: \'0.25\'\n          memory: 256M\n#    env_file:\n#      - ./env_vars/.env_web_service\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.6\n  zabbix-agent:\n    container_name: zabbix-agent\n    image: zabbix/zabbix-agent:alpine-5.4-latest\n    links:\n      - zabbix-server:zabbix-server\n    ports:                                                                                                                                              \n      - "10050:10050"\n    privileged: true\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.7\nnetworks:\n  zabb:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.21.0.0/24\n          gateway: 172.21.0.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n',normalizedContent:'# docker-compose.yaml文件\n\nversion: \'2\'\nservices:\n zabbix-server:\n  image: zabbix/zabbix-server-mysql:centos-5.0-latest\n  networks:\n   zabb:\n    ipv4_address: 172.19.0.2\n  ports:\n   - "10051:10051"\n  volumes:\n   - /etc/localtime:/etc/localtime\n   - /etc/timezone:/etc/timezone \n   - ./zbx_env/usr/lib/zabbix/alertscripts:/usr/lib/zabbix/alertscripts\n   #- ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts\n   #- ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules\n   #- ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc\n   #- ./zbx_env/var/lib/zabbix/ssh_keys:/var/lib/zabbix/ssh_keys\n   #- ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs\n  privileged: true\n  volumes_from:\n   - zabbix-snmptraps:rw\n\n  links:\n   - mysql-server:mysql-server\n   - zabbix-java-gateway:zabbix-java-gateway\n  ulimits:\n   nproc: 65535\n   nofile:\n    soft: 20000\n    hard: 40000\n  mem_limit: 1024m\n  env_file:\n   - .env_db_mysql\n   - .env_srv\n\n zabbix-web-nginx-mysql:\n  image: zabbix/zabbix-web-nginx-mysql:centos-5.0-latest\n  networks:\n   zabb:\n    ipv4_address: 172.19.0.3\n  ports:\n   - 82:8080\n   - 9010:9000\n   #- "8443:443"\n  links:\n   - mysql-server:mysql-server\n   - zabbix-server:zabbix-server\n  mem_limit: 512m\n  volumes:\n   - /etc/localtime:/etc/localtime\n   - /etc/timezone:/etc/timezone\n   - /usr/share/zabbix:/usr/share/zabbix\n   #- ./zbx_env/etc/php-fpm.d:/etc/php-fpm.d\n   #- ./zbx_env/phpsock/:/tmp/proxy\n   #- ./zbx_env/etc/php-fpm.conf:/etc/php-fpm.conf\n   #- ./zbx_env/etc/php.ini:/etc/php.ini\n   #- ./zbx_env/etc/nginx/conf.d:/etc/nginx/conf.d\n  env_file:\n   - .env_db_mysql\n   - .env_web\n zabbix-java-gateway:\n   image: zabbix/zabbix-java-gateway:centos-5.0-latest\n   networks:\n    zabb:\n     ipv4_address: 172.19.0.5\n   ports:\n    - "10052:10052"\n   env_file:\n    - .env_java\n zabbix-snmptraps:\n   image: zabbix/zabbix-snmptraps:centos-5.0-latest\n   networks:\n    zabb:\n     ipv4_address: 172.19.0.6\n   ports:\n    - "162:162/udp"\n   privileged: true\n   #volumes:\n    #- ./zbx_env/var/lib/zabbix/snmptraps:/var/lib/zabbix/snmptraps:rw\n\n mysql-server:\n  image: mysql:5.7\n  networks:\n   zabb:\n    ipv4_address: 172.19.0.7\n  ports:\n   - "3306:3306"\n  volumes:\n   - /data/zabbix_mysql/mysql:/var/lib/mysql\n   #- /data/zabbix_mysql/zabbix_mysqld.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf\n  environment:\n   mysql_root_password: "mysql_pwd"\n  command: [mysqld, --character-set-server=utf8, --collation-server=utf8_bin]\n  #volumes_from:\n    #- db_data_mysql\n  #volume_driver: local\n  #env_file:\n   #- .env_db_mysql\n\n #db_data_mysql:\n    #image: busybox\n    #networks:\n     #zabb:\n      #ipv4_address: 172.19.0.8\n    #volumes:\n    #- ./zbx_env/var/lib/mysql:/var/lib/mysql:rw\nnetworks:\n zabb:\n  driver: bridge\n  ipam:\n   config:\n    - subnet: 172.19.0.0/24\n      gateway: 172.19.0.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n\n\n# 字体乱码修改\n\n将中文字体放到这个目录下\n\n/opt/apps/zabbix/zbx_env/usr/share/zabbix/assets/fonts\n\n修改php配置\n\n/opt/apps/zabbix/zbx_env/usr/share/zabbix/include/defines.inc.php\n\ndefine(\'zbx_graph_font_name\', \'simhei\');\n\ndefine(\'zbx_font_name\', \'simhei\');\n\n重启web服务\n\n# docker-compose 5.4安装文件\n\nversion: "3"\nservices:\n  mysql-server:\n    container_name: mysql-server\n    image: mysql:8.0\n    ports:\n      - "3306:3306"\n    volumes:\n      - /etc/localtime:/etc/localtime\n      - /data/mysql:/var/lib/mysql\n    environment:\n      mysql_root_password: root_pwd\n      mysql_database: zabbix\n      mysql_user: zabbix\n      mysql_password: zabbix_pwd\n      tz: asia/shanghai\n    command: \n      --character-set-server=utf8\n      --collation-server=utf8_bin\n    restart: always\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.2\n  zabbix-java-gateway:\n    container_name: zabbix-java-gateway\n    image: zabbix/zabbix-java-gateway:alpine-5.4-latest\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.3\n  zabbix-server:\n    container_name: zabbix-server\n    image: zabbix/zabbix-server-mysql:alpine-5.4-latest\n    ports:\n      - "10051:10051"\n    volumes:\n      - ./zbx_env/usr/lib/zabbix/alertscripts:/usr/lib/zabbix/alertscripts\n      - ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts\n      - ./zbx_env/var/lib/zabbix/export:/var/lib/zabbix/export\n      - ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules\n      - ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc\n      - ./zbx_env/var/lib/zabbix/ssh_keys:/var/lib/zabbix/ssh_keys\n      - ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs\n      - ./zbx_env/var/lib/zabbix/snmptraps:/var/lib/zabbix/snmptraps\n      - ./zbx_env/etc/zabbix/zabbix_server.conf:/etc/zabbix/zabbix_server.conf\n    environment:\n      db_server_host: mysql-server\n      db_server_port: 3306\n      mysql_database: zabbix\n      mysql_user: zabbix\n      mysql_password: zabbix_pwd\n      zbx_javagateway: "zabbix-java-gateway"\n      tz: asia/shanghai\n    depends_on:\n      - mysql-server\n      - zabbix-java-gateway\n      - zabbix-web-service\n    restart: always\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.4\n  zabbix-web:\n    container_name: zabbix-web\n    image: zabbix/zabbix-web-nginx-mysql:alpine-5.4-latest\n    ports:\n      - "80:8080"\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /etc/timezone:/etc/timezone:ro\n      - ./zbx_env/usr/share/zabbix:/usr/share/zabbix\n      - ./zbx_env/etc/php-fpm.d:/etc/php7/php-fpm.d\n      - ./zbx_env/etc/php-fpm.conf:/etc/php7/php-fpm.conf\n      - ./zbx_env/etc/php.ini:/etc/php7/php.ini\n    environment:\n      db_server_host: mysql-server\n      db_server_port: 3306\n      mysql_database: zabbix\n      mysql_user: zabbix\n      mysql_password: zabbix_pwd\n      mysql_root_password: root_pwd\n      tz: asia/shanghai\n      zbx_server_host: zabbix-server\n    depends_on:\n      - mysql-server\n      - zabbix-server\n    restart: always\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.5\n  zabbix-web-service:\n    container_name: zabbix-web-service\n    image: zabbix/zabbix-web-service:alpine-5.4-latest\n#    profiles:\n#      - full\n#      - all\n    ports:\n      - "10053:10053"\n#    volumes:\n#      - ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc:ro\n    cap_add:\n      - sys_admin\n    extra_hosts:\n      - "zabbix-server:172.21.0.4"\n    deploy:\n      resources:\n        limits:\n          cpus: \'0.5\'\n          memory: 512m\n        reservations:\n          cpus: \'0.25\'\n          memory: 256m\n#    env_file:\n#      - ./env_vars/.env_web_service\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.6\n  zabbix-agent:\n    container_name: zabbix-agent\n    image: zabbix/zabbix-agent:alpine-5.4-latest\n    links:\n      - zabbix-server:zabbix-server\n    ports:                                                                                                                                              \n      - "10050:10050"\n    privileged: true\n    networks:\n      zabb:\n        ipv4_address: 172.21.0.7\nnetworks:\n  zabb:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.21.0.0/24\n          gateway: 172.21.0.1\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n',charsets:{cjk:!0}},{title:"zabbix配置钉钉告警",frontmatter:{title:"zabbix配置钉钉告警",date:"2022-12-20T16:16:02.000Z",permalink:"/pages/a4e3ce/",categories:["专题","zabbix"],tags:[null],readingShow:"top",description:"脚本如下",meta:[{name:"image",content:"https://gitee.com/zhang_peng_jie/images/raw/master/img/2022121220221220162025.jpg"},{name:"twitter:title",content:"zabbix配置钉钉告警"},{name:"twitter:description",content:"脚本如下"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://gitee.com/zhang_peng_jie/images/raw/master/img/2022121220221220162025.jpg"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/04.zabbix%E9%85%8D%E7%BD%AE%E9%92%89%E9%92%89%E5%91%8A%E8%AD%A6.html"},{property:"og:type",content:"article"},{property:"og:title",content:"zabbix配置钉钉告警"},{property:"og:description",content:"脚本如下"},{property:"og:image",content:"https://gitee.com/zhang_peng_jie/images/raw/master/img/2022121220221220162025.jpg"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/04.zabbix%E9%85%8D%E7%BD%AE%E9%92%89%E9%92%89%E5%91%8A%E8%AD%A6.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-20T16:16:02.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"zabbix配置钉钉告警"},{itemprop:"description",content:"脚本如下"},{itemprop:"image",content:"https://gitee.com/zhang_peng_jie/images/raw/master/img/2022121220221220162025.jpg"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/04.zabbix%E9%85%8D%E7%BD%AE%E9%92%89%E9%92%89%E5%91%8A%E8%AD%A6.html",relativePath:"04.运维/12.监控/01.zabbix/04.zabbix配置钉钉告警.md",key:"v-282498df",path:"/pages/a4e3ce/",headers:[{level:4,title:"将脚本放到zabbix-server服务器路径：/usr/lib/zabbix/alertscripts",slug:"将脚本放到zabbix-server服务器路径-usr-lib-zabbix-alertscripts",normalizedTitle:"将脚本放到zabbix-server服务器路径：/usr/lib/zabbix/alertscripts",charIndex:2},{level:4,title:"报警媒介配置",slug:"报警媒介配置",normalizedTitle:"报警媒介配置",charIndex:1227},{level:4,title:"配置触发器动作",slug:"配置触发器动作",normalizedTitle:"配置触发器动作",charIndex:1239},{level:4,title:"设置告警用户，添加报警媒介",slug:"设置告警用户-添加报警媒介",normalizedTitle:"设置告警用户，添加报警媒介",charIndex:1482}],headersStr:"将脚本放到zabbix-server服务器路径：/usr/lib/zabbix/alertscripts 报警媒介配置 配置触发器动作 设置告警用户，添加报警媒介",content:'# 将脚本放到zabbix-server服务器路径：/usr/lib/zabbix/alertscripts\n\n脚本如下\n\n#!/usr/bin/python\n#-*- coding: utf-8 -*-\nimport requests,json,sys,os,datetime\nwebhook="https://oapi.dingtalk.com/robot/send?access_token=712c3d89c613a3cce737bd1ed5ce8c7b9ebd5ee48c115236873b2a2a54c5efcf"\n\n#说明：这里改为自己创建的机器人的webhook的值 \nuser=sys.argv[1]\n#发给钉钉群中哪个用户\ntext=sys.argv[3]\n\ndata={ \n    "msgtype": "text", \n    "text": { \n        "content": text \n    }, \n    "at": { \n        "atMobiles": [\n            user \n        ],\n        "isAtAll": False\n    } \n}\n#钉钉API固定数据格式\nheaders = {\'Content-Type\': \'application/json\'}\nx=requests.post(url=webhook,data=json.dumps(data),headers=headers)\nif os.path.exists("/usr/lib/zabbix/alertscripts/dingding.log"):\n    f=open("/usr/lib/zabbix/alertscripts/dingding.log","a+")\nelse:\n    f=open("/usr/lib/zabbix/alertscripts/dingding.log","w+")\nf.write("\\n"+"--"*30)\nif x.json()["errcode"] == 0:\n    f.write("\\n"+str(datetime.datetime.now())+"    "+str(user)+"    "+"发送成功"+"\\n"+str(text))\n    f.close()\nelse:\n    f.write("\\n"+str(datetime.datetime.now()) + "    " + str(user) + "    " + "发送失败" + "\\n" + str(text))\n    f.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n# 报警媒介配置\n\n\n\n# 配置触发器动作\n\n\n\n\n\n\n\n故障名称：{EVENT.NAME}\n\n服务器：{HOST.NAME} 发生：{TRIGGER.NAME}故障！ 告警主机：{HOST.NAME} 监控项目：{ITEM.NAME} 监控取值：{ITEM.LASTVALUE} 告警等级：{TRIGGER.SEVERITY} 当前状态：{TRIGGER.STATUS} 告警信息：{TRIGGER.NAME} 告警时间：{EVENT.DATE} {EVENT.TIME} 事件ID:{EVENT.ID}\n\n# 设置告警用户，添加报警媒介\n\n',normalizedContent:'# 将脚本放到zabbix-server服务器路径：/usr/lib/zabbix/alertscripts\n\n脚本如下\n\n#!/usr/bin/python\n#-*- coding: utf-8 -*-\nimport requests,json,sys,os,datetime\nwebhook="https://oapi.dingtalk.com/robot/send?access_token=712c3d89c613a3cce737bd1ed5ce8c7b9ebd5ee48c115236873b2a2a54c5efcf"\n\n#说明：这里改为自己创建的机器人的webhook的值 \nuser=sys.argv[1]\n#发给钉钉群中哪个用户\ntext=sys.argv[3]\n\ndata={ \n    "msgtype": "text", \n    "text": { \n        "content": text \n    }, \n    "at": { \n        "atmobiles": [\n            user \n        ],\n        "isatall": false\n    } \n}\n#钉钉api固定数据格式\nheaders = {\'content-type\': \'application/json\'}\nx=requests.post(url=webhook,data=json.dumps(data),headers=headers)\nif os.path.exists("/usr/lib/zabbix/alertscripts/dingding.log"):\n    f=open("/usr/lib/zabbix/alertscripts/dingding.log","a+")\nelse:\n    f=open("/usr/lib/zabbix/alertscripts/dingding.log","w+")\nf.write("\\n"+"--"*30)\nif x.json()["errcode"] == 0:\n    f.write("\\n"+str(datetime.datetime.now())+"    "+str(user)+"    "+"发送成功"+"\\n"+str(text))\n    f.close()\nelse:\n    f.write("\\n"+str(datetime.datetime.now()) + "    " + str(user) + "    " + "发送失败" + "\\n" + str(text))\n    f.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n# 报警媒介配置\n\n\n\n# 配置触发器动作\n\n\n\n\n\n\n\n故障名称：{event.name}\n\n服务器：{host.name} 发生：{trigger.name}故障！ 告警主机：{host.name} 监控项目：{item.name} 监控取值：{item.lastvalue} 告警等级：{trigger.severity} 当前状态：{trigger.status} 告警信息：{trigger.name} 告警时间：{event.date} {event.time} 事件id:{event.id}\n\n# 设置告警用户，添加报警媒介\n\n',charsets:{cjk:!0}},{title:"zabbix添加端口和进程监控",frontmatter:{title:"zabbix添加端口和进程监控",date:"2022-12-14T15:03:30.000Z",permalink:"/pages/a0a3ae/",categories:["专题","zabbix"],tags:[null],readingShow:"top",description:"在监控生产环境的服务的时候，通常需要对多个端口进行监控，如果我们手动一个一个的添加，这将是一件很麻烦的事情，为了减少这样的情况，我们采用批量添加监控端口的方法，这是非常常见的需求，zabbix也是支持这种方式的，需要使用zabbix的Discovery功能来实现。\n>\n> 使用zabbix监控如何监控服务器端口状态，大概的流程：zabbix监控服务自带端口监控的监控项，所以需要我们自己手动定义所监控的item，客户端获取的端口列表通过agent传送到服务端。\n>\n> 只需在服务端进行端口监控模板配置，然后自定义监控图形，添加监控项即可！",meta:[{name:"image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212Dingtalk_20221220163305.jpg"},{name:"twitter:title",content:"zabbix添加端口和进程监控"},{name:"twitter:description",content:"在监控生产环境的服务的时候，通常需要对多个端口进行监控，如果我们手动一个一个的添加，这将是一件很麻烦的事情，为了减少这样的情况，我们采用批量添加监控端口的方法，这是非常常见的需求，zabbix也是支持这种方式的，需要使用zabbix的Discovery功能来实现。\n>\n> 使用zabbix监控如何监控服务器端口状态，大概的流程：zabbix监控服务自带端口监控的监控项，所以需要我们自己手动定义所监控的item，客户端获取的端口列表通过agent传送到服务端。\n>\n> 只需在服务端进行端口监控模板配置，然后自定义监控图形，添加监控项即可！"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212Dingtalk_20221220163305.jpg"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/02.zabbix%E6%B7%BB%E5%8A%A0%E7%AB%AF%E5%8F%A3%E5%92%8C%E8%BF%9B%E7%A8%8B%E7%9B%91%E6%8E%A7.html"},{property:"og:type",content:"article"},{property:"og:title",content:"zabbix添加端口和进程监控"},{property:"og:description",content:"在监控生产环境的服务的时候，通常需要对多个端口进行监控，如果我们手动一个一个的添加，这将是一件很麻烦的事情，为了减少这样的情况，我们采用批量添加监控端口的方法，这是非常常见的需求，zabbix也是支持这种方式的，需要使用zabbix的Discovery功能来实现。\n>\n> 使用zabbix监控如何监控服务器端口状态，大概的流程：zabbix监控服务自带端口监控的监控项，所以需要我们自己手动定义所监控的item，客户端获取的端口列表通过agent传送到服务端。\n>\n> 只需在服务端进行端口监控模板配置，然后自定义监控图形，添加监控项即可！"},{property:"og:image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212Dingtalk_20221220163305.jpg"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/02.zabbix%E6%B7%BB%E5%8A%A0%E7%AB%AF%E5%8F%A3%E5%92%8C%E8%BF%9B%E7%A8%8B%E7%9B%91%E6%8E%A7.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-14T15:03:30.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"zabbix添加端口和进程监控"},{itemprop:"description",content:"在监控生产环境的服务的时候，通常需要对多个端口进行监控，如果我们手动一个一个的添加，这将是一件很麻烦的事情，为了减少这样的情况，我们采用批量添加监控端口的方法，这是非常常见的需求，zabbix也是支持这种方式的，需要使用zabbix的Discovery功能来实现。\n>\n> 使用zabbix监控如何监控服务器端口状态，大概的流程：zabbix监控服务自带端口监控的监控项，所以需要我们自己手动定义所监控的item，客户端获取的端口列表通过agent传送到服务端。\n>\n> 只需在服务端进行端口监控模板配置，然后自定义监控图形，添加监控项即可！"},{itemprop:"image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212Dingtalk_20221220163305.jpg"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/02.zabbix%E6%B7%BB%E5%8A%A0%E7%AB%AF%E5%8F%A3%E5%92%8C%E8%BF%9B%E7%A8%8B%E7%9B%91%E6%8E%A7.html",relativePath:"04.运维/12.监控/01.zabbix/02.zabbix添加端口和进程监控.md",key:"v-7b122aa1",path:"/pages/a0a3ae/",headers:[{level:4,title:"扫描端口脚本",slug:"扫描端口脚本",normalizedTitle:"扫描端口脚本",charIndex:279},{level:4,title:"新建zabbix模板",slug:"新建zabbix模板",normalizedTitle:"新建zabbix模板",charIndex:1214},{level:4,title:"自动发现批量监测指定进程服务方案优化（对于没有固定端口的服务可采用此方式）",slug:"自动发现批量监测指定进程服务方案优化-对于没有固定端口的服务可采用此方式",normalizedTitle:"自动发现批量监测指定进程服务方案优化（对于没有固定端口的服务可采用此方式）",charIndex:1264}],headersStr:"扫描端口脚本 新建zabbix模板 自动发现批量监测指定进程服务方案优化（对于没有固定端口的服务可采用此方式）",content:'> 在监控生产环境的服务的时候，通常需要对多个端口进行监控，如果我们手动一个一个的添加，这将是一件很麻烦的事情，为了减少这样的情况，我们采用批量添加监控端口的方法，这是非常常见的需求，zabbix也是支持这种方式的，需要使用zabbix的Discovery功能来实现。\n> \n> 使用zabbix监控如何监控服务器端口状态，大概的流程：zabbix监控服务自带端口监控的监控项，所以需要我们自己手动定义所监控的item，客户端获取的端口列表通过agent传送到服务端。\n> \n> 只需在服务端进行端口监控模板配置，然后自定义监控图形，添加监控项即可！\n\n# 扫描端口脚本\n\ncheck_port.py\n\n#!/usr/bin/env python\n#coding:utf-8\n\nimport json\n\ntotal_dict={"data":[\n{"{#TCP_PORT}":"10690","{#SERVICE_NAME}":"bms_frontend"},\n{"{#TCP_PORT}":"10691","{#SERVICE_NAME}":"bms_backend"},\n{"{#TCP_PORT}":"30004","{#SERVICE_NAME}":"idc_web_uniappp"},\n{"{#TCP_PORT}":"30003","{#SERVICE_NAME}":"idc_service_go"},\n{"{#TCP_PORT}":"30000","{#SERVICE_NAME}":"idc_web_website"},\n{"{#TCP_PORT}":"30002","{#SERVICE_NAME}":"idc_web_h5"},\n{"{#TCP_PORT}":"30001","{#SERVICE_NAME}":"idc_web_admin"},\n{"{#TCP_PORT}":"10585","{#SERVICE_NAME}":"gg_h5_fz"},\n{"{#TCP_PORT}":"80","{#SERVICE_NAME}":"proxy"},\n{"{#TCP_PORT}":"16379","{#SERVICE_NAME}":"redis"}\n]}\n\njsonStr = json.dumps(total_dict,sort_keys=True,indent=4,ensure_ascii=False)\n\n#python3\n#print(jsonStr)\n\n#python2\nprint jsonStr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\ncheck_port.conf\n\nUserParameter=tcpportlisten,/etc/zabbix/sh/check_port.py\n\n# 新建zabbix模板\n\n\n\n创建自动发现\n\n\n\n创建自动发现的监控项原型和触发器类型\n\n\n\n\n\n# 自动发现批量监测指定进程服务方案优化（对于没有固定端口的服务可采用此方式）\n\n进程服务发现脚本 check_proc.py\n\n#!/usr/bin/env python\n#coding:utf-8\n\nimport json\n\ntotal_dict={"data":[\n{"{#CMDLINE}":"accountinfo-provider","{#SERVICE_NAME}":"accountinfo-provider"},\n{"{#CMDLINE}":"account-provider","{#SERVICE_NAME}":"account-provider"},\n{"{#CMDLINE}":"account-user-provider","{#SERVICE_NAME}":"account-user-provider"},\n{"{#CMDLINE}":"account-userSeq-provider","{#SERVICE_NAME}":"account-userSeq-provider"},\n{"{#CMDLINE}":"loginLogger-provider","{#SERVICE_NAME}":"loginLogger-provider"}\n]}\n\njsonStr = json.dumps(total_dict,sort_keys=True,indent=4,ensure_ascii=False)\n\n#python3\n#print(jsonStr)\n\n#python2\nprint jsonStr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\nuserparameter_checkproc.conf 文件内容：\n\n\n\n自动发现规则：\n\n\n\n监控项原型优化：\n\n\n\n触发器原型优化：\n\n\n\n原文链接：https://blog.csdn.net/wudinaniya/article/details/94362623',normalizedContent:'> 在监控生产环境的服务的时候，通常需要对多个端口进行监控，如果我们手动一个一个的添加，这将是一件很麻烦的事情，为了减少这样的情况，我们采用批量添加监控端口的方法，这是非常常见的需求，zabbix也是支持这种方式的，需要使用zabbix的discovery功能来实现。\n> \n> 使用zabbix监控如何监控服务器端口状态，大概的流程：zabbix监控服务自带端口监控的监控项，所以需要我们自己手动定义所监控的item，客户端获取的端口列表通过agent传送到服务端。\n> \n> 只需在服务端进行端口监控模板配置，然后自定义监控图形，添加监控项即可！\n\n# 扫描端口脚本\n\ncheck_port.py\n\n#!/usr/bin/env python\n#coding:utf-8\n\nimport json\n\ntotal_dict={"data":[\n{"{#tcp_port}":"10690","{#service_name}":"bms_frontend"},\n{"{#tcp_port}":"10691","{#service_name}":"bms_backend"},\n{"{#tcp_port}":"30004","{#service_name}":"idc_web_uniappp"},\n{"{#tcp_port}":"30003","{#service_name}":"idc_service_go"},\n{"{#tcp_port}":"30000","{#service_name}":"idc_web_website"},\n{"{#tcp_port}":"30002","{#service_name}":"idc_web_h5"},\n{"{#tcp_port}":"30001","{#service_name}":"idc_web_admin"},\n{"{#tcp_port}":"10585","{#service_name}":"gg_h5_fz"},\n{"{#tcp_port}":"80","{#service_name}":"proxy"},\n{"{#tcp_port}":"16379","{#service_name}":"redis"}\n]}\n\njsonstr = json.dumps(total_dict,sort_keys=true,indent=4,ensure_ascii=false)\n\n#python3\n#print(jsonstr)\n\n#python2\nprint jsonstr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\ncheck_port.conf\n\nuserparameter=tcpportlisten,/etc/zabbix/sh/check_port.py\n\n# 新建zabbix模板\n\n\n\n创建自动发现\n\n\n\n创建自动发现的监控项原型和触发器类型\n\n\n\n\n\n# 自动发现批量监测指定进程服务方案优化（对于没有固定端口的服务可采用此方式）\n\n进程服务发现脚本 check_proc.py\n\n#!/usr/bin/env python\n#coding:utf-8\n\nimport json\n\ntotal_dict={"data":[\n{"{#cmdline}":"accountinfo-provider","{#service_name}":"accountinfo-provider"},\n{"{#cmdline}":"account-provider","{#service_name}":"account-provider"},\n{"{#cmdline}":"account-user-provider","{#service_name}":"account-user-provider"},\n{"{#cmdline}":"account-userseq-provider","{#service_name}":"account-userseq-provider"},\n{"{#cmdline}":"loginlogger-provider","{#service_name}":"loginlogger-provider"}\n]}\n\njsonstr = json.dumps(total_dict,sort_keys=true,indent=4,ensure_ascii=false)\n\n#python3\n#print(jsonstr)\n\n#python2\nprint jsonstr\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\nuserparameter_checkproc.conf 文件内容：\n\n\n\n自动发现规则：\n\n\n\n监控项原型优化：\n\n\n\n触发器原型优化：\n\n\n\n原文链接：https://blog.csdn.net/wudinaniya/article/details/94362623',charsets:{cjk:!0}},{title:"zabbix添加证书监控",frontmatter:{title:"zabbix添加证书监控",date:"2022-12-14T15:03:30.000Z",permalink:"/pages/3dac52/",categories:["专题","zabbix"],tags:[null],readingShow:"top",description:"获取证书过期时间脚本：/etc/zabbix/scripts/check-cert-expire.sh",meta:[{name:"image",content:"https://img2018.cnblogs.com/blog/1279026/201907/1279026-20190725190623268-1734764150.png"},{name:"twitter:title",content:"zabbix添加证书监控"},{name:"twitter:description",content:"获取证书过期时间脚本：/etc/zabbix/scripts/check-cert-expire.sh"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://img2018.cnblogs.com/blog/1279026/201907/1279026-20190725190623268-1734764150.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/01.zabbix%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6%E7%9B%91%E6%8E%A7.html"},{property:"og:type",content:"article"},{property:"og:title",content:"zabbix添加证书监控"},{property:"og:description",content:"获取证书过期时间脚本：/etc/zabbix/scripts/check-cert-expire.sh"},{property:"og:image",content:"https://img2018.cnblogs.com/blog/1279026/201907/1279026-20190725190623268-1734764150.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/01.zabbix%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6%E7%9B%91%E6%8E%A7.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-14T15:03:30.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"zabbix添加证书监控"},{itemprop:"description",content:"获取证书过期时间脚本：/etc/zabbix/scripts/check-cert-expire.sh"},{itemprop:"image",content:"https://img2018.cnblogs.com/blog/1279026/201907/1279026-20190725190623268-1734764150.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/01.zabbix%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6%E7%9B%91%E6%8E%A7.html",relativePath:"04.运维/12.监控/01.zabbix/01.zabbix添加证书监控.md",key:"v-787b413e",path:"/pages/3dac52/",headers:[{level:4,title:"zabbix监控ssl证书过期时间",slug:"zabbix监控ssl证书过期时间",normalizedTitle:"zabbix监控ssl证书过期时间",charIndex:2},{level:4,title:"域名自动发现脚本：",slug:"域名自动发现脚本",normalizedTitle:"域名自动发现脚本：",charIndex:804},{level:4,title:"zabbix配置",slug:"zabbix配置",normalizedTitle:"zabbix配置",charIndex:1532},{level:4,title:"在zabbix中添加模板：",slug:"在zabbix中添加模板",normalizedTitle:"在zabbix中添加模板：",charIndex:1782}],headersStr:"zabbix监控ssl证书过期时间 域名自动发现脚本： zabbix配置 在zabbix中添加模板：",content:'# zabbix监控ssl证书过期时间\n\n获取证书过期时间脚本：/etc/zabbix/scripts/check-cert-expire.sh\n\n/etc/zabbix/scripts/check-cert-expire.sh:\n\n#!/bin/bash\nhost=$1\nport=$2\nend_date=`/usr/bin/openssl s_client -servername $host -host $host -port $port -showcerts </dev/null 2>/dev/null |\n  sed -n \'/BEGIN CERTIFICATE/,/END CERT/p\' |\n  /usr/bin/openssl  x509 -text 2>/dev/null |\n  sed -n \'s/ *Not After : *//p\'`\n# openssl 检验和验证SSL证书。\n# -servername $host 因一台主机存在多个证书，利用SNI特性检查\n# </dev/null 定向标准输入，防止交互式程序。从/dev/null 读时，直接读出0 。\n# sed -n 和p 一起使用，仅显示匹配到的部分。 //,// 区间匹配。\n# openssl x509 -text 解码证书信息，包含证书的有效期。\n\nif [ -n "$end_date" ]\nthen\n    end_date_seconds=`date \'+%s\' --date "$end_date"`\n    now_seconds=`date \'+%s\'`\n    echo "($end_date_seconds-$now_seconds)/24/3600" | bc\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 域名自动发现脚本：\n\n#!/usr/bin/env python\n#coding:utf-8\n\nimport os\nimport sys\nimport json\n\n#这个函数主要是构造出一个特定格式的字典，用于zabbix\ndef ssl_cert_discovery():\n    web_list=[]\n    web_dict={"data":None}\n    with open("/etc/zabbix/scripts/ssl_cert_list","r") as f:\n        for sslcert in f:\n            dict={}\n            dict["{#DOMAINNAME}"]=sslcert.strip().split()[0]\n            dict["{#PORT}"]=sslcert.strip().split()[1]\n            web_list.append(dict)\n    web_dict["data"]=web_list\n    jsonStr = json.dumps(web_dict,indent=4)\n    return jsonStr\nif __name__ == "__main__":\n    print ssl_cert_discovery()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n域名列表：\n\n/etc/zabbix/scripts/ssl_cert_list: www.baidu.com 443 www.qq.com 443\n\n# zabbix配置\n\n/etc/zabbix/zabbix_agentd.conf.d/userparameter_sslcert.conf: UserParameter=sslcert_discovery,/usr/bin/python /etc/zabbix/scripts/sshcert_discovery.py UserParameter=sslcert.info[*],/bin/bash /etc/zabbix/scripts/check-cert-expire.sh $1 $2\n\n# 在zabbix中添加模板：\n\nTemplate_ssl_cert_info.xml：<?xml version="1.0" encoding="UTF-8"?>\n<zabbix_export>\n    <version>3.0</version>\n    <date>2019-07-25T10:31:57Z</date>\n    <groups>\n        <group>\n            <name>Templates</name>\n        </group>\n    </groups>\n    <templates>\n        <template>\n            <template>Template ssl cert Information</template>\n            <name>Template ssl cert Information</name>\n            <description/>\n            <groups>\n                <group>\n                    <name>Templates</name>\n                </group>\n            </groups>\n            <applications/>\n            <items/>\n            <discovery_rules>\n                <discovery_rule>\n                    <name>ssl cert information</name>\n                    <type>0</type>\n                    <snmp_community/>\n                    <snmp_oid/>\n                    <key>sslcert_discovery</key>\n                    <delay>10800</delay>\n                    <status>0</status>\n                    <allowed_hosts/>\n                    <snmpv3_contextname/>\n                    <snmpv3_securityname/>\n                    <snmpv3_securitylevel>0</snmpv3_securitylevel>\n                    <snmpv3_authprotocol>0</snmpv3_authprotocol>\n                    <snmpv3_authpassphrase/>\n                    <snmpv3_privprotocol>0</snmpv3_privprotocol>\n                    <snmpv3_privpassphrase/>\n                    <delay_flex/>\n                    <params/>\n                    <ipmi_sensor/>\n                    <authtype>0</authtype>\n                    <username/>\n                    <password/>\n                    <publickey/>\n                    <privatekey/>\n                    <port/>\n                    <filter>\n                        <evaltype>0</evaltype>\n                        <formula/>\n                        <conditions/>\n                    </filter>\n                    <lifetime>30</lifetime>\n                    <description/>\n                    <item_prototypes>\n                        <item_prototype>\n                            <name>sslinfo[{#DOMAINNAME}]</name>\n                            <type>0</type>\n                            <snmp_community/>\n                            <multiplier>1</multiplier>\n                            <snmp_oid/>\n                            <key>sslcert.info[{#DOMAINNAME},{#PORT}]</key>\n                            <delay>10800</delay>\n                            <history>90</history>\n                            <trends>365</trends>\n                            <status>0</status>\n                            <value_type>3</value_type>\n                            <allowed_hosts/>\n                            <units>天</units>\n                            <delta>0</delta>\n                            <snmpv3_contextname/>\n                            <snmpv3_securityname/>\n                            <snmpv3_securitylevel>0</snmpv3_securitylevel>\n                            <snmpv3_authprotocol>0</snmpv3_authprotocol>\n                            <snmpv3_authpassphrase/>\n                            <snmpv3_privprotocol>0</snmpv3_privprotocol>\n                            <snmpv3_privpassphrase/>\n                            <formula>1</formula>\n                            <delay_flex/>\n                            <params/>\n                            <ipmi_sensor/>\n                            <data_type>0</data_type>\n                            <authtype>0</authtype>\n                            <username/>\n                            <password/>\n                            <publickey/>\n                            <privatekey/>\n                            <port/>\n                            <description/>\n                            <inventory_link>0</inventory_link>\n                            <applications/>\n                            <valuemap/>\n                            <logtimefmt/>\n                            <application_prototypes/>\n                        </item_prototype>\n                    </item_prototypes>\n                    <trigger_prototypes>\n                        <trigger_prototype>\n                            <expression>{Template ssl cert Information:sslcert.info[{#DOMAINNAME},{#PORT}].last()}<7</expression>\n                            <name>{#DOMAINNAME} will retire after 7 days,Attention Please!</name>\n                            <url/>\n                            <status>0</status>\n                            <priority>4</priority>\n                            <description/>\n                            <type>0</type>\n                            <dependencies/>\n                        </trigger_prototype>\n                    </trigger_prototypes>\n                    <graph_prototypes/>\n                    <host_prototypes/>\n                </discovery_rule>\n            </discovery_rules>\n            <macros/>\n            <templates/>\n            <screens/>\n        </template>\n    </templates>\n</zabbix_export>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n\n\n调整触发器时长，验证配置是否正确。\n\n\n\n原文链接：https://www.cnblogs.com/imcati/p/11246158.html',normalizedContent:'# zabbix监控ssl证书过期时间\n\n获取证书过期时间脚本：/etc/zabbix/scripts/check-cert-expire.sh\n\n/etc/zabbix/scripts/check-cert-expire.sh:\n\n#!/bin/bash\nhost=$1\nport=$2\nend_date=`/usr/bin/openssl s_client -servername $host -host $host -port $port -showcerts </dev/null 2>/dev/null |\n  sed -n \'/begin certificate/,/end cert/p\' |\n  /usr/bin/openssl  x509 -text 2>/dev/null |\n  sed -n \'s/ *not after : *//p\'`\n# openssl 检验和验证ssl证书。\n# -servername $host 因一台主机存在多个证书，利用sni特性检查\n# </dev/null 定向标准输入，防止交互式程序。从/dev/null 读时，直接读出0 。\n# sed -n 和p 一起使用，仅显示匹配到的部分。 //,// 区间匹配。\n# openssl x509 -text 解码证书信息，包含证书的有效期。\n\nif [ -n "$end_date" ]\nthen\n    end_date_seconds=`date \'+%s\' --date "$end_date"`\n    now_seconds=`date \'+%s\'`\n    echo "($end_date_seconds-$now_seconds)/24/3600" | bc\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 域名自动发现脚本：\n\n#!/usr/bin/env python\n#coding:utf-8\n\nimport os\nimport sys\nimport json\n\n#这个函数主要是构造出一个特定格式的字典，用于zabbix\ndef ssl_cert_discovery():\n    web_list=[]\n    web_dict={"data":none}\n    with open("/etc/zabbix/scripts/ssl_cert_list","r") as f:\n        for sslcert in f:\n            dict={}\n            dict["{#domainname}"]=sslcert.strip().split()[0]\n            dict["{#port}"]=sslcert.strip().split()[1]\n            web_list.append(dict)\n    web_dict["data"]=web_list\n    jsonstr = json.dumps(web_dict,indent=4)\n    return jsonstr\nif __name__ == "__main__":\n    print ssl_cert_discovery()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n域名列表：\n\n/etc/zabbix/scripts/ssl_cert_list: www.baidu.com 443 www.qq.com 443\n\n# zabbix配置\n\n/etc/zabbix/zabbix_agentd.conf.d/userparameter_sslcert.conf: userparameter=sslcert_discovery,/usr/bin/python /etc/zabbix/scripts/sshcert_discovery.py userparameter=sslcert.info[*],/bin/bash /etc/zabbix/scripts/check-cert-expire.sh $1 $2\n\n# 在zabbix中添加模板：\n\ntemplate_ssl_cert_info.xml：<?xml version="1.0" encoding="utf-8"?>\n<zabbix_export>\n    <version>3.0</version>\n    <date>2019-07-25t10:31:57z</date>\n    <groups>\n        <group>\n            <name>templates</name>\n        </group>\n    </groups>\n    <templates>\n        <template>\n            <template>template ssl cert information</template>\n            <name>template ssl cert information</name>\n            <description/>\n            <groups>\n                <group>\n                    <name>templates</name>\n                </group>\n            </groups>\n            <applications/>\n            <items/>\n            <discovery_rules>\n                <discovery_rule>\n                    <name>ssl cert information</name>\n                    <type>0</type>\n                    <snmp_community/>\n                    <snmp_oid/>\n                    <key>sslcert_discovery</key>\n                    <delay>10800</delay>\n                    <status>0</status>\n                    <allowed_hosts/>\n                    <snmpv3_contextname/>\n                    <snmpv3_securityname/>\n                    <snmpv3_securitylevel>0</snmpv3_securitylevel>\n                    <snmpv3_authprotocol>0</snmpv3_authprotocol>\n                    <snmpv3_authpassphrase/>\n                    <snmpv3_privprotocol>0</snmpv3_privprotocol>\n                    <snmpv3_privpassphrase/>\n                    <delay_flex/>\n                    <params/>\n                    <ipmi_sensor/>\n                    <authtype>0</authtype>\n                    <username/>\n                    <password/>\n                    <publickey/>\n                    <privatekey/>\n                    <port/>\n                    <filter>\n                        <evaltype>0</evaltype>\n                        <formula/>\n                        <conditions/>\n                    </filter>\n                    <lifetime>30</lifetime>\n                    <description/>\n                    <item_prototypes>\n                        <item_prototype>\n                            <name>sslinfo[{#domainname}]</name>\n                            <type>0</type>\n                            <snmp_community/>\n                            <multiplier>1</multiplier>\n                            <snmp_oid/>\n                            <key>sslcert.info[{#domainname},{#port}]</key>\n                            <delay>10800</delay>\n                            <history>90</history>\n                            <trends>365</trends>\n                            <status>0</status>\n                            <value_type>3</value_type>\n                            <allowed_hosts/>\n                            <units>天</units>\n                            <delta>0</delta>\n                            <snmpv3_contextname/>\n                            <snmpv3_securityname/>\n                            <snmpv3_securitylevel>0</snmpv3_securitylevel>\n                            <snmpv3_authprotocol>0</snmpv3_authprotocol>\n                            <snmpv3_authpassphrase/>\n                            <snmpv3_privprotocol>0</snmpv3_privprotocol>\n                            <snmpv3_privpassphrase/>\n                            <formula>1</formula>\n                            <delay_flex/>\n                            <params/>\n                            <ipmi_sensor/>\n                            <data_type>0</data_type>\n                            <authtype>0</authtype>\n                            <username/>\n                            <password/>\n                            <publickey/>\n                            <privatekey/>\n                            <port/>\n                            <description/>\n                            <inventory_link>0</inventory_link>\n                            <applications/>\n                            <valuemap/>\n                            <logtimefmt/>\n                            <application_prototypes/>\n                        </item_prototype>\n                    </item_prototypes>\n                    <trigger_prototypes>\n                        <trigger_prototype>\n                            <expression>{template ssl cert information:sslcert.info[{#domainname},{#port}].last()}<7</expression>\n                            <name>{#domainname} will retire after 7 days,attention please!</name>\n                            <url/>\n                            <status>0</status>\n                            <priority>4</priority>\n                            <description/>\n                            <type>0</type>\n                            <dependencies/>\n                        </trigger_prototype>\n                    </trigger_prototypes>\n                    <graph_prototypes/>\n                    <host_prototypes/>\n                </discovery_rule>\n            </discovery_rules>\n            <macros/>\n            <templates/>\n            <screens/>\n        </template>\n    </templates>\n</zabbix_export>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n\n\n调整触发器时长，验证配置是否正确。\n\n\n\n原文链接：https://www.cnblogs.com/imcati/p/11246158.html',charsets:{cjk:!0}},{title:"zabbix添加日志监控",frontmatter:{title:"zabbix添加日志监控",date:"2022-12-20T16:41:01.000Z",permalink:"/pages/334bec/",categories:["专题","zabbix"],tags:[null],readingShow:"top",description:"今天来了解一下关于ELK的“L”-Logstash,没错，就是这个神奇小组件，我们都知道，它是ELK不可缺少的组件，完成了输入（input），过滤（fileter），output（输出）工作量，也是我们作为运维人员需要掌握的难点，说到这里 ，又爱又恨；“爱之好，恨之难”；这个Logstash拥有这强大的插件功能，除了帮我们过滤，高效的输出日志，还能帮我们与Zabbix监控相结合？\n>\n> 因为我们的Logstash支持多种输出类型，能够收集web服务日志，系统日志，内核日志；但是；竟然是有日志输出，肯定避免不了错误（error）日志的出现；当Error日志出现的时候，虽然可以通过ELK查找出来，但是ELK不能实时提供报警，这就有点尴尬了，我们要做的就是能够像zabbix，nagios监控那样，不能要做到监控，还要做到报警，这一点，ELK只做到了监控，但是没有做到报警；不过没关系，我们的Logstash插件能够与zabbix结合起来，将需要告警 的日志收集起来（比如说有错误标识的日志）完成日志监控触发告警~\n>\n> logstash支持多种输出介质，比如说syslog，http，tcp，elasticsearch,kafka等，如果我们将logstash收集的日志输出到zabbix告警，就必须要用到logstash-output-zabbix插件，通过这个插件将logstash与zabbix整合，logstash收集到的数据过滤出错误信息的日志输出到zabbix中，最后通过zabbix告警机制触发；",meta:[{name:"image",content:"https://images2018.cnblogs.com/blog/1166362/201809/1166362-20180906220312074-1735973576.png"},{name:"twitter:title",content:"zabbix添加日志监控"},{name:"twitter:description",content:"今天来了解一下关于ELK的“L”-Logstash,没错，就是这个神奇小组件，我们都知道，它是ELK不可缺少的组件，完成了输入（input），过滤（fileter），output（输出）工作量，也是我们作为运维人员需要掌握的难点，说到这里 ，又爱又恨；“爱之好，恨之难”；这个Logstash拥有这强大的插件功能，除了帮我们过滤，高效的输出日志，还能帮我们与Zabbix监控相结合？\n>\n> 因为我们的Logstash支持多种输出类型，能够收集web服务日志，系统日志，内核日志；但是；竟然是有日志输出，肯定避免不了错误（error）日志的出现；当Error日志出现的时候，虽然可以通过ELK查找出来，但是ELK不能实时提供报警，这就有点尴尬了，我们要做的就是能够像zabbix，nagios监控那样，不能要做到监控，还要做到报警，这一点，ELK只做到了监控，但是没有做到报警；不过没关系，我们的Logstash插件能够与zabbix结合起来，将需要告警 的日志收集起来（比如说有错误标识的日志）完成日志监控触发告警~\n>\n> logstash支持多种输出介质，比如说syslog，http，tcp，elasticsearch,kafka等，如果我们将logstash收集的日志输出到zabbix告警，就必须要用到logstash-output-zabbix插件，通过这个插件将logstash与zabbix整合，logstash收集到的数据过滤出错误信息的日志输出到zabbix中，最后通过zabbix告警机制触发；"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://images2018.cnblogs.com/blog/1166362/201809/1166362-20180906220312074-1735973576.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/05.zabbix%E6%B7%BB%E5%8A%A0%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7.html"},{property:"og:type",content:"article"},{property:"og:title",content:"zabbix添加日志监控"},{property:"og:description",content:"今天来了解一下关于ELK的“L”-Logstash,没错，就是这个神奇小组件，我们都知道，它是ELK不可缺少的组件，完成了输入（input），过滤（fileter），output（输出）工作量，也是我们作为运维人员需要掌握的难点，说到这里 ，又爱又恨；“爱之好，恨之难”；这个Logstash拥有这强大的插件功能，除了帮我们过滤，高效的输出日志，还能帮我们与Zabbix监控相结合？\n>\n> 因为我们的Logstash支持多种输出类型，能够收集web服务日志，系统日志，内核日志；但是；竟然是有日志输出，肯定避免不了错误（error）日志的出现；当Error日志出现的时候，虽然可以通过ELK查找出来，但是ELK不能实时提供报警，这就有点尴尬了，我们要做的就是能够像zabbix，nagios监控那样，不能要做到监控，还要做到报警，这一点，ELK只做到了监控，但是没有做到报警；不过没关系，我们的Logstash插件能够与zabbix结合起来，将需要告警 的日志收集起来（比如说有错误标识的日志）完成日志监控触发告警~\n>\n> logstash支持多种输出介质，比如说syslog，http，tcp，elasticsearch,kafka等，如果我们将logstash收集的日志输出到zabbix告警，就必须要用到logstash-output-zabbix插件，通过这个插件将logstash与zabbix整合，logstash收集到的数据过滤出错误信息的日志输出到zabbix中，最后通过zabbix告警机制触发；"},{property:"og:image",content:"https://images2018.cnblogs.com/blog/1166362/201809/1166362-20180906220312074-1735973576.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/05.zabbix%E6%B7%BB%E5%8A%A0%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-20T16:41:01.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"zabbix添加日志监控"},{itemprop:"description",content:"今天来了解一下关于ELK的“L”-Logstash,没错，就是这个神奇小组件，我们都知道，它是ELK不可缺少的组件，完成了输入（input），过滤（fileter），output（输出）工作量，也是我们作为运维人员需要掌握的难点，说到这里 ，又爱又恨；“爱之好，恨之难”；这个Logstash拥有这强大的插件功能，除了帮我们过滤，高效的输出日志，还能帮我们与Zabbix监控相结合？\n>\n> 因为我们的Logstash支持多种输出类型，能够收集web服务日志，系统日志，内核日志；但是；竟然是有日志输出，肯定避免不了错误（error）日志的出现；当Error日志出现的时候，虽然可以通过ELK查找出来，但是ELK不能实时提供报警，这就有点尴尬了，我们要做的就是能够像zabbix，nagios监控那样，不能要做到监控，还要做到报警，这一点，ELK只做到了监控，但是没有做到报警；不过没关系，我们的Logstash插件能够与zabbix结合起来，将需要告警 的日志收集起来（比如说有错误标识的日志）完成日志监控触发告警~\n>\n> logstash支持多种输出介质，比如说syslog，http，tcp，elasticsearch,kafka等，如果我们将logstash收集的日志输出到zabbix告警，就必须要用到logstash-output-zabbix插件，通过这个插件将logstash与zabbix整合，logstash收集到的数据过滤出错误信息的日志输出到zabbix中，最后通过zabbix告警机制触发；"},{itemprop:"image",content:"https://images2018.cnblogs.com/blog/1166362/201809/1166362-20180906220312074-1735973576.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/05.zabbix%E6%B7%BB%E5%8A%A0%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7.html",relativePath:"04.运维/12.监控/01.zabbix/05.zabbix添加日志监控.md",key:"v-68775ace",path:"/pages/334bec/",headers:[{level:4,title:"zabbix整合elk收集系统异常日志触发告警",slug:"zabbix整合elk收集系统异常日志触发告警",normalizedTitle:"zabbix整合elk收集系统异常日志触发告警",charIndex:2}],headersStr:"zabbix整合elk收集系统异常日志触发告警",content:'# zabbix整合elk收集系统异常日志触发告警\n\n> 今天来了解一下关于ELK的“L”-Logstash,没错，就是这个神奇小组件，我们都知道，它是ELK不可缺少的组件，完成了输入（input），过滤（fileter），output（输出）工作量，也是我们作为运维人员需要掌握的难点，说到这里 ，又爱又恨；“爱之好，恨之难”；这个Logstash拥有这强大的插件功能，除了帮我们过滤，高效的输出日志，还能帮我们与Zabbix监控相结合？\n> \n> 因为我们的Logstash支持多种输出类型，能够收集web服务日志，系统日志，内核日志；但是；竟然是有日志输出，肯定避免不了错误（error）日志的出现；当Error日志出现的时候，虽然可以通过ELK查找出来，但是ELK不能实时提供报警，这就有点尴尬了，我们要做的就是能够像zabbix，nagios监控那样，不能要做到监控，还要做到报警，这一点，ELK只做到了监控，但是没有做到报警；不过没关系，我们的Logstash插件能够与zabbix结合起来，将需要告警 的日志收集起来（比如说有错误标识的日志）完成日志监控触发告警~\n> \n> logstash支持多种输出介质，比如说syslog，http，tcp，elasticsearch,kafka等，如果我们将logstash收集的日志输出到zabbix告警，就必须要用到logstash-output-zabbix插件，通过这个插件将logstash与zabbix整合，logstash收集到的数据过滤出错误信息的日志输出到zabbix中，最后通过zabbix告警机制触发；\n\n[root@localhost ~]# /usr/local/logstash/bin/logstash-plugin install logstash-output-zabbix      #安装logstash-output-zabbix插件\nValidating logstash-output-zabbix\nInstalling logstash-output-zabbix\nInstallation successful\n\n\n1\n2\n3\n4\n\n\n环境案例需求：\n\n通过读系统日志文件的监控，过滤掉日志信息中的异常关键词，如ERR,error，Failed，warning等信息，将这些带有异常关键词的异常日志信息过滤出来，然后输出到zabbix，通过zabbix告警机制实现触发告警；下面环境是filebeat作为采集端；输出到kafaka消息队列，最后由logsatsh拉取日志并过滤，输出到zabbix\n\nfilebeat\n\n- type: log\n\n  # Change to true to enable this input configuration.\n  enabled: true\n\n  # Paths that should be crawled and fetched. Glob based paths.\n  paths:\n    -  /opt/apps/nginx_proxy/log/*.log\n    - /opt/apps/idc/idc_service_go/logs/*.log\n    - /workspace/apps/goldmerry/business_server/storage/logs/*.log\n    - /workspace/apps/goldmerry/cms_server/storage/logs/*.log\n    - /opt/apps/bms/bms_go_server/logs/backend/*.log\n    - /opt/apps/bms/bms_go_server/logs/contractmgr/*.log\n    - /opt/apps/bms/bms_go_server/logs/crm/*.log\n    - /opt/apps/bms/bms_go_server/logs/frontend/*.log\n    - /opt/apps/bms/bms_go_server/logs/product/*.log\n    - /opt/apps/idc/idc_service_go/logs/*.log\n    - /workspace/apps/goldmerry/crm_server/runtime/log/**/*.log\n    - /opt/apps/idc/idc_python/logs/*.log\n    - /opt/apps/idc/idc_python_2/logs/*.log\n    #- c:\\programdata\\elasticsearch\\logs\\*\n  fields:\n    topic: pro-idc_bms-logs\nname: pro_bms_idc\noutput.kafka:\n  hosts: ["xx:9093","xx:9094","xx:9095"]\n  topic: \'%{[fields.topic]}\'\n  partition.round_robin:\n    reachable_only: true\n  required_acks: 1\n\nlogging:\n    to_syslog: false\n    to_files: true\n    files:\n        rotateeverybytes: 10485760 # 默认的10MB\n        level: info\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\ninput {\n kafka {\n    type => "pro-idc_bms-logs"\n    bootstrap_servers => ["172.16.30.247:9093, 172.16.30.247:9094, 172.16.30.247:9095"]\n    client_id => "pro-idc_bms-logs"\n    topics => ["pro-idc_bms-logs"]\n    consumer_threads => 3\n    codec => "json"\n  }\n}\n\nfilter {\n    if [fields][topic] == "pro-idc_bms-logs" {    #指定filebeat产生的日志主题\n             mutate {\n             add_field => [ "[zabbix_key]", "oslogs" ]      #新增的字段，字段名是zabbix_key，值为oslogs。\n             add_field => [ "[zabbix_host]", "%{[host][name]}" ]   #新增的字段，字段名是zabbix_host，值可以在这里直接定义，也可以引用字段变量来获取。这里的%{[host][name]}获取的就是日志数据的来源IP，这个来源IP在filebeat配置中的name选项进行定义。\n             }\n\n        }\n}\n\n\noutput {\n\n\nif [type] == "pro-idc_bms-logs" {\n\n        if [message]  =~ /(error_code:!=0|ERROR|Failed|错误原因|status:404|process day|数据库操作失败)/  {\n              zabbix {\n                        zabbix_host => "[zabbix_host]"      #这个zabbix_host将获取上面filter部分定义的字段变量%{[host][name]}的值\n                        zabbix_key => "[zabbix_key]"        #这个zabbix_key将获取上面filter部分中给出的值\n                        zabbix_server_host => "172.16.30.241"  #这是指定zabbix server的IP地址\n                        zabbix_server_port => "10051"           #这是指定zabbix server的监听端口\n                        zabbix_value => "message"\n                        }\n                    }\n\n\n  }\n\n}\n\n\n【zabbix-监控模板创建到 告警一触即发】\n\n1.创建模板\n\n将词模板链接到192.168.37.147上，创建的模板上的监控项就会在192.168.37.147上自动生效了\n\n\n\n2.创建应用集，点击应用集-创建应用集\n\n\n\n3.创建监控项，点击监控项，创建监控项\n\n\n\n4.告警触发，创建 触发器\n\n\n\n将咱们创建的收集日志的模板连接到 需要收集日志的主机，验证告警触发效果\n\n\n\n注意事项：filebeat配置name必须要和zabbix主机名称一致，不然收集不到日志信息\n\n关闭日志告警脚本\n\n#!/usr/bin/python\n# -*- coding:utf-8 -*-\n\nimport json\nimport requests\n\npost_headers = {\'Content-Type\': \'application/json\'}\n#http://127.0.0.1/zabbix.php zabbix的web访问地址改成api访问\nurl = \'http://172.16.30.241:82/api_jsonrpc.php\' \n#这里的鉴权key可以通过user.login 接口获取 百度下一堆教程\n#我用4.0版本的zabbix API 官方文档连接：https://www.zabbix.com/documentation/4.0/zh/manual/api/reference/user/login\nkey = \'###############\'\n\npost_data = {\n    "jsonrpc": "2.0",\n    "method": "problem.get",\n    "params": {\n               "output": "extend",\n               "severities": 1\n    },\n    "auth": key,\n    "id": 1\n}\nret = requests.post(url, data=json.dumps(post_data), headers=post_headers)\nresult = json.loads(ret.text)\n#相关接口文档：https://www.zabbix.com/documentation/4.0/zh/manual/api/reference/event/acknowledge\nfor id in json.loads(ret.text)[\'result\']:\n    print(id)\n    post_data = {\n        "jsonrpc": "2.0",\n        "method": "event.acknowledge",\n        "params": {\n            "eventids": id[\'eventid\'],\n            "action": 1,\n            "message": "Problem resolved."\n        },\n        "auth": key,\n        "id": 1\n    }\n    ret = requests.post(url, data=json.dumps(post_data), headers=post_headers)\n    result = json.loads(ret.text)\n    print(result)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n获取zabbix token\n\n\n\n参考链接\n\nhttps://www.cnblogs.com/bixiaoyu/p/9595698.html\n\nhttps://blog.csdn.net/JineD/article/details/114440164',normalizedContent:'# zabbix整合elk收集系统异常日志触发告警\n\n> 今天来了解一下关于elk的“l”-logstash,没错，就是这个神奇小组件，我们都知道，它是elk不可缺少的组件，完成了输入（input），过滤（fileter），output（输出）工作量，也是我们作为运维人员需要掌握的难点，说到这里 ，又爱又恨；“爱之好，恨之难”；这个logstash拥有这强大的插件功能，除了帮我们过滤，高效的输出日志，还能帮我们与zabbix监控相结合？\n> \n> 因为我们的logstash支持多种输出类型，能够收集web服务日志，系统日志，内核日志；但是；竟然是有日志输出，肯定避免不了错误（error）日志的出现；当error日志出现的时候，虽然可以通过elk查找出来，但是elk不能实时提供报警，这就有点尴尬了，我们要做的就是能够像zabbix，nagios监控那样，不能要做到监控，还要做到报警，这一点，elk只做到了监控，但是没有做到报警；不过没关系，我们的logstash插件能够与zabbix结合起来，将需要告警 的日志收集起来（比如说有错误标识的日志）完成日志监控触发告警~\n> \n> logstash支持多种输出介质，比如说syslog，http，tcp，elasticsearch,kafka等，如果我们将logstash收集的日志输出到zabbix告警，就必须要用到logstash-output-zabbix插件，通过这个插件将logstash与zabbix整合，logstash收集到的数据过滤出错误信息的日志输出到zabbix中，最后通过zabbix告警机制触发；\n\n[root@localhost ~]# /usr/local/logstash/bin/logstash-plugin install logstash-output-zabbix      #安装logstash-output-zabbix插件\nvalidating logstash-output-zabbix\ninstalling logstash-output-zabbix\ninstallation successful\n\n\n1\n2\n3\n4\n\n\n环境案例需求：\n\n通过读系统日志文件的监控，过滤掉日志信息中的异常关键词，如err,error，failed，warning等信息，将这些带有异常关键词的异常日志信息过滤出来，然后输出到zabbix，通过zabbix告警机制实现触发告警；下面环境是filebeat作为采集端；输出到kafaka消息队列，最后由logsatsh拉取日志并过滤，输出到zabbix\n\nfilebeat\n\n- type: log\n\n  # change to true to enable this input configuration.\n  enabled: true\n\n  # paths that should be crawled and fetched. glob based paths.\n  paths:\n    -  /opt/apps/nginx_proxy/log/*.log\n    - /opt/apps/idc/idc_service_go/logs/*.log\n    - /workspace/apps/goldmerry/business_server/storage/logs/*.log\n    - /workspace/apps/goldmerry/cms_server/storage/logs/*.log\n    - /opt/apps/bms/bms_go_server/logs/backend/*.log\n    - /opt/apps/bms/bms_go_server/logs/contractmgr/*.log\n    - /opt/apps/bms/bms_go_server/logs/crm/*.log\n    - /opt/apps/bms/bms_go_server/logs/frontend/*.log\n    - /opt/apps/bms/bms_go_server/logs/product/*.log\n    - /opt/apps/idc/idc_service_go/logs/*.log\n    - /workspace/apps/goldmerry/crm_server/runtime/log/**/*.log\n    - /opt/apps/idc/idc_python/logs/*.log\n    - /opt/apps/idc/idc_python_2/logs/*.log\n    #- c:\\programdata\\elasticsearch\\logs\\*\n  fields:\n    topic: pro-idc_bms-logs\nname: pro_bms_idc\noutput.kafka:\n  hosts: ["xx:9093","xx:9094","xx:9095"]\n  topic: \'%{[fields.topic]}\'\n  partition.round_robin:\n    reachable_only: true\n  required_acks: 1\n\nlogging:\n    to_syslog: false\n    to_files: true\n    files:\n        rotateeverybytes: 10485760 # 默认的10mb\n        level: info\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\ninput {\n kafka {\n    type => "pro-idc_bms-logs"\n    bootstrap_servers => ["172.16.30.247:9093, 172.16.30.247:9094, 172.16.30.247:9095"]\n    client_id => "pro-idc_bms-logs"\n    topics => ["pro-idc_bms-logs"]\n    consumer_threads => 3\n    codec => "json"\n  }\n}\n\nfilter {\n    if [fields][topic] == "pro-idc_bms-logs" {    #指定filebeat产生的日志主题\n             mutate {\n             add_field => [ "[zabbix_key]", "oslogs" ]      #新增的字段，字段名是zabbix_key，值为oslogs。\n             add_field => [ "[zabbix_host]", "%{[host][name]}" ]   #新增的字段，字段名是zabbix_host，值可以在这里直接定义，也可以引用字段变量来获取。这里的%{[host][name]}获取的就是日志数据的来源ip，这个来源ip在filebeat配置中的name选项进行定义。\n             }\n\n        }\n}\n\n\noutput {\n\n\nif [type] == "pro-idc_bms-logs" {\n\n        if [message]  =~ /(error_code:!=0|error|failed|错误原因|status:404|process day|数据库操作失败)/  {\n              zabbix {\n                        zabbix_host => "[zabbix_host]"      #这个zabbix_host将获取上面filter部分定义的字段变量%{[host][name]}的值\n                        zabbix_key => "[zabbix_key]"        #这个zabbix_key将获取上面filter部分中给出的值\n                        zabbix_server_host => "172.16.30.241"  #这是指定zabbix server的ip地址\n                        zabbix_server_port => "10051"           #这是指定zabbix server的监听端口\n                        zabbix_value => "message"\n                        }\n                    }\n\n\n  }\n\n}\n\n\n【zabbix-监控模板创建到 告警一触即发】\n\n1.创建模板\n\n将词模板链接到192.168.37.147上，创建的模板上的监控项就会在192.168.37.147上自动生效了\n\n\n\n2.创建应用集，点击应用集-创建应用集\n\n\n\n3.创建监控项，点击监控项，创建监控项\n\n\n\n4.告警触发，创建 触发器\n\n\n\n将咱们创建的收集日志的模板连接到 需要收集日志的主机，验证告警触发效果\n\n\n\n注意事项：filebeat配置name必须要和zabbix主机名称一致，不然收集不到日志信息\n\n关闭日志告警脚本\n\n#!/usr/bin/python\n# -*- coding:utf-8 -*-\n\nimport json\nimport requests\n\npost_headers = {\'content-type\': \'application/json\'}\n#http://127.0.0.1/zabbix.php zabbix的web访问地址改成api访问\nurl = \'http://172.16.30.241:82/api_jsonrpc.php\' \n#这里的鉴权key可以通过user.login 接口获取 百度下一堆教程\n#我用4.0版本的zabbix api 官方文档连接：https://www.zabbix.com/documentation/4.0/zh/manual/api/reference/user/login\nkey = \'###############\'\n\npost_data = {\n    "jsonrpc": "2.0",\n    "method": "problem.get",\n    "params": {\n               "output": "extend",\n               "severities": 1\n    },\n    "auth": key,\n    "id": 1\n}\nret = requests.post(url, data=json.dumps(post_data), headers=post_headers)\nresult = json.loads(ret.text)\n#相关接口文档：https://www.zabbix.com/documentation/4.0/zh/manual/api/reference/event/acknowledge\nfor id in json.loads(ret.text)[\'result\']:\n    print(id)\n    post_data = {\n        "jsonrpc": "2.0",\n        "method": "event.acknowledge",\n        "params": {\n            "eventids": id[\'eventid\'],\n            "action": 1,\n            "message": "problem resolved."\n        },\n        "auth": key,\n        "id": 1\n    }\n    ret = requests.post(url, data=json.dumps(post_data), headers=post_headers)\n    result = json.loads(ret.text)\n    print(result)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n获取zabbix token\n\n\n\n参考链接\n\nhttps://www.cnblogs.com/bixiaoyu/p/9595698.html\n\nhttps://blog.csdn.net/jined/article/details/114440164',charsets:{cjk:!0}},{title:"zabbix监控windows进程",frontmatter:{title:"zabbix监控windows进程",date:"2022-12-20T17:04:24.000Z",permalink:"/pages/269839/",categories:["专题","zabbix"],tags:[null],readingShow:"top",description:"小Z同学最近遇到个项目需求，需求是用zabbix监控运行在windows的进程。然鹅，当小Z同学在网上搜了一大堆，发现基本上都是使用官方的proc.num(*.exe)键值拿到进程数量，很显然，这并不符合客户实际的需求，如果是只运行1个的还好，若是运行了3到6个，例如java，就特别的dan疼，在任务管理器发现全是java.exe的。也许，你可以找到有些文章的说可以使用任务管理器可以看到命令行（图一），tasklist /V查看进程详情（图二）等等，然鹅，对于java来说还是看不到的。\n> 当然了拿到java.exe和pid也可以，但作为运维人员最关注的是要知道是谁停了，而不是PID，况且人家也不知道PID对应的进程是啥。所以，本着上进负责的精神，在查阅了几天的资料后，终于实现了！！！在这里记录下来，为热爱zabbix的小伙伴做个参考，也可以自行拓展结合脚本等其他技术实现。教程不复杂，多做几次就上手了。",meta:[{name:"image",content:"https://img-blog.csdnimg.cn/20200417105538630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYzMTYzMQ==,size_16,color_FFFFFF,t_70"},{name:"twitter:title",content:"zabbix监控windows进程"},{name:"twitter:description",content:"小Z同学最近遇到个项目需求，需求是用zabbix监控运行在windows的进程。然鹅，当小Z同学在网上搜了一大堆，发现基本上都是使用官方的proc.num(*.exe)键值拿到进程数量，很显然，这并不符合客户实际的需求，如果是只运行1个的还好，若是运行了3到6个，例如java，就特别的dan疼，在任务管理器发现全是java.exe的。也许，你可以找到有些文章的说可以使用任务管理器可以看到命令行（图一），tasklist /V查看进程详情（图二）等等，然鹅，对于java来说还是看不到的。\n> 当然了拿到java.exe和pid也可以，但作为运维人员最关注的是要知道是谁停了，而不是PID，况且人家也不知道PID对应的进程是啥。所以，本着上进负责的精神，在查阅了几天的资料后，终于实现了！！！在这里记录下来，为热爱zabbix的小伙伴做个参考，也可以自行拓展结合脚本等其他技术实现。教程不复杂，多做几次就上手了。"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://img-blog.csdnimg.cn/20200417105538630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYzMTYzMQ==,size_16,color_FFFFFF,t_70"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/07.zabbix%E7%9B%91%E6%8E%A7windows%E8%BF%9B%E7%A8%8B.html"},{property:"og:type",content:"article"},{property:"og:title",content:"zabbix监控windows进程"},{property:"og:description",content:"小Z同学最近遇到个项目需求，需求是用zabbix监控运行在windows的进程。然鹅，当小Z同学在网上搜了一大堆，发现基本上都是使用官方的proc.num(*.exe)键值拿到进程数量，很显然，这并不符合客户实际的需求，如果是只运行1个的还好，若是运行了3到6个，例如java，就特别的dan疼，在任务管理器发现全是java.exe的。也许，你可以找到有些文章的说可以使用任务管理器可以看到命令行（图一），tasklist /V查看进程详情（图二）等等，然鹅，对于java来说还是看不到的。\n> 当然了拿到java.exe和pid也可以，但作为运维人员最关注的是要知道是谁停了，而不是PID，况且人家也不知道PID对应的进程是啥。所以，本着上进负责的精神，在查阅了几天的资料后，终于实现了！！！在这里记录下来，为热爱zabbix的小伙伴做个参考，也可以自行拓展结合脚本等其他技术实现。教程不复杂，多做几次就上手了。"},{property:"og:image",content:"https://img-blog.csdnimg.cn/20200417105538630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYzMTYzMQ==,size_16,color_FFFFFF,t_70"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/07.zabbix%E7%9B%91%E6%8E%A7windows%E8%BF%9B%E7%A8%8B.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-20T17:04:24.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"zabbix监控windows进程"},{itemprop:"description",content:"小Z同学最近遇到个项目需求，需求是用zabbix监控运行在windows的进程。然鹅，当小Z同学在网上搜了一大堆，发现基本上都是使用官方的proc.num(*.exe)键值拿到进程数量，很显然，这并不符合客户实际的需求，如果是只运行1个的还好，若是运行了3到6个，例如java，就特别的dan疼，在任务管理器发现全是java.exe的。也许，你可以找到有些文章的说可以使用任务管理器可以看到命令行（图一），tasklist /V查看进程详情（图二）等等，然鹅，对于java来说还是看不到的。\n> 当然了拿到java.exe和pid也可以，但作为运维人员最关注的是要知道是谁停了，而不是PID，况且人家也不知道PID对应的进程是啥。所以，本着上进负责的精神，在查阅了几天的资料后，终于实现了！！！在这里记录下来，为热爱zabbix的小伙伴做个参考，也可以自行拓展结合脚本等其他技术实现。教程不复杂，多做几次就上手了。"},{itemprop:"image",content:"https://img-blog.csdnimg.cn/20200417105538630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYzMTYzMQ==,size_16,color_FFFFFF,t_70"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/07.zabbix%E7%9B%91%E6%8E%A7windows%E8%BF%9B%E7%A8%8B.html",relativePath:"04.运维/12.监控/01.zabbix/07.zabbix监控windows进程.md",key:"v-3bd0d01e",path:"/pages/269839/",headersStr:null,content:"> 小Z同学最近遇到个项目需求，需求是用zabbix监控运行在windows的进程。然鹅，当小Z同学在网上搜了一大堆，发现基本上都是使用官方的proc.num(***.exe)键值拿到进程数量，很显然，这并不符合客户实际的需求，如果是只运行1个的还好，若是运行了3到6个，例如java，就特别的dan疼，在任务管理器发现全是java.exe的。也许，你可以找到有些文章的说可以使用任务管理器可以看到命令行（图一），tasklist /V查看进程详情（图二）等等，然鹅，对于java来说还是看不到的。 当然了拿到java.exe和pid也可以，但作为运维人员最关注的是要知道是谁停了，而不是PID，况且人家也不知道PID对应的进程是啥。所以，本着上进负责的精神，在查阅了几天的资料后，终于实现了！！！在这里记录下来，为热爱zabbix的小伙伴做个参考，也可以自行拓展结合脚本等其他技术实现。教程不复杂，多做几次就上手了。\n\n\n\n\n\n方法：\n\n对于非java的进程，确实使用tasklist可以监控到具体的进程名，例如监控邮箱服务器程序，方法是打开cmd，输入 tasklist | findstr “mailServer.exe” 这里只输出具体的进程名 或者 tasklist | find “mailServer.exe” 这里会输出进程，使用的资源，PID等等。然后再zabbix_agentd.conf后面加上 UserParameter=mailServer, tasklist | findstr “mailServer.exe” 保存重启agent；在zabbix做监控时，信息类型选择字符或者文本即可。 重点来了，对于java进程亦或者是在任务管理器只看到运行了一堆相同的进程名，但看不到具体的实例名可以利用windows自带的 WMIC命令来获取！打开cmd，输入 wmic process 回车，会输出目前在运行的所有进程，比tasklist还详细，输出的结果包含命令行、执行路径、PID、使用的资源、所有者等等。有兴趣的小伙伴可以到官网多多了解，拓展一下其他的WMIC参数。 当然了，对于封装好的jar包程序，常规的执行命令也就是java –jar ****.jar，是可以用wmic拿到的，命令是 wmic process where name=”java.exe” 就可以输出当前正在运行的所有java进程，但结果不太美观，所以可以指定一下要找的jar包名，例如： wimc process where name=”java.exe” | find “jar包名” ，这样出来的结果就是客户想要的了。\n\n\n\n以下是agent配置示例： UserParameter=java_jar, WMIC.exe process where name=”java.exe” | find “jar包名” 这里保险一点，写上完整的WMIC.exe就不会报命令找不到的错。监控项照上一步。 触发器 在给非数值的监控配置触发器时，要注意类型要选择红框的 str()\n\n\n\n因为返回的结果是字符串，所以在V这里填上jar包进程名即可。可以只写包名+版本，不用整行都写。\n\n\n\n问题和恢复的表达式：监控项是每60s检测一次，触发器这里也是每60s检测一次，发现和取值不符则发送告警信息。如果发现乱报警，那就多注意调整下。\n\n\n\n完成后，可以停止程序检测下是否符合需求。\n\n原文链接：\n\nhttps://blog.csdn.net/weixin_43631631/article/details/105575412",normalizedContent:"> 小z同学最近遇到个项目需求，需求是用zabbix监控运行在windows的进程。然鹅，当小z同学在网上搜了一大堆，发现基本上都是使用官方的proc.num(***.exe)键值拿到进程数量，很显然，这并不符合客户实际的需求，如果是只运行1个的还好，若是运行了3到6个，例如java，就特别的dan疼，在任务管理器发现全是java.exe的。也许，你可以找到有些文章的说可以使用任务管理器可以看到命令行（图一），tasklist /v查看进程详情（图二）等等，然鹅，对于java来说还是看不到的。 当然了拿到java.exe和pid也可以，但作为运维人员最关注的是要知道是谁停了，而不是pid，况且人家也不知道pid对应的进程是啥。所以，本着上进负责的精神，在查阅了几天的资料后，终于实现了！！！在这里记录下来，为热爱zabbix的小伙伴做个参考，也可以自行拓展结合脚本等其他技术实现。教程不复杂，多做几次就上手了。\n\n\n\n\n\n方法：\n\n对于非java的进程，确实使用tasklist可以监控到具体的进程名，例如监控邮箱服务器程序，方法是打开cmd，输入 tasklist | findstr “mailserver.exe” 这里只输出具体的进程名 或者 tasklist | find “mailserver.exe” 这里会输出进程，使用的资源，pid等等。然后再zabbix_agentd.conf后面加上 userparameter=mailserver, tasklist | findstr “mailserver.exe” 保存重启agent；在zabbix做监控时，信息类型选择字符或者文本即可。 重点来了，对于java进程亦或者是在任务管理器只看到运行了一堆相同的进程名，但看不到具体的实例名可以利用windows自带的 wmic命令来获取！打开cmd，输入 wmic process 回车，会输出目前在运行的所有进程，比tasklist还详细，输出的结果包含命令行、执行路径、pid、使用的资源、所有者等等。有兴趣的小伙伴可以到官网多多了解，拓展一下其他的wmic参数。 当然了，对于封装好的jar包程序，常规的执行命令也就是java –jar ****.jar，是可以用wmic拿到的，命令是 wmic process where name=”java.exe” 就可以输出当前正在运行的所有java进程，但结果不太美观，所以可以指定一下要找的jar包名，例如： wimc process where name=”java.exe” | find “jar包名” ，这样出来的结果就是客户想要的了。\n\n\n\n以下是agent配置示例： userparameter=java_jar, wmic.exe process where name=”java.exe” | find “jar包名” 这里保险一点，写上完整的wmic.exe就不会报命令找不到的错。监控项照上一步。 触发器 在给非数值的监控配置触发器时，要注意类型要选择红框的 str()\n\n\n\n因为返回的结果是字符串，所以在v这里填上jar包进程名即可。可以只写包名+版本，不用整行都写。\n\n\n\n问题和恢复的表达式：监控项是每60s检测一次，触发器这里也是每60s检测一次，发现和取值不符则发送告警信息。如果发现乱报警，那就多注意调整下。\n\n\n\n完成后，可以停止程序检测下是否符合需求。\n\n原文链接：\n\nhttps://blog.csdn.net/weixin_43631631/article/details/105575412",charsets:{cjk:!0}},{title:"zabbix添加进程pid监控",frontmatter:{title:"zabbix添加进程pid监控",date:"2022-12-20T17:02:20.000Z",permalink:"/pages/20f905/",categories:["专题","zabbix"],tags:[null],readingShow:"top",description:"Zabbix 监控进程宕机\n>\n>   业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；\n>\n> 当然zabbix设置报警设置就不再一一",meta:[{name:"image",content:"https://s1.51cto.com/images/20180223/1519371651993152.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="},{name:"twitter:title",content:"zabbix添加进程pid监控"},{name:"twitter:description",content:"Zabbix 监控进程宕机\n>\n>   业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；\n>\n> 当然zabbix设置报警设置就不再一一"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://s1.51cto.com/images/20180223/1519371651993152.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/06.zabbix%E6%B7%BB%E5%8A%A0%E8%BF%9B%E7%A8%8Bpid%E7%9B%91%E6%8E%A7.html"},{property:"og:type",content:"article"},{property:"og:title",content:"zabbix添加进程pid监控"},{property:"og:description",content:"Zabbix 监控进程宕机\n>\n>   业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；\n>\n> 当然zabbix设置报警设置就不再一一"},{property:"og:image",content:"https://s1.51cto.com/images/20180223/1519371651993152.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/06.zabbix%E6%B7%BB%E5%8A%A0%E8%BF%9B%E7%A8%8Bpid%E7%9B%91%E6%8E%A7.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-20T17:02:20.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"zabbix添加进程pid监控"},{itemprop:"description",content:"Zabbix 监控进程宕机\n>\n>   业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；\n>\n> 当然zabbix设置报警设置就不再一一"},{itemprop:"image",content:"https://s1.51cto.com/images/20180223/1519371651993152.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk="}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/06.zabbix%E6%B7%BB%E5%8A%A0%E8%BF%9B%E7%A8%8Bpid%E7%9B%91%E6%8E%A7.html",relativePath:"04.运维/12.监控/01.zabbix/06.zabbix添加进程pid监控.md",key:"v-1955a3de",path:"/pages/20f905/",headersStr:null,content:"> Zabbix 监控进程宕机\n> \n> 业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；\n> \n> 当然zabbix设置报警设置就不再一一\n\n在每台服务器/etc/zabbix/zabbix_agentd.conf设置路径：此例只需要piddiff.sh\n\nUserParameter=checkpid,sh /usr/local/script/piddiff.sh\n\nUserParameter=test,sh /usr/local/script/test.sh\n\nUserParameter=discovery.process,/usr/local/script/disprocess.sh\n\nUserParameter=process.check[*],/usr/local/script/proc_check.sh $1 $2 $3\n\n\n/usr/local/script下面存放脚本\n\nVim piddiff.sh\n\naapid为业务监控id 取值根据业务需求；\n\n#/bin/sh\n\nonl_ok=1\n\nonl_cored=3\n\ndir=/usr/local/script\n\nif [[ ! -f \"$dir/old.txt\" ]];then\n\n ps aux|grep aapid |grep -v grep|grep -v /bin/bash|awk  '{print $2,$11}' > $dir/old.txt\n\n      else\n\n         sleep 1s           \n\nfi\n\n ps aux|grep aapid |grep -v grep|grep -v /bin/bash|awk  '{print $2,$11}' > $dir/now.txt\n\nif ! diff -q $dir/old.txt  $dir/now.txt > /dev/null; then\n\n          echo $onl_cored\n\n          diff -c $dir/old.txt  $dir/now.txt > $dir/`date \"+%Y%m%d%H%M\"`_diff.txt\n\n          cat $dir/now.txt >$dir/old.txt\n\n     else\n\n          echo $onl_ok    \n\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n一个简单的判断脚本；\n\nZabbix30秒会抓取一次，正常没变化为1，有变化为3，那么zabbix抓取数值为3则表示pid有变化，会发出警报；\n\nZabbix设置：\n\n监控项模板添加如下：\n\n\n\n触发器：{Template OS Linux:checkpid.last()}=3\n\n\n\n转载于:https://blog.51cto.com/xpu2001/2072366",normalizedContent:"> zabbix 监控进程宕机\n> \n> 业务需求后端进程宕机以后能在短时间内迅速拉起，业务影响不大，但是开发需要查看coredump，要求能监控到pid变化；在现有构架下zabbix能监控并报警；\n> \n> 当然zabbix设置报警设置就不再一一\n\n在每台服务器/etc/zabbix/zabbix_agentd.conf设置路径：此例只需要piddiff.sh\n\nuserparameter=checkpid,sh /usr/local/script/piddiff.sh\n\nuserparameter=test,sh /usr/local/script/test.sh\n\nuserparameter=discovery.process,/usr/local/script/disprocess.sh\n\nuserparameter=process.check[*],/usr/local/script/proc_check.sh $1 $2 $3\n\n\n/usr/local/script下面存放脚本\n\nvim piddiff.sh\n\naapid为业务监控id 取值根据业务需求；\n\n#/bin/sh\n\nonl_ok=1\n\nonl_cored=3\n\ndir=/usr/local/script\n\nif [[ ! -f \"$dir/old.txt\" ]];then\n\n ps aux|grep aapid |grep -v grep|grep -v /bin/bash|awk  '{print $2,$11}' > $dir/old.txt\n\n      else\n\n         sleep 1s           \n\nfi\n\n ps aux|grep aapid |grep -v grep|grep -v /bin/bash|awk  '{print $2,$11}' > $dir/now.txt\n\nif ! diff -q $dir/old.txt  $dir/now.txt > /dev/null; then\n\n          echo $onl_cored\n\n          diff -c $dir/old.txt  $dir/now.txt > $dir/`date \"+%y%m%d%h%m\"`_diff.txt\n\n          cat $dir/now.txt >$dir/old.txt\n\n     else\n\n          echo $onl_ok    \n\nfi\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n一个简单的判断脚本；\n\nzabbix30秒会抓取一次，正常没变化为1，有变化为3，那么zabbix抓取数值为3则表示pid有变化，会发出警报；\n\nzabbix设置：\n\n监控项模板添加如下：\n\n\n\n触发器：{template os linux:checkpid.last()}=3\n\n\n\n转载于:https://blog.51cto.com/xpu2001/2072366",charsets:{cjk:!0}},{title:"zabbix添加web监控",frontmatter:{title:"zabbix添加web监控",date:"2022-12-21T17:35:46.000Z",permalink:"/pages/b70eb8/",categories:["专题","zabbix"],tags:[null],readingShow:"top",description:"点击配置-主机-我的服务器-web场景-创建web场景-按下面填写步骤",meta:[{name:"image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212Dingtalk_20221221174005.jpg"},{name:"twitter:title",content:"zabbix添加web监控"},{name:"twitter:description",content:"点击配置-主机-我的服务器-web场景-创建web场景-按下面填写步骤"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212Dingtalk_20221221174005.jpg"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/08.zabbix%E6%B7%BB%E5%8A%A0web%E7%9B%91%E6%8E%A7.html"},{property:"og:type",content:"article"},{property:"og:title",content:"zabbix添加web监控"},{property:"og:description",content:"点击配置-主机-我的服务器-web场景-创建web场景-按下面填写步骤"},{property:"og:image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212Dingtalk_20221221174005.jpg"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/08.zabbix%E6%B7%BB%E5%8A%A0web%E7%9B%91%E6%8E%A7.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-21T17:35:46.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"zabbix添加web监控"},{itemprop:"description",content:"点击配置-主机-我的服务器-web场景-创建web场景-按下面填写步骤"},{itemprop:"image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/20221212Dingtalk_20221221174005.jpg"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/08.zabbix%E6%B7%BB%E5%8A%A0web%E7%9B%91%E6%8E%A7.html",relativePath:"04.运维/12.监控/01.zabbix/08.zabbix添加web监控.md",key:"v-8105e1aa",path:"/pages/b70eb8/",headers:[{level:4,title:"web监控",slug:"web监控",normalizedTitle:"web监控",charIndex:2},{level:4,title:"创建触发器",slug:"创建触发器",normalizedTitle:"创建触发器",charIndex:52}],headersStr:"web监控 创建触发器",content:"# web监控\n\n点击配置-主机-我的服务器-web场景-创建web场景-按下面填写步骤\n\n\n\n\n\n# 创建触发器\n\n\n\n参考链接：使用zabbix监控端口及web页面触发报警配置方法 | IT博客站-盛行",normalizedContent:"# web监控\n\n点击配置-主机-我的服务器-web场景-创建web场景-按下面填写步骤\n\n\n\n\n\n# 创建触发器\n\n\n\n参考链接：使用zabbix监控端口及web页面触发报警配置方法 | it博客站-盛行",charsets:{cjk:!0}},{title:"centos7编译安装zabbix5.0 proxy端",frontmatter:{title:"centos7编译安装zabbix5.0 proxy端",date:"2023-03-01T14:31:52.000Z",permalink:"/pages/723e7e/",categories:["运维","监控","zabbix"],tags:[null],readingShow:"top",description:"zabbix-prxoy是一个代理服务器，它收集监控到的数据，先存放数据库，然后再传送到zabbix-server；所以代理需要一个单独的数据库",meta:[{name:"image",content:"http://pic.zzppjj.top/LightPicture/2023/03/e44061ad1ce81c4d.png"},{name:"twitter:title",content:"centos7编译安装zabbix5.0 proxy端"},{name:"twitter:description",content:"zabbix-prxoy是一个代理服务器，它收集监控到的数据，先存放数据库，然后再传送到zabbix-server；所以代理需要一个单独的数据库"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"http://pic.zzppjj.top/LightPicture/2023/03/e44061ad1ce81c4d.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/09.centos7%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85zabbix%20proxy%E7%AB%AF.html"},{property:"og:type",content:"article"},{property:"og:title",content:"centos7编译安装zabbix5.0 proxy端"},{property:"og:description",content:"zabbix-prxoy是一个代理服务器，它收集监控到的数据，先存放数据库，然后再传送到zabbix-server；所以代理需要一个单独的数据库"},{property:"og:image",content:"http://pic.zzppjj.top/LightPicture/2023/03/e44061ad1ce81c4d.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/09.centos7%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85zabbix%20proxy%E7%AB%AF.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-03-01T14:31:52.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"centos7编译安装zabbix5.0 proxy端"},{itemprop:"description",content:"zabbix-prxoy是一个代理服务器，它收集监控到的数据，先存放数据库，然后再传送到zabbix-server；所以代理需要一个单独的数据库"},{itemprop:"image",content:"http://pic.zzppjj.top/LightPicture/2023/03/e44061ad1ce81c4d.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/09.centos7%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85zabbix%20proxy%E7%AB%AF.html",relativePath:"04.运维/12.监控/01.zabbix/09.centos7编译安装zabbix proxy端.md",key:"v-c5ed0f8c",path:"/pages/723e7e/",headers:[{level:2,title:"1、准备环境",slug:"_1、准备环境",normalizedTitle:"1、准备环境",charIndex:2},{level:2,title:"2、开始安装",slug:"_2、开始安装",normalizedTitle:"2、开始安装",charIndex:263},{level:3,title:"添加zabbix用户用于运行zabbix服务",slug:"添加zabbix用户用于运行zabbix服务",normalizedTitle:"添加zabbix用户用于运行zabbix服务",charIndex:274},{level:3,title:"安装依赖包",slug:"安装依赖包",normalizedTitle:"安装依赖包",charIndex:366},{level:3,title:"解压源码包",slug:"解压源码包",normalizedTitle:"解压源码包",charIndex:470},{level:3,title:"编译参数",slug:"编译参数",normalizedTitle:"编译参数",charIndex:541},{level:3,title:"初始化数据库信息",slug:"初始化数据库信息",normalizedTitle:"初始化数据库信息",charIndex:1147},{level:4,title:"登录数据库创建zabbix-proxy库",slug:"登录数据库创建zabbix-proxy库",normalizedTitle:"登录数据库创建zabbix-proxy库",charIndex:1220},{level:4,title:"导入数据",slug:"导入数据",normalizedTitle:"导入数据",charIndex:1686},{level:4,title:"配置zabbix_proxy.conf配置文件",slug:"配置zabbix-proxy-conf配置文件",normalizedTitle:"配置zabbix_proxy.conf配置文件",charIndex:1804},{level:4,title:"修改以下信息",slug:"修改以下信息",normalizedTitle:"修改以下信息",charIndex:1833},{level:4,title:"启动zabbix-proxy",slug:"启动zabbix-proxy",normalizedTitle:"启动zabbix-proxy",charIndex:2054},{level:4,title:"启动报错提示找不到mysql相关lib文件",slug:"启动报错提示找不到mysql相关lib文件",normalizedTitle:"启动报错提示找不到mysql相关lib文件",charIndex:2202},{level:4,title:"将mysql lib目录添加到lib环境变量中",slug:"将mysql-lib目录添加到lib环境变量中",normalizedTitle:"将mysql lib目录添加到lib环境变量中",charIndex:2229},{level:4,title:"再次启动zabbix-proxy",slug:"再次启动zabbix-proxy",normalizedTitle:"再次启动zabbix-proxy",charIndex:2402},{level:3,title:"打开zabbix-server页面，选择配置---》agent代理程序",slug:"打开zabbix-server页面-选择配置-》agent代理程序",normalizedTitle:"打开zabbix-server页面，选择配置---》agent代理程序",charIndex:2431},{level:3,title:"选择创建代理程序",slug:"选择创建代理程序",normalizedTitle:"选择创建代理程序",charIndex:2473},{level:3,title:"填写相关信息，保存即可",slug:"填写相关信息-保存即可",normalizedTitle:"填写相关信息，保存即可",charIndex:2488}],headersStr:"1、准备环境 2、开始安装 添加zabbix用户用于运行zabbix服务 安装依赖包 解压源码包 编译参数 初始化数据库信息 登录数据库创建zabbix-proxy库 导入数据 配置zabbix_proxy.conf配置文件 修改以下信息 启动zabbix-proxy 启动报错提示找不到mysql相关lib文件 将mysql lib目录添加到lib环境变量中 再次启动zabbix-proxy 打开zabbix-server页面，选择配置---》agent代理程序 选择创建代理程序 填写相关信息，保存即可",content:"# 1、准备环境\n\nzabbix-prxoy是一个代理服务器，它收集监控到的数据，先存放数据库，然后再传送到zabbix-server；所以代理需要一个单独的数据库\n\nzabbix-prxoy的作用：\n\n1、当监控设备过多时，用于分担zabbix-server监控压力\n\n2、针对不同的网络环境，可以组成分布式监控\n\n环境：已安装MySQL 5.7.31\n\nzabbix5.0 ：https://cdn.zabbix.com/zabbix/sources/stable/5.0/zabbix-5.0.2.tar.gz\n\n\n# 2、开始安装\n\n\n# 添加zabbix用户用于运行zabbix服务\n\n[root@swarm-node2 local]# useradd -s /sbin/nologin zabbix　\n\n\n1\n\n\n\n# 安装依赖包\n\nyum install unixODBC-devel net-snmp-devel libevent-devel libxml2-devel libcurl-devel -y\n\n\n1\n\n\n\n# 解压源码包\n\n[root@swarm-node2 local]# tar -zxf zabbix-5.0.2.tar.gz\n\n\n1\n\n\n\n# 编译参数\n\n./configure --prefix=/usr/local/zabbix-proxy --enable-proxy --with-mysql=/usr/local/mysql/bin/mysql_config --with-net-snmp --with-libcurl --with-libxml2 --with-unixodbc\n\n\n1\n\n\n--prefix=/usr/local/zabbix-proxy        # 编译安装路径\n\n--enable-prxoy　　　　　　　　　 # 启动proxy端\n\n--with-mysql=/usr/local/mysql/bin/mysql_config    #编译连接mysql相关依赖包，并指定mysql_config的路径\n\n--with-net-snmp　　　　　　　　　#启动snmp相关，用于snmp监控设备\n\n--with-libcurl　　　　　　　　　　 #启动curl相关\n\n--with-libxml2　　　　　　　　　　#启动xml相关\n\n--with-unixodbc　　　　　　　　　#启动odbc相关用于监控数据库\n\n\n\n确认无误后就开始make && make install\n\n[root@swarm-node2 zabbix-5.0.2]# make && make install\n\n\n1\n\n\n\n# 初始化数据库信息\n\n数据库文件在zabbix-5.0.2/database/mysql 中\n\nproxy只需要导入schema.sql即可\n\n# 登录数据库创建zabbix-proxy库\n\nmysql> create database proxy character set UTF8 collate utf8_bin;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> \nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| proxy              |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n\nmysql> \nmysql> \nmysql>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 导入数据\n\nmysql> \nmysql> use proxy;\nDatabase changed\nmysql> source /usr/local/zabbix-5.0.2/database/mysql/schema.sql;\n\n\n# 配置zabbix_proxy.conf配置文件\n\n\n\n# 修改以下信息\n\nServer=192.168.137.128                #指定zabbix-server的ip地址\nDBHost=localhost　　　　　　　　　　　　　#以下都是数据库信息\nDBName=proxy\nDBUser=root\nDBPassword=123.com\nDBSocket=/usr/local/mysql/mysql.sock\nDBPort=3306\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 启动zabbix-proxy\n\n[root@swarm-node2 zabbix-proxy]# /usr/local/zabbix-proxy/sbin/zabbix_proxy -c /usr/local/zabbix-proxy/etc/zabbix_proxy.conf\n\n\n1\n\n\n# 启动报错提示找不到mysql相关lib文件\n\n\n\n# 将mysql lib目录添加到lib环境变量中\n\n[root@swarm-node2 zabbix-proxy]# echo '/usr/local/mysql/lib/' > /etc/ld.so.conf.d/mysql.conf\n[root@swarm-node2 zabbix-proxy]# ldconfig -v\n\n\n1\n2\n\n\n# 再次启动zabbix-proxy\n\n启动成功\n\n\n\n\n# 打开zabbix-server页面，选择配置---》agent代理程序\n\n\n\n\n# 选择创建代理程序\n\n\n\n\n# 填写相关信息，保存即可\n\nagent代理程序名称：为zabbix-proxy配置文件中的Hostname\n\n系统代理程序模式：选择主动式\n\n\n\n----------------------------------------\n\n原文链接：CentOS-7 编译安装zabbix5.0（proxy端） - cchenppp - 博客园",normalizedContent:"# 1、准备环境\n\nzabbix-prxoy是一个代理服务器，它收集监控到的数据，先存放数据库，然后再传送到zabbix-server；所以代理需要一个单独的数据库\n\nzabbix-prxoy的作用：\n\n1、当监控设备过多时，用于分担zabbix-server监控压力\n\n2、针对不同的网络环境，可以组成分布式监控\n\n环境：已安装mysql 5.7.31\n\nzabbix5.0 ：https://cdn.zabbix.com/zabbix/sources/stable/5.0/zabbix-5.0.2.tar.gz\n\n\n# 2、开始安装\n\n\n# 添加zabbix用户用于运行zabbix服务\n\n[root@swarm-node2 local]# useradd -s /sbin/nologin zabbix　\n\n\n1\n\n\n\n# 安装依赖包\n\nyum install unixodbc-devel net-snmp-devel libevent-devel libxml2-devel libcurl-devel -y\n\n\n1\n\n\n\n# 解压源码包\n\n[root@swarm-node2 local]# tar -zxf zabbix-5.0.2.tar.gz\n\n\n1\n\n\n\n# 编译参数\n\n./configure --prefix=/usr/local/zabbix-proxy --enable-proxy --with-mysql=/usr/local/mysql/bin/mysql_config --with-net-snmp --with-libcurl --with-libxml2 --with-unixodbc\n\n\n1\n\n\n--prefix=/usr/local/zabbix-proxy        # 编译安装路径\n\n--enable-prxoy　　　　　　　　　 # 启动proxy端\n\n--with-mysql=/usr/local/mysql/bin/mysql_config    #编译连接mysql相关依赖包，并指定mysql_config的路径\n\n--with-net-snmp　　　　　　　　　#启动snmp相关，用于snmp监控设备\n\n--with-libcurl　　　　　　　　　　 #启动curl相关\n\n--with-libxml2　　　　　　　　　　#启动xml相关\n\n--with-unixodbc　　　　　　　　　#启动odbc相关用于监控数据库\n\n\n\n确认无误后就开始make && make install\n\n[root@swarm-node2 zabbix-5.0.2]# make && make install\n\n\n1\n\n\n\n# 初始化数据库信息\n\n数据库文件在zabbix-5.0.2/database/mysql 中\n\nproxy只需要导入schema.sql即可\n\n# 登录数据库创建zabbix-proxy库\n\nmysql> create database proxy character set utf8 collate utf8_bin;\nquery ok, 1 row affected (0.00 sec)\n\nmysql> \nmysql> show databases;\n+--------------------+\n| database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| proxy              |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n\nmysql> \nmysql> \nmysql>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 导入数据\n\nmysql> \nmysql> use proxy;\ndatabase changed\nmysql> source /usr/local/zabbix-5.0.2/database/mysql/schema.sql;\n\n\n# 配置zabbix_proxy.conf配置文件\n\n\n\n# 修改以下信息\n\nserver=192.168.137.128                #指定zabbix-server的ip地址\ndbhost=localhost　　　　　　　　　　　　　#以下都是数据库信息\ndbname=proxy\ndbuser=root\ndbpassword=123.com\ndbsocket=/usr/local/mysql/mysql.sock\ndbport=3306\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 启动zabbix-proxy\n\n[root@swarm-node2 zabbix-proxy]# /usr/local/zabbix-proxy/sbin/zabbix_proxy -c /usr/local/zabbix-proxy/etc/zabbix_proxy.conf\n\n\n1\n\n\n# 启动报错提示找不到mysql相关lib文件\n\n\n\n# 将mysql lib目录添加到lib环境变量中\n\n[root@swarm-node2 zabbix-proxy]# echo '/usr/local/mysql/lib/' > /etc/ld.so.conf.d/mysql.conf\n[root@swarm-node2 zabbix-proxy]# ldconfig -v\n\n\n1\n2\n\n\n# 再次启动zabbix-proxy\n\n启动成功\n\n\n\n\n# 打开zabbix-server页面，选择配置---》agent代理程序\n\n\n\n\n# 选择创建代理程序\n\n\n\n\n# 填写相关信息，保存即可\n\nagent代理程序名称：为zabbix-proxy配置文件中的hostname\n\n系统代理程序模式：选择主动式\n\n\n\n----------------------------------------\n\n原文链接：centos-7 编译安装zabbix5.0（proxy端） - cchenppp - 博客园",charsets:{cjk:!0}},{title:"rpm安装zabbix proxy过程简记",frontmatter:{title:"rpm安装zabbix proxy过程简记",date:"2023-03-02T10:13:05.000Z",permalink:"/pages/ef347e/",categories:["运维","监控","zabbix"],tags:[null],readingShow:"top",description:"查验版本",meta:[{name:"twitter:title",content:"rpm安装zabbix proxy过程简记"},{name:"twitter:description",content:"查验版本"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/10.rpm%E5%AE%89%E8%A3%85zabbix%20proxy%E8%BF%87%E7%A8%8B%E7%AE%80%E8%AE%B0.html"},{property:"og:type",content:"article"},{property:"og:title",content:"rpm安装zabbix proxy过程简记"},{property:"og:description",content:"查验版本"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/10.rpm%E5%AE%89%E8%A3%85zabbix%20proxy%E8%BF%87%E7%A8%8B%E7%AE%80%E8%AE%B0.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-03-02T10:13:05.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"rpm安装zabbix proxy过程简记"},{itemprop:"description",content:"查验版本"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/01.zabbix/10.rpm%E5%AE%89%E8%A3%85zabbix%20proxy%E8%BF%87%E7%A8%8B%E7%AE%80%E8%AE%B0.html",relativePath:"04.运维/12.监控/01.zabbix/10.rpm安装zabbix proxy过程简记.md",key:"v-20df7df7",path:"/pages/ef347e/",headers:[{level:3,title:"环境初始化",slug:"环境初始化",normalizedTitle:"环境初始化",charIndex:2},{level:3,title:"安装依赖包",slug:"安装依赖包",normalizedTitle:"安装依赖包",charIndex:260},{level:3,title:"下载对应版本的zabbix-proxy-mysql安装包",slug:"下载对应版本的zabbix-proxy-mysql安装包",normalizedTitle:"下载对应版本的zabbix-proxy-mysql安装包",charIndex:534},{level:3,title:"安装并初始化数据库",slug:"安装并初始化数据库",normalizedTitle:"安装并初始化数据库",charIndex:837},{level:3,title:"配置zabbix-proxy数据库",slug:"配置zabbix-proxy数据库",normalizedTitle:"配置zabbix-proxy数据库",charIndex:1002},{level:4,title:"登录数据库",slug:"登录数据库",normalizedTitle:"登录数据库",charIndex:1023},{level:4,title:"创建zabbix_proxy库并设置字符集",slug:"创建zabbix-proxy库并设置字符集",normalizedTitle:"创建zabbix_proxy库并设置字符集",charIndex:1068},{level:4,title:"对root用户进行授权",slug:"对root用户进行授权",normalizedTitle:"对root用户进行授权",charIndex:1186},{level:4,title:"创建zabbix用户",slug:"创建zabbix用户",normalizedTitle:"创建zabbix用户",charIndex:1290},{level:4,title:"对zabbix进行授权(授权其仅能访问zabbix_proxy库）",slug:"对zabbix进行授权-授权其仅能访问zabbix-proxy库",normalizedTitle:"对zabbix进行授权(授权其仅能访问zabbix_proxy库）",charIndex:1386},{level:4,title:"quit推出数据库",slug:"quit推出数据库",normalizedTitle:"quit推出数据库",charIndex:1535},{level:3,title:"导入zabbix-proxy-mysql库文件",slug:"导入zabbix-proxy-mysql库文件",normalizedTitle:"导入zabbix-proxy-mysql库文件",charIndex:1549},{level:3,title:"修改配置文件",slug:"修改配置文件",normalizedTitle:"修改配置文件",charIndex:1690},{level:3,title:"重启zabbix-proxy和zabbix-agent服务",slug:"重启zabbix-proxy和zabbix-agent服务",normalizedTitle:"重启zabbix-proxy和zabbix-agent服务",charIndex:2140},{level:3,title:"添加agent代理配置",slug:"添加agent代理配置",normalizedTitle:"添加agent代理配置",charIndex:2335}],headersStr:"环境初始化 安装依赖包 下载对应版本的zabbix-proxy-mysql安装包 安装并初始化数据库 配置zabbix-proxy数据库 登录数据库 创建zabbix_proxy库并设置字符集 对root用户进行授权 创建zabbix用户 对zabbix进行授权(授权其仅能访问zabbix_proxy库） quit推出数据库 导入zabbix-proxy-mysql库文件 修改配置文件 重启zabbix-proxy和zabbix-agent服务 添加agent代理配置",content:"# 环境初始化\n\n查验版本\n\n#cat /etc/redhat-release\n\n关闭防火墙，selinux systemctl stop firewalld.service && systemctl disable firewalld.service sed -i ‘s/^SELINUX=enforcing/#SELINUX=enforcing\\nSELINUX=disabled/g’ /etc/selinux/config 配置时间同步 配置yum源 优化ssh服务 安装必要软件包 以上过程不一一细讲\n\n\n# 安装依赖包\n\nyum -y install OpenIPMI OpenIPMI-modalias net-snmp-libs net-snmp-agent-libs fping unixODBC\n\nfping需要epel源，若没有的话也可以使用rpm包进行安装 wget http://www.rpmfind.net/linux/dag/redhat/el7/en/x86_64/dag/RPMS/fping-3.10-1.el7.rf.x86_64.rpm rpm -ivh fping-3.10-1.el7.rf.x86_64.rpm\n\n\n# 下载对应版本的zabbix-proxy-mysql安装包\n\n官网：https://repo.zabbix.com/zabbix/5.0/ 由于我这里的zabbix-server端的版本是5.0.19，因此这里的zabbix-proxy版本也用的是5.0.X; yum -y install wget wget https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-proxy-mysql-5.0.25-1.el7.x86_64.rpm rpm -ivh zabbix-proxy-mysql-5.0.25-1.el7.x86_64.rpm\n\n\n# 安装并初始化数据库\n\nyum -y install mariadb-server systemctl restart mariadb && systemctl enable mariadb netstat -anptu | grep -i mysqld *初始化数据库 mysql_secure_installation\n\n\n# 配置zabbix-proxy数据库\n\n# 登录数据库\n\n[root@CentOS79 ~]# mysql -uroot -p\n\n# 创建zabbix_proxy库并设置字符集\n\nMariaDB [(none)]> create database zabbix_proxy default character set utf8 collate utf8_bin;\n\n# 对root用户进行授权\n\nMariaDB [(none)]> grant all privileges on . to root@127.0.0.1 identified by ‘root@123’;\n\n# 创建zabbix用户\n\nMariaDB [zabbix_proxy]> create user zabbix@localhost identified by ‘zabbix@123’;\n\n# 对zabbix进行授权(授权其仅能访问zabbix_proxy库）\n\nMariaDB [zabbix_proxy]> grant all privileges on zabbix_proxy.* to zabbix@localhost identified by ‘zabbix@123’;\n\n# quit推出数据库\n\n\n# 导入zabbix-proxy-mysql库文件\n\n[root@CentOS79 ~]# zcat /usr/share/doc/zabbix-proxy-mysql-5.0.25/schema.sql.gz | mysql -uzabbix -p zabbix_proxy\n\n\n# 修改配置文件\n\n[root@CentOS79 ~]# vim /etc/zabbix/zabbix_proxy.conf 编辑如下参数： 30 Server=10.0.3.246 ##配置zabbix-server服务端地址，zabbix-proxy会将收集到数 据发往该地址 49 Hostname=YCGF-Zabbix-Proxy ##配置主机名 163 DBHost=localhost ##配置数据库主机（默认localhost即可） 174 DBName=zabbix_proxy ##配置数据库名成（就是你刚创建的库名） 189 DBUser=zabbix ##配置书库据操作用户 198 DBPassword=Longi@123 ##输入数据库用户密码 其他配置默认即可 [root@CentOS79 zabbix]# vim /etc/zabbix/zabbix_agentd.conf 100 Server=10.0.3.246 ##配置zabbix-server端地址\n\n\n# 重启zabbix-proxy和zabbix-agent服务\n\nsystemctl restart zabbix-proxy.service && systemctl enable zabbix-proxy.service systemctl restart zabbix-agent.service && systemctl enable zabbix-agent.service\n\n\n# 添加agent代理配置\n\n登录zabbix-web——管理——agent代理程序——创建代理——根据页面填空即可\n\n此处我选择的是被动式。 主动模式：客户端每隔一段时间主动向服务端发起连接请求–>服务端收到请求,查询客户端需要取的item信息,发送给客户端–>客户端收集数据发送服务端–>结束。（收集数据时间一致） 被动模式：客户端开一个端口默认10050,等待服务端来取数据,然后客户端收集数据发送到服务端,然后结束。\n\n原文链接：CentOS7.9部署zabbix-proxy-5.0.25过程简记_王大江1018的博客-CSDN博客",normalizedContent:"# 环境初始化\n\n查验版本\n\n#cat /etc/redhat-release\n\n关闭防火墙，selinux systemctl stop firewalld.service && systemctl disable firewalld.service sed -i ‘s/^selinux=enforcing/#selinux=enforcing\\nselinux=disabled/g’ /etc/selinux/config 配置时间同步 配置yum源 优化ssh服务 安装必要软件包 以上过程不一一细讲\n\n\n# 安装依赖包\n\nyum -y install openipmi openipmi-modalias net-snmp-libs net-snmp-agent-libs fping unixodbc\n\nfping需要epel源，若没有的话也可以使用rpm包进行安装 wget http://www.rpmfind.net/linux/dag/redhat/el7/en/x86_64/dag/rpms/fping-3.10-1.el7.rf.x86_64.rpm rpm -ivh fping-3.10-1.el7.rf.x86_64.rpm\n\n\n# 下载对应版本的zabbix-proxy-mysql安装包\n\n官网：https://repo.zabbix.com/zabbix/5.0/ 由于我这里的zabbix-server端的版本是5.0.19，因此这里的zabbix-proxy版本也用的是5.0.x; yum -y install wget wget https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-proxy-mysql-5.0.25-1.el7.x86_64.rpm rpm -ivh zabbix-proxy-mysql-5.0.25-1.el7.x86_64.rpm\n\n\n# 安装并初始化数据库\n\nyum -y install mariadb-server systemctl restart mariadb && systemctl enable mariadb netstat -anptu | grep -i mysqld *初始化数据库 mysql_secure_installation\n\n\n# 配置zabbix-proxy数据库\n\n# 登录数据库\n\n[root@centos79 ~]# mysql -uroot -p\n\n# 创建zabbix_proxy库并设置字符集\n\nmariadb [(none)]> create database zabbix_proxy default character set utf8 collate utf8_bin;\n\n# 对root用户进行授权\n\nmariadb [(none)]> grant all privileges on . to root@127.0.0.1 identified by ‘root@123’;\n\n# 创建zabbix用户\n\nmariadb [zabbix_proxy]> create user zabbix@localhost identified by ‘zabbix@123’;\n\n# 对zabbix进行授权(授权其仅能访问zabbix_proxy库）\n\nmariadb [zabbix_proxy]> grant all privileges on zabbix_proxy.* to zabbix@localhost identified by ‘zabbix@123’;\n\n# quit推出数据库\n\n\n# 导入zabbix-proxy-mysql库文件\n\n[root@centos79 ~]# zcat /usr/share/doc/zabbix-proxy-mysql-5.0.25/schema.sql.gz | mysql -uzabbix -p zabbix_proxy\n\n\n# 修改配置文件\n\n[root@centos79 ~]# vim /etc/zabbix/zabbix_proxy.conf 编辑如下参数： 30 server=10.0.3.246 ##配置zabbix-server服务端地址，zabbix-proxy会将收集到数 据发往该地址 49 hostname=ycgf-zabbix-proxy ##配置主机名 163 dbhost=localhost ##配置数据库主机（默认localhost即可） 174 dbname=zabbix_proxy ##配置数据库名成（就是你刚创建的库名） 189 dbuser=zabbix ##配置书库据操作用户 198 dbpassword=longi@123 ##输入数据库用户密码 其他配置默认即可 [root@centos79 zabbix]# vim /etc/zabbix/zabbix_agentd.conf 100 server=10.0.3.246 ##配置zabbix-server端地址\n\n\n# 重启zabbix-proxy和zabbix-agent服务\n\nsystemctl restart zabbix-proxy.service && systemctl enable zabbix-proxy.service systemctl restart zabbix-agent.service && systemctl enable zabbix-agent.service\n\n\n# 添加agent代理配置\n\n登录zabbix-web——管理——agent代理程序——创建代理——根据页面填空即可\n\n此处我选择的是被动式。 主动模式：客户端每隔一段时间主动向服务端发起连接请求–>服务端收到请求,查询客户端需要取的item信息,发送给客户端–>客户端收集数据发送服务端–>结束。（收集数据时间一致） 被动模式：客户端开一个端口默认10050,等待服务端来取数据,然后客户端收集数据发送到服务端,然后结束。\n\n原文链接：centos7.9部署zabbix-proxy-5.0.25过程简记_王大江1018的博客-csdn博客",charsets:{cjk:!0}},{title:"prometheus监控、告警与存储",frontmatter:{title:"prometheus监控、告警与存储",date:"2022-12-12T09:57:58.000Z",permalink:"/pages/40794d/",categories:["监控","prometheus"],tags:[null],readingShow:"top",description:"github地址：https://github.com/kubernetes/kube-state-metrics",meta:[{name:"image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/27521399-c2fb1fc8fbb0563f.png"},{name:"twitter:title",content:"prometheus监控、告警与存储"},{name:"twitter:description",content:"github地址：https://github.com/kubernetes/kube-state-metrics"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/27521399-c2fb1fc8fbb0563f.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/01.prometheus%E7%9B%91%E6%8E%A7%E3%80%81%E5%91%8A%E8%AD%A6%E4%B8%8E%E5%AD%98%E5%82%A8.html"},{property:"og:type",content:"article"},{property:"og:title",content:"prometheus监控、告警与存储"},{property:"og:description",content:"github地址：https://github.com/kubernetes/kube-state-metrics"},{property:"og:image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/27521399-c2fb1fc8fbb0563f.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/01.prometheus%E7%9B%91%E6%8E%A7%E3%80%81%E5%91%8A%E8%AD%A6%E4%B8%8E%E5%AD%98%E5%82%A8.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-12T09:57:58.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"prometheus监控、告警与存储"},{itemprop:"description",content:"github地址：https://github.com/kubernetes/kube-state-metrics"},{itemprop:"image",content:"https://raw.githubusercontent.com/zpj874878956/images/main/img/27521399-c2fb1fc8fbb0563f.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/01.prometheus%E7%9B%91%E6%8E%A7%E3%80%81%E5%91%8A%E8%AD%A6%E4%B8%8E%E5%AD%98%E5%82%A8.html",relativePath:"04.运维/12.监控/02.prometheus/01.prometheus监控、告警与存储.md",key:"v-4008dcfe",path:"/pages/40794d/",headers:[{level:2,title:"1.1 kube-state-metrics介绍",slug:"_1-1-kube-state-metrics介绍",normalizedTitle:"1.1 kube-state-metrics介绍",charIndex:27},{level:2,title:"1.2 部署kube-state-metrics",slug:"_1-2-部署kube-state-metrics",normalizedTitle:"1.2 部署kube-state-metrics",charIndex:512},{level:2,title:"1.3 验证数据",slug:"_1-3-验证数据",normalizedTitle:"1.3 验证数据",charIndex:605},{level:2,title:"1.4 prometheus数据采集",slug:"_1-4-prometheus数据采集",normalizedTitle:"1.4 prometheus数据采集",charIndex:622},{level:2,title:"1.5 验证prometheus状态",slug:"_1-5-验证prometheus状态",normalizedTitle:"1.5 验证prometheus状态",charIndex:769},{level:2,title:"1.6 grafana导入模板",slug:"_1-6-grafana导入模板",normalizedTitle:"1.6 grafana导入模板",charIndex:792},{level:2,title:"2.1 tomcat",slug:"_2-1-tomcat",normalizedTitle:"2.1 tomcat",charIndex:950},{level:2,title:"2.2 redis",slug:"_2-2-redis",normalizedTitle:"2.2 redis",charIndex:1673},{level:2,title:"2.3 mysql",slug:"_2-3-mysql",normalizedTitle:"2.3 mysql",charIndex:1989},{level:2,title:"2.4 haproxy",slug:"_2-4-haproxy",normalizedTitle:"2.4 haproxy",charIndex:3402},{level:2,title:"2.5 nginx",slug:"_2-5-nginx",normalizedTitle:"2.5 nginx",charIndex:5022},{level:2,title:"2.6 blockbox监控url",slug:"_2-6-blockbox监控url",normalizedTitle:"2.6 blockbox监控url",charIndex:7199},{level:2,title:"3.1 Alertmanager",slug:"_3-1-alertmanager",normalizedTitle:"3.1 alertmanager",charIndex:10420},{level:2,title:"3.2 邮件",slug:"_3-2-邮件",normalizedTitle:"3.2 邮件",charIndex:11588},{level:2,title:"3.3 钉钉",slug:"_3-3-钉钉",normalizedTitle:"3.3 钉钉",charIndex:15377}],headersStr:"1.1 kube-state-metrics介绍 1.2 部署kube-state-metrics 1.3 验证数据 1.4 prometheus数据采集 1.5 验证prometheus状态 1.6 grafana导入模板 2.1 tomcat 2.2 redis 2.3 mysql 2.4 haproxy 2.5 nginx 2.6 blockbox监控url 3.1 Alertmanager 3.2 邮件 3.3 钉钉",content:'# 一、kube-state-metrics\n\n\n# 1.1 kube-state-metrics介绍\n\ngithub地址：https://github.com/kubernetes/kube-state-metrics\n\n镜像地址：https://hub.docker.com/r/bitnami/kube-state-metrics\n\n博客介绍：https://xie.infoq.cn/article/9e1fff6306649e65480a96bb1\n\nkube-state-metrics是通过监听API Server生成有关资源对象的状态指标，比如Deployment、Node、Pod，需要注意的是kube-state-metrics只是简单的提供一个metrics数据，并不会存储这些指标数据，所以我们可以使用Prometheus来抓取这些数据然后存储，主要关注的是业务相关的一些元数据，比如Deployment、Pod、副本状态等，调度了多少个replicas？现在可用的有几个？多少个Pod是running/stopped/terminated状态？Pod重启了多少次？目前由多少job在运行中\n\n\n# 1.2 部署kube-state-metrics\n\n 1. 编写基于deploy控制器的yaml文件\n 2. 编写svc的yaml文件，端口暴露为NodePort\n 3. 部署\n\n\n# 1.3 验证数据\n\n\n\n\n\n\n# 1.4 prometheus数据采集\n\n    - job_name: \'kube-state-metrics\'\n      static_configs:\n        - targets: ["IP:PORT"]\n\n\n1\n2\n3\n\n\n> k8s配置文件configmap缩进格式\n\n\n# 1.5 验证prometheus状态\n\n\n# 1.6 grafana导入模板\n\n 1. 13824\n 2. 14518\n\n> 因为版本不同，可根据对应版本进行设置\n> \n> Dashboard模板网址：https://grafana.com/grafana/dashboards/\n\n\n# 二、监控示例\n\n基于第三方exporter实现对目标服务的监控\n\n\n# 2.1 tomcat\n\n * 构建镜像\n\ngithub地址：https://github.com/nlighten/tomcat_exporter\n\n根据tomcat官方镜像添加jar包\n\nADD metrics.war /data/tomcat/webapps\nADD simpleclient-0.8.0.jar  /usr/local/tomcat/lib/\nADD simpleclient_common-0.8.0.jar /usr/local/tomcat/lib/\nADD simpleclient_hotspot-0.8.0.jar /usr/local/tomcat/lib/\nADD simpleclient_servlet-0.8.0.jar /usr/local/tomcat/lib/\nADD tomcat_exporter_client-0.0.12.jar /usr/local/tomcat/lib/\n\n\n1\n2\n3\n4\n5\n6\n\n * prometheus采集\n\n    - job_name: \'kube-state-metrics\'\n      static_configs:\n        - targets: ["IP:PORT"]\n\n\n1\n2\n3\n\n\n> k8s配置文件configmap缩进格式\n\n * prometheus验证\n\n * grafana导入模板\n\ngithub地址：https://github.com/nlighten/tomcat_exporter/blob/master/dashboard/example.json\n\n> 下载这个json文件导入grafana即可\n\n\n# 2.2 redis\n\n通过redis_exporter监控redis服务装态\n\ngithub网址：https://github.com/oliver006/redis_exporter\n\n * 部署redis\n\n> 一个pod两个容器，redis和redis-exporter\n\n * prometheus采集\n\n    - job_name: \'redis-metrics\'\n      static_configs:\n        - targets: ["IP:PORT"]\n\n\n1\n2\n3\n\n\n> k8s配置文件configmap缩进格式\n\n * grafana导入模板\n\n 1. 14615\n 2. 11692\n\n\n# 2.3 mysql\n\n通过mysqld_exporter监控MySQL服务的运行状态\n\ngithub网址：https://github.com/prometheus/mysqld_exporter\n\n * 安装mariadb-server\n\napt install -y mariadb\n\n\n1\n\n * 修改配置文件/etc/mysql/mariadb.conf.d/50-server.cnf监听地址，修改为0.0.0.0\n\n> bind-address = 0.0.0.0\n> \n> 重启mariadb\n\n * 创建mysql_exporter用户\n\ncreate user \'mysql_exporter\'@\'localhost\' identified by \'password\';\n\n\n1\n\n * 测试用户名密码连接\n\nmysql -umysql_exporter -hlocalhost -ppassword\n\n\n1\n\n * 下载mysql_exporter\n\n# 下载\nwget https://github.com/prometheus/mysqld_exporter/releases/download/v0.13.0/mysqld_exporter-0.13.0.linux-amd64.tar.gz\n# 解压\ntar xvf mysqld_exporter-0.13.0.linux-amd64.tar.gz\n# 查看启动参数\n./mysqld_exporter --help\n\n\n1\n2\n3\n4\n5\n6\n\n * 创建免密登陆文件/root/.my.cnf\n\ncat >> /root/.my.cnf <<EOF\n[client]\nuser=mysql_exporter\npassword=123321\nEOF\n\n\n1\n2\n3\n4\n5\n\n * 创建mysqld_service文件并启动\n\n# 创建软链接\nln -sv /apps/mysqld_exporter-0.13.0.linux-amd64 /apps/mysqld_exporter\n\n# 创建service文件\ncat >> /etc/systemd/system/mysqld_exporter.service <<EOF\n[Unit]\nDescription=Prometheus Mysql Exporter\nAfter=network.target\n\n[Service]\nExecStart=/apps/mysqld_exporter/mysqld_exporter --config.my-cnf=/root/.my.cnf\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n# 重新加载配置\nsystemctl daemon-reload\n\n# 启动mysqld_exporter\nsystemctl start mysqld_exporter.service\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n * 验证metrics\n\n1647316276383.png\n\n * 验证prometheus\n\n1647316313220.png\n\n * grafana导入模板\n\n 1. 13106\n 2. 11323\n\n\n# 2.4 haproxy\n\n通过haproxy_exporter监控haproxy\n\ngithub网址：https://github.com/prometheus/haproxy_exporter\n\n * 安装haproxy\n\napt install -y haproxy\n\n\n1\n\n * 修改配置文件，监听一个服务\n\nlisten SERVICE\n  bind BIND_IP:PORT\n  mode tcp\n  server SERVER_NAME LISTEN_IP:PORT check inter 3s fall 3 rise 3\n\n\n1\n2\n3\n4\n\n\n> 确保sock文件是admin用户（level后边的admin）\n> \n>     stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners\n> \n> \n> 1\n\n * 重启haproxy\n\nsystemctl restart haproxy\n\n\n1\n\n\n> 检查监听端口是否正常\n\n * 下载haproxy_exporter\n\n# 下载\nwget https://github.com/prometheus/haproxy_exporter/releases/download/v0.13.0/haproxy_exporter-0.13.0.linux-amd64.tar.gz\n# 解压\ntar xvf haproxy_exporter-0.13.0.linux-amd64.tar.gz\n# 创建软链接\nln -sv /apps/haproxy_exporter-0.13.0.linux-amd64 /apps/haproxy_exporter\n\n\n1\n2\n3\n4\n5\n6\n\n * 配置文件启动haproxy_expoter\n\n./haproxy_exporter --haproxy.scrape-uri=unix:/run/haproxy/admin.sock\n\n\n1\n\n\n> 端口默认监听9101\n\n * 配置haproxy状态页\n\nlisten stats\n  bind :PORT\n  stats enable\n  stats uri /haproxy-status\n  stats realm HAProxy\\ Stats\\ Page\n  stats auth haadmin:123456\n  stats auth admin:123456\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n> 编辑/etc/haproxy/haproxy.cfg添加如上配置内容\n\n * 状态页启动haproxy\n\n./haproxy_exporter --haproxy.scrape-uri="http://admin:123456@127.0.0.1:PORT/haproxy-status;csv"\n\n\n1\n\n\n> 需要指定用户名密码，csv是指定以csv形式展示\n\n * 验证exporter\n\n1647397779547.png\n\n * prometheus数据采集\n\n  - job_name: "haproxy-exporter"\n    static_configs:\n      - targets: ["127.0.0.1:9101"]\n\n\n1\n2\n3\n\n\n> 虚拟机prometheus.yml配置缩进格式\n> \n> ./promtool check config prometheus.yml # 修改后检查配置文件是否正确\n\n * 重启prometheus\n\nsystemctl restart prometheus.service\n\n\n1\n\n\n * 验证prometheus\n\n * grafana导入模板\n\n 1. 367\n 2. 2428\n\n\n# 2.5 nginx\n\n通过nginx_exporter监控ngix\n\ngithub模块依赖网址：https://github.com/vozlt/nginx-module-vts\n\n * 安装nginx\n\n# 克隆依赖模块\ngit clone https://github.com/vozlt/nginx-module-vts.git\n# 下载nginx源码\nwget http://nginx.org/download/nginx-1.20.2.tar.gz\n# 解压\ntar xvf nginx-1.20.2.tar.gz\n# 安装nginx编译依赖包\napt install -y libgd-dev libgeoip-dev libpcre3 libpcre3-dev libssl-dev gcc make\n# 编译nginx\ncd nginx-1.20.2\n./configure --prefix=/apps/nginx \\\n--with-http_ssl_module \\\n--with-http_v2_module \\\n--with-http_realip_module \\\n--with-http_stub_status_module  \\\n--with-http_gzip_static_module \\\n--with-pcre \\\n--with-file-aio \\\n--with-stream \\\n--with-stream_ssl_module \\\n--with-stream_realip_module \\\n--add-module=/usr/local/src/nginx-module-vts/\n# make\nmake\n# make install\nmake install\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n * 修改配置文件\n\nhttp {\n    vhost_traffic_status_zone;\n\n    ...\n\n    server {\n\n        ...\n\n        location /status {\n            vhost_traffic_status_display;\n            vhost_traffic_status_display_format html;\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n> 参考https://github.com/vozlt/nginx-module-vts添加配置\n\n * 启动nginx\n\n# 检查配置文件\n/apps/nginx/sbin/nginx -t\n# 启动nginx\n/apps/nginx/sbin/nginx\n\n\n1\n2\n3\n4\n\n * 配置nginx的upstream\n\n    # http模块里边，server模块同级\n    upstream SERVICE {\n      server IP:PORT;\n    }\n        # server模块里边，转发首页\n        location / {\n            #root   html;\n            #index  index.html index.htm;\n            proxy_pass http://SERVICE;\n        }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * 检查状态页\n\n> 可以以json模式显示数据\n\n * 安装nginx_exporter\n\n# 下载\nwget https://github.com/hnlq715/nginx-vts-exporter/releases/download/v0.10.3/nginx-vts-exporter-0.10.3.linux-amd64.tar.gz\n# 解压\ntar xvf nginx-vts-exporter-0.10.3.linux-amd64.tar.gz\n# 创建软链接\nln -sv /apps/nginx-vts-exporter-0.10.3.linux-amd64 nginx-vts-exporter\n# 启动nginx_exporter\n./nginx-vts-exporter  -nginx.scrape_uri http://IP/status/format/json\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n> 默认监听端口号9913\n\n * 验证数据\n\n1647402416180.png\n\n * prometheus数据采集\n\n  - job_name: "nginx-exporter"\n    static_configs:\n      - targets: ["127.0.0.1:9913"]\n\n\n1\n2\n3\n\n\n> 虚拟机prometheus.yml配置文件缩进格式\n\n * prometheus验证数据\n\n * grafana导入模板\n\n 1. 2949\n\n\n# 2.6 blockbox监控url\n\n官方地址：https://prometheus.io/download/#blackbox_exporter\n\nblockbox_exporter是prometheus官方提供的一个exporter，可以通过http，https，dns，tcp和icmp对被监控节点进行监控和数据采集\n\n> http/https：url/api可用性检测\n> \n> TCP：端口监听检测\n> \n> ICMP：主机存活检测\n> \n> DNS：域名解析\n\n * 部署blackbox_exporter\n\n# 下载\nwget https://github.com/prometheus/blackbox_exporter/releases/download/v0.19.0/blackbox_exporter-0.19.0.linux-amd64.tar.gz\n# 解压\ntar xvf blackbox_exporter-0.19.0.linux-amd64.tar.gz\n# 创建软链接\nln -sv /apps/blackbox_exporter-0.19.0.linux-amd64 blackbox_exporter\n\n\n1\n2\n3\n4\n5\n6\n\n * 创建blackbox-exporter.service文件\n\ncat > /etc/systemd/system/blackbox-exporter.service <<EOF\n[Unit]\nDescription=Prometheus Blackbox Exporter\nDocumentation=https://prometheus.io/download/#blackbox_exporter\n\nAfter=network.target\n\n[Service]\nType=simple\nUser=root\nGroup=root\nRestart=on-failure\nExecStart=/apps/blackbox_exporter/blackbox_exporter --config.file=/apps/blackbox_exporter/blackbox.yml     --web.listen-address=:9115\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n * 启动blackbox_exporter\n\nsystemctl daemon-reload\nsystemctl restart blackbox-exporter\n\n\n1\n2\n\n * 验证数据\n\n1647413347284.png\n\n> 默认监听端口9115\n\n * blackbox exporter监控url\n\nprometheus数据采集\n\n  - job_name: "http_status"\n    metrics_path: /probe\n    params:\n      module: [http_2xx]\n    static_configs:\n      - targets: ["domainname1","domainname2"]\n        labels:\n          instance: http_status\n          group: web\n    relabel_configs:\n      - source_labels: [__address__] # relbel通过将__address__(当前目标地址)写入__param_tartget标签来创建一个label\n        target_label: __param_target # 监控目标domainname，作为__address__的value\n      - source_labels: [__param_target] # 监控目标\n        target_label: url # 将监控目标与url创建一个label\n      - target_label: __address__\n        replacement: BLACKBOX_EXPORTER:PORT\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n> 虚拟机prometheus.yml配置文件缩进格式\n\n * 验证prometheus状态\n\n * 查看blackbox页面\n\n1647415523065.png\n\n * blockbox_exporter监控icmp\n\nprometheus数据采集\n\n  - job_name: "ping_status"\n    metrics_path: /probe\n    params:\n      module: [icmp]\n    static_configs:\n      - targets: ["IP1","IP2"]\n        labels:\n          instance: \'ping_status\'\n          group: \'icmp\'\n    relabel_configs:\n      - source_labels: [__address__] \n        target_label: __param_target \n      - source_labels: [__param_target]\n        target_label: ip # 将ip与__param_target创建一个label\n      - target_label: __address__\n        replacement: BLACKBOX_EXPORTER:PORT\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n> 虚拟机prometheus.yml配置文件缩进格式\n\n * 验证prometheus状态\n\n1647417886518.png\n\n * blackbox_exporter监控端口\n\nprometheus数据采集\n\n# 端口监控\n  - job_name: "port_status"\n    metrics_path: /probe\n    params:\n      module: [tcp_connect]\n    static_configs:\n      - targets: ["IP:PORT","IP:PORT"]\n        labels:\n          instance: \'port_status\'\n          group: \'port\'\n    relabel_configs:\n      - source_labels: [__address__]\n        target_label: __param_target \n      - source_labels: [__param_target] \n        target_label: ip\n      - target_label: __address__\n        replacement: BLACKBOX_EXPORTER:PORT\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n * 验证prometheus状态\n\n1647418487723.png\n\n * grafana导入模板\n\n 1. 9965\n 2. 13587\n\n\n# 三、告警\n\n\n# 3.1 Alertmanager\n\nprometheus--\x3e触发阈值--\x3e超出持续时间--\x3ealertmanager--\x3e分组|抑制|静默--\x3e媒体类型--\x3e邮件|钉钉|微信等\n\n> prometheus server通过配置监控规则，实现告警发送，然后把告警push给Alertmanager，匹配Alertmanager配置的Router，以WeChat、Email或Webhook方式发送给对应的Receiver\n\n分组（group）：将类似性质的告警合并为单个通知，比如网络通知、主机通知、服务通知\n\n静默（silences）：是一种简单的特定时间静音的机制，例如：服务器要升级维护可以先设置这个时间段告警静默\n\n抑制（inhibition）：当告警发出后，停止重复发送由此告警引发的其他告警；即合并由一个故障引起的多个告警事件，可以消除冗余告警\n\n * 安装alertmanager\n\n# 下载\nwget https://github.com/prometheus/alertmanager/releases/download/v0.23.0/alertmanager-0.23.0.linux-amd64.tar.gz\n# 解压\ntar xvf alertmanager-0.23.0.linux-amd64.tar.gz\n# 创建软链接\nln -sv /apps/alertmanager-0.23.0.linux-amd64 /apps/alertmanager\n\n\n1\n2\n3\n4\n5\n6\n\n * 创建alertmanager.service文件\n\ncat > /etc/systemd/system/alertmanager.service <<EOF\n[Unit]\nDescription=Prometheus alertmanager\nAfter=network.target\n\n[Service]\nExecStart=/apps/alertmanager/alertmamager --config.file="/apps/alertmanager/alertmanager.yml"\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * 启动alertmanager\n\nsystemctl start alertmanager.service\n\n\n1\n\n\n> 默认监听端口9093，9094\n\n监控配置官方网址：https://prometheus.io/docs/alerting/latest/configuration/\n\n * 验证alertmanager状态\n\n1647481599806.png\n\n\n# 3.2 邮件\n\n官方网址：https://prometheus.io/docs/alerting/latest/configuration/#email_config\n\n * 配置文件介绍\n\nalertmanager.yml配置文件\n\nglobal:\n  resolve_timeout: 5m # alertmanager在持续多久没有收到新告警后标记为resolved\n  smtp_from:  # 发件人邮箱地址\n  smtp_smarthost:  # 邮箱smtp地址\n  smtp_auth_username:  # 发件人的登陆用户名，默认和发件人地址一致\n  smtp_auth_password:  # 发件人的登陆密码，有时候是授权码\n  smtp_hello:\n  smtp_require_tls:  # 是否需要tls协议。默认是true\n\nroute:\n  group_by: [alertname] # 通过alertname的值对告警进行分类\n  group_wait: 10s # 一组告警第一次发送之前等待的时延，即产生告警10s将组内新产生的消息合并发送，通常是0s~几分钟（默认是30s）\n  group_interval: 2m # 一组已发送过初始告警通知的告警，接收到新告警后，下次发送通知前等待时延，通常是5m或更久（默认是5m）\n  repeat_interval: 5m # 一组已经发送过通知的告警，重复发送告警的间隔，通常设置为3h或者更久（默认是4h）\n  receiver: \'default-receiver\' # 设置告警接收人\n\nreceivers:\n- name: \'default-receiver\'\n  email_configs:\n  - to: \'EMAIL@DOMAIN.com\'\n    send_resolved: true # 发送恢复告警通知\n\ninhibit_rules: # 抑制规则\n  - source_match:  # 源匹配级别，当匹配成功发出通知，其他级别产生告警将被抑制\n      severity: \'critical\' # 告警时间级别（告警级别根据规则自定义）\n    target_match: \n      severity: \'warning\' # 匹配目标成功后，新产生的目标告警为\'warning\'将被抑制\n    equal: [\'alertname\',\'dev\',\'instance\'] # 基于这些标签抑制匹配告警的级别\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n> # 时间示例解析\n> # group_wait: 10s # 第一次产生告警，等待10s，组内有新增告警，一起发出，没有则单独发出\n> # group_interval: 2m # 第二次产生告警，先等待2m，2m后没有恢复就进入repeat_interval\n> # repeat_interval: 5m # 在第二次告警时延过后，再等待5m，5m后没有恢复，就发送第二次告警\n> \n> \n> 1\n> 2\n> 3\n> 4\n> \n> \n> 如上配置，如果告警没有恢复，第二次告警会等待2m+5m，即7分钟后发出\n\n * 配置告警规则\n\ngroups:\n  - name: alertmanager_pod.rules\n    rules:\n    - alert: Pod_all_cpu_usage\n      expr: (sum by(name)(rate(container_cpu_usage_seconds_total{image!=""}[5m]))*100) > 1\n      for: 2m\n      labels:\n        serverity: critical\n        service: pods\n      annotations:\n        description: 容器 {{ $labels.name }} CPU 资源利用率大于 10% , (current value is {{ $value }})\n        summary: Dev CPU 负载告警\n\n    - alert: Pod_all_memory_usage\n      expr: sort_desc(avg by(name)(irate(node_memory_MemFree_bytes {name!=""}[5m]))) > 2147483648 # 内存大于2G\n      for: 2m\n      labels:\n        severity: critical\n      annotations:\n        description: 容器 {{ $labels.name }} Memory 资源利用大于 2G , (current value is {{ $value }})\n        summary: Dev Memory 负载告警\n\n    - alert: Pod_all_network_receive_usage\n      expr: sum by(name)(irate(container_network_reveive_bytes_total{container_name="POD"}[1m])) > 52428800 # 大于50M\n      for: 2m\n      labels:\n        severity: critical\n      annotations:\n        description: 容器 {{ $labels.name }} network_receive 资源利用大于 50M , (current value is {{ $value }})\n\n    - alert: node内存可用大小\n      expr: node_memory_MemFree_bytes < 4294967296 # 内存小于4G\n      for: 2m\n      labels:\n        severity: critical\n      annotations:\n        description: node可用内存小于4G\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n> 在/apps/prometheus/目录下创建rules目录，创建pods_rule.yaml文件，内容如上\n> \n> 注意缩进格式，如果文件格式有误，重启prometheus的时候，promethues会一直起不来，可以先用promtool检查配置文件格式，因为加载告警配置的时候，引入了这个文件，所以在检查promethues.yml文件的时候也会检查自定义的pods_rule.yaml文件\n\n * promethues加载告警配置\n\n# Alertmanager configuration\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          - IP:9093\n\n# Load rules once and periodically evaluate them according to the global \'evaluation_interval\'.\nrule_files:\n  - "/apps/prometheus/rules/pods_rule.yaml"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n> 注：如果修改rule_files中的内容，需要先重启prometheus，加载修改后的配置，然后修改alertmanager，不然修改后的告警内容不会生效\n\n * 重启prometheus\n\nsystemctl restart prometheus.service\n\n\n1\n\n * 验证prometheus状态\n\n> 在prometheus页面，点击Alerts查看告警状态，当前为PENDING，说明已经检测到告警，还没满足发邮件的时间规则\n\n1647485888388.png\n\n> FIRING证明告警已成功，此时应该已经收到邮件\n\n * 查看alertmanager告警\n\n * 查看告警邮件\n\n1647485953827.png\n\n> 点击Source链接，跳转的是主机名加prometheus-server的端口，无法解析就跳转不过去\n\n * 使用amtool查看告警\n\n./amtool alert --alertmanager.url=http://IP:9093\n\n\n1\n\n\n1647485846885.png\n\n\n# 3.3 钉钉\n\n * 钉钉添加机器人\n\n创建机器人官方网址：https://open.dingtalk.com/document/robots/custom-robot-access\n\n * 发送消息脚本\n\nvim /data/scripts/dingding-keywords.sh\nMESSAGE=$1\n\n/usr/bin/curl -X POST \'https://oapi.dingtalk.com/robot/send?access_token=TOKEN\'\\\n-H \'Content-Type: application/json\' \\\n-d \'{"msgtype": "text",\n     "text": {\n        "content": "${MESSAGE}"\n     }\n    }\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * 测试发送消息\n\n1647506928206.png\n\n> 发送的消息内容中，必须包含自定义的关键字，不然发送消息会失败，发送脚本发送消息成功后，群里会收到\n\n * 部署webhook-dingtalk\n\n# 下载\nwget https://github.com/timonwong/prometheus-webhook-dingtalk/releases/download/v1.4.0/prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz\n# 解压\ntar xvf prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz\n# 创建软链接\nln -sv /apps/prometheus-webhook-dingtalk-1.4.0.linux-amd64 prometheus-webhook-dingtalk\n\n\n1\n2\n3\n4\n5\n6\n\n\n> 下载的webhook-dingtalk版本最好跟这个保持一直，新版本有些地方不兼容\n\n * 启动webhook-dingtalk\n\ncd /apps/prometheus-webhook-dingtalk\n./prometheus-webhook-dingtalk --web.listen-address="0.0.0.0:8060" --ding.profile="KEYWORD=https://oapi.dingtalk.com/robot/send?access_token=TOKEN" \n\n\n1\n2\n\n\n> 指定监听端口8060\n> \n> KEYWORD必须是创建机器人时的自定义关键字，不然告警发布出去，会报错\n\n * 配置alertmanager\n\n- name: \'dingding\'\n  webhook_configs:\n  - url: \'http://IP:8060/dingtalk/alertsen/send\'\n    send_resolved: true\n\n\n1\n2\n3\n4\n\n * 重启alertmanager\n\nsystemctl restart alertmanager.service\n\n\n1\n',normalizedContent:'# 一、kube-state-metrics\n\n\n# 1.1 kube-state-metrics介绍\n\ngithub地址：https://github.com/kubernetes/kube-state-metrics\n\n镜像地址：https://hub.docker.com/r/bitnami/kube-state-metrics\n\n博客介绍：https://xie.infoq.cn/article/9e1fff6306649e65480a96bb1\n\nkube-state-metrics是通过监听api server生成有关资源对象的状态指标，比如deployment、node、pod，需要注意的是kube-state-metrics只是简单的提供一个metrics数据，并不会存储这些指标数据，所以我们可以使用prometheus来抓取这些数据然后存储，主要关注的是业务相关的一些元数据，比如deployment、pod、副本状态等，调度了多少个replicas？现在可用的有几个？多少个pod是running/stopped/terminated状态？pod重启了多少次？目前由多少job在运行中\n\n\n# 1.2 部署kube-state-metrics\n\n 1. 编写基于deploy控制器的yaml文件\n 2. 编写svc的yaml文件，端口暴露为nodeport\n 3. 部署\n\n\n# 1.3 验证数据\n\n\n\n\n\n\n# 1.4 prometheus数据采集\n\n    - job_name: \'kube-state-metrics\'\n      static_configs:\n        - targets: ["ip:port"]\n\n\n1\n2\n3\n\n\n> k8s配置文件configmap缩进格式\n\n\n# 1.5 验证prometheus状态\n\n\n# 1.6 grafana导入模板\n\n 1. 13824\n 2. 14518\n\n> 因为版本不同，可根据对应版本进行设置\n> \n> dashboard模板网址：https://grafana.com/grafana/dashboards/\n\n\n# 二、监控示例\n\n基于第三方exporter实现对目标服务的监控\n\n\n# 2.1 tomcat\n\n * 构建镜像\n\ngithub地址：https://github.com/nlighten/tomcat_exporter\n\n根据tomcat官方镜像添加jar包\n\nadd metrics.war /data/tomcat/webapps\nadd simpleclient-0.8.0.jar  /usr/local/tomcat/lib/\nadd simpleclient_common-0.8.0.jar /usr/local/tomcat/lib/\nadd simpleclient_hotspot-0.8.0.jar /usr/local/tomcat/lib/\nadd simpleclient_servlet-0.8.0.jar /usr/local/tomcat/lib/\nadd tomcat_exporter_client-0.0.12.jar /usr/local/tomcat/lib/\n\n\n1\n2\n3\n4\n5\n6\n\n * prometheus采集\n\n    - job_name: \'kube-state-metrics\'\n      static_configs:\n        - targets: ["ip:port"]\n\n\n1\n2\n3\n\n\n> k8s配置文件configmap缩进格式\n\n * prometheus验证\n\n * grafana导入模板\n\ngithub地址：https://github.com/nlighten/tomcat_exporter/blob/master/dashboard/example.json\n\n> 下载这个json文件导入grafana即可\n\n\n# 2.2 redis\n\n通过redis_exporter监控redis服务装态\n\ngithub网址：https://github.com/oliver006/redis_exporter\n\n * 部署redis\n\n> 一个pod两个容器，redis和redis-exporter\n\n * prometheus采集\n\n    - job_name: \'redis-metrics\'\n      static_configs:\n        - targets: ["ip:port"]\n\n\n1\n2\n3\n\n\n> k8s配置文件configmap缩进格式\n\n * grafana导入模板\n\n 1. 14615\n 2. 11692\n\n\n# 2.3 mysql\n\n通过mysqld_exporter监控mysql服务的运行状态\n\ngithub网址：https://github.com/prometheus/mysqld_exporter\n\n * 安装mariadb-server\n\napt install -y mariadb\n\n\n1\n\n * 修改配置文件/etc/mysql/mariadb.conf.d/50-server.cnf监听地址，修改为0.0.0.0\n\n> bind-address = 0.0.0.0\n> \n> 重启mariadb\n\n * 创建mysql_exporter用户\n\ncreate user \'mysql_exporter\'@\'localhost\' identified by \'password\';\n\n\n1\n\n * 测试用户名密码连接\n\nmysql -umysql_exporter -hlocalhost -ppassword\n\n\n1\n\n * 下载mysql_exporter\n\n# 下载\nwget https://github.com/prometheus/mysqld_exporter/releases/download/v0.13.0/mysqld_exporter-0.13.0.linux-amd64.tar.gz\n# 解压\ntar xvf mysqld_exporter-0.13.0.linux-amd64.tar.gz\n# 查看启动参数\n./mysqld_exporter --help\n\n\n1\n2\n3\n4\n5\n6\n\n * 创建免密登陆文件/root/.my.cnf\n\ncat >> /root/.my.cnf <<eof\n[client]\nuser=mysql_exporter\npassword=123321\neof\n\n\n1\n2\n3\n4\n5\n\n * 创建mysqld_service文件并启动\n\n# 创建软链接\nln -sv /apps/mysqld_exporter-0.13.0.linux-amd64 /apps/mysqld_exporter\n\n# 创建service文件\ncat >> /etc/systemd/system/mysqld_exporter.service <<eof\n[unit]\ndescription=prometheus mysql exporter\nafter=network.target\n\n[service]\nexecstart=/apps/mysqld_exporter/mysqld_exporter --config.my-cnf=/root/.my.cnf\n\n[install]\nwantedby=multi-user.target\neof\n\n# 重新加载配置\nsystemctl daemon-reload\n\n# 启动mysqld_exporter\nsystemctl start mysqld_exporter.service\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n * 验证metrics\n\n1647316276383.png\n\n * 验证prometheus\n\n1647316313220.png\n\n * grafana导入模板\n\n 1. 13106\n 2. 11323\n\n\n# 2.4 haproxy\n\n通过haproxy_exporter监控haproxy\n\ngithub网址：https://github.com/prometheus/haproxy_exporter\n\n * 安装haproxy\n\napt install -y haproxy\n\n\n1\n\n * 修改配置文件，监听一个服务\n\nlisten service\n  bind bind_ip:port\n  mode tcp\n  server server_name listen_ip:port check inter 3s fall 3 rise 3\n\n\n1\n2\n3\n4\n\n\n> 确保sock文件是admin用户（level后边的admin）\n> \n>     stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners\n> \n> \n> 1\n\n * 重启haproxy\n\nsystemctl restart haproxy\n\n\n1\n\n\n> 检查监听端口是否正常\n\n * 下载haproxy_exporter\n\n# 下载\nwget https://github.com/prometheus/haproxy_exporter/releases/download/v0.13.0/haproxy_exporter-0.13.0.linux-amd64.tar.gz\n# 解压\ntar xvf haproxy_exporter-0.13.0.linux-amd64.tar.gz\n# 创建软链接\nln -sv /apps/haproxy_exporter-0.13.0.linux-amd64 /apps/haproxy_exporter\n\n\n1\n2\n3\n4\n5\n6\n\n * 配置文件启动haproxy_expoter\n\n./haproxy_exporter --haproxy.scrape-uri=unix:/run/haproxy/admin.sock\n\n\n1\n\n\n> 端口默认监听9101\n\n * 配置haproxy状态页\n\nlisten stats\n  bind :port\n  stats enable\n  stats uri /haproxy-status\n  stats realm haproxy\\ stats\\ page\n  stats auth haadmin:123456\n  stats auth admin:123456\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n> 编辑/etc/haproxy/haproxy.cfg添加如上配置内容\n\n * 状态页启动haproxy\n\n./haproxy_exporter --haproxy.scrape-uri="http://admin:123456@127.0.0.1:port/haproxy-status;csv"\n\n\n1\n\n\n> 需要指定用户名密码，csv是指定以csv形式展示\n\n * 验证exporter\n\n1647397779547.png\n\n * prometheus数据采集\n\n  - job_name: "haproxy-exporter"\n    static_configs:\n      - targets: ["127.0.0.1:9101"]\n\n\n1\n2\n3\n\n\n> 虚拟机prometheus.yml配置缩进格式\n> \n> ./promtool check config prometheus.yml # 修改后检查配置文件是否正确\n\n * 重启prometheus\n\nsystemctl restart prometheus.service\n\n\n1\n\n\n * 验证prometheus\n\n * grafana导入模板\n\n 1. 367\n 2. 2428\n\n\n# 2.5 nginx\n\n通过nginx_exporter监控ngix\n\ngithub模块依赖网址：https://github.com/vozlt/nginx-module-vts\n\n * 安装nginx\n\n# 克隆依赖模块\ngit clone https://github.com/vozlt/nginx-module-vts.git\n# 下载nginx源码\nwget http://nginx.org/download/nginx-1.20.2.tar.gz\n# 解压\ntar xvf nginx-1.20.2.tar.gz\n# 安装nginx编译依赖包\napt install -y libgd-dev libgeoip-dev libpcre3 libpcre3-dev libssl-dev gcc make\n# 编译nginx\ncd nginx-1.20.2\n./configure --prefix=/apps/nginx \\\n--with-http_ssl_module \\\n--with-http_v2_module \\\n--with-http_realip_module \\\n--with-http_stub_status_module  \\\n--with-http_gzip_static_module \\\n--with-pcre \\\n--with-file-aio \\\n--with-stream \\\n--with-stream_ssl_module \\\n--with-stream_realip_module \\\n--add-module=/usr/local/src/nginx-module-vts/\n# make\nmake\n# make install\nmake install\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n * 修改配置文件\n\nhttp {\n    vhost_traffic_status_zone;\n\n    ...\n\n    server {\n\n        ...\n\n        location /status {\n            vhost_traffic_status_display;\n            vhost_traffic_status_display_format html;\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n> 参考https://github.com/vozlt/nginx-module-vts添加配置\n\n * 启动nginx\n\n# 检查配置文件\n/apps/nginx/sbin/nginx -t\n# 启动nginx\n/apps/nginx/sbin/nginx\n\n\n1\n2\n3\n4\n\n * 配置nginx的upstream\n\n    # http模块里边，server模块同级\n    upstream service {\n      server ip:port;\n    }\n        # server模块里边，转发首页\n        location / {\n            #root   html;\n            #index  index.html index.htm;\n            proxy_pass http://service;\n        }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * 检查状态页\n\n> 可以以json模式显示数据\n\n * 安装nginx_exporter\n\n# 下载\nwget https://github.com/hnlq715/nginx-vts-exporter/releases/download/v0.10.3/nginx-vts-exporter-0.10.3.linux-amd64.tar.gz\n# 解压\ntar xvf nginx-vts-exporter-0.10.3.linux-amd64.tar.gz\n# 创建软链接\nln -sv /apps/nginx-vts-exporter-0.10.3.linux-amd64 nginx-vts-exporter\n# 启动nginx_exporter\n./nginx-vts-exporter  -nginx.scrape_uri http://ip/status/format/json\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n> 默认监听端口号9913\n\n * 验证数据\n\n1647402416180.png\n\n * prometheus数据采集\n\n  - job_name: "nginx-exporter"\n    static_configs:\n      - targets: ["127.0.0.1:9913"]\n\n\n1\n2\n3\n\n\n> 虚拟机prometheus.yml配置文件缩进格式\n\n * prometheus验证数据\n\n * grafana导入模板\n\n 1. 2949\n\n\n# 2.6 blockbox监控url\n\n官方地址：https://prometheus.io/download/#blackbox_exporter\n\nblockbox_exporter是prometheus官方提供的一个exporter，可以通过http，https，dns，tcp和icmp对被监控节点进行监控和数据采集\n\n> http/https：url/api可用性检测\n> \n> tcp：端口监听检测\n> \n> icmp：主机存活检测\n> \n> dns：域名解析\n\n * 部署blackbox_exporter\n\n# 下载\nwget https://github.com/prometheus/blackbox_exporter/releases/download/v0.19.0/blackbox_exporter-0.19.0.linux-amd64.tar.gz\n# 解压\ntar xvf blackbox_exporter-0.19.0.linux-amd64.tar.gz\n# 创建软链接\nln -sv /apps/blackbox_exporter-0.19.0.linux-amd64 blackbox_exporter\n\n\n1\n2\n3\n4\n5\n6\n\n * 创建blackbox-exporter.service文件\n\ncat > /etc/systemd/system/blackbox-exporter.service <<eof\n[unit]\ndescription=prometheus blackbox exporter\ndocumentation=https://prometheus.io/download/#blackbox_exporter\n\nafter=network.target\n\n[service]\ntype=simple\nuser=root\ngroup=root\nrestart=on-failure\nexecstart=/apps/blackbox_exporter/blackbox_exporter --config.file=/apps/blackbox_exporter/blackbox.yml     --web.listen-address=:9115\n\n[install]\nwantedby=multi-user.target\neof\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n * 启动blackbox_exporter\n\nsystemctl daemon-reload\nsystemctl restart blackbox-exporter\n\n\n1\n2\n\n * 验证数据\n\n1647413347284.png\n\n> 默认监听端口9115\n\n * blackbox exporter监控url\n\nprometheus数据采集\n\n  - job_name: "http_status"\n    metrics_path: /probe\n    params:\n      module: [http_2xx]\n    static_configs:\n      - targets: ["domainname1","domainname2"]\n        labels:\n          instance: http_status\n          group: web\n    relabel_configs:\n      - source_labels: [__address__] # relbel通过将__address__(当前目标地址)写入__param_tartget标签来创建一个label\n        target_label: __param_target # 监控目标domainname，作为__address__的value\n      - source_labels: [__param_target] # 监控目标\n        target_label: url # 将监控目标与url创建一个label\n      - target_label: __address__\n        replacement: blackbox_exporter:port\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n> 虚拟机prometheus.yml配置文件缩进格式\n\n * 验证prometheus状态\n\n * 查看blackbox页面\n\n1647415523065.png\n\n * blockbox_exporter监控icmp\n\nprometheus数据采集\n\n  - job_name: "ping_status"\n    metrics_path: /probe\n    params:\n      module: [icmp]\n    static_configs:\n      - targets: ["ip1","ip2"]\n        labels:\n          instance: \'ping_status\'\n          group: \'icmp\'\n    relabel_configs:\n      - source_labels: [__address__] \n        target_label: __param_target \n      - source_labels: [__param_target]\n        target_label: ip # 将ip与__param_target创建一个label\n      - target_label: __address__\n        replacement: blackbox_exporter:port\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n> 虚拟机prometheus.yml配置文件缩进格式\n\n * 验证prometheus状态\n\n1647417886518.png\n\n * blackbox_exporter监控端口\n\nprometheus数据采集\n\n# 端口监控\n  - job_name: "port_status"\n    metrics_path: /probe\n    params:\n      module: [tcp_connect]\n    static_configs:\n      - targets: ["ip:port","ip:port"]\n        labels:\n          instance: \'port_status\'\n          group: \'port\'\n    relabel_configs:\n      - source_labels: [__address__]\n        target_label: __param_target \n      - source_labels: [__param_target] \n        target_label: ip\n      - target_label: __address__\n        replacement: blackbox_exporter:port\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n * 验证prometheus状态\n\n1647418487723.png\n\n * grafana导入模板\n\n 1. 9965\n 2. 13587\n\n\n# 三、告警\n\n\n# 3.1 alertmanager\n\nprometheus--\x3e触发阈值--\x3e超出持续时间--\x3ealertmanager--\x3e分组|抑制|静默--\x3e媒体类型--\x3e邮件|钉钉|微信等\n\n> prometheus server通过配置监控规则，实现告警发送，然后把告警push给alertmanager，匹配alertmanager配置的router，以wechat、email或webhook方式发送给对应的receiver\n\n分组（group）：将类似性质的告警合并为单个通知，比如网络通知、主机通知、服务通知\n\n静默（silences）：是一种简单的特定时间静音的机制，例如：服务器要升级维护可以先设置这个时间段告警静默\n\n抑制（inhibition）：当告警发出后，停止重复发送由此告警引发的其他告警；即合并由一个故障引起的多个告警事件，可以消除冗余告警\n\n * 安装alertmanager\n\n# 下载\nwget https://github.com/prometheus/alertmanager/releases/download/v0.23.0/alertmanager-0.23.0.linux-amd64.tar.gz\n# 解压\ntar xvf alertmanager-0.23.0.linux-amd64.tar.gz\n# 创建软链接\nln -sv /apps/alertmanager-0.23.0.linux-amd64 /apps/alertmanager\n\n\n1\n2\n3\n4\n5\n6\n\n * 创建alertmanager.service文件\n\ncat > /etc/systemd/system/alertmanager.service <<eof\n[unit]\ndescription=prometheus alertmanager\nafter=network.target\n\n[service]\nexecstart=/apps/alertmanager/alertmamager --config.file="/apps/alertmanager/alertmanager.yml"\n\n[install]\nwantedby=multi-user.target\neof\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n * 启动alertmanager\n\nsystemctl start alertmanager.service\n\n\n1\n\n\n> 默认监听端口9093，9094\n\n监控配置官方网址：https://prometheus.io/docs/alerting/latest/configuration/\n\n * 验证alertmanager状态\n\n1647481599806.png\n\n\n# 3.2 邮件\n\n官方网址：https://prometheus.io/docs/alerting/latest/configuration/#email_config\n\n * 配置文件介绍\n\nalertmanager.yml配置文件\n\nglobal:\n  resolve_timeout: 5m # alertmanager在持续多久没有收到新告警后标记为resolved\n  smtp_from:  # 发件人邮箱地址\n  smtp_smarthost:  # 邮箱smtp地址\n  smtp_auth_username:  # 发件人的登陆用户名，默认和发件人地址一致\n  smtp_auth_password:  # 发件人的登陆密码，有时候是授权码\n  smtp_hello:\n  smtp_require_tls:  # 是否需要tls协议。默认是true\n\nroute:\n  group_by: [alertname] # 通过alertname的值对告警进行分类\n  group_wait: 10s # 一组告警第一次发送之前等待的时延，即产生告警10s将组内新产生的消息合并发送，通常是0s~几分钟（默认是30s）\n  group_interval: 2m # 一组已发送过初始告警通知的告警，接收到新告警后，下次发送通知前等待时延，通常是5m或更久（默认是5m）\n  repeat_interval: 5m # 一组已经发送过通知的告警，重复发送告警的间隔，通常设置为3h或者更久（默认是4h）\n  receiver: \'default-receiver\' # 设置告警接收人\n\nreceivers:\n- name: \'default-receiver\'\n  email_configs:\n  - to: \'email@domain.com\'\n    send_resolved: true # 发送恢复告警通知\n\ninhibit_rules: # 抑制规则\n  - source_match:  # 源匹配级别，当匹配成功发出通知，其他级别产生告警将被抑制\n      severity: \'critical\' # 告警时间级别（告警级别根据规则自定义）\n    target_match: \n      severity: \'warning\' # 匹配目标成功后，新产生的目标告警为\'warning\'将被抑制\n    equal: [\'alertname\',\'dev\',\'instance\'] # 基于这些标签抑制匹配告警的级别\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n> # 时间示例解析\n> # group_wait: 10s # 第一次产生告警，等待10s，组内有新增告警，一起发出，没有则单独发出\n> # group_interval: 2m # 第二次产生告警，先等待2m，2m后没有恢复就进入repeat_interval\n> # repeat_interval: 5m # 在第二次告警时延过后，再等待5m，5m后没有恢复，就发送第二次告警\n> \n> \n> 1\n> 2\n> 3\n> 4\n> \n> \n> 如上配置，如果告警没有恢复，第二次告警会等待2m+5m，即7分钟后发出\n\n * 配置告警规则\n\ngroups:\n  - name: alertmanager_pod.rules\n    rules:\n    - alert: pod_all_cpu_usage\n      expr: (sum by(name)(rate(container_cpu_usage_seconds_total{image!=""}[5m]))*100) > 1\n      for: 2m\n      labels:\n        serverity: critical\n        service: pods\n      annotations:\n        description: 容器 {{ $labels.name }} cpu 资源利用率大于 10% , (current value is {{ $value }})\n        summary: dev cpu 负载告警\n\n    - alert: pod_all_memory_usage\n      expr: sort_desc(avg by(name)(irate(node_memory_memfree_bytes {name!=""}[5m]))) > 2147483648 # 内存大于2g\n      for: 2m\n      labels:\n        severity: critical\n      annotations:\n        description: 容器 {{ $labels.name }} memory 资源利用大于 2g , (current value is {{ $value }})\n        summary: dev memory 负载告警\n\n    - alert: pod_all_network_receive_usage\n      expr: sum by(name)(irate(container_network_reveive_bytes_total{container_name="pod"}[1m])) > 52428800 # 大于50m\n      for: 2m\n      labels:\n        severity: critical\n      annotations:\n        description: 容器 {{ $labels.name }} network_receive 资源利用大于 50m , (current value is {{ $value }})\n\n    - alert: node内存可用大小\n      expr: node_memory_memfree_bytes < 4294967296 # 内存小于4g\n      for: 2m\n      labels:\n        severity: critical\n      annotations:\n        description: node可用内存小于4g\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n> 在/apps/prometheus/目录下创建rules目录，创建pods_rule.yaml文件，内容如上\n> \n> 注意缩进格式，如果文件格式有误，重启prometheus的时候，promethues会一直起不来，可以先用promtool检查配置文件格式，因为加载告警配置的时候，引入了这个文件，所以在检查promethues.yml文件的时候也会检查自定义的pods_rule.yaml文件\n\n * promethues加载告警配置\n\n# alertmanager configuration\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          - ip:9093\n\n# load rules once and periodically evaluate them according to the global \'evaluation_interval\'.\nrule_files:\n  - "/apps/prometheus/rules/pods_rule.yaml"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n> 注：如果修改rule_files中的内容，需要先重启prometheus，加载修改后的配置，然后修改alertmanager，不然修改后的告警内容不会生效\n\n * 重启prometheus\n\nsystemctl restart prometheus.service\n\n\n1\n\n * 验证prometheus状态\n\n> 在prometheus页面，点击alerts查看告警状态，当前为pending，说明已经检测到告警，还没满足发邮件的时间规则\n\n1647485888388.png\n\n> firing证明告警已成功，此时应该已经收到邮件\n\n * 查看alertmanager告警\n\n * 查看告警邮件\n\n1647485953827.png\n\n> 点击source链接，跳转的是主机名加prometheus-server的端口，无法解析就跳转不过去\n\n * 使用amtool查看告警\n\n./amtool alert --alertmanager.url=http://ip:9093\n\n\n1\n\n\n1647485846885.png\n\n\n# 3.3 钉钉\n\n * 钉钉添加机器人\n\n创建机器人官方网址：https://open.dingtalk.com/document/robots/custom-robot-access\n\n * 发送消息脚本\n\nvim /data/scripts/dingding-keywords.sh\nmessage=$1\n\n/usr/bin/curl -x post \'https://oapi.dingtalk.com/robot/send?access_token=token\'\\\n-h \'content-type: application/json\' \\\n-d \'{"msgtype": "text",\n     "text": {\n        "content": "${message}"\n     }\n    }\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n * 测试发送消息\n\n1647506928206.png\n\n> 发送的消息内容中，必须包含自定义的关键字，不然发送消息会失败，发送脚本发送消息成功后，群里会收到\n\n * 部署webhook-dingtalk\n\n# 下载\nwget https://github.com/timonwong/prometheus-webhook-dingtalk/releases/download/v1.4.0/prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz\n# 解压\ntar xvf prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz\n# 创建软链接\nln -sv /apps/prometheus-webhook-dingtalk-1.4.0.linux-amd64 prometheus-webhook-dingtalk\n\n\n1\n2\n3\n4\n5\n6\n\n\n> 下载的webhook-dingtalk版本最好跟这个保持一直，新版本有些地方不兼容\n\n * 启动webhook-dingtalk\n\ncd /apps/prometheus-webhook-dingtalk\n./prometheus-webhook-dingtalk --web.listen-address="0.0.0.0:8060" --ding.profile="keyword=https://oapi.dingtalk.com/robot/send?access_token=token" \n\n\n1\n2\n\n\n> 指定监听端口8060\n> \n> keyword必须是创建机器人时的自定义关键字，不然告警发布出去，会报错\n\n * 配置alertmanager\n\n- name: \'dingding\'\n  webhook_configs:\n  - url: \'http://ip:8060/dingtalk/alertsen/send\'\n    send_resolved: true\n\n\n1\n2\n3\n4\n\n * 重启alertmanager\n\nsystemctl restart alertmanager.service\n\n\n1\n',charsets:{cjk:!0}},{title:"使用docker-compose搭建promethes+grafana监控系统",frontmatter:{title:"使用docker-compose搭建promethes+grafana监控系统",date:"2023-01-09T14:26:28.000Z",permalink:"/pages/712f5c/",categories:["专题","prometheus"],tags:[null],readingShow:"top",description:"如果你之前安装过 docker，请先删掉",meta:[{name:"twitter:title",content:"使用docker-compose搭建promethes+grafana监控系统"},{name:"twitter:description",content:"如果你之前安装过 docker，请先删掉"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/02.%E4%BD%BF%E7%94%A8docker-compose%E6%90%AD%E5%BB%BApromethes+grafana%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F.html"},{property:"og:type",content:"article"},{property:"og:title",content:"使用docker-compose搭建promethes+grafana监控系统"},{property:"og:description",content:"如果你之前安装过 docker，请先删掉"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/02.%E4%BD%BF%E7%94%A8docker-compose%E6%90%AD%E5%BB%BApromethes+grafana%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-09T14:26:28.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"使用docker-compose搭建promethes+grafana监控系统"},{itemprop:"description",content:"如果你之前安装过 docker，请先删掉"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/02.%E4%BD%BF%E7%94%A8docker-compose%E6%90%AD%E5%BB%BApromethes+grafana%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F.html",relativePath:"04.运维/12.监控/02.prometheus/02.使用docker-compose搭建promethes+grafana监控系统.md",key:"v-6f6532a7",path:"/pages/712f5c/",headers:[{level:3,title:"安装docker",slug:"安装docker",normalizedTitle:"安装docker",charIndex:2},{level:3,title:"docker-compose安装",slug:"docker-compose安装",normalizedTitle:"docker-compose安装",charIndex:550},{level:3,title:"部署prometheus和grafana、alertmanager",slug:"部署prometheus和grafana、alertmanager",normalizedTitle:"部署prometheus和grafana、alertmanager",charIndex:793}],headersStr:"安装docker docker-compose安装 部署prometheus和grafana、alertmanager",content:"# 安装docker\n\n如果你之前安装过 docker，请先删掉\n\nsudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine\n\n安装一些依赖\n\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n\n**根据你的发行版下载repo文件: **\n\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n\n把软件仓库地址替换为 TUNA:\n\nsudo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo\n\n最后安装:\n\nsudo yum makecache fast\n\nsudo yum install docker-ce\n\n\n# docker-compose安装\n\n下载地址\n\nhttps://github.com/docker/compose/releases/download/v2.15.0/docker-compose-linux-x86_64\n\n点击下载，完成后上传到服务器上，然后执行如下命令\n\nmv docker-compose-Linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose\n\n\n# 部署prometheus和grafana、alertmanager\n\n新增prometheus配置文件，创建prometheus.yml,内容如下\n\n# my global config\nglobal:\n  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n  # scrape_timeout is set to the global default (10s).\n\n# Alertmanager configuration\n\nalerting:\n  alertmanagers:\n    - static_configs:\n         - targets: [\"alertmanager:9093\"]\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\nrule_files:\n  - \"rules.yml\"\n  - \"quote.yml\"\n  #- \"rabbitmq.yml\"\n  # - \"second_rules.yml\"\n\n# A scrape configuration containing exactly one endpoint to scrape:\n# Here it's Prometheus itself.\nscrape_configs:\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n  - job_name: \"prometheus\"\n\n    # metrics_path defaults to '/metrics'\n    # scheme defaults to 'http'.\n\n    static_configs:\n      - targets: [\"localhost:9090\"]\n#  - job_name: 'cadvisor'\n#    static_configs:\n#      - targets: ['10.10.10.235:8088']\n#        labels:\n#          instance: cadvisor\n  - job_name: 'airm-php'\n    static_configs:\n      - targets: ['10.10.10.13:8199']\n        labels:\n          instance: airm-php\n  - job_name: 'airm-c++'\n    static_configs:\n      - targets: ['10.10.10.155:8199']\n        labels:\n          instance: airm-c++\n  - job_name: 'get-front'\n    static_configs:\n      - targets: ['10.10.10.75:8199']\n        labels:\n          instance: get-front\n#  - job_name: 'gg-website'\n#    static_configs:\n#      - targets: ['10.10.10.200:8199']\n#        labels:\n#          instance: gg-website\n  - job_name: 'IDC-BMS'\n    static_configs:\n      - targets: ['10.10.10.16:8199']\n        labels:\n          instance: IDC-BMS\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n接着创建rules.yml，quote.yml\n\ngroups:\n- name: Node_Down\n  rules:\n  - alert: Node实例宕机\n    expr: up == 0\n    for: 3m\n    labels:\n      severity: critical\n      level: 1\n    annotations:\n      summary: \"{{ $labels.instance }} 服务宕机\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been Down.\"\n- name: airm_RabbitMQ_queue_messages\n  interval: 5m\n  rules:\n  - alert: airm_RabbitMQ\n    expr: rabbitmq_queue_messages_unacked{job=\"airm_RabbitMQ\"} >=1\n    for: 3m\n    labels:\n      severity: critical\n      level: 1\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n- name: gf_RabbitMQ_queue_messages\n  interval: 5m\n  rules:\n  - alert: gf_RabbitMQ_queue_messages\n    expr: rabbitmq_queue_messages_unacked{job=\"gf_RabbitMQ\"} !=0\n    for: 10s\n    labels:\n      severity: critical\n      level: 1\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n- name: gf_RabbitMQ_published\n  interval: 5m\n  rules:\n  - alert: gf_RabbitMQ_published\n    expr: irate(rabbitmq_channel_messages_published_total{instance=\"gf_RabbitMQ\", job=\"gf_RabbitMQ\"}[2m]) ==0\n    for: 3m\n    labels:\n      severity: critical\n      level: 1\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n\n- name: RabbitMQ_Down\n  rules:\n  - alert: Rabbitmq-down\n    expr: rabbitmq_up{job='airm_RabbitMQ'} !=1\n    labels:\n      status: critical\n      team: Rabbitmq_monitor\n      level: 1\n    annotations:\n      description: \"Instance: {{ $labels.instance }} is Down ! ! !\"\n      value: '{{ $value }}'\n      summary:  \"The host node is down\"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\ngroups:\n#- name: gf-md\n#  interval: 10m # 查询间隔\n#  rules:\n#  - alert: gf_md\n#    expr: irate(source_server_receive_tick_counter{instance=\"gf-md_hq\"}[2m]) ==0\n#    for: 10m # 查询时间间隔\n#    labels:\n#      severity: critical\n#      level: 2\n#    annotations:\n#      summary: \"{{ $labels.instance }} is problem\"\n#      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n- name: airm-nano\n  interval: 5m # 查询间隔\n  rules:\n  - alert: airm_nano\n    expr: irate(source_server_receive_tick_counter{instance=\"airm-nano_hq\", job=\"airm-nano_hq\", source=\"nano_source\"}[2m]) ==0\n    for: 5m # 查询时间间隔\n    labels:\n      severity: critical\n      level: 2\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n- name: gf-md_hq_qianhai\n  interval: 5m # 查询间隔\n  rules:\n  - alert: gf-md_hq_qianhai\n    expr: irate(source_server_receive_tick_counter{instance=\"gf-md_hq_qianhai\"}[2m]) ==0\n    for: 5m\n    labels:\n      severity: critical\n      level: 2\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n- name: airm-fortex\n  rules:\n  - alert: airm_fortex\n    expr: irate(source_server_receive_tick_counter{instance=\"airm-fortex_hq\"}[2m]) ==0\n    for: 3m\n    labels:\n      severity: critical\n      level: 2\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n创建alertmanager.yml文件\n\nglobal:\n  #每一分钟检查一次是否恢复\n  resolve_timeout: 5m\nroute:\n  #设置默认接收人\n  receiver: 'webhook'\n  #组告警等待时间。也就是告警产生后等待10s，如果有同组告警一起发出\n  group_wait: 10s\n  #两组告警的间隔时间\n  group_interval: 30m\n  #重复告警的间隔时间，减少相同微信告警的发送频率\n  repeat_interval: 2h\n  #repeat_interval: 60s\n  #采用哪个标签来作为分组依据\n  group_by: ['alertname','severity','channel','env']\n  routes:\n  - receiver: webhook\n    match_re:\n      severity: critical|warning\n    continue: true\n  - receiver: webhook1\n    match:\n      severity: critical\n    continue: true\n  - receiver: prometheusalert-phone    #短信告警\n    match_re:    #正则匹配\n      level: 1|2\nreceivers:\n- name: 'webhook'\n  webhook_configs:\n#  - url: http://10.10.10.250:8060/dingtalk/ops_dingding/send \n  - url: http://10.10.10.250:8060/dingtalk/webhook/send \n    #警报被解决之后是否通知\n    send_resolved: true\n- name: 'webhook1'\n  webhook_configs:\n#  - url: http://10.10.10.250:8060/dingtalk/ops_dingding/send \n  - url: http://10.10.10.250:8060/dingtalk/webhook1/send\n    #警报被解决之后是否通知\n    send_resolved: true\n- name: 'prometheusalert-phone'\n  webhook_configs:\n  - url: 'http://10.10.10.250:8081/prometheusalert?type=alydx&tpl=ali-phone&phone=xxx'\n  #- url: 'http://10.10.10.250:8081/prometheusalert?type=alydx&tpl=ali-phone&phone=xxx'\n    send_resolved: true\n#inhibit_rules:\n##  - source_match:  # 当此告警发生，其他的告警被抑制\n##      severity: 'critical'   \n##    target_match:   # 被抑制的对象\n##      severity: 'High'  \n##    equal: ['id', 'instance']\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\ndocker-compose文件\n\nversion: '2'\nservices:\n  # 添加 普罗米修斯服务\n  prometheus:\n    # Docker Hub 镜像\n    image: prom/prometheus:latest\n    # 容器名称\n    container_name: prometheus\n    # 容器内部 hostname\n    hostname: prometheus\n    # 容器支持自启动\n    restart: always\n    # 容器与宿主机 端口映射\n    ports:\n      - '9090:9090'\n    # 将宿主机中的config文件夹，挂载到容器中/config文件夹\n    volumes:\n      - './prometheus/config:/config'\n      - '/data/prometheus/data/prometheus:/prometheus/data'\n      - '/etc/localtime:/etc/localtime'\n    # 指定容器中的配置文件\n    command:\n      - '--config.file=/config/prometheus.yml'\n      # 支持热更新\n      - '--web.enable-lifecycle'\n    networks:\n      - monitor\n\n  # 添加告警模块\n  alertmanager:\n    image: prom/alertmanager:latest\n    container_name: alertmanager\n    hostname: alertmanager\n    restart: always\n    ports:\n      - '9093:9093'\n    volumes:\n      - './prometheus/config:/config'\n      - '/data/prometheus/data/alertmanager:/alertmanager/data'\n      - '/etc/localtime:/etc/localtime'\n    command:\n      - '--config.file=/config/alertmanager.yml'\n    networks:\n      - monitor\n\n  # 添加监控可视化面板\n  grafana:\n    image: grafana/grafana:latest\n    container_name: grafana\n    hostname: grafana\n    restart: always\n    ports:\n      - '3000:3000'\n    volumes:\n      # 配置grafana 邮件服务器\n      - './grafana/config/grafana.ini:/etc/grafana/grafana.ini'\n      - '/data/grafana/data/grafana:/var/lib/grafana'\n      - '/etc/localtime:/etc/localtime'\n      - './grafana/config/ldap.toml:/etc/grafana/ldap.toml'\n    networks:\n      - monitor\nnetworks:\n  monitor:\n    driver: bridge\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n启动docker-compose\n\ndocker-compose up -d",normalizedContent:"# 安装docker\n\n如果你之前安装过 docker，请先删掉\n\nsudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine\n\n安装一些依赖\n\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n\n**根据你的发行版下载repo文件: **\n\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n\n把软件仓库地址替换为 tuna:\n\nsudo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo\n\n最后安装:\n\nsudo yum makecache fast\n\nsudo yum install docker-ce\n\n\n# docker-compose安装\n\n下载地址\n\nhttps://github.com/docker/compose/releases/download/v2.15.0/docker-compose-linux-x86_64\n\n点击下载，完成后上传到服务器上，然后执行如下命令\n\nmv docker-compose-linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose\n\n\n# 部署prometheus和grafana、alertmanager\n\n新增prometheus配置文件，创建prometheus.yml,内容如下\n\n# my global config\nglobal:\n  scrape_interval: 15s # set the scrape interval to every 15 seconds. default is every 1 minute.\n  evaluation_interval: 15s # evaluate rules every 15 seconds. the default is every 1 minute.\n  # scrape_timeout is set to the global default (10s).\n\n# alertmanager configuration\n\nalerting:\n  alertmanagers:\n    - static_configs:\n         - targets: [\"alertmanager:9093\"]\n# load rules once and periodically evaluate them according to the global 'evaluation_interval'.\nrule_files:\n  - \"rules.yml\"\n  - \"quote.yml\"\n  #- \"rabbitmq.yml\"\n  # - \"second_rules.yml\"\n\n# a scrape configuration containing exactly one endpoint to scrape:\n# here it's prometheus itself.\nscrape_configs:\n  # the job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n  - job_name: \"prometheus\"\n\n    # metrics_path defaults to '/metrics'\n    # scheme defaults to 'http'.\n\n    static_configs:\n      - targets: [\"localhost:9090\"]\n#  - job_name: 'cadvisor'\n#    static_configs:\n#      - targets: ['10.10.10.235:8088']\n#        labels:\n#          instance: cadvisor\n  - job_name: 'airm-php'\n    static_configs:\n      - targets: ['10.10.10.13:8199']\n        labels:\n          instance: airm-php\n  - job_name: 'airm-c++'\n    static_configs:\n      - targets: ['10.10.10.155:8199']\n        labels:\n          instance: airm-c++\n  - job_name: 'get-front'\n    static_configs:\n      - targets: ['10.10.10.75:8199']\n        labels:\n          instance: get-front\n#  - job_name: 'gg-website'\n#    static_configs:\n#      - targets: ['10.10.10.200:8199']\n#        labels:\n#          instance: gg-website\n  - job_name: 'idc-bms'\n    static_configs:\n      - targets: ['10.10.10.16:8199']\n        labels:\n          instance: idc-bms\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n接着创建rules.yml，quote.yml\n\ngroups:\n- name: node_down\n  rules:\n  - alert: node实例宕机\n    expr: up == 0\n    for: 3m\n    labels:\n      severity: critical\n      level: 1\n    annotations:\n      summary: \"{{ $labels.instance }} 服务宕机\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down.\"\n- name: airm_rabbitmq_queue_messages\n  interval: 5m\n  rules:\n  - alert: airm_rabbitmq\n    expr: rabbitmq_queue_messages_unacked{job=\"airm_rabbitmq\"} >=1\n    for: 3m\n    labels:\n      severity: critical\n      level: 1\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n- name: gf_rabbitmq_queue_messages\n  interval: 5m\n  rules:\n  - alert: gf_rabbitmq_queue_messages\n    expr: rabbitmq_queue_messages_unacked{job=\"gf_rabbitmq\"} !=0\n    for: 10s\n    labels:\n      severity: critical\n      level: 1\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n- name: gf_rabbitmq_published\n  interval: 5m\n  rules:\n  - alert: gf_rabbitmq_published\n    expr: irate(rabbitmq_channel_messages_published_total{instance=\"gf_rabbitmq\", job=\"gf_rabbitmq\"}[2m]) ==0\n    for: 3m\n    labels:\n      severity: critical\n      level: 1\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n\n- name: rabbitmq_down\n  rules:\n  - alert: rabbitmq-down\n    expr: rabbitmq_up{job='airm_rabbitmq'} !=1\n    labels:\n      status: critical\n      team: rabbitmq_monitor\n      level: 1\n    annotations:\n      description: \"instance: {{ $labels.instance }} is down ! ! !\"\n      value: '{{ $value }}'\n      summary:  \"the host node is down\"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\ngroups:\n#- name: gf-md\n#  interval: 10m # 查询间隔\n#  rules:\n#  - alert: gf_md\n#    expr: irate(source_server_receive_tick_counter{instance=\"gf-md_hq\"}[2m]) ==0\n#    for: 10m # 查询时间间隔\n#    labels:\n#      severity: critical\n#      level: 2\n#    annotations:\n#      summary: \"{{ $labels.instance }} is problem\"\n#      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n- name: airm-nano\n  interval: 5m # 查询间隔\n  rules:\n  - alert: airm_nano\n    expr: irate(source_server_receive_tick_counter{instance=\"airm-nano_hq\", job=\"airm-nano_hq\", source=\"nano_source\"}[2m]) ==0\n    for: 5m # 查询时间间隔\n    labels:\n      severity: critical\n      level: 2\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n- name: gf-md_hq_qianhai\n  interval: 5m # 查询间隔\n  rules:\n  - alert: gf-md_hq_qianhai\n    expr: irate(source_server_receive_tick_counter{instance=\"gf-md_hq_qianhai\"}[2m]) ==0\n    for: 5m\n    labels:\n      severity: critical\n      level: 2\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n- name: airm-fortex\n  rules:\n  - alert: airm_fortex\n    expr: irate(source_server_receive_tick_counter{instance=\"airm-fortex_hq\"}[2m]) ==0\n    for: 3m\n    labels:\n      severity: critical\n      level: 2\n    annotations:\n      summary: \"{{ $labels.instance }} is problem\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been problem.\"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n创建alertmanager.yml文件\n\nglobal:\n  #每一分钟检查一次是否恢复\n  resolve_timeout: 5m\nroute:\n  #设置默认接收人\n  receiver: 'webhook'\n  #组告警等待时间。也就是告警产生后等待10s，如果有同组告警一起发出\n  group_wait: 10s\n  #两组告警的间隔时间\n  group_interval: 30m\n  #重复告警的间隔时间，减少相同微信告警的发送频率\n  repeat_interval: 2h\n  #repeat_interval: 60s\n  #采用哪个标签来作为分组依据\n  group_by: ['alertname','severity','channel','env']\n  routes:\n  - receiver: webhook\n    match_re:\n      severity: critical|warning\n    continue: true\n  - receiver: webhook1\n    match:\n      severity: critical\n    continue: true\n  - receiver: prometheusalert-phone    #短信告警\n    match_re:    #正则匹配\n      level: 1|2\nreceivers:\n- name: 'webhook'\n  webhook_configs:\n#  - url: http://10.10.10.250:8060/dingtalk/ops_dingding/send \n  - url: http://10.10.10.250:8060/dingtalk/webhook/send \n    #警报被解决之后是否通知\n    send_resolved: true\n- name: 'webhook1'\n  webhook_configs:\n#  - url: http://10.10.10.250:8060/dingtalk/ops_dingding/send \n  - url: http://10.10.10.250:8060/dingtalk/webhook1/send\n    #警报被解决之后是否通知\n    send_resolved: true\n- name: 'prometheusalert-phone'\n  webhook_configs:\n  - url: 'http://10.10.10.250:8081/prometheusalert?type=alydx&tpl=ali-phone&phone=xxx'\n  #- url: 'http://10.10.10.250:8081/prometheusalert?type=alydx&tpl=ali-phone&phone=xxx'\n    send_resolved: true\n#inhibit_rules:\n##  - source_match:  # 当此告警发生，其他的告警被抑制\n##      severity: 'critical'   \n##    target_match:   # 被抑制的对象\n##      severity: 'high'  \n##    equal: ['id', 'instance']\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\ndocker-compose文件\n\nversion: '2'\nservices:\n  # 添加 普罗米修斯服务\n  prometheus:\n    # docker hub 镜像\n    image: prom/prometheus:latest\n    # 容器名称\n    container_name: prometheus\n    # 容器内部 hostname\n    hostname: prometheus\n    # 容器支持自启动\n    restart: always\n    # 容器与宿主机 端口映射\n    ports:\n      - '9090:9090'\n    # 将宿主机中的config文件夹，挂载到容器中/config文件夹\n    volumes:\n      - './prometheus/config:/config'\n      - '/data/prometheus/data/prometheus:/prometheus/data'\n      - '/etc/localtime:/etc/localtime'\n    # 指定容器中的配置文件\n    command:\n      - '--config.file=/config/prometheus.yml'\n      # 支持热更新\n      - '--web.enable-lifecycle'\n    networks:\n      - monitor\n\n  # 添加告警模块\n  alertmanager:\n    image: prom/alertmanager:latest\n    container_name: alertmanager\n    hostname: alertmanager\n    restart: always\n    ports:\n      - '9093:9093'\n    volumes:\n      - './prometheus/config:/config'\n      - '/data/prometheus/data/alertmanager:/alertmanager/data'\n      - '/etc/localtime:/etc/localtime'\n    command:\n      - '--config.file=/config/alertmanager.yml'\n    networks:\n      - monitor\n\n  # 添加监控可视化面板\n  grafana:\n    image: grafana/grafana:latest\n    container_name: grafana\n    hostname: grafana\n    restart: always\n    ports:\n      - '3000:3000'\n    volumes:\n      # 配置grafana 邮件服务器\n      - './grafana/config/grafana.ini:/etc/grafana/grafana.ini'\n      - '/data/grafana/data/grafana:/var/lib/grafana'\n      - '/etc/localtime:/etc/localtime'\n      - './grafana/config/ldap.toml:/etc/grafana/ldap.toml'\n    networks:\n      - monitor\nnetworks:\n  monitor:\n    driver: bridge\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n启动docker-compose\n\ndocker-compose up -d",charsets:{cjk:!0}},{title:"prometheus添加钉钉消息告警",frontmatter:{title:"prometheus添加钉钉消息告警",date:"2023-01-10T17:38:03.000Z",permalink:"/pages/22b836/",categories:["专题","prometheus"],tags:[null],readingShow:"top",description:"cd /opt/apps/dingding/prometheus-webhook-dingtalk",meta:[{name:"image",content:"https://oss-test-v1.1quant.com/monitor/logo.png"},{name:"twitter:title",content:"prometheus添加钉钉消息告警"},{name:"twitter:description",content:"cd /opt/apps/dingding/prometheus-webhook-dingtalk"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://oss-test-v1.1quant.com/monitor/logo.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/03.prometheus%E6%B7%BB%E5%8A%A0%E9%92%89%E9%92%89%E6%B6%88%E6%81%AF%E5%91%8A%E8%AD%A6.html"},{property:"og:type",content:"article"},{property:"og:title",content:"prometheus添加钉钉消息告警"},{property:"og:description",content:"cd /opt/apps/dingding/prometheus-webhook-dingtalk"},{property:"og:image",content:"https://oss-test-v1.1quant.com/monitor/logo.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/03.prometheus%E6%B7%BB%E5%8A%A0%E9%92%89%E9%92%89%E6%B6%88%E6%81%AF%E5%91%8A%E8%AD%A6.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-10T17:38:03.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"prometheus添加钉钉消息告警"},{itemprop:"description",content:"cd /opt/apps/dingding/prometheus-webhook-dingtalk"},{itemprop:"image",content:"https://oss-test-v1.1quant.com/monitor/logo.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/03.prometheus%E6%B7%BB%E5%8A%A0%E9%92%89%E9%92%89%E6%B6%88%E6%81%AF%E5%91%8A%E8%AD%A6.html",relativePath:"04.运维/12.监控/02.prometheus/03.prometheus添加钉钉消息告警.md",key:"v-003c987e",path:"/pages/22b836/",headers:[{level:4,title:"安装prometheus-webhook-dingtalk插件",slug:"安装prometheus-webhook-dingtalk插件",normalizedTitle:"安装prometheus-webhook-dingtalk插件",charIndex:2},{level:4,title:"创建启动服务文件",slug:"创建启动服务文件",normalizedTitle:"创建启动服务文件",charIndex:433}],headersStr:"安装prometheus-webhook-dingtalk插件 创建启动服务文件",content:'# 安装prometheus-webhook-dingtalk插件\n\n#下载\nwget https://github.com/timonwong/prometheus-webhook-dingtalk/releases/download/v1.4.0/prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz\n#解压\ntar -zxf prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz\n# 移动到安装目录\nmv prometheus-webhook-dingtalk-1.4.0.linux-amd64 /opt/apps/dingding/prometheus-webhook-dingtalk\n# 进入目录\ncd /opt/apps/dingding/prometheus-webhook-dingtalk\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 创建启动服务文件\n\n#prometheus-webhook-dingtalk.service\n[Unit]\nDescription=prometheus-webhook-dingtalk\nAfter=network-online.target\n\n[Service]\nRestart=on-failure\nExecStart=/opt/apps/dingding/prometheus-webhook-dingtalk/prometheus-webhook-dingtalk --config.file=/opt/apps/dingding/prometheus-webhook-dingtalk/config.yml\n\n[Install]\nWantedBy=default.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n将prometheus-webhook-dingtalk.service文件移动到/usr/lib/systemd/system下\n\n\n#生效系统文件\nsystemctl daemon-reload\n# 启动\nsystemctl start prometheus-webhook-dingtalk.service\n# 停止\nsystemctl stop prometheus-webhook-dingtalk.service \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n配置文件config.yml\n\n## Request timeout\n# timeout: 5s\n\n## Customizable templates path\ntemplates:\n  - /opt/apps/dingding/prometheus-webhook-dingtalk/contrib/templates/default.tmpl\n\n## You can also override default template using `default_message`\n## The following example to use the \'legacy\' template from v0.3.0\n# default_message:\n#   title: \'{{ template "legacy.title" . }}\'\n#   text: \'{{ template "legacy.content" . }}\'\n\n## Targets, previously was known as "profiles"\ntargets:\n  webhook:\n    url: https://oapi.dingtalk.com/robot/send?access_token=xxx\n    message:\n      title: \'{{ template "default.title" . }}\'\n      text: \'{{ template "default.content" . }}\'\n  webhook1:\n    url: https://oapi.dingtalk.com/robot/send?access_token=xxx\n    message:\n      title: \'{{ template "default.title" . }}\'\n      text: \'{{ template "default.content" . }}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n告警模板default.tmpl\n\n{{ define "__subject" }}\n[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}]\n{{ end }}\n\n{{ define "__alert_list" }}{{ range . }}\n---\n**告警名称**: {{ index .Annotations "summary" }}\n\n**告警级别**: {{ .Labels.severity }}\n\n**告警主机**: {{ .Labels.instance }}\n\n**告警信息**: {{ index .Annotations "description" }}\n\n**维护团队**: {{ .Labels.team | upper }}\n\n**告警时间**: {{ dateInZone "2006.01.02 15:04:05" (.StartsAt) "Asia/Shanghai" }}\n\n{{ end }}{{ end }}\n\n{{ define "__resolved_list" }}{{ range . }}\n---\n**告警名称**: {{ index .Annotations "summary" }}\n\n**告警级别**: {{ .Labels.severity }}\n\n**告警主机**: {{ .Labels.instance }}\n\n**告警信息**: {{ index .Annotations "description" }}\n\n**维护团队**: {{ .Labels.team | upper }}\n\n**告警时间**: {{ dateInZone "2006.01.02 15:04:05" (.StartsAt) "Asia/Shanghai" }}\n\n**恢复时间**: {{ dateInZone "2006.01.02 15:04:05" (.EndsAt) "Asia/Shanghai" }}\n\n{{ end }}{{ end }}\n\n\n{{ define "default.title" }}\n{{ template "__subject" . }}\n{{ end }}\n\n{{ define "default.content" }}\n![警报 图标](https://oss-test-v1.1quant.com/monitor/logo.png)\n\n{{ if gt (len .Alerts.Firing) 0 }}\n\n**====侦测到{{ .Alerts.Firing | len  }}个故障====**\n{{ template "__alert_list" .Alerts.Firing }}\n---\n{{ end }}\n\n{{ if gt (len .Alerts.Resolved) 0 }}\n**====恢复{{ .Alerts.Resolved | len  }}个故障====**\n{{ template "__resolved_list" .Alerts.Resolved }}\n{{ end }}\n{{ end }}\n\n\n{{ define "ding.link.title" }}{{ template "default.title" . }}{{ end }}\n{{ define "ding.link.content" }}{{ template "default.content" . }}{{ end }}\n{{ template "default.title" . }}\n{{ template "default.content" . }}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n',normalizedContent:'# 安装prometheus-webhook-dingtalk插件\n\n#下载\nwget https://github.com/timonwong/prometheus-webhook-dingtalk/releases/download/v1.4.0/prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz\n#解压\ntar -zxf prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz\n# 移动到安装目录\nmv prometheus-webhook-dingtalk-1.4.0.linux-amd64 /opt/apps/dingding/prometheus-webhook-dingtalk\n# 进入目录\ncd /opt/apps/dingding/prometheus-webhook-dingtalk\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 创建启动服务文件\n\n#prometheus-webhook-dingtalk.service\n[unit]\ndescription=prometheus-webhook-dingtalk\nafter=network-online.target\n\n[service]\nrestart=on-failure\nexecstart=/opt/apps/dingding/prometheus-webhook-dingtalk/prometheus-webhook-dingtalk --config.file=/opt/apps/dingding/prometheus-webhook-dingtalk/config.yml\n\n[install]\nwantedby=default.target\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n将prometheus-webhook-dingtalk.service文件移动到/usr/lib/systemd/system下\n\n\n#生效系统文件\nsystemctl daemon-reload\n# 启动\nsystemctl start prometheus-webhook-dingtalk.service\n# 停止\nsystemctl stop prometheus-webhook-dingtalk.service \n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n配置文件config.yml\n\n## request timeout\n# timeout: 5s\n\n## customizable templates path\ntemplates:\n  - /opt/apps/dingding/prometheus-webhook-dingtalk/contrib/templates/default.tmpl\n\n## you can also override default template using `default_message`\n## the following example to use the \'legacy\' template from v0.3.0\n# default_message:\n#   title: \'{{ template "legacy.title" . }}\'\n#   text: \'{{ template "legacy.content" . }}\'\n\n## targets, previously was known as "profiles"\ntargets:\n  webhook:\n    url: https://oapi.dingtalk.com/robot/send?access_token=xxx\n    message:\n      title: \'{{ template "default.title" . }}\'\n      text: \'{{ template "default.content" . }}\'\n  webhook1:\n    url: https://oapi.dingtalk.com/robot/send?access_token=xxx\n    message:\n      title: \'{{ template "default.title" . }}\'\n      text: \'{{ template "default.content" . }}\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n告警模板default.tmpl\n\n{{ define "__subject" }}\n[{{ .status | toupper }}{{ if eq .status "firing" }}:{{ .alerts.firing | len }}{{ end }}]\n{{ end }}\n\n{{ define "__alert_list" }}{{ range . }}\n---\n**告警名称**: {{ index .annotations "summary" }}\n\n**告警级别**: {{ .labels.severity }}\n\n**告警主机**: {{ .labels.instance }}\n\n**告警信息**: {{ index .annotations "description" }}\n\n**维护团队**: {{ .labels.team | upper }}\n\n**告警时间**: {{ dateinzone "2006.01.02 15:04:05" (.startsat) "asia/shanghai" }}\n\n{{ end }}{{ end }}\n\n{{ define "__resolved_list" }}{{ range . }}\n---\n**告警名称**: {{ index .annotations "summary" }}\n\n**告警级别**: {{ .labels.severity }}\n\n**告警主机**: {{ .labels.instance }}\n\n**告警信息**: {{ index .annotations "description" }}\n\n**维护团队**: {{ .labels.team | upper }}\n\n**告警时间**: {{ dateinzone "2006.01.02 15:04:05" (.startsat) "asia/shanghai" }}\n\n**恢复时间**: {{ dateinzone "2006.01.02 15:04:05" (.endsat) "asia/shanghai" }}\n\n{{ end }}{{ end }}\n\n\n{{ define "default.title" }}\n{{ template "__subject" . }}\n{{ end }}\n\n{{ define "default.content" }}\n![警报 图标](https://oss-test-v1.1quant.com/monitor/logo.png)\n\n{{ if gt (len .alerts.firing) 0 }}\n\n**====侦测到{{ .alerts.firing | len  }}个故障====**\n{{ template "__alert_list" .alerts.firing }}\n---\n{{ end }}\n\n{{ if gt (len .alerts.resolved) 0 }}\n**====恢复{{ .alerts.resolved | len  }}个故障====**\n{{ template "__resolved_list" .alerts.resolved }}\n{{ end }}\n{{ end }}\n\n\n{{ define "ding.link.title" }}{{ template "default.title" . }}{{ end }}\n{{ define "ding.link.content" }}{{ template "default.content" . }}{{ end }}\n{{ template "default.title" . }}\n{{ template "default.content" . }}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n',charsets:{cjk:!0}},{title:"归档",frontmatter:{archivesPage:!0,title:"归档",permalink:"/archives/",article:!1,readingShow:"top",description:"",meta:[{name:"twitter:title",content:"归档"},{name:"twitter:description",content:""},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/@pages/archivesPage.html"},{property:"og:type",content:"article"},{property:"og:title",content:"归档"},{property:"og:description",content:""},{property:"og:url",content:"https://blog.zzppjj.top/@pages/archivesPage.html"},{property:"og:site_name",content:"zpj"},{itemprop:"name",content:"归档"},{itemprop:"description",content:""}]},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-403d9ae8",path:"/archives/",headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"alertmanager实现某个时间段静默某些告警项",frontmatter:{title:"alertmanager实现某个时间段静默某些告警项",date:"2023-01-11T11:40:32.000Z",permalink:"/pages/96c6ce/",categories:["专题","prometheus"],tags:[null],readingShow:"top",description:"需求：我需要每天屏蔽某个时间段的某些告警项",meta:[{name:"image",content:"https://img2022.cnblogs.com/blog/1302413/202203/1302413-20220317170106813-275993103.png"},{name:"twitter:title",content:"alertmanager实现某个时间段静默某些告警项"},{name:"twitter:description",content:"需求：我需要每天屏蔽某个时间段的某些告警项"},{name:"twitter:card",content:"summary_large_image"},{name:"twitter:image",content:"https://img2022.cnblogs.com/blog/1302413/202203/1302413-20220317170106813-275993103.png"},{name:"twitter:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/04.alertmanager%E5%AE%9E%E7%8E%B0%E6%9F%90%E4%B8%AA%E6%97%B6%E9%97%B4%E6%AE%B5%E9%9D%99%E9%BB%98%E6%9F%90%E4%BA%9B%E5%91%8A%E8%AD%A6%E9%A1%B9.html"},{property:"og:type",content:"article"},{property:"og:title",content:"alertmanager实现某个时间段静默某些告警项"},{property:"og:description",content:"需求：我需要每天屏蔽某个时间段的某些告警项"},{property:"og:image",content:"https://img2022.cnblogs.com/blog/1302413/202203/1302413-20220317170106813-275993103.png"},{property:"og:url",content:"https://blog.zzppjj.top/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/04.alertmanager%E5%AE%9E%E7%8E%B0%E6%9F%90%E4%B8%AA%E6%97%B6%E9%97%B4%E6%AE%B5%E9%9D%99%E9%BB%98%E6%9F%90%E4%BA%9B%E5%91%8A%E8%AD%A6%E9%A1%B9.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2023-01-11T11:40:32.000Z"},{property:"article:tag",content:null},{itemprop:"name",content:"alertmanager实现某个时间段静默某些告警项"},{itemprop:"description",content:"需求：我需要每天屏蔽某个时间段的某些告警项"},{itemprop:"image",content:"https://img2022.cnblogs.com/blog/1302413/202203/1302413-20220317170106813-275993103.png"}]},regularPath:"/04.%E8%BF%90%E7%BB%B4/12.%E7%9B%91%E6%8E%A7/02.prometheus/04.alertmanager%E5%AE%9E%E7%8E%B0%E6%9F%90%E4%B8%AA%E6%97%B6%E9%97%B4%E6%AE%B5%E9%9D%99%E9%BB%98%E6%9F%90%E4%BA%9B%E5%91%8A%E8%AD%A6%E9%A1%B9.html",relativePath:"04.运维/12.监控/02.prometheus/04.alertmanager实现某个时间段静默某些告警项.md",key:"v-75545c56",path:"/pages/96c6ce/",headers:[{level:2,title:"【0】解决思路分析",slug:"【0】解决思路分析",normalizedTitle:"【0】解决思路分析",charIndex:2},{level:2,title:"【1】API",slug:"【1】api",normalizedTitle:"【1】api",charIndex:165},{level:3,title:"API Version",slug:"api-version",normalizedTitle:"api version",charIndex:215},{level:3,title:"POST /api/v2/alerts",slug:"post-api-v2-alerts",normalizedTitle:"post /api/v2/alerts",charIndex:2326},{level:3,title:"GET /api/v2/alerts",slug:"get-api-v2-alerts",normalizedTitle:"get /api/v2/alerts",charIndex:2703},{level:3,title:"GET /api/v2/alerts/groups",slug:"get-api-v2-alerts-groups",normalizedTitle:"get /api/v2/alerts/groups",charIndex:3650},{level:2,title:"【2】API使用实践",slug:"【2】api使用实践",normalizedTitle:"【2】api使用实践",charIndex:4289},{level:3,title:"（2.1）基本案例",slug:"_2-1-基本案例",normalizedTitle:"（2.1）基本案例",charIndex:4342},{level:2,title:"【3】最佳实践（定时静默）",slug:"【3】最佳实践-定时静默",normalizedTitle:"【3】最佳实践（定时静默）",charIndex:5073},{level:3,title:"（3.1）立马构造出故障的告警信息",slug:"_3-1-立马构造出故障的告警信息",normalizedTitle:"（3.1）立马构造出故障的告警信息",charIndex:5115},{level:3,title:"（3.2）构建好静默，获取静默语句",slug:"_3-2-构建好静默-获取静默语句",normalizedTitle:"（3.2）构建好静默，获取静默语句",charIndex:5143},{level:3,title:"（3.3）使用生成静默规则",slug:"_3-3-使用生成静默规则",normalizedTitle:"（3.3）使用生成静默规则",charIndex:6135},{level:3,title:"（3.4）自动化脚本",slug:"_3-4-自动化脚本",normalizedTitle:"（3.4）自动化脚本",charIndex:7214}],headersStr:"【0】解决思路分析 【1】API API Version POST /api/v2/alerts GET /api/v2/alerts GET /api/v2/alerts/groups 【2】API使用实践 （2.1）基本案例 【3】最佳实践（定时静默） （3.1）立马构造出故障的告警信息 （3.2）构建好静默，获取静默语句 （3.3）使用生成静默规则 （3.4）自动化脚本",content:'# 【0】解决思路分析\n\n需求：我需要每天屏蔽某个时间段的某些告警项\n\n比如：凌晨4点会异地备份，导致流量报警，我想屏蔽每天4点-4点30分的该告警项\n\n思路：\n\n直接操作是没有这个步骤的，曲线救国吧；\n\n（1）定时任务，每天凌晨四点的 silences\n\n（2）定时任务中使用 alertmanager api 来建设\n\n\n# 【1】API\n\n官网：AlertManager的API · prometheus · 看云\n\n\n# API Version\n\nAlertManager有两套API，v1与v2，不过两套API的内部逻辑基本是一致的，调用哪套都没有关系。v1没有相关的文档，不过我们可以找到v2的相关文档。\nAPI-v2的swagger文件的链接为：\n\nalertmanager/openapi.yaml at main · prometheus/alertmanager · GitHub\n\n把这个文件的内容拷贝到 https://editor.swagger.io 里面，便可以查看API。下面罗列了v2版本的所有API：\n\n# Alert\nGET    /api/v2/alerts\nPOST   /api/v2/alerts\n\n# AlertGroup\nGET    /api/v2/alerts/groups  \n\n# General\nGET    /api/v2/status\n\n# Receiver\nGET    /api/v2/receivers\n\n# Silence\nGET    /api/v2/silences\nPOST   /api/v2/silences\nGET    /api/v2/silence/{silenceID}\nDELETE /api/v2/silence/{silenceID}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n其中最重要的是Alert与AlertGroup的那三个API，接下来我们详细地讲解一下\n\n使用其中提供的url：http://127.0.0.1:9090/api/v1/alerts ，可以获取到报警的json信息，即可获得json的格式\n\n[{"annotations":\n{"description":\n"localhost:9100 of job exporter has been down for more than 5 minutes.",\n"summary":"Instance localhost:9100 down"},\n"endsAt":"2021-11-25T08:13:56.026Z",\n"fingerprint":"d44e90ffc89b2ea1",\n"receivers":[{"name":"mail"}],\n"startsAt":"2021-11-25T07:36:11.026Z",\n"status":{"inhibitedBy":[],"silencedBy":[],"state":"active"},\n"updatedAt":"2021-11-25T16:09:56.030+08:00",\n"generatorURL":"http://localhost.localdomain:9090/graph?g0.expr=up+%3D%3D+0\\u0026g0.tab=1",\n"labels":{"alertname":"InstanceDown","instance":"localhost:9100","job":"exporter","severity":"page"}\n}]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n编写好的json文件可以使用curl语句进行测试\n\ncurl -i -k -H "Content-type: application/json" -X POST -d\n[{"annotations":\n{"description":\n"localhost:9100 of job exporter has been down for more than 5 minutes.",\n"summary":"Instance localhost:9100 down"},\n"endsAt":"2021-11-25T08:13:56.026Z",\n"fingerprint":"d44e90ffc89b2ea1",\n"receivers":[{"name":"mail"}],\n"startsAt":"2021-11-25T07:36:11.026Z",\n"status":{"inhibitedBy":[],"silencedBy":[],"state":"active"},\n"updatedAt":"2021-11-25T16:09:56.030+08:00",\n"generatorURL":"http://localhost.localdomain:9090/graph?g0.expr=up+%3D%3D+0\\u0026g0.tab=1",\n"labels":{"alertname":"InstanceDown","instance":"localhost:9100","job":"exporter","severity":"page"}\n}]\n"http://192.168.217.22:9093/api/v1/alerts"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# POST /api/v2/alerts\n\nBody参数示例如下：\n\n[\n  {\n    "labels": {"label": "value", ...},\n    "annotations": {"label": "value", ...},\n    "generatorURL": "string",\n    "startsAt": "2020-01-01T00:00:00.000+08:00", # optional\n    "endsAt": "2020-01-01T01:00:00.000+08:00" # optional\n  },\n  ...\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nBody参数是一个数组，里面是一个个的告警。其中startsAt与endsAt是可选参数，且格式必须是上面的那种，不能是时间戳。\n\n\n# GET /api/v2/alerts\n\nQuery参数如下，以下参数用来过滤告警\n\n参数名           类型              默认值    是否必须       其他说明\nactive        bool            true   optional   -\nsilenced      bool            true   optional   -\ninhibited     bool            true   optional   -\nunprocessed   bool            true   optional   -\nfilter        array[string]   无      optional   -\nreceiver      string          无      optional   -\n\n其返回值如下：\n\n[\n  {\n    "labels": {"label": "value", ...},\n    "annotations": {"label": "value", ...},\n    "generatorURL": "string",\n    "startsAt": "2020-01-01T00:00:00.000+08:00", \n    "endsAt": "2020-01-01T01:00:00.000+08:00",\n    "updatedAt": "2020-01-01T01:00:00.000+08:00",\n    "fingerprint": "string"\n    "receivers": [{"name": "string"}, ...],\n    "status": {\n      "state": "active", # active, unprocessed, ...\n      "silencedBy": ["string", ...],\n      "inhibitedBy": ["string", ...]\n  },\n  ...\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# GET /api/v2/alerts/groups\n\nQuery参数如下，以下参数用来过滤告警\n\n参数名           类型              默认值    是否必须       其他说明\nactive        bool            true   optional   -\nsilenced      bool            true   optional   -\ninhibited     bool            true   optional   -\nunprocessed   bool            true   optional   -\nfilter        array[string]   无      optional   -\nreceiver      string          无      optional   -\n\n其返回值如下：\n\n[\n  { \n    "labels": {"label": "value", ...},\n    "receiver": {"name": "string"},  # 注意与alert的receivers的区别\n    "alerts": [alert1, alert2, ...]  # alert的Json结构与 `GET /api/v2/alerts` 返回值中的结构一致\n  },\n  ...\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 【2】API使用实践\n\n参考：实践：AlertManager · prometheus · 看云\n\n\n# （2.1）基本案例\n\n构造测试数据：\n\n# 构造测试数据\n\naa=\'[\n    {\n        "Labels": {\n            "alertname": "NodeCpuPressure",\n            "IP": "192.168.2.101"\n        },\n        "Annotations": {\n            "summary": "NodeCpuPressure, IP: 192.168.2.101, Value: 90%, Threshold: 85%"\n        },\n        "StartsAt": "2020-02-17T23:00:00.000+08:00", \n        "EndsAt": "2023-02-18T23:00:00.000+08:00"\n    }\n]\'\n\n# 执行 POST\ncurl http://127.0.0.1:9093/api/v2/alerts -XPOST -H\'Content-Type: application/json\' -d"$aa"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n如果是使用V1版本，可以不用加  -H\'Content-Type: application/json\'\n\n\n\n怎么取消掉？把结束时间修改一下就好了；\n\n我们可以通过  curl -X GET localhost:9093/api/v2/alerts  获取\n\n\n\n然后修改其时间\n\n\n\n然后我们在上述的操作前：\n\n活跃且没被静默的 Alert\n\n\n\n操作后：\n\n\n\n回到顶部\n\n\n# 【3】最佳实践（定时静默）\n\n注意，静默条目所生成、显示的时间是UTC时间\n\n\n# （3.1）立马构造出故障的告警信息\n\n\n\n\n\n\n\n\n# （3.2）构建好静默，获取静默语句\n\n\n\n\n\n最终结果：\n\n\n\ncurl \'127.0.0.1:9093/api/v2/silences\' \\\n  -H \'Connection: keep-alive\' \\\n  -H \'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\' \\\n  -H \'Content-Type: application/json\' \\\n  -H \'Accept: */*\' \\\n  -H \'Origin: http://127.0.0.1:9093\' \\\n  -H \'Referer: http://127.0.0.1:9093/\' \\\n  -H \'Accept-Language: zh-CN,zh;q=0.9\' \\\n  -H \'Cookie: grafana_session=a504e4a78501efe7009fa8b7587d5fb4\' \\\n  --data-raw \'{"matchers":[{"name":"alertname","value":"磁盘读吞吐过高","isRegex":false},{"name":"instance","value":"127.0.0.1:9182","isRegex":false},{"name":"job","value":"测试_win","isRegex":false},{"name":"name","value":"测试数据库鸭[47.103.57.124]","isRegex":false},{"name":"severity","value":"warning","isRegex":false}],"startsAt":"2022-03-17T07:44:45.446Z","endsAt":"2022-03-17T09:44:45.446Z","createdBy":"guochaoqun","comment":"tmp","id":null}\' \\\n  --compressed \\\n  --insecure\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# （3.3）使用生成静默规则\n\nV1方式\n\ncurl -X POST http://127.0.0.1:9093/api/v1/silences -d\'{"matchers":[{"name":"alertname","value":"磁盘读吞 吐过高","isRegex":false},{"name":"instance","value":"47.103.57.124:9182","isRegex":false},{"name":"job","value":"test","isRegex":false},{"name":"name","value":"test库[47.103.57.124]","isRegex":false},{"name":"severity","value":"warning","isRegex":false}],"startsAt":"2022-03-17T07:44:45.446Z","endsAt":"2022-03-17T09:44:45.446Z","createdBy":"guochaoqun","comment":"tmp","id":null}\' \n\n\n1\n\n\nV2 方式，就必须要加 -H\n\ncurl \'127.0.0.1:9093/api/v2/silences\' \\-H \'Content-Type: application/json\' \\\n  --data \'{"matchers":[{"name":"alertname","value":"磁盘读吞吐过高","isRegex":false},{"name":"instance","value":"127.0.0.1:9182","isRegex":false},{"name":"job","value":"测试_win","isRegex":false},{"name":"name","value":"测试库[47.103.57.124]","isRegex":false},{"name":"severity","value":"warning","isRegex":false}],"startsAt":"2022-03-17T07:44:45.446Z","endsAt":"2022-03-17T09:44:45.446Z","createdBy":"guochaoqun","comment":"tmp","id":null}\' \\\n  --compressed \\\n  --insecure\n\n\n1\n2\n3\n4\n\n\n\n\n\n# （3.4）自动化脚本\n\nnow_date=`date +%F --date="-1 day"`\ncurl \'http://127.0.0.1:9093/api/v2/silences\' \\\n-H \'Content-Type: application/json\' \\\n--data \'{"matchers":[{"name":"alertname","value":"磁盘读吞吐过高","isRegex":false},{"name":"instance","value":"47.103.57.124:9182","isRegex":false},{"name":"job","value":"金游世界_win","isRegex":false},{"name":"name","value":"8833主库_详细对局[47.103.57.124]","isRegex":false},{"name":"severity","value":"warning","isRegex":false}],"startsAt":"\'${now_date}\'T20:10:45.446Z","endsAt":"\'${now_date}\'T20:40:45.446Z","createdBy":"guochaoqun","comment":"tmp","id":null}\' \\\n--compressed --insecure\n\n\n1\n2\n3\n4\n5\n\n\n原文链接：\n\nhttps://www.cnblogs.com/gered/p/15946822.html',normalizedContent:'# 【0】解决思路分析\n\n需求：我需要每天屏蔽某个时间段的某些告警项\n\n比如：凌晨4点会异地备份，导致流量报警，我想屏蔽每天4点-4点30分的该告警项\n\n思路：\n\n直接操作是没有这个步骤的，曲线救国吧；\n\n（1）定时任务，每天凌晨四点的 silences\n\n（2）定时任务中使用 alertmanager api 来建设\n\n\n# 【1】api\n\n官网：alertmanager的api · prometheus · 看云\n\n\n# api version\n\nalertmanager有两套api，v1与v2，不过两套api的内部逻辑基本是一致的，调用哪套都没有关系。v1没有相关的文档，不过我们可以找到v2的相关文档。\napi-v2的swagger文件的链接为：\n\nalertmanager/openapi.yaml at main · prometheus/alertmanager · github\n\n把这个文件的内容拷贝到 https://editor.swagger.io 里面，便可以查看api。下面罗列了v2版本的所有api：\n\n# alert\nget    /api/v2/alerts\npost   /api/v2/alerts\n\n# alertgroup\nget    /api/v2/alerts/groups  \n\n# general\nget    /api/v2/status\n\n# receiver\nget    /api/v2/receivers\n\n# silence\nget    /api/v2/silences\npost   /api/v2/silences\nget    /api/v2/silence/{silenceid}\ndelete /api/v2/silence/{silenceid}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n其中最重要的是alert与alertgroup的那三个api，接下来我们详细地讲解一下\n\n使用其中提供的url：http://127.0.0.1:9090/api/v1/alerts ，可以获取到报警的json信息，即可获得json的格式\n\n[{"annotations":\n{"description":\n"localhost:9100 of job exporter has been down for more than 5 minutes.",\n"summary":"instance localhost:9100 down"},\n"endsat":"2021-11-25t08:13:56.026z",\n"fingerprint":"d44e90ffc89b2ea1",\n"receivers":[{"name":"mail"}],\n"startsat":"2021-11-25t07:36:11.026z",\n"status":{"inhibitedby":[],"silencedby":[],"state":"active"},\n"updatedat":"2021-11-25t16:09:56.030+08:00",\n"generatorurl":"http://localhost.localdomain:9090/graph?g0.expr=up+%3d%3d+0\\u0026g0.tab=1",\n"labels":{"alertname":"instancedown","instance":"localhost:9100","job":"exporter","severity":"page"}\n}]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n编写好的json文件可以使用curl语句进行测试\n\ncurl -i -k -h "content-type: application/json" -x post -d\n[{"annotations":\n{"description":\n"localhost:9100 of job exporter has been down for more than 5 minutes.",\n"summary":"instance localhost:9100 down"},\n"endsat":"2021-11-25t08:13:56.026z",\n"fingerprint":"d44e90ffc89b2ea1",\n"receivers":[{"name":"mail"}],\n"startsat":"2021-11-25t07:36:11.026z",\n"status":{"inhibitedby":[],"silencedby":[],"state":"active"},\n"updatedat":"2021-11-25t16:09:56.030+08:00",\n"generatorurl":"http://localhost.localdomain:9090/graph?g0.expr=up+%3d%3d+0\\u0026g0.tab=1",\n"labels":{"alertname":"instancedown","instance":"localhost:9100","job":"exporter","severity":"page"}\n}]\n"http://192.168.217.22:9093/api/v1/alerts"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# post /api/v2/alerts\n\nbody参数示例如下：\n\n[\n  {\n    "labels": {"label": "value", ...},\n    "annotations": {"label": "value", ...},\n    "generatorurl": "string",\n    "startsat": "2020-01-01t00:00:00.000+08:00", # optional\n    "endsat": "2020-01-01t01:00:00.000+08:00" # optional\n  },\n  ...\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nbody参数是一个数组，里面是一个个的告警。其中startsat与endsat是可选参数，且格式必须是上面的那种，不能是时间戳。\n\n\n# get /api/v2/alerts\n\nquery参数如下，以下参数用来过滤告警\n\n参数名           类型              默认值    是否必须       其他说明\nactive        bool            true   optional   -\nsilenced      bool            true   optional   -\ninhibited     bool            true   optional   -\nunprocessed   bool            true   optional   -\nfilter        array[string]   无      optional   -\nreceiver      string          无      optional   -\n\n其返回值如下：\n\n[\n  {\n    "labels": {"label": "value", ...},\n    "annotations": {"label": "value", ...},\n    "generatorurl": "string",\n    "startsat": "2020-01-01t00:00:00.000+08:00", \n    "endsat": "2020-01-01t01:00:00.000+08:00",\n    "updatedat": "2020-01-01t01:00:00.000+08:00",\n    "fingerprint": "string"\n    "receivers": [{"name": "string"}, ...],\n    "status": {\n      "state": "active", # active, unprocessed, ...\n      "silencedby": ["string", ...],\n      "inhibitedby": ["string", ...]\n  },\n  ...\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# get /api/v2/alerts/groups\n\nquery参数如下，以下参数用来过滤告警\n\n参数名           类型              默认值    是否必须       其他说明\nactive        bool            true   optional   -\nsilenced      bool            true   optional   -\ninhibited     bool            true   optional   -\nunprocessed   bool            true   optional   -\nfilter        array[string]   无      optional   -\nreceiver      string          无      optional   -\n\n其返回值如下：\n\n[\n  { \n    "labels": {"label": "value", ...},\n    "receiver": {"name": "string"},  # 注意与alert的receivers的区别\n    "alerts": [alert1, alert2, ...]  # alert的json结构与 `get /api/v2/alerts` 返回值中的结构一致\n  },\n  ...\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 【2】api使用实践\n\n参考：实践：alertmanager · prometheus · 看云\n\n\n# （2.1）基本案例\n\n构造测试数据：\n\n# 构造测试数据\n\naa=\'[\n    {\n        "labels": {\n            "alertname": "nodecpupressure",\n            "ip": "192.168.2.101"\n        },\n        "annotations": {\n            "summary": "nodecpupressure, ip: 192.168.2.101, value: 90%, threshold: 85%"\n        },\n        "startsat": "2020-02-17t23:00:00.000+08:00", \n        "endsat": "2023-02-18t23:00:00.000+08:00"\n    }\n]\'\n\n# 执行 post\ncurl http://127.0.0.1:9093/api/v2/alerts -xpost -h\'content-type: application/json\' -d"$aa"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n如果是使用v1版本，可以不用加  -h\'content-type: application/json\'\n\n\n\n怎么取消掉？把结束时间修改一下就好了；\n\n我们可以通过  curl -x get localhost:9093/api/v2/alerts  获取\n\n\n\n然后修改其时间\n\n\n\n然后我们在上述的操作前：\n\n活跃且没被静默的 alert\n\n\n\n操作后：\n\n\n\n回到顶部\n\n\n# 【3】最佳实践（定时静默）\n\n注意，静默条目所生成、显示的时间是utc时间\n\n\n# （3.1）立马构造出故障的告警信息\n\n\n\n\n\n\n\n\n# （3.2）构建好静默，获取静默语句\n\n\n\n\n\n最终结果：\n\n\n\ncurl \'127.0.0.1:9093/api/v2/silences\' \\\n  -h \'connection: keep-alive\' \\\n  -h \'user-agent: mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/98.0.4758.102 safari/537.36\' \\\n  -h \'content-type: application/json\' \\\n  -h \'accept: */*\' \\\n  -h \'origin: http://127.0.0.1:9093\' \\\n  -h \'referer: http://127.0.0.1:9093/\' \\\n  -h \'accept-language: zh-cn,zh;q=0.9\' \\\n  -h \'cookie: grafana_session=a504e4a78501efe7009fa8b7587d5fb4\' \\\n  --data-raw \'{"matchers":[{"name":"alertname","value":"磁盘读吞吐过高","isregex":false},{"name":"instance","value":"127.0.0.1:9182","isregex":false},{"name":"job","value":"测试_win","isregex":false},{"name":"name","value":"测试数据库鸭[47.103.57.124]","isregex":false},{"name":"severity","value":"warning","isregex":false}],"startsat":"2022-03-17t07:44:45.446z","endsat":"2022-03-17t09:44:45.446z","createdby":"guochaoqun","comment":"tmp","id":null}\' \\\n  --compressed \\\n  --insecure\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# （3.3）使用生成静默规则\n\nv1方式\n\ncurl -x post http://127.0.0.1:9093/api/v1/silences -d\'{"matchers":[{"name":"alertname","value":"磁盘读吞 吐过高","isregex":false},{"name":"instance","value":"47.103.57.124:9182","isregex":false},{"name":"job","value":"test","isregex":false},{"name":"name","value":"test库[47.103.57.124]","isregex":false},{"name":"severity","value":"warning","isregex":false}],"startsat":"2022-03-17t07:44:45.446z","endsat":"2022-03-17t09:44:45.446z","createdby":"guochaoqun","comment":"tmp","id":null}\' \n\n\n1\n\n\nv2 方式，就必须要加 -h\n\ncurl \'127.0.0.1:9093/api/v2/silences\' \\-h \'content-type: application/json\' \\\n  --data \'{"matchers":[{"name":"alertname","value":"磁盘读吞吐过高","isregex":false},{"name":"instance","value":"127.0.0.1:9182","isregex":false},{"name":"job","value":"测试_win","isregex":false},{"name":"name","value":"测试库[47.103.57.124]","isregex":false},{"name":"severity","value":"warning","isregex":false}],"startsat":"2022-03-17t07:44:45.446z","endsat":"2022-03-17t09:44:45.446z","createdby":"guochaoqun","comment":"tmp","id":null}\' \\\n  --compressed \\\n  --insecure\n\n\n1\n2\n3\n4\n\n\n\n\n\n# （3.4）自动化脚本\n\nnow_date=`date +%f --date="-1 day"`\ncurl \'http://127.0.0.1:9093/api/v2/silences\' \\\n-h \'content-type: application/json\' \\\n--data \'{"matchers":[{"name":"alertname","value":"磁盘读吞吐过高","isregex":false},{"name":"instance","value":"47.103.57.124:9182","isregex":false},{"name":"job","value":"金游世界_win","isregex":false},{"name":"name","value":"8833主库_详细对局[47.103.57.124]","isregex":false},{"name":"severity","value":"warning","isregex":false}],"startsat":"\'${now_date}\'t20:10:45.446z","endsat":"\'${now_date}\'t20:40:45.446z","createdby":"guochaoqun","comment":"tmp","id":null}\' \\\n--compressed --insecure\n\n\n1\n2\n3\n4\n5\n\n\n原文链接：\n\nhttps://www.cnblogs.com/gered/p/15946822.html',charsets:{cjk:!0}},{title:"关于",frontmatter:{title:"关于",date:"2022-12-15T13:14:01.000Z",permalink:"/about/",sidebar:!0,article:!1,description:"",readingShow:"top",meta:[{name:"twitter:title",content:"关于"},{name:"twitter:description",content:""},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/05.%E5%85%B3%E4%BA%8E/01.%E5%85%B3%E4%BA%8E.html"},{property:"og:type",content:"article"},{property:"og:title",content:"关于"},{property:"og:description",content:""},{property:"og:url",content:"https://blog.zzppjj.top/05.%E5%85%B3%E4%BA%8E/01.%E5%85%B3%E4%BA%8E.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T13:14:01.000Z"},{itemprop:"name",content:"关于"},{itemprop:"description",content:""}]},regularPath:"/05.%E5%85%B3%E4%BA%8E/01.%E5%85%B3%E4%BA%8E.html",relativePath:"05.关于/01.关于.md",key:"v-17a259f2",path:"/about/",headers:[{level:2,title:"关于博主",slug:"关于博主",normalizedTitle:"关于博主",charIndex:2}],headersStr:"关于博主",content:"# 关于博主\n\n7年运维经验的打工人，分享内容，分享生活",normalizedContent:"# 关于博主\n\n7年运维经验的打工人，分享内容，分享生活",charsets:{cjk:!0}},{title:"友链",frontmatter:{title:"友链",date:"2022-12-15T09:17:44.000Z",permalink:"/friends",categories:["友链"],tags:["friend"],description:"::: cardList",readingShow:"top",meta:[{name:"twitter:title",content:"友链"},{name:"twitter:description",content:"::: cardList"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/06.%E5%8F%8B%E9%93%BE/01.%E5%8F%8B%E9%93%BE.html"},{property:"og:type",content:"article"},{property:"og:title",content:"友链"},{property:"og:description",content:"::: cardList"},{property:"og:url",content:"https://blog.zzppjj.top/06.%E5%8F%8B%E9%93%BE/01.%E5%8F%8B%E9%93%BE.html"},{property:"og:site_name",content:"zpj"},{property:"article:published_time",content:"2022-12-15T09:17:44.000Z"},{property:"article:tag",content:"friend"},{itemprop:"name",content:"友链"},{itemprop:"description",content:"::: cardList"}]},regularPath:"/06.%E5%8F%8B%E9%93%BE/01.%E5%8F%8B%E9%93%BE.html",relativePath:"06.友链/01.友链.md",key:"v-50528ad2",path:"/friends/",headers:[{level:2,title:"我的友链",slug:"我的友链",normalizedTitle:"我的友链",charIndex:2},{level:2,title:"友链申请",slug:"友链申请",normalizedTitle:"友链申请",charIndex:181}],headersStr:"我的友链 友链申请",content:"# 我的友链\n\n二丫讲梵\n\n💻学习📝记录🔗分享\n\n- name: 二丫讲梵\n  desc: '💻学习📝记录🔗分享'\n  avatar: https://wiki.eryajf.net/img/logo.png\n  link: https://wiki.eryajf.net\n  bgColor: '#f8f9fa'\n\n\n1\n2\n3\n4\n5\n\n\n\n# 友链申请\n\n与我联系或者 在本页面评论区留言您的友链信息，格式：(点击代码块右上角一键复制)\n\n- name: 章工运维 # 昵称\n  desc: 好好学习，天天向上\n  avatar: http://pic.zzppjj.top/LightPicture/2023/02/b8f9973008054c97.png # 头像\n  link: https://blog.zzppjj.top/  # 链接\n\n\n1\n2\n3\n4\n\n\n申请前记得先添加本站哦~",normalizedContent:"# 我的友链\n\n二丫讲梵\n\n💻学习📝记录🔗分享\n\n- name: 二丫讲梵\n  desc: '💻学习📝记录🔗分享'\n  avatar: https://wiki.eryajf.net/img/logo.png\n  link: https://wiki.eryajf.net\n  bgcolor: '#f8f9fa'\n\n\n1\n2\n3\n4\n5\n\n\n\n# 友链申请\n\n与我联系或者 在本页面评论区留言您的友链信息，格式：(点击代码块右上角一键复制)\n\n- name: 章工运维 # 昵称\n  desc: 好好学习，天天向上\n  avatar: http://pic.zzppjj.top/lightpicture/2023/02/b8f9973008054c97.png # 头像\n  link: https://blog.zzppjj.top/  # 链接\n\n\n1\n2\n3\n4\n\n\n申请前记得先添加本站哦~",charsets:{cjk:!0}},{title:"分类",frontmatter:{categoriesPage:!0,title:"分类",permalink:"/categories/",article:!1,readingShow:"top",description:"",meta:[{name:"twitter:title",content:"分类"},{name:"twitter:description",content:""},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/@pages/categoriesPage.html"},{property:"og:type",content:"article"},{property:"og:title",content:"分类"},{property:"og:description",content:""},{property:"og:url",content:"https://blog.zzppjj.top/@pages/categoriesPage.html"},{property:"og:site_name",content:"zpj"},{itemprop:"name",content:"分类"},{itemprop:"description",content:""}]},regularPath:"/@pages/categoriesPage.html",relativePath:"@pages/categoriesPage.md",key:"v-65b5a670",path:"/categories/",headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"标签",frontmatter:{tagsPage:!0,title:"标签",permalink:"/tags/",article:!1,readingShow:"top",description:"",meta:[{name:"twitter:title",content:"标签"},{name:"twitter:description",content:""},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/@pages/tagsPage.html"},{property:"og:type",content:"article"},{property:"og:title",content:"标签"},{property:"og:description",content:""},{property:"og:url",content:"https://blog.zzppjj.top/@pages/tagsPage.html"},{property:"og:site_name",content:"zpj"},{itemprop:"name",content:"标签"},{itemprop:"description",content:""}]},regularPath:"/@pages/tagsPage.html",relativePath:"@pages/tagsPage.md",key:"v-8923d930",path:"/tags/",headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"Home",frontmatter:{home:!0,heroText:"章工运维",tagline:"努力、拼搏",bannerBg:"http://pic.zzppjj.top/LightPicture/2023/02/7c6d3b8425f739ab.jpg",features:[{title:"运维",details:"好好学习，天天向上",link:"/ops/"},{title:"生活",details:"记录、回忆",link:"/life/"},{title:"编程",details:"分享、记录",link:"/code/"}],readingShow:"top",description:"好好学习，天天向上",meta:[{name:"twitter:title",content:"章工运维"},{name:"twitter:description",content:"好好学习，天天向上"},{name:"twitter:card",content:"summary"},{name:"twitter:url",content:"https://blog.zzppjj.top/"},{property:"og:type",content:"website"},{property:"og:title",content:"章工运维"},{property:"og:description",content:"好好学习，天天向上"},{property:"og:url",content:"https://blog.zzppjj.top/"},{property:"og:site_name",content:"zpj"},{itemprop:"name",content:"章工运维"},{itemprop:"description",content:"好好学习，天天向上"}]},regularPath:"/",relativePath:"README.md",key:"v-7e9c1840",path:"/",headersStr:null,content:"努力、拼搏、自信\n\n",normalizedContent:"努力、拼搏、自信\n\n",charsets:{cjk:!0}}],themeConfig:{nav:[{text:"首页",link:"/"},{text:"运维",link:"/ops/",items:[{text:"linux",link:"/linux/"},{text:"windows",link:"/windows/"},{text:"中间件",link:"/middleware/"},{text:"监控",link:"/monitor/"},{text:"网络",link:"/network/"},{text:"存储",link:"/storage/"},{text:"安全",link:"/safety/"},{text:"防火墙",link:"/firewalld/"},{text:"数据库",link:"/db/"},{text:"系统",link:"/sys/"},{text:"docker",link:"/docker/"},{text:"运维工具",link:"/tool/"},{text:"other",link:"/other/"}]},{text:"专题",link:"/topic/",items:[{text:"elk",link:"/elk/"},{text:"k8s",link:"/k8s/"},{text:"ansible",link:"/ansible/"},{text:"Jenkins",link:"/jenkins/"}]},{text:"生活",link:"/life/",items:[{text:"随笔",link:"/suibi/"},{text:"面试",link:"/mianshi/"},{text:"工具",link:"/gongju/"}]},{text:"编程",link:"/code/",items:[{text:"Shell",link:"/shell/"},{text:"python",link:"/python/"}]},{text:"关于",link:"/about/"},{text:"友链",link:"/friends/"},{text:"索引",link:"/categories/",items:[{text:"分类",link:"/categories/"},{text:"标签",link:"/tags/"},{text:"归档",link:"/archives/"}]},{text:"页面",link:"/nav/",items:[{items:[{text:"chatgpt",link:"https://aichat.zzppjj.top"},{text:"图床",link:"https://pic.zzppjj.top"},{text:"评论",link:"https://artalk.zzppjj.top"}]}]},{text:"开往",link:"https://www.travellings.cn/go.html"}],sidebarDepth:2,logo:"http://pic.zzppjj.top/LightPicture/2023/02/b8f9973008054c97.png",repo:"zpj874878956",searchMaxSuggestions:10,lastUpdated:"上次更新",editLinks:!1,sidebarHoverTriggerOpen:!0,searchPlaceholder:"按下 𝑺 搜索",pageButton:!1,sidebar:{"/00.目录页/":[["01.ansible.md","ansible","/ansible/"],["02.k8s.md","k8s","/k8s/"],["03.elk.md","elk","/elk/"],["04.jenkins.md","jenkins","/jenkins/"],["05.随笔.md","随笔","/suibi/"],["06.面试.md","面试","/mianshi/"],["07.工具.md","工具","/gongju/"],["08.生活.md","生活","/life/"],["09.编程.md","编程","/code/"],["10.python.md","python","/python/"],["11.shell.md","shell","/shell/"],["12.专题.md","专题","/topic/"],["13.运维.md","运维","/ops/"],["14.中间件.md","中间件","/middleware/"],["15.linux.md","linux","/linux/"],["16.windows.md","windows","/windows/"],["17.网络.md","网络","/network/"],["18.安全.md","安全","/safety/"],["19.存储.md","存储","/storage/"],["20.防火墙.md","防火墙","/firewalld/"],["21.数据库.md","数据库","/db/"],["22.系统.md","系统","/sys/"],["23.docker.md","docker","/docker/"],["24.运维工具.md","运维工具","/tool/"],["25.监控.md","监控","/monitor/"],["26.other.md","other","/other/"]],catalogue:{ansible:"/ansible/",k8s:"/k8s/",elk:"/elk/",jenkins:"/jenkins/","随笔":"/suibi/","面试":"/mianshi/","工具":"/gongju/","生活":"/life/","编程":"/code/",python:"/python/",shell:"/shell/","专题":"/topic/","运维":"/ops/","中间件":"/middleware/",linux:"/linux/",windows:"/windows/","网络":"/network/","安全":"/safety/","存储":"/storage/","防火墙":"/firewalld/","数据库":"/db/","系统":"/sys/",docker:"/docker/","运维工具":"/tool/","监控":"/monitor/",other:"/other/"},"/01.专题/":[{title:"ansible系列文章",collapsable:!0,children:[["01.ansible系列文章/01.ansible入门.md","ansible入门","/pages/12c5da01/"],["01.ansible系列文章/02.anisble批量安装node_exporter.md","anisble批量安装node_exporter","/pages/f8f66c/"],["01.ansible系列文章/03.ansible-playbook中的变量.md","ansible-playbook中的变量","/pages/d740bc/"],["01.ansible系列文章/04.Ansible性能优化——提升ansible执行效率.md","Ansible性能优化——提升ansible执行效率","/pages/a06506/"],["01.ansible系列文章/05.ansible之roles简单使用.md","ansible之roles简单使用","/pages/847542/"],["01.ansible系列文章/06.ansible中template简单使用.md","ansible中template简单使用","/pages/4189b8/"],["01.ansible系列文章/07.Ansible中的playbook 详解.md","playbook详解","/pages/14eda9/"],["01.ansible系列文章/08.ansible-playbook编排使用tips.md","ansible-playbook编排使用tips","/pages/7eca6b/"],["01.ansible系列文章/09.ansible优秀案例.md","ansible优秀案例","/pages/4fbba1/"]]},{title:"k8s",collapsable:!0,children:[["02.k8s/01.快速部署k8s集群.md","快速部署k8s集群","/pages/fc6bfb/"]]},{title:"elk",collapsable:!0,children:[["03.elk/01.elk安装.md","elk安装","/pages/98c9f5/"],["03.elk/02.docker-compose安装elk.md","docker-compose安装elk","/pages/bbe108/"],["03.elk/03.filebeat-log相关配置指南.md","filebeat-log相关配置指南","/pages/ed9e8d/"]]},{title:"jenkins",collapsable:!0,children:[["04.jenkins/01.jenkins容器安装.md","jenkins容器安装","/pages/ce2b89/"],["04.jenkins/02.jenkins流水线部署.md","jenkins流水线部署","/pages/a9ff44/"]]}],"/02.生活/":[{title:"随笔",collapsable:!0,children:[["01.随笔/01.男人心智成熟的九大表现.md","男人心智成熟的九大表现","/pages/aa6ada/"],["01.随笔/02.如果面试时大家都说真话.md","如果面试时大家都说真话","/pages/0cd089/"],["01.随笔/03.这四个故事小段，够你受用一生.md","这四个故事小段，够你受用一生","/pages/7b0831/"],["01.随笔/04.小说活着摘录.md","小说活着摘录","/pages/e7e74b/"],["01.随笔/05.人生格言.md","人生格言","/pages/58b3c0/"]]},{title:"面试",collapsable:!0,children:[["02.面试/01.运维10道基础面试题.md","运维10道基础面试题","/pages/f6c9c9/"],["02.面试/02.http状态码.md","http状态码","/pages/a2e381/"],["02.面试/03.高级运维工程需要掌握的技能.md","高级运维工程需要掌握的技能","/pages/9245d9/"]]},{title:"工具",collapsable:!0,children:[{title:"vpn",collapsable:!0,children:[["03.工具/01.vpn/01.企业级openvpn搭建.md","企业级openvpn搭建","/pages/45fa3c/"]]},{title:"翻墙教程",collapsable:!0,children:[["03.工具/02.翻墙教程/01.实现V2Ray通过CloudFlare自选IP加速.md","实现V2Ray通过CloudFlare自选IP加速","/pages/e9bb88/"]]},["03.工具/03.mac电脑常用软件.md","mac电脑常用软件","/pages/4ad301/"],["03.工具/04.个人工具链接.md","工具链接","/pages/8994e9/"],["03.工具/05.vuepress配置artalk.md","vuepress配置artalk","/pages/468f43/"],["03.工具/06.windows目录实时同步工具.md","windows目录实时同步工具","/pages/112008/"]]}],"/03.编程/":[{title:"python",collapsable:!0,children:[["01.python/01.监控目录或文件变化.md","监控目录或文件变化","/pages/f8da74/"],["01.python/02.批量更改文件.md","批量更改文件","/pages/f659cd/"],["01.python/03.python引用数据库.md","python引用数据库","/pages/a109ec/"],["01.python/04.python3给防火墙添加放行.md","python3给防火墙添加放行","/pages/457fb7/"],["01.python/05.python生成部署脚本.md","python生成部署脚本","/pages/d0fcb5/"],["01.python/06.python将多个文件内容输出到一个文件中.md","python将多个文件内容输出到一个文件中","/pages/8d0b76/"]]},{title:"shell",collapsable:!0,children:[["02.shell/01.进程pid判断脚本.md","进程pid判断脚本","/pages/cae02d/"],["02.shell/02.日志切割脚本.md","日志切割脚本","/pages/3b937e/"],["02.shell/03.设置跳板机脚本.md","设置跳板机脚本","/pages/69b699/"],["02.shell/04.编写启动、停止、重启的脚本.md","编写启动、停止、重启的脚本","/pages/fe0782/"],["02.shell/05.mysql数据库备份的三种方式.md","mysql数据库备份的三种方式","/pages/93dcc7/"],["02.shell/06.jenkins编译服务脚本.md","jenkins编译服务脚本","/pages/2019f8/"],["02.shell/07.app编译脚本.md","app编译脚本","/pages/fd3678/"],["02.shell/08.常用shell脚本.md","常用shell脚本","/pages/8fdac1/"],["02.shell/09.字符串的截取拼接.md","字符串的截取拼接","/pages/f5c376/"],["02.shell/10.shell基础.md","shell基础","/pages/665355/"]]}],"/04.运维/":[{title:"linux",collapsable:!0,children:[{title:"rsync",collapsable:!0,children:[["01.linux/01.rsync/01.rsync用法及参数详解.md","rsync用法及参数详解","/pages/98cac7/"],["01.linux/01.rsync/02.rsync服务实现推送，拉取.md","rsync服务实现推送，拉取","/pages/c7f1bc/"]]},{title:"dns",collapsable:!0,children:[["01.linux/02.dns/01.centos7搭建dns,bind配置.md","centos7搭建dns,bind配置","/pages/78c801/"]]},{title:"sed、awk、grep、find四剑客",collapsable:!0,children:[["01.linux/03.sed、awk、grep、find四剑客/01.sed命令在文本每行,行尾或行首添加字符.md","sed命令在文本每行,行尾或行首添加字符","/pages/4baba0/"]]},["01.linux/04.LVM管理.md","LVM管理","/pages/5f261c/"],["01.linux/05.sudo权限规划.md","sudo权限规划","/pages/0bac05/"],["01.linux/06.linux修改网卡为eth0的两种方法.md","linux修改网卡为eth0的两种方法","/pages/c17655/"],["01.linux/07.Logrotate入门了解及生产实践.md","Logrotate入门了解及生产实践","/pages/a8c469/"]]},{title:"windows",collapsable:!0,children:[["02.windows/01.windows支持多用户远程登录.md","windows支持多用户远程登录","/pages/18b06c/"],["02.windows/02.windows应用服务部署脚本.md","windows应用服务部署脚本","/pages/adbe78/"]]},{title:"中间件",collapsable:!0,children:[{title:"nginx",collapsable:!0,children:[["03.中间件/01.nginx/01.nginx配置教程.md","nginx配置教程","/pages/5ed327/"],["03.中间件/01.nginx/02.nginx常用配置.md","nginx常用配置","/pages/df9ea8/"]]},{title:"kafka",collapsable:!0,children:[["03.中间件/02.kafka/01.kafka介绍和常见操作.md","kafka介绍和常见操作","/pages/98b071/"],["03.中间件/02.kafka/02.docker-compose安装kafka集群.md","docker-compose安装kafka集群","/pages/43f361/"],["03.中间件/02.kafka/03.kafka工作原理.md","kafka工作原理","/pages/b73d79/"]]},["03.中间件/03.apollo部署.md","apollo部署","/pages/fd6cf8/"]]},{title:"网络",collapsable:!0,children:[["04.网络/01.抓包工具 tcpdump 用法说明.md","抓包工具 tcpdump 用法说明","/pages/2f613d/"],["04.网络/02.网络代理.md","网络代理","/pages/5933ee/"],["04.网络/03.网络工具.md","网络工具","/pages/38ce88/"]]},{title:"安全",collapsable:!0,children:[["05.安全/01.docker部署openvas.md","docker部署openvas","/pages/7f4bdd/"],["05.安全/02.白帽子讲web安全.md","白帽子讲web安全","/pages/fcba46/"],["05.安全/03.ssh安全.md","ssh安全","/pages/3913e5/"],["05.安全/04.系统安全及应用基础.md","系统安全及应用基础","/pages/8dcd54/"]]},{title:"存储",collapsable:!0,children:[["06.存储/01.fastdfs.md","fastdfs","/pages/988870/"],["06.存储/02.glusterfs.md","glusterfs","/pages/70e7ad/"],{title:"ceph",collapsable:!0,children:[["06.存储/03.ceph/01.部署ceph集群 Nautilus版.md","部署ceph集群 Nautilus版","/pages/88a4de/"]]}]},{title:"防火墙",collapsable:!0,children:[["07.防火墙/01.centos7下配置firewalld实现nat转发软路由.md","centos7下配置firewalld实现nat转发软路由","/pages/ce46b6/"]]},{title:"数据库",collapsable:!0,children:[{title:"mysql",collapsable:!0,children:[["08.数据库/01.mysql/01.数据库二进制安装.md","数据库二进制安装","/pages/1f5460/"]]},{title:"mongodb",collapsable:!0,children:[["08.数据库/02.mongodb/01.mongodb相关概念.md","mongodb相关概念","/pages/598be1/"],["08.数据库/02.mongodb/02.mongodb副本集.md","mongodb副本集","/pages/359db9/"]]},{title:"oracle",collapsable:!0,children:[["08.数据库/03.oracle/01.oracle数据库安装.md","oracle数据库安装","/pages/e977f3/"]]}]},{title:"系统",collapsable:!0,children:[{title:"vmware",collapsable:!0,children:[["09.系统/01.vmware/01.服务器虚拟化VMware ESXI搭建集群.md","服务器虚拟化VMware ESXI搭建集群","/pages/26f193/"]]},{title:"ftp",collapsable:!0,children:[["09.系统/02.ftp/01.proftpd环境部署.md","proftpd环境部署","/pages/affcf2/"]]},{title:"nexus",collapsable:!0,children:[["09.系统/03.nexus/01.nexus安装.md","nexus安装","/pages/af4fce/"],["09.系统/03.nexus/02.使用nexus3配置npm私有仓库.md","使用nexus3配置npm私有仓库","/pages/b851fe/"],["09.系统/03.nexus/03.使用nexus3配置maven私有仓库.md","使用nexus3配置maven私有仓库","/pages/12e16a/"]]}]},{title:"docker",collapsable:!0,children:[["10.docker/01.Docker构建镜像.md","Docker构建镜像","/pages/83a393/"],["10.docker/02.docker和docker-compose安装.md","docker和docker-compose安装","/pages/217e32/"],["10.docker/03.如何选择docker基础镜像.md","如何选择docker基础镜像","/pages/8884ac/"],["10.docker/04.常见的dockerfile汇总.md","常见的dockerfile汇总","/pages/382a6c/"],["10.docker/05.基于官方php7-2-34镜像构建生产可用镜像.md","基于官方php7.2.34镜像构建生产可用镜像","/pages/6e3cb9/"]]},{title:"other",collapsable:!0,children:[["11.other/01.debian中科大替换教程.md","debian中科大替换教程","/pages/767641/"],["11.other/02.Alpine安装php各种扩展.md","Alpine安装php各种扩展","/pages/aa9eee/"]]},{title:"监控",collapsable:!0,children:[{title:"zabbix",collapsable:!0,children:[["12.监控/01.zabbix/01.zabbix添加证书监控.md","zabbix添加证书监控","/pages/3dac52/"],["12.监控/01.zabbix/02.zabbix添加端口和进程监控.md","zabbix添加端口和进程监控","/pages/a0a3ae/"],["12.监控/01.zabbix/03.docker文件安装zabbix5.md","docker文件安装zabbix5","/pages/e0085e/"],["12.监控/01.zabbix/04.zabbix配置钉钉告警.md","zabbix配置钉钉告警","/pages/a4e3ce/"],["12.监控/01.zabbix/05.zabbix添加日志监控.md","zabbix添加日志监控","/pages/334bec/"],["12.监控/01.zabbix/06.zabbix添加进程pid监控.md","zabbix添加进程pid监控","/pages/20f905/"],["12.监控/01.zabbix/07.zabbix监控windows进程.md","zabbix监控windows进程","/pages/269839/"],["12.监控/01.zabbix/08.zabbix添加web监控.md","zabbix添加web监控","/pages/b70eb8/"],["12.监控/01.zabbix/09.centos7编译安装zabbix proxy端.md","centos7编译安装zabbix5.0 proxy端","/pages/723e7e/"],["12.监控/01.zabbix/10.rpm安装zabbix proxy过程简记.md","rpm安装zabbix proxy过程简记","/pages/ef347e/"]]},{title:"prometheus",collapsable:!0,children:[["12.监控/02.prometheus/01.prometheus监控、告警与存储.md","prometheus监控、告警与存储","/pages/40794d/"],["12.监控/02.prometheus/02.使用docker-compose搭建promethes+grafana监控系统.md","使用docker-compose搭建promethes+grafana监控系统","/pages/712f5c/"],["12.监控/02.prometheus/03.prometheus添加钉钉消息告警.md","prometheus添加钉钉消息告警","/pages/22b836/"],["12.监控/02.prometheus/04.alertmanager实现某个时间段静默某些告警项.md","alertmanager实现某个时间段静默某些告警项","/pages/96c6ce/"]]}]}],"/05.关于/":[["01.关于.md","关于","/about/"]],"/06.友链/":[["01.友链.md","友链","/friends"]]},blogInfo:{blogCreate:"2019-09-20",indexView:!0,pageView:!0,mdFileCountType:"archives",totalWords:"archives",eachFileWords:[{name:"ansible",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\01.ansible.md",wordsCount:46,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"01.专题/01.ansible系列文章",description:"ansible"}},title:"ansible",date:"2022-12-15T10:37:52.000Z",permalink:"/ansible/",categories:["目录页"],tags:[null]},{name:"k8s",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\02.k8s.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"01.专题/02.k8s",description:"k8s"}},title:"k8s",date:"2022-12-15T14:40:52.000Z",permalink:"/k8s/",categories:["目录页"],tags:[null]},{name:"elk",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\03.elk.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"01.专题/03.elk",description:"elk"}},title:"elk",date:"2022-12-15T10:41:32.000Z",permalink:"/elk/",categories:["目录页"],tags:[null]},{name:"jenkins",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\04.jenkins.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"01.专题/06.jenkins",description:"jenkins"}},title:"jenkins",date:"2022-12-15T14:43:23.000Z",permalink:"/jenkins/",categories:["目录页"],tags:[null]},{name:"随笔",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\05.随笔.md",wordsCount:45,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"02.生活/01.随笔",description:"随笔"}},title:"随笔",date:"2022-12-15T14:43:23.000Z",permalink:"/suibi/",categories:["目录页"],tags:[null]},{name:"面试",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\06.面试.md",wordsCount:45,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"02.生活/02.面试",description:"面试"}},title:"面试",date:"2022-12-15T14:43:23.000Z",permalink:"/mianshi/",categories:["目录页"],tags:[null]},{name:"工具",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\07.工具.md",wordsCount:45,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"02.生活/03.工具",description:"工具"}},title:"工具",date:"2022-12-15T14:43:23.000Z",permalink:"/gongju/",categories:["目录页"],tags:[null]},{name:"生活",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\08.生活.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"02.生活",description:"生活"}},title:"生活",date:"2022-12-15T14:43:23.000Z",permalink:"/life/",categories:["目录页"],tags:[null]},{name:"编程",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\09.编程.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"03.编程",description:"编程"}},title:"编程",date:"2022-12-15T14:43:23.000Z",permalink:"/code/",categories:["目录页"],tags:[null]},{name:"python",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\10.python.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"03.编程/01.python",description:"python"}},title:"python",date:"2022-12-15T14:43:23.000Z",permalink:"/python/",categories:["目录页"],tags:[null]},{name:"shell",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\11.shell.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"03.编程/02.shell",description:"shell"}},title:"shell",date:"2022-12-15T14:43:23.000Z",permalink:"/shell/",categories:["目录页"],tags:[null]},{name:"专题",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\12.专题.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"01.专题",description:"专题"}},title:"专题",date:"2022-12-15T14:43:23.000Z",permalink:"/topic/",categories:["目录页"],tags:[null]},{name:"运维",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\13.运维.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维",description:"运维"}},title:"运维",date:"2022-12-15T14:43:23.000Z",permalink:"/ops/",categories:["目录页"],tags:[null]},{name:"中间件",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\14.中间件.md",wordsCount:48,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/03.中间件",description:"中间件"}},title:"中间件",date:"2022-12-15T14:43:23.000Z",permalink:"/middleware/",categories:["目录页"],tags:[null]},{name:"linux",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\15.linux.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/01.linux",description:"linux"}},title:"linux",date:"2022-12-15T14:43:23.000Z",permalink:"/linux/",categories:["目录页"],tags:[null]},{name:"windows",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\16.windows.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/02.windows",description:"windows"}},title:"windows",date:"2022-12-15T14:43:23.000Z",permalink:"/windows/",categories:["目录页"],tags:[null]},{name:"网络",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\17.网络.md",wordsCount:45,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/04.网络",description:"网络"}},title:"网络",date:"2022-12-15T14:43:23.000Z",permalink:"/network/",categories:["目录页"],tags:[null]},{name:"安全",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\18.安全.md",wordsCount:45,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/05.安全",description:"安全"}},title:"安全",date:"2022-12-15T14:43:23.000Z",permalink:"/safety/",categories:["目录页"],tags:[null]},{name:"存储",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\19.存储.md",wordsCount:45,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/06.存储",description:"存储"}},title:"存储",date:"2022-12-15T14:43:23.000Z",permalink:"/storage/",categories:["目录页"],tags:[null]},{name:"防火墙",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\20.防火墙.md",wordsCount:48,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/07.防火墙",description:"防火墙"}},title:"防火墙",date:"2022-12-15T14:43:23.000Z",permalink:"/firewalld/",categories:["目录页"],tags:[null]},{name:"数据库",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\21.数据库.md",wordsCount:48,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/08.数据库",description:"数据库"}},title:"数据库",date:"2022-12-15T14:43:23.000Z",permalink:"/db/",categories:["目录页"],tags:[null]},{name:"系统",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\22.系统.md",wordsCount:45,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/09.系统",description:"系统"}},title:"系统",date:"2022-12-15T14:43:23.000Z",permalink:"/sys/",categories:["目录页"],tags:[null]},{name:"docker",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\23.docker.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/10.docker",description:"docker"}},title:"docker",date:"2022-12-15T14:43:23.000Z",permalink:"/docker/",categories:["目录页"],tags:[null]},{name:"运维工具",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\24.运维工具.md",wordsCount:47,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"02.生活/03.工具",description:"工具"}},title:"运维工具",date:"2022-12-15T14:43:23.000Z",permalink:"/tool/",categories:["目录页"],tags:[null]},{name:"监控",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\25.监控.md",wordsCount:45,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/12.监控",description:"监控"}},title:"监控",date:"2022-12-15T14:43:23.000Z",permalink:"/monitor/",categories:["目录页"],tags:[null]},{name:"other",filePath:"D:\\blog\\blog-master\\docs\\00.目录页\\26.other.md",wordsCount:42,readingTime:"1",pageComponent:{name:"Catalogue",data:{path:"04.运维/11.other",description:"other"}},title:"other",date:"2022-12-15T14:43:23.000Z",permalink:"/other/",categories:["目录页"],tags:[null]},{name:"ansible入门",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\01.ansible系列文章\\01.ansible入门.md",wordsCount:"6.8k",readingTime:"30.7m",title:"ansible入门",date:null,permalink:"/pages/12c5da01/",categories:["系列专题","ansible"],tags:["ansible"],description:null},{name:"anisble批量安装node_exporter",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\01.ansible系列文章\\02.anisble批量安装node_exporter.md",wordsCount:361,readingTime:"1.9m",title:"anisble批量安装node_exporter",categories:["系列专题","ansible"],tags:["ansible"],date:"2022-12-09T20:53:14.000Z",permalink:"/pages/f8f66c/"},{name:"ansible-playbook中的变量",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\01.ansible系列文章\\03.ansible-playbook中的变量.md",wordsCount:"3.2k",readingTime:"15.3m",title:"ansible-playbook中的变量",categories:["系列专题","ansible"],tags:["ansible"],date:"2022-12-09T20:53:14.000Z",permalink:"/pages/d740bc/"},{name:"Ansible性能优化——提升ansible执行效率",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\01.ansible系列文章\\04.Ansible性能优化——提升ansible执行效率.md",wordsCount:"3k",readingTime:"12.9m",title:"Ansible性能优化——提升ansible执行效率",categories:["ansible"],tags:["ansible"],date:"2022-12-09T20:53:07.000Z",permalink:"/pages/a06506/"},{name:"ansible之roles简单使用",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\01.ansible系列文章\\05.ansible之roles简单使用.md",wordsCount:"1.1k",readingTime:"5m",title:"ansible之roles简单使用",categories:["系列专题","ansible"],tags:["ansible"],date:"2022-12-09T20:53:07.000Z",permalink:"/pages/847542/"},{name:"ansible中template简单使用",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\01.ansible系列文章\\06.ansible中template简单使用.md",wordsCount:"1.5k",readingTime:"7.1m",title:"ansible中template简单使用",categories:["系列专题","ansible"],tags:["ansible"],date:"2022-12-09T20:53:07.000Z",permalink:"/pages/4189b8/"},{name:"Ansible中的playbook 详解",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\01.ansible系列文章\\07.Ansible中的playbook 详解.md",wordsCount:"8.3k",readingTime:"40.5m",title:"playbook详解",categories:["系列专题","ansible"],tags:["ansible"],date:"2022-12-09T20:53:07.000Z",permalink:"/pages/14eda9/"},{name:"ansible-playbook编排使用tips",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\01.ansible系列文章\\08.ansible-playbook编排使用tips.md",wordsCount:"2.2k",readingTime:"10m",title:"ansible-playbook编排使用tips",date:"2023-01-11T15:06:48.000Z",permalink:"/pages/7eca6b/",categories:["专题","ansible系列文章"],tags:[null]},{name:"ansible优秀案例",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\01.ansible系列文章\\09.ansible优秀案例.md",wordsCount:144,readingTime:"1",title:"ansible优秀案例",date:"2023-01-29T15:16:58.000Z",permalink:"/pages/4fbba1/",categories:["专题","ansible系列文章"],tags:[null]},{name:"快速部署k8s集群",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\02.k8s\\01.快速部署k8s集群.md",wordsCount:878,readingTime:"4.2m",title:"快速部署k8s集群",categories:["k8s"],tags:["k8s"],date:"2022-12-09T20:51:06.000Z",permalink:"/pages/fc6bfb/"},{name:"elk安装",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\03.elk\\01.elk安装.md",wordsCount:"1.3k",readingTime:"5.9m",title:"elk安装",categories:"elk",tags:["elk"],date:"2022-12-09T20:50:18.000Z",permalink:"/pages/98c9f5/"},{name:"docker-compose安装elk",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\03.elk\\02.docker-compose安装elk.md",wordsCount:386,readingTime:"2.3m",title:"docker-compose安装elk",date:"2023-01-13T16:45:28.000Z",permalink:"/pages/bbe108/",categories:["专题","elk"],tags:[null]},{name:"filebeat-log相关配置指南",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\03.elk\\03.filebeat-log相关配置指南.md",wordsCount:"3.7k",readingTime:"14.6m",title:"filebeat-log相关配置指南",date:"2023-02-23T09:01:59.000Z",permalink:"/pages/ed9e8d/",categories:["专题","elk"],tags:[null]},{name:"jenkins容器安装",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\04.jenkins\\01.jenkins容器安装.md",wordsCount:218,readingTime:"1.3m",title:"jenkins容器安装",date:"2022-12-15T14:01:31.000Z",permalink:"/pages/ce2b89/",categories:["专题","jenkins"],tags:[null]},{name:"jenkins流水线部署",filePath:"D:\\blog\\blog-master\\docs\\01.专题\\04.jenkins\\02.jenkins流水线部署.md",wordsCount:"1.4k",readingTime:"8m",title:"jenkins流水线部署",date:"2023-01-06T14:41:01.000Z",permalink:"/pages/a9ff44/",categories:["专题","jenkins"],tags:[null]},{name:"男人心智成熟的九大表现",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\01.随笔\\01.男人心智成熟的九大表现.md",wordsCount:"1k",readingTime:"3.5m",title:"男人心智成熟的九大表现",categories:"随笔",tags:["随笔"],date:"2022-12-09T20:46:02.000Z",permalink:"/pages/aa6ada/"},{name:"如果面试时大家都说真话",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\01.随笔\\02.如果面试时大家都说真话.md",wordsCount:583,readingTime:"2m",title:"如果面试时大家都说真话",date:"2020-01-06T14:28:22.000Z",categories:"随笔",tags:["面试题集"],permalink:"/pages/0cd089/"},{name:"这四个故事小段，够你受用一生",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\01.随笔\\03.这四个故事小段，够你受用一生.md",wordsCount:557,readingTime:"1.9m",title:"这四个故事小段，够你受用一生",date:"2023-02-23T12:14:52.000Z",permalink:"/pages/7b0831/",categories:["生活","随笔"],tags:[null]},{name:"小说活着摘录",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\01.随笔\\04.小说活着摘录.md",wordsCount:"2.3k",readingTime:"7.8m",title:"小说活着摘录",date:"2023-02-24T16:40:24.000Z",permalink:"/pages/e7e74b/",categories:["生活","随笔"],tags:[null]},{name:"人生格言",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\01.随笔\\05.人生格言.md",wordsCount:113,readingTime:"1",title:"人生格言",date:"2023-02-24T16:42:33.000Z",permalink:"/pages/58b3c0/",categories:["生活","随笔"],tags:[null]},{name:"运维10道基础面试题",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\02.面试\\01.运维10道基础面试题.md",wordsCount:956,readingTime:"3.9m",title:"运维10道基础面试题",date:"2020-01-13T14:28:22.000Z",categories:["面试"],tags:["面试题集"],permalink:"/pages/f6c9c9/"},{name:"http状态码",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\02.面试\\02.http状态码.md",wordsCount:183,readingTime:"1",title:"http状态码",date:"2020-01-06T14:28:22.000Z",categories:"技术",tags:["面试题集"],permalink:"/pages/a2e381/"},{name:"高级运维工程需要掌握的技能",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\02.面试\\03.高级运维工程需要掌握的技能.md",wordsCount:826,readingTime:"3.1m",title:"高级运维工程需要掌握的技能",date:"2022-12-15T10:29:16.000Z",permalink:"/pages/9245d9/",categories:["专题","面试"],tags:[null]},{name:"企业级openvpn搭建",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\03.工具\\01.vpn\\01.企业级openvpn搭建.md",wordsCount:"2.6k",readingTime:"12.6m",title:"企业级openvpn搭建",date:"2022-12-15T19:13:50.000Z",permalink:"/pages/45fa3c/",categories:["运维","vpn"],tags:[null]},{name:"实现V2Ray通过CloudFlare自选IP加速",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\03.工具\\02.翻墙教程\\01.实现V2Ray通过CloudFlare自选IP加速.md",wordsCount:641,readingTime:"2.8m",title:"实现V2Ray通过CloudFlare自选IP加速",date:"2022-12-15T19:05:28.000Z",permalink:"/pages/e9bb88/",categories:["运维","翻墙教程"],tags:[null]},{name:"mac电脑常用软件",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\03.工具\\03.mac电脑常用软件.md",wordsCount:60,readingTime:"1",title:"mac电脑常用软件",date:"2022-12-15T14:59:50.000Z",permalink:"/pages/4ad301/",categories:["生活","工具"],tags:[null]},{name:"个人工具链接",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\03.工具\\04.个人工具链接.md",wordsCount:58,readingTime:"1",title:"工具链接",date:"2022-12-16T14:39:26.000Z",permalink:"/pages/8994e9/",categories:["生活","工具"],tags:[null]},{name:"vuepress配置artalk",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\03.工具\\05.vuepress配置artalk.md",wordsCount:306,readingTime:"1.5m",title:"vuepress配置artalk",date:"2023-01-11T14:19:31.000Z",permalink:"/pages/468f43/",categories:["生活","工具"],tags:[null]},{name:"windows目录实时同步工具",filePath:"D:\\blog\\blog-master\\docs\\02.生活\\03.工具\\06.windows目录实时同步工具.md",wordsCount:42,readingTime:"1",title:"windows目录实时同步工具",date:"2022-12-15T15:00:54.000Z",permalink:"/pages/112008/",categories:["生活","工具"],tags:[null]},{name:"监控目录或文件变化",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\01.python\\01.监控目录或文件变化.md",wordsCount:373,readingTime:"1.7m",title:"监控目录或文件变化",date:"2022-12-13T18:01:34.000Z",permalink:"/pages/f8da74/",categories:["编程","python"],tags:[null]},{name:"批量更改文件",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\01.python\\02.批量更改文件.md",wordsCount:82,readingTime:"1",title:"批量更改文件",date:"2022-12-13T18:01:34.000Z",permalink:"/pages/f659cd/",categories:["编程","python"],tags:[null]},{name:"python引用数据库",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\01.python\\03.python引用数据库.md",wordsCount:424,readingTime:"2.3m",title:"python引用数据库",date:"2022-12-13T18:35:51.000Z",permalink:"/pages/a109ec/",categories:["编程","python"],tags:[null]},{name:"python3给防火墙添加放行",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\01.python\\04.python3给防火墙添加放行.md",wordsCount:258,readingTime:"1.5m",title:"python3给防火墙添加放行",date:"2022-12-14T14:51:21.000Z",permalink:"/pages/457fb7/",categories:["编程","python"],tags:[null]},{name:"python生成部署脚本",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\01.python\\05.python生成部署脚本.md",wordsCount:"1.7k",readingTime:"10.6m",title:"python生成部署脚本",date:"2023-01-06T11:10:56.000Z",permalink:"/pages/d0fcb5/",categories:["编程","python"],tags:[null]},{name:"python将多个文件内容输出到一个文件中",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\01.python\\06.python将多个文件内容输出到一个文件中.md",wordsCount:224,readingTime:"1.1m",title:"python将多个文件内容输出到一个文件中",date:"2023-01-09T17:43:27.000Z",permalink:"/pages/8d0b76/",categories:["编程","python"],tags:[null]},{name:"进程pid判断脚本",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\02.shell\\01.进程pid判断脚本.md",wordsCount:244,readingTime:"1.2m",title:"进程pid判断脚本",date:"2022-12-14T14:30:33.000Z",permalink:"/pages/cae02d/",categories:["编程","shell"],tags:[null]},{name:"日志切割脚本",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\02.shell\\02.日志切割脚本.md",wordsCount:86,readingTime:"1",title:"日志切割脚本",date:"2022-12-15T12:32:31.000Z",permalink:"/pages/3b937e/",categories:["编程","shell"],tags:[null]},{name:"设置跳板机脚本",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\02.shell\\03.设置跳板机脚本.md",wordsCount:261,readingTime:"1.5m",title:"设置跳板机脚本",date:"2022-12-15T12:41:13.000Z",permalink:"/pages/69b699/",categories:["编程","shell"],tags:[null]},{name:"编写启动、停止、重启的脚本",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\02.shell\\04.编写启动、停止、重启的脚本.md",wordsCount:148,readingTime:"1",title:"编写启动、停止、重启的脚本",date:"2022-12-21T18:07:20.000Z",permalink:"/pages/fe0782/",categories:["编程","shell"],tags:[null]},{name:"mysql数据库备份的三种方式",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\02.shell\\05.mysql数据库备份的三种方式.md",wordsCount:799,readingTime:"4.1m",title:"mysql数据库备份的三种方式",date:"2023-01-04T14:22:29.000Z",permalink:"/pages/93dcc7/",categories:["编程","shell"],tags:[null]},{name:"jenkins编译服务脚本",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\02.shell\\06.jenkins编译服务脚本.md",wordsCount:433,readingTime:"2.5m",title:"jenkins编译服务脚本",date:"2023-01-06T11:01:07.000Z",permalink:"/pages/2019f8/",categories:["编程","shell"],tags:[null]},{name:"app编译脚本",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\02.shell\\07.app编译脚本.md",wordsCount:"1.4k",readingTime:"8.2m",title:"app编译脚本",date:"2023-01-06T11:06:30.000Z",permalink:"/pages/fd3678/",categories:["编程","shell"],tags:[null]},{name:"常用shell脚本",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\02.shell\\08.常用shell脚本.md",wordsCount:"1.6k",readingTime:"8.4m",title:"常用shell脚本",date:"2023-02-24T15:30:36.000Z",permalink:"/pages/8fdac1/",categories:["编程","shell"],tags:[null]},{name:"字符串的截取拼接",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\02.shell\\09.字符串的截取拼接.md",wordsCount:828,readingTime:"3.7m",title:"字符串的截取拼接",date:"2023-02-24T17:28:59.000Z",permalink:"/pages/f5c376/",categories:["编程","shell"],tags:[null]},{name:"shell基础",filePath:"D:\\blog\\blog-master\\docs\\03.编程\\02.shell\\10.shell基础.md",wordsCount:"5k",readingTime:"20.3m",title:"shell基础",date:"2023-02-24T17:31:50.000Z",permalink:"/pages/665355/",categories:["编程","shell"],tags:[null]},{name:"rsync用法及参数详解",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\01.linux\\01.rsync\\01.rsync用法及参数详解.md",wordsCount:"1.9k",readingTime:"7.2m",title:"rsync用法及参数详解",categories:["rsync"],tags:["rsync"],date:"2022-12-09T20:51:06.000Z",permalink:"/pages/98cac7/",feed:{enable:!0},description:"Logrotate入门了解及生产实践"},{name:"rsync服务实现推送，拉取",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\01.linux\\01.rsync\\02.rsync服务实现推送，拉取.md",wordsCount:"1.2k",readingTime:"5.3m",title:"rsync服务实现推送，拉取",categories:["rsync"],tags:["rsync"],date:"2022-12-09T20:51:06.000Z",permalink:"/pages/c7f1bc/",feed:{enable:!0},description:"Logrotate入门了解及生产实践"},{name:"centos7搭建dns,bind配置",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\01.linux\\02.dns\\01.centos7搭建dns,bind配置.md",wordsCount:969,readingTime:"4.7m",title:"centos7搭建dns,bind配置",date:"2022-12-15T19:18:37.000Z",permalink:"/pages/78c801/",categories:["运维","dns"],tags:[null],feed:{enable:!0},description:"Logrotate入门了解及生产实践"},{name:"sed命令在文本每行,行尾或行首添加字符",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\01.linux\\03.sed、awk、grep、find四剑客\\01.sed命令在文本每行,行尾或行首添加字符.md",wordsCount:370,readingTime:"1.5m",title:"sed命令在文本每行,行尾或行首添加字符",date:"2022-12-21T20:56:09.000Z",permalink:"/pages/4baba0/",categories:["运维","linux","sed、awk、grep、find四剑客"],tags:[null],feed:{enable:!0},description:"Logrotate入门了解及生产实践"},{name:"LVM管理",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\01.linux\\04.LVM管理.md",wordsCount:"4k",readingTime:"21m",title:"LVM管理",date:"2022-12-15T17:38:10.000Z",permalink:"/pages/5f261c/",categories:["运维","linux系统"],tags:[null],feed:{enable:!0},description:null},{name:"sudo权限规划",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\01.linux\\05.sudo权限规划.md",wordsCount:"2.7k",readingTime:"11.8m",title:"sudo权限规划",date:"2022-12-15T18:49:07.000Z",permalink:"/pages/0bac05/",categories:["运维","linux系统"],tags:[null],feed:{enable:!0},description:"sudo权限规划"},{name:"linux修改网卡为eth0的两种方法",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\01.linux\\06.linux修改网卡为eth0的两种方法.md",wordsCount:255,readingTime:"1.1m",title:"linux修改网卡为eth0的两种方法",date:"2022-12-21T14:34:00.000Z",permalink:"/pages/c17655/",categories:["运维","linux"],tags:[null],feed:{enable:!0},description:"sudo权限规划"},{name:"Logrotate入门了解及生产实践",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\01.linux\\07.Logrotate入门了解及生产实践.md",wordsCount:"1.5k",readingTime:"5.7m",title:"Logrotate入门了解及生产实践",date:"2023-01-11T15:19:54.000Z",permalink:"/pages/a8c469/",categories:["运维","linux"],tags:[null],feed:{enable:!0},description:"Logrotate入门了解及生产实践"},{name:"windows支持多用户远程登录",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\02.windows\\01.windows支持多用户远程登录.md",wordsCount:"1k",readingTime:"3.8m",title:"windows支持多用户远程登录",date:"2022-12-15T18:56:36.000Z",permalink:"/pages/18b06c/",categories:["运维","windows系统"],tags:[null]},{name:"windows应用服务部署脚本",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\02.windows\\02.windows应用服务部署脚本.md",wordsCount:287,readingTime:"1.6m",title:"windows应用服务部署脚本",date:"2023-02-28T22:13:50.000Z",permalink:"/pages/adbe78/",categories:["运维","windows"],tags:[null]},{name:"nginx配置教程",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\03.中间件\\01.nginx\\01.nginx配置教程.md",wordsCount:"10.4k",readingTime:"45.3m",title:"nginx配置教程",categories:"nginx",tags:["nginx"],date:"2022-12-09T20:48:36.000Z",permalink:"/pages/5ed327/"},{name:"nginx常用配置",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\03.中间件\\01.nginx\\02.nginx常用配置.md",wordsCount:"3.1k",readingTime:"14m",title:"nginx常用配置",date:"2023-02-09T20:26:18.000Z",permalink:"/pages/df9ea8/",categories:["运维","中间件","nginx"],tags:[null]},{name:"kafka介绍和常见操作",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\03.中间件\\02.kafka\\01.kafka介绍和常见操作.md",wordsCount:"1k",readingTime:"4.6m",title:"kafka介绍和常见操作",date:"2023-02-23T09:16:02.000Z",permalink:"/pages/98b071/",categories:["运维","中间件","kafka"],tags:[null]},{name:"docker-compose安装kafka集群",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\03.中间件\\02.kafka\\02.docker-compose安装kafka集群.md",wordsCount:189,readingTime:"1.1m",title:"docker-compose安装kafka集群",date:"2023-01-13T16:48:12.000Z",permalink:"/pages/43f361/",categories:["运维","系统"],tags:[null]},{name:"kafka工作原理",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\03.中间件\\02.kafka\\03.kafka工作原理.md",wordsCount:"1.3k",readingTime:"5.1m",title:"kafka工作原理",date:"2023-03-01T14:14:49.000Z",permalink:"/pages/b73d79/",categories:["运维","中间件","kafka"],tags:[null]},{name:"apollo部署",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\03.中间件\\03.apollo部署.md",wordsCount:"5.5k",readingTime:"23.8m",title:"apollo部署",date:"2022-12-15T19:20:45.000Z",permalink:"/pages/fd6cf8/",categories:["运维","程序"],tags:[null]},{name:"抓包工具 tcpdump 用法说明",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\04.网络\\01.抓包工具 tcpdump 用法说明.md",wordsCount:"2.7k",readingTime:"12.3m",title:"抓包工具 tcpdump 用法说明",date:"2022-12-15T19:00:25.000Z",permalink:"/pages/2f613d/",categories:["运维","网络"],tags:[null]},{name:"网络代理",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\04.网络\\02.网络代理.md",wordsCount:"4.9k",readingTime:"21.5m",title:"网络代理",date:"2023-02-20T09:19:56.000Z",permalink:"/pages/5933ee/",categories:["运维","网络"],tags:[null]},{name:"网络工具",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\04.网络\\03.网络工具.md",wordsCount:148,readingTime:"1",title:"网络工具",date:"2023-02-20T09:33:21.000Z",permalink:"/pages/38ce88/",categories:["运维","网络"],tags:[null]},{name:"docker部署openvas",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\05.安全\\01.docker部署openvas.md",wordsCount:611,readingTime:"2.5m",title:"docker部署openvas",date:"2022-12-15T19:21:33.000Z",permalink:"/pages/7f4bdd/",categories:["运维","安全"],tags:[null]},{name:"白帽子讲web安全",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\05.安全\\02.白帽子讲web安全.md",wordsCount:"1.2k",readingTime:"4.1m",title:"白帽子讲web安全",date:"2023-02-24T15:38:46.000Z",permalink:"/pages/fcba46/",categories:["运维","安全"],tags:[null]},{name:"ssh安全",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\05.安全\\03.ssh安全.md",wordsCount:390,readingTime:"2m",title:"ssh安全",date:"2023-02-24T15:44:20.000Z",permalink:"/pages/3913e5/",categories:["运维","安全"],tags:[null]},{name:"系统安全及应用基础",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\05.安全\\04.系统安全及应用基础.md",wordsCount:"2.1k",readingTime:"9m",title:"系统安全及应用基础",date:"2023-02-24T15:49:26.000Z",permalink:"/pages/8dcd54/",categories:["运维","安全"],tags:[null]},{name:"fastdfs",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\06.存储\\01.fastdfs.md",wordsCount:"6.1k",readingTime:"26.6m",title:"fastdfs",categories:"fastdfs",tags:["分布式文件系统"],date:"2022-12-09T20:47:55.000Z",permalink:"/pages/988870/"},{name:"glusterfs",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\06.存储\\02.glusterfs.md",wordsCount:"2k",readingTime:"9.5m",title:"glusterfs",categories:"glusterfs",tags:["分布式文件系统"],date:"2022-12-09T20:47:55.000Z",permalink:"/pages/70e7ad/"},{name:"部署ceph集群 Nautilus版",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\06.存储\\03.ceph\\01.部署ceph集群 Nautilus版.md",wordsCount:"3.8k",readingTime:"17.8m",title:"部署ceph集群 Nautilus版",date:"2023-03-01T15:47:49.000Z",permalink:"/pages/88a4de/",categories:["运维","存储","ceph"],tags:[null]},{name:"centos7下配置firewalld实现nat转发软路由",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\07.防火墙\\01.centos7下配置firewalld实现nat转发软路由.md",wordsCount:932,readingTime:"4.1m",title:"centos7下配置firewalld实现nat转发软路由",date:"2022-12-15T19:03:12.000Z",permalink:"/pages/ce46b6/",categories:["运维","防火墙"],tags:[null]},{name:"数据库二进制安装",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\08.数据库\\01.mysql\\01.数据库二进制安装.md",wordsCount:"2.1k",readingTime:"10.2m",title:"数据库二进制安装",categories:"mysql",tags:["mysql"],date:"2022-12-09T20:49:19.000Z",permalink:"/pages/1f5460/"},{name:"mongodb相关概念",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\08.数据库\\02.mongodb\\01.mongodb相关概念.md",wordsCount:"12.3k",readingTime:"49.8m",title:"mongodb相关概念",categories:"mongodb",tags:["mongodb"],date:"2022-12-09T20:50:33.000Z",permalink:"/pages/598be1/"},{name:"mongodb副本集",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\08.数据库\\02.mongodb\\02.mongodb副本集.md",wordsCount:"23.9k",readingTime:"1h43m",title:"mongodb副本集",categories:"mongodb",tags:["mongodb"],date:"2022-12-09T20:50:33.000Z",permalink:"/pages/359db9/"},{name:"oracle数据库安装",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\08.数据库\\03.oracle\\01.oracle数据库安装.md",wordsCount:"1.9k",readingTime:"8.9m",title:"oracle数据库安装",date:"2023-03-01T15:15:52.000Z",permalink:"/pages/e977f3/",categories:["运维","数据库","oracle"],tags:[null]},{name:"服务器虚拟化VMware ESXI搭建集群",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\09.系统\\01.vmware\\01.服务器虚拟化VMware ESXI搭建集群.md",wordsCount:"2.8k",readingTime:"12.4m",title:"服务器虚拟化VMware ESXI搭建集群",date:"2022-12-16T15:57:11.000Z",permalink:"/pages/26f193/",categories:["运维","系统","vmware"],tags:[null]},{name:"proftpd环境部署",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\09.系统\\02.ftp\\01.proftpd环境部署.md",wordsCount:"2.9k",readingTime:"13.4m",title:"proftpd环境部署",date:"2022-12-21T17:59:19.000Z",permalink:"/pages/affcf2/",categories:["运维","系统","ftp"],tags:[null]},{name:"nexus安装",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\09.系统\\03.nexus\\01.nexus安装.md",wordsCount:"1.9k",readingTime:"7.6m",title:"nexus安装",date:"2023-02-17T10:48:27.000Z",permalink:"/pages/af4fce/",categories:["运维","系统","nexus"],tags:[null]},{name:"使用nexus3配置npm私有仓库",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\09.系统\\03.nexus\\02.使用nexus3配置npm私有仓库.md",wordsCount:"1.2k",readingTime:"5.3m",title:"使用nexus3配置npm私有仓库",date:"2023-02-20T10:57:40.000Z",permalink:"/pages/b851fe/",categories:["运维","系统","nexus"],tags:[null]},{name:"使用nexus3配置maven私有仓库",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\09.系统\\03.nexus\\03.使用nexus3配置maven私有仓库.md",wordsCount:"2.3k",readingTime:"12.5m",title:"使用nexus3配置maven私有仓库",date:"2023-02-20T12:03:31.000Z",permalink:"/pages/12e16a/",categories:["运维","系统","nexus"],tags:[null]},{name:"Docker构建镜像",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\10.docker\\01.Docker构建镜像.md",wordsCount:"8.4k",readingTime:"34.2m",title:"Docker构建镜像",date:"2023-02-24T16:15:35.000Z",permalink:"/pages/83a393/",categories:["运维","docker"],tags:[null]},{name:"docker和docker-compose安装",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\10.docker\\02.docker和docker-compose安装.md",wordsCount:331,readingTime:"1.7m",title:"docker和docker-compose安装",date:"2022-12-15T18:52:58.000Z",permalink:"/pages/217e32/",categories:["运维","docker"],tags:[null]},{name:"如何选择docker基础镜像",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\10.docker\\03.如何选择docker基础镜像.md",wordsCount:656,readingTime:"2.8m",title:"如何选择docker基础镜像",date:"2023-01-30T16:28:23.000Z",permalink:"/pages/8884ac/",categories:["运维","docker"],tags:[null]},{name:"常见的dockerfile汇总",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\10.docker\\04.常见的dockerfile汇总.md",wordsCount:546,readingTime:"3.3m",title:"常见的dockerfile汇总",date:"2023-02-01T12:38:37.000Z",permalink:"/pages/382a6c/",categories:["运维","docker"],tags:[null]},{name:"基于官方php7-2-34镜像构建生产可用镜像",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\10.docker\\05.基于官方php7-2-34镜像构建生产可用镜像.md",wordsCount:"2k",readingTime:"9.1m",title:"基于官方php7.2.34镜像构建生产可用镜像",date:"2023-01-30T20:21:15.000Z",permalink:"/pages/6e3cb9/",categories:["docker"],tags:[null]},{name:"debian中科大替换教程",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\11.other\\01.debian中科大替换教程.md",wordsCount:400,readingTime:"1.9m",title:"debian中科大替换教程",date:"2022-12-15T18:58:50.000Z",permalink:"/pages/767641/",categories:["运维","镜像源"],tags:[null]},{name:"Alpine安装php各种扩展",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\11.other\\02.Alpine安装php各种扩展.md",wordsCount:462,readingTime:"2.7m",title:"Alpine安装php各种扩展",date:"2023-01-30T20:43:25.000Z",permalink:"/pages/aa9eee/",categories:["运维","中间件"],tags:[null]},{name:"zabbix添加证书监控",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\01.zabbix\\01.zabbix添加证书监控.md",wordsCount:642,readingTime:"3.5m",title:"zabbix添加证书监控",date:"2022-12-14T15:03:30.000Z",permalink:"/pages/3dac52/",categories:["专题","zabbix"],tags:[null]},{name:"zabbix添加端口和进程监控",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\01.zabbix\\02.zabbix添加端口和进程监控.md",wordsCount:594,readingTime:"2.7m",title:"zabbix添加端口和进程监控",date:"2022-12-14T15:03:30.000Z",permalink:"/pages/a0a3ae/",categories:["专题","zabbix"],tags:[null]},{name:"docker文件安装zabbix5",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\01.zabbix\\03.docker文件安装zabbix5.md",wordsCount:873,readingTime:"5.3m",title:"docker文件安装zabbix5",date:"2022-12-20T16:08:39.000Z",permalink:"/pages/e0085e/",categories:["专题","zabbix"],tags:[null]},{name:"zabbix配置钉钉告警",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\01.zabbix\\04.zabbix配置钉钉告警.md",wordsCount:353,readingTime:"1.8m",title:"zabbix配置钉钉告警",date:"2022-12-20T16:16:02.000Z",permalink:"/pages/a4e3ce/",categories:["专题","zabbix"],tags:[null]},{name:"zabbix添加日志监控",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\01.zabbix\\05.zabbix添加日志监控.md",wordsCount:"1.4k",readingTime:"6.3m",title:"zabbix添加日志监控",date:"2022-12-20T16:41:01.000Z",permalink:"/pages/334bec/",categories:["专题","zabbix"],tags:[null]},{name:"zabbix添加进程pid监控",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\01.zabbix\\06.zabbix添加进程pid监控.md",wordsCount:405,readingTime:"2m",title:"zabbix添加进程pid监控",date:"2022-12-20T17:02:20.000Z",permalink:"/pages/20f905/",categories:["专题","zabbix"],tags:[null]},{name:"zabbix监控windows进程",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\01.zabbix\\07.zabbix监控windows进程.md",wordsCount:972,readingTime:"3.7m",title:"zabbix监控windows进程",date:"2022-12-20T17:04:24.000Z",permalink:"/pages/269839/",categories:["专题","zabbix"],tags:[null]},{name:"zabbix添加web监控",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\01.zabbix\\08.zabbix添加web监控.md",wordsCount:119,readingTime:"1",title:"zabbix添加web监控",date:"2022-12-21T17:35:46.000Z",permalink:"/pages/b70eb8/",categories:["专题","zabbix"],tags:[null]},{name:"centos7编译安装zabbix proxy端",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\01.zabbix\\09.centos7编译安装zabbix proxy端.md",wordsCount:737,readingTime:"3.5m",title:"centos7编译安装zabbix5.0 proxy端",date:"2023-03-01T14:31:52.000Z",permalink:"/pages/723e7e/",categories:["运维","监控","zabbix"],tags:[null]},{name:"rpm安装zabbix proxy过程简记",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\01.zabbix\\10.rpm安装zabbix proxy过程简记.md",wordsCount:790,readingTime:"3.6m",title:"rpm安装zabbix proxy过程简记",date:"2023-03-02T10:13:05.000Z",permalink:"/pages/ef347e/",categories:["运维","监控","zabbix"],tags:[null]},{name:"prometheus监控、告警与存储",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\02.prometheus\\01.prometheus监控、告警与存储.md",wordsCount:"3.8k",readingTime:"18m",title:"prometheus监控、告警与存储",date:"2022-12-12T09:57:58.000Z",permalink:"/pages/40794d/",categories:["监控","prometheus"],tags:[null]},{name:"使用docker-compose搭建promethes+grafana监控系统",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\02.prometheus\\02.使用docker-compose搭建promethes+grafana监控系统.md",wordsCount:"1.3k",readingTime:"7.2m",title:"使用docker-compose搭建promethes+grafana监控系统",date:"2023-01-09T14:26:28.000Z",permalink:"/pages/712f5c/",categories:["专题","prometheus"],tags:[null]},{name:"prometheus添加钉钉消息告警",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\02.prometheus\\03.prometheus添加钉钉消息告警.md",wordsCount:517,readingTime:"2.8m",title:"prometheus添加钉钉消息告警",date:"2023-01-10T17:38:03.000Z",permalink:"/pages/22b836/",categories:["专题","prometheus"],tags:[null]},{name:"alertmanager实现某个时间段静默某些告警项",filePath:"D:\\blog\\blog-master\\docs\\04.运维\\12.监控\\02.prometheus\\04.alertmanager实现某个时间段静默某些告警项.md",wordsCount:"1.7k",readingTime:"8.8m",title:"alertmanager实现某个时间段静默某些告警项",date:"2023-01-11T11:40:32.000Z",permalink:"/pages/96c6ce/",categories:["专题","prometheus"],tags:[null]},{name:"关于",filePath:"D:\\blog\\blog-master\\docs\\05.关于\\01.关于.md",wordsCount:39,readingTime:"1",title:"关于",date:"2022-12-15T13:14:01.000Z",permalink:"/about/",sidebar:!0,article:!1,description:null},{name:"友链",filePath:"D:\\blog\\blog-master\\docs\\06.友链\\01.友链.md",wordsCount:212,readingTime:"1",title:"友链",date:"2022-12-15T09:17:44.000Z",permalink:"/friends",categories:["友链"],tags:["friend"],description:null}],moutedEvent:".tags-wrapper",readingTime:!0,indexIteration:2500,pageIteration:2500},author:{name:"章工运维",link:"https://github.com/zpj874878956"},blogger:{avatar:"http://pic.zzppjj.top/LightPicture/2023/02/c7ac9de8fb6cfb17.jpg",name:"章工运维",slogan:"好好学习，天天向上"},social:{icons:[{iconClass:"icon-github",title:"GitHub",link:"https://github.com/zpj874878956"},{iconClass:"icon-youjian",title:"发邮件",link:"874878956@qq.com"},{iconClass:"icon-rss",title:"订阅",link:"https://blog.zzppjj.top/rss.xml"}]},htmlModules:{homeSidebarB:'<div style="padding: 0.95rem">\n      <p style="\n        color: var(--textColor);\n        opacity: 0.9;\n        font-size: 20px;\n        font-weight: bold;\n        margin: 0 0 8px 0;\n      ">公众号</p>\n      <img src="http://pic.zzppjj.top/LightPicture/2023/01/72a7c6cb076f44bc.jpg"  style="width:100%;" />\n      <p>\n      章工运维，扫码或者搜索关注\n      </p>\n      </div>',pageB:'<div class="donation">\n  <button>打赏</button>\n  <div class="main">\n      <div class="pic">\n          <img src="http://pic.zzppjj.top/LightPicture/2023/02/cebf13bbcea9264d.jpg" alt="微信">\n          <img src="http://pic.zzppjj.top/LightPicture/2023/02/d98d15501306c7c4.jpg" alt="支付宝">\n      </div>\n  </div>\n</div>'},footer:{createYear:2019,copyrightInfo:'| <a href="https://www.foreverblog.cn/" class="d-inline-block text-muted" target="_blank" rel="external nofollow"><img src="/img/964560013b68c2e4.png" alt="点击查看十年之约" style="width:auto;height:11px;">'}}};var kl=t(98),xl=t(99),wl=t(11);var El={computed:{$filterPosts(){return this.$site.pages.filter(n=>{const{frontmatter:{pageComponent:e,article:t,home:o}}=n;return!(e||!1===t||!0===o)})},$sortPosts(){return(n=this.$filterPosts).sort((n,e)=>{const t=n.frontmatter.sticky,o=e.frontmatter.sticky;return t&&o?t==o?Object(wl.a)(n,e):t-o:t&&!o?-1:!t&&o?1:Object(wl.a)(n,e)}),n;var n},$sortPostsByDate(){return(n=this.$filterPosts).sort((n,e)=>Object(wl.a)(n,e)),n;var n},$groupPosts(){return function(n){const e={},t={};for(let o=0,a=n.length;o<a;o++){const{frontmatter:{categories:a,tags:s}}=n[o];"array"===Object(wl.n)(a)&&a.forEach(t=>{t&&(e[t]||(e[t]=[]),e[t].push(n[o]))}),"array"===Object(wl.n)(s)&&s.forEach(e=>{e&&(t[e]||(t[e]=[]),t[e].push(n[o]))})}return{categories:e,tags:t}}(this.$sortPosts)},$categoriesAndTags(){return function(n){const e=[],t=[];for(let t in n.categories)e.push({key:t,length:n.categories[t].length});for(let e in n.tags)t.push({key:e,length:n.tags[e].length});return{categories:e,tags:t}}(this.$groupPosts)}}};Vt.component(kl.default),Vt.component(xl.default);function Al(n){return n.toString().padStart(2,"0")}t(247);Vt.component("BlockToggle",()=>Promise.all([t.e(0),t.e(4)]).then(t.bind(null,354))),Vt.component("WebInfo",()=>Promise.all([t.e(0),t.e(3)]).then(t.bind(null,352))),Vt.component("PageInfo",()=>t.e(7).then(t.bind(null,355))),Vt.component("CodeBlock",()=>Promise.resolve().then(t.bind(null,98))),Vt.component("Badge",()=>Promise.all([t.e(0),t.e(5)]).then(t.bind(null,490))),Vt.component("CodeGroup",()=>Promise.resolve().then(t.bind(null,99)));t(248);var zl=(n,e,t)=>{if(!e.has(n))throw TypeError("Cannot "+t)},Bl=(n,e,t)=>(zl(n,e,"read from private field"),t?t.call(n):e.get(n)),Tl=(n,e,t)=>{if(e.has(n))throw TypeError("Cannot add the same private member more than once");e instanceof WeakSet?e.add(n):e.set(n,t)},Sl=(n,e,t,o)=>(zl(n,e,"write to private field"),o?o.call(n,t):e.set(n,t),t);var $l,jl;const Il=class{constructor(n,e,t,o=!0){Tl(this,$l,void 0),Tl(this,jl,void 0),Sl(this,$l,{width:0,height:0});const{el:a,ctx:s}=Il.initCanvas(n);this.el=a,this.ctx=s,Sl(this,jl,o),this.size={width:e||window.innerWidth,height:t||window.innerHeight}}get size(){return{...Bl(this,$l)}}set size({width:n,height:e}){var t;if(Bl(this,$l).width===n&&Bl(this,$l).height===e)return;Bl(this,$l).width=n,Bl(this,$l).height=e;const o=null!=(t=Bl(this,jl)?window.devicePixelRatio:1)?t:1;this.el.width=Math.round(Bl(this,$l).width*o),this.el.height=Math.round(Bl(this,$l).height*o),this.el.style.width=Bl(this,$l).width+"px",this.el.style.height=Bl(this,$l).height+"px",Bl(this,jl)&&this.ctx.scale(o,o)}clear(){Il.clearCanvas(this.ctx,{...Bl(this,$l)})}to(n){n.ctx.drawImage(this.el,0,0,Bl(this,$l).width,Bl(this,$l).height)}handleResize(n){this.size={width:window.innerWidth,height:window.innerHeight}}static setCanvasStyle(n,e,t){const o=n.style,{zIndex:a=0,opacity:s=1}=e;o.position="fixed",o.top="0",o.left="0",o.zIndex=a.toString(),o.width=(t?t.width:n.width).toString()+"px",o.height=(t?t.height:n.height).toString()+"px",1!==s&&(o.opacity=s.toString()),o.pointerEvents="none"}static initCanvas(n){n||(n=document.createElement("canvas"));const e=n.getContext("2d");return{el:n,ctx:e}}static createOffscreenCanvas(){return new Il}static clearCanvas(n,e){const{width:t,height:o}=e;n.clearRect(0,0,t,o)}};let Pl=Il;var Dl,Cl;$l=new WeakMap,jl=new WeakMap;class Rl{constructor(n,e,t,o=!0,a=!0,s={zIndex:0,opacity:1}){Tl(this,Dl,void 0),Tl(this,Cl,void 0),Sl(this,Dl,new Pl(n,e,t,o)),Pl.setCanvasStyle(Bl(this,Dl).el,s,{width:e,height:t}),Sl(this,Cl,a?new Pl(void 0,e,t,o):null)}get size(){return Bl(this,Dl).size}draw(n){var e;const t=null!=(e=Bl(this,Cl))?e:Bl(this,Dl);t.clear(),n(t.ctx,{...t.size})}render(){!Bl(this,Cl)||(Bl(this,Dl).clear(),Bl(this,Cl).to(Bl(this,Dl)))}handleResize(n){Bl(this,Dl).handleResize(n),Bl(this,Cl)&&Bl(this,Cl).handleResize(n)}clear(){Bl(this,Dl).clear(),Bl(this,Cl)&&Bl(this,Cl).clear()}}Dl=new WeakMap,Cl=new WeakMap;var Ol=(n,e,t)=>{if(!e.has(n))throw TypeError("Cannot "+t)},Ll=(n,e,t)=>(Ol(n,e,"read from private field"),t?t.call(n):e.get(n));var Ml;class ql{constructor(){((n,e,t)=>{if(e.has(n))throw TypeError("Cannot add the same private member more than once");e instanceof WeakSet?e.add(n):e.set(n,t)})(this,Ml,void 0),((n,e,t,o)=>{Ol(n,e,"write to private field"),o?o.call(n,t):e.set(n,t)})(this,Ml,new Map)}add(n,e,t=window){Ll(this,Ml).has(t)||Ll(this,Ml).set(t,new Map);const o=Ll(this,Ml).get(t);o.has(n)||o.set(n,new Set),o.get(n).add(e)}startAll(){for(const[n,e]of Ll(this,Ml))for(const[t,o]of e)for(const e of o)n.addEventListener(t,e)}stopAll(){for(const[n,e]of Ll(this,Ml))for(const[t,o]of e)for(const e of o)n.removeEventListener(t,e)}clear(){Ll(this,Ml).clear()}}function Nl(n){return!!n.touches}Ml=new WeakMap;class Fl{static randomFloat(n,e){return Math.random()*(e-n)+n}static randomInt(n,e){return Math.floor(Fl.randomFloat(n,e))}static choice(n){const e=n.length;return n[Math.floor(e*Math.random())]}static color(n="0123456789ABCDEF"){return"#"+Fl.choice(n)+Fl.choice(n)+Fl.choice(n)+Fl.choice(n)+Fl.choice(n)+Fl.choice(n)}}var Ul,Gl,Hl,Vl,Zl,Kl=(n,e,t)=>{if(!e.has(n))throw TypeError("Cannot "+t)},Wl=(n,e,t)=>(Kl(n,e,"read from private field"),t?t.call(n):e.get(n)),Yl=(n,e,t)=>{if(e.has(n))throw TypeError("Cannot add the same private member more than once");e instanceof WeakSet?e.add(n):e.set(n,t)},Xl=(n,e,t,o)=>(Kl(n,e,"write to private field"),o?o.call(n,t):e.set(n,t),t),Ql=(n,e,t)=>(Kl(n,e,"access private method"),t);class Jl{constructor(n,e,t,o,a){Yl(this,Ul,void 0),Yl(this,Gl,void 0),Yl(this,Hl,void 0),this.size=t,this.color=o,Xl(this,Hl,0),Xl(this,Ul,a),Xl(this,Gl,e),this.position={...n}}move(){this.position.x=Math.sin(Wl(this,Ul))*Wl(this,Gl)+this.position.x,this.position.y=Math.cos(Wl(this,Ul))*Wl(this,Gl)+this.position.y+.3*Wl(this,Hl),((n,e,t,o)=>({set _(o){Xl(n,e,o,t)},get _(){return Wl(n,e,o)}}))(this,Hl)._++}shouleRemove(n){return this.position.x<0||this.position.x>n.width||this.position.y>n.height}}Ul=new WeakMap,Gl=new WeakMap,Hl=new WeakMap;Vl=new WeakMap;class nc{static create(n,e,t,o,a,s){return new(this.shapeMap.get(n))(e,t,o,a,s)}}nc.shapeMap=new Map([["star",class extends Jl{constructor(n,e,t,o,a){super(n,e,t,o,a),Yl(this,Vl,0)}draw(n,e){n.fillStyle=this.color,n.beginPath();const t=2*this.size,o=this.size;for(let e=0;e<5;e++)n.lineTo(Math.cos((18+72*e-Wl(this,Vl))/180*Math.PI)*t+this.position.x,-Math.sin((18+72*e-Wl(this,Vl))/180*Math.PI)*t+this.position.y),n.lineTo(Math.cos((54+72*e-Wl(this,Vl))/180*Math.PI)*o+this.position.x,-Math.sin((54+72*e-Wl(this,Vl))/180*Math.PI)*o+this.position.y);n.fill(),Xl(this,Vl,Wl(this,Vl)+5)}}],["circle",class extends Jl{draw(n,e){n.fillStyle=this.color,n.beginPath(),n.arc(this.position.x,this.position.y,this.size,0,2*Math.PI),n.fill()}}]]);class ec{constructor(n,e,t,o){Yl(this,Zl,void 0),this.stopped=!1,Xl(this,Zl,new Set);for(let a=0;a<o;a++){const o=nc.create(n,e,Fl.randomFloat(1,6),t,Fl.color("89ABCDEF"),Fl.randomFloat(Math.PI-1,Math.PI+1));Wl(this,Zl).add(o)}}move(n){for(const e of Wl(this,Zl))e.shouleRemove(n)?Wl(this,Zl).delete(e):e.move();0===Wl(this,Zl).size&&(this.stopped=!0)}draw(n,e){for(const t of Wl(this,Zl))t.draw(n,e)}}Zl=new WeakMap;var tc,oc,ac,sc,rc,ic,lc,cc,pc,dc,mc,uc,hc,gc,bc,fc,yc,_c,vc,kc;class xc{constructor({shape:n="star",size:e=2,numParticles:t=10}={},o={}){Yl(this,pc),Yl(this,mc),Yl(this,hc),Yl(this,bc),Yl(this,yc),Yl(this,vc),Yl(this,tc,void 0),Yl(this,oc,void 0),Yl(this,ac,void 0),Yl(this,sc,null),Yl(this,rc,new Set),Yl(this,ic,!1),Yl(this,lc,void 0),Yl(this,cc,new ql),Xl(this,tc,n),Xl(this,oc,e),Xl(this,ac,t),Xl(this,lc,o),this.animate=this.animate.bind(this)}mount(n){Xl(this,sc,new Rl(n,window.innerWidth,window.innerHeight,!0,!0,Wl(this,lc))),Ql(this,pc,dc).call(this),function(n,{leftColor:e="#fff",rightColor:t="#444",leftBgColor:o="#35495e",rightBgColor:a="#00ffc0"}={}){console.log(`%c ${n} %c v0.5.2 112fa81 %c`,`background: ${o}; padding: 2px; color: ${e}; font-weight: bold; text-transform: uppercase;`,`background: ${a}; padding: 2px; color: ${t}; font-weight: bold; text-transform: uppercase;`,"background: transparent")}("Theme Popper 🎉",{leftBgColor:"#ffb366"})}unmount(){Ql(this,mc,uc).call(this),Xl(this,ic,!1)}animate(){if(Xl(this,ic,!0),0===Wl(this,rc).size)return Xl(this,ic,!1),void Wl(this,sc).clear();requestAnimationFrame(this.animate);for(const n of Wl(this,rc)){if(n.stopped)return void Wl(this,rc).delete(n);n.move(Wl(this,sc).size)}Wl(this,sc).draw((n,e)=>{for(const t of Wl(this,rc))t.draw(n,e)}),Wl(this,sc).render()}}tc=new WeakMap,oc=new WeakMap,ac=new WeakMap,sc=new WeakMap,rc=new WeakMap,ic=new WeakMap,lc=new WeakMap,cc=new WeakMap,pc=new WeakSet,dc=function(){/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)?Wl(this,cc).add("touchstart",Ql(this,hc,gc).bind(this)):Wl(this,cc).add("mousedown",Ql(this,hc,gc).bind(this)),Wl(this,cc).add("visibilitychange",Ql(this,yc,_c).bind(this)),Wl(this,cc).add("resize",function(n,e,t){var o,a,s;void 0===e&&(e=50),void 0===t&&(t={});var r=null!=(o=t.isImmediate)&&o,i=null!=(a=t.callback)&&a,l=t.maxWait,c=Date.now(),p=[];function d(){if(void 0!==l){var n=Date.now()-c;if(n+e>=l)return l-n}return e}var m=function(){var e=[].slice.call(arguments),t=this;return new Promise((function(o,a){var l=r&&void 0===s;if(void 0!==s&&clearTimeout(s),s=setTimeout((function(){if(s=void 0,c=Date.now(),!r){var o=n.apply(t,e);i&&i(o),p.forEach((function(n){return(0,n.resolve)(o)})),p=[]}}),d()),l){var m=n.apply(t,e);return i&&i(m),o(m)}p.push({resolve:o,reject:a})}))};return m.cancel=function(n){void 0!==s&&clearTimeout(s),p.forEach((function(e){return(0,e.reject)(n)})),p=[]},m}(Ql(this,bc,fc).bind(this),500)),Wl(this,cc).startAll()},mc=new WeakSet,uc=function(){Wl(this,cc).stopAll(),Wl(this,cc).clear()},hc=new WeakSet,gc=function(n){const e={x:Nl(n)?n.touches[0].clientX:n.clientX,y:Nl(n)?n.touches[0].clientY:n.clientY},t=new ec(Wl(this,tc),{...e},Wl(this,oc),Wl(this,ac));Wl(this,rc).add(t),Wl(this,ic)||Ql(this,vc,kc).call(this)},bc=new WeakSet,fc=function(n){Wl(this,sc).handleResize(n)},yc=new WeakSet,_c=function(n){Wl(this,rc).clear(),Xl(this,ic,!1)},vc=new WeakSet,kc=function(){requestAnimationFrame(this.animate)};var wc={name:"CursorEffects",data:()=>({popper:new xc({shape:"star",size:2},{opacity:1,zIndex:999999999})}),mounted(){this.popper.mount(this.$el)},beforeDestroy(){this.popper.unmount()}},Ec=Object(fl.a)(wc,(function(){return(0,this._self._c)("canvas",{attrs:{id:"vuepress-canvas-cursor"}})}),[],!1,null,null,null).exports,Ac={name:"ReadingProgress",data:()=>({readingTop:0,readingHeight:1,progressStyle:null,transform:void 0,running:!1}),watch:{$readingShow(){this.progressStyle=this.getProgressStyle(),this.$readingShow&&window.addEventListener("scroll",this.base)}},mounted(){this.transform=this.getTransform(),this.progressStyle=this.getProgressStyle(),this.$readingShow&&window.addEventListener("scroll",this.base)},beforeDestroy(){this.$readingShow&&window.removeEventListener("scroll",this.base)},methods:{base(){this.running||(this.running=!0,requestAnimationFrame(this.getReadingBase))},getReadingBase(){this.readingHeight=this.getReadingHeight()-this.getScreenHeight(),this.readingTop=this.getReadingTop(),this.progressStyle=this.getProgressStyle(),this.running=!1},getReadingHeight:()=>Math.max(document.body.scrollHeight,document.body.offsetHeight,0),getScreenHeight:()=>Math.max(window.innerHeight,document.documentElement.clientHeight,0),getReadingTop:()=>Math.max(window.pageYOffset,document.documentElement.scrollTop,0),getTransform(){const n=document.createElement("div");return["transform","-webkit-transform","-moz-transform","-o-transform","-ms-transform"].find(e=>e in n.style)||void 0},getProgressStyle(){const n=this.readingTop/this.readingHeight;switch(this.$readingShow){case"top":case"bottom":return this.transform?`${this.transform}: scaleX(${n})`:`width: ${100*n}%`;case"left":case"right":return this.transform?`${this.transform}: scaleY(${n})`:`height: ${100*n}%`;default:return null}}}},zc=(t(249),Object(fl.a)(Ac,(function(){var n=this._self._c;return n("ClientOnly",[this.$readingShow?n("div",{staticClass:"reading-progress",class:this.$readingShow},[n("div",{staticClass:"progress",style:this.progressStyle})]):this._e()])}),[],!1,null,"3640397f",null).exports),Bc={props:{color:{required:!1,default:"rgb(66, 185, 131)"}}},Tc=(t(250),Object(fl.a)(Bc,(function(){return(0,this._self._c)("div",{staticClass:"spinner",style:{background:this.color}})}),[],!1,null,"1bbcb91a",null).exports);const Sc={name:"Mermaid",props:{id:{type:String,required:!1,default:()=>"diagram_"+Date.now()},graph:{type:String,required:!1}},data:()=>({svg:void 0}),computed:{graphData(){return this.graph?this.graph:this.$slots.default[0].text}},render(n){return void 0===this.svg?n("Loading"):n("div",{class:["mermaid-diagram"],domProps:{innerHTML:this.svg,style:"width: 100%"}})},mounted(){t.e(144).then(t.t.bind(null,344,7)).then(n=>{n.initialize({startOnLoad:!0}),n.render(this.id,this.graphData,n=>{this.svg=n})})},components:{Loading:Tc}};t(251);var $c=t(97),jc=t.n($c),Ic=t(17);let Pc,Dc,Cc;var Rc;"valine"===(Rc="artalk")?t.e(148).then(t.t.bind(null,345,7)).then(n=>Dc=n.default):"gitalk"===Rc?Promise.all([t.e(0),t.e(146)]).then(t.t.bind(null,346,7)).then(()=>t.e(143).then(t.t.bind(null,347,7))).then(n=>Pc=n.default):"artalk"===Rc&&Promise.all([t.e(0),t.e(145)]).then(t.t.bind(null,348,7)).then(()=>Promise.all([t.e(0),t.e(147)]).then(t.t.bind(null,349,7))).then(()=>t.e(8).then(t.bind(null,350))).then(n=>Cc=n.default);function Oc(n,e){const t={};return Reflect.ownKeys(n).forEach(o=>{if("string"==typeof n[o])try{t[o]=jc.a.render(n[o],e)}catch(e){console.warn(`Comment config option error at key named "${o}"`),console.warn("More info: "+e.message),t[o]=n[o]}else t[o]=n[o]}),t}console.log(`Current frontend version is ${Ic.dependencies.artalk} , plugin version is ${Ic.name}@v${Ic.version} , more details:`,Ic.homepage);const Lc={gitalk:{render(n,e){const t=document.createElement("div");t.id=e;document.querySelector("main.page").appendChild(t);new Pc(Oc({server:"https://artalk.zzppjj.top",site:"zzppjj",disableEmotion:!1,disablePicture:!1,disablePreview:!1},{frontmatter:n})).render(e)},clear(n){const e=document.querySelector("#"+n);return e&&e.remove(),!0}},valine:{render(n,e){const t=document.createElement("div");t.id=e;document.querySelector("main.page").appendChild(t),new Dc({...Oc({server:"https://artalk.zzppjj.top",site:"zzppjj",disableEmotion:!1,disablePicture:!1,disablePreview:!1},{frontmatter:n}),el:"#"+e})},clear(n){const e=document.querySelector("#"+n);return e&&e.remove(),!0}},artalk:{render(n,e){const t=document.createElement("div");t.id=e;document.querySelector("main.page").appendChild(t),new Cc({el:"#"+e,pageKey:"",pageTitle:"",server:"https://artalk.zzppjj.top",site:"zzppjj"}),function(){const n=setInterval((function(){const e=document.querySelectorAll(".atk-plug-btn");e&&e.length>0&&(3===e.length?console.log("Artalk自带图片已开启"):console.log("Artalk自带图片已关闭"),clearInterval(n))}),500)}()},clear(n){const e=document.querySelector("#"+n);return e&&e.remove(),!0}}},Mc="vuepress-plugin-vdoing-comment";let qc=null;function Nc(n){return Lc.artalk.clear(Mc)}function Fc(n){return!1!==n.comment&&!1!==n.comments}function Uc(n){clearTimeout(qc);if(document.querySelector("main.page"))return Lc.artalk.render(n,Mc);qc=setTimeout(()=>Uc(n),200)}var Gc={mounted(){qc=setTimeout(()=>{const n={to:{},from:{},...this.$frontmatter};Nc()&&Fc(n)&&Uc(n)},1e3),this.$router.afterEach((n,e)=>{if(n&&e&&n.path===e.path)return;const t={to:n,from:e,...this.$frontmatter};Nc()&&Fc(t)&&Uc(t)})}},Hc=Object(fl.a)(Gc,(function(){return(0,this._self._c)("div")}),[],!1,null,null,null).exports,Vc=[({Vue:n,options:e,router:t,siteData:o})=>{o.pages.map(n=>{const{frontmatter:{date:e,author:t}}=n;"string"==typeof e&&"Z"===e.charAt(e.length-1)&&(n.frontmatter.date=function(n){n instanceof Date||(n=new Date(n));return`${n.getUTCFullYear()}-${Al(n.getUTCMonth()+1)}-${Al(n.getUTCDate())} ${Al(n.getUTCHours())}:${Al(n.getUTCMinutes())}:${Al(n.getUTCSeconds())}`}(e)),t?n.author=t:o.themeConfig.author&&(n.author=o.themeConfig.author)}),n.mixin(El)},{},({Vue:n})=>{n.mixin({computed:{$dataBlock(){return this.$options.__data__block__}}})},{},{},({router:n})=>{"undefined"!=typeof window&&function(){var n=document.createElement("script"),e=window.location.protocol.split(":")[0];n.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(n,t)}()},({Vue:n})=>{n.component("CursorEffects",Ec)},({Vue:n})=>{n.component(zc.name,zc),n.mixin({computed:{$readingShow(){return this.$page.frontmatter.readingShow}}})},({router:n})=>{"undefined"!=typeof window&&(window._hmt=window._hmt||[],function(){var n=document.createElement("script");n.src="https://hm.baidu.com/hm.js?a2eb5aefa7df511c4cda2ae0aecea086";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(n,e)}(),n.afterEach((function(n){_hmt.push(["_trackPageview",n.fullPath])})))},({Vue:n})=>{n.component(Sc.name,Sc)},({Vue:n})=>{n.component("Comment",Hc)}],Zc=["CursorEffects","ReadingProgress","PageInfo","BlockToggle","Comment"];class Kc extends class{constructor(){this.store=new Vt({data:{state:{}}})}$get(n){return this.store.state[n]}$set(n,e){Vt.set(this.store.state,n,e)}$emit(...n){this.store.$emit(...n)}$on(...n){this.store.$on(...n)}}{}Object.assign(Kc.prototype,{getPageAsyncComponent:ri,getLayoutAsyncComponent:ii,getAsyncComponent:li,getVueComponent:ci});var Wc={install(n){const e=new Kc;n.$vuepress=e,n.prototype.$vuepress=e}};function Yc(n,e){const t=e.toLowerCase();return n.options.routes.some(n=>n.path.toLowerCase()===t)}var Xc={props:{pageKey:String,slotKey:{type:String,default:"default"}},render(n){const e=this.pageKey||this.$parent.$page.key;return di("pageKey",e),Vt.component(e)||Vt.component(e,ri(e)),Vt.component(e)?n(e):n("")}},Qc={functional:!0,props:{slotKey:String,required:!0},render:(n,{props:e,slots:t})=>n("div",{class:["content__"+e.slotKey]},t()[e.slotKey])},Jc={computed:{openInNewWindowTitle(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},np=(t(257),t(258),Object(fl.a)(Jc,(function(){var n=this._self._c;return n("span",[n("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[n("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),n("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),n("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports),ep={functional:!0,render(n,{parent:e,children:t}){if(e._isMounted)return t;e.$once("hook:mounted",()=>{e.$forceUpdate()})}};Vt.config.productionTip=!1,Vt.use(Gr),Vt.use(Wc),Vt.mixin(function(n,e,t=Vt){!function(n){n.locales&&Object.keys(n.locales).forEach(e=>{n.locales[e].path=e});Object.freeze(n)}(e),t.$vuepress.$set("siteData",e);const o=new(n(t.$vuepress.$get("siteData"))),a=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(o)),s={};return Object.keys(a).reduce((n,e)=>(e.startsWith("$")&&(n[e]=a[e].get),n),s),{computed:s}}(n=>class{setPage(n){this.__page=n}get $site(){return n}get $themeConfig(){return this.$site.themeConfig}get $frontmatter(){return this.$page.frontmatter}get $localeConfig(){const{locales:n={}}=this.$site;let e,t;for(const o in n)"/"===o?t=n[o]:0===this.$page.path.indexOf(o)&&(e=n[o]);return e||t||{}}get $siteTitle(){return this.$localeConfig.title||this.$site.title||""}get $canonicalUrl(){const{canonicalUrl:n}=this.$page.frontmatter;return"string"==typeof n&&n}get $title(){const n=this.$page,{metaTitle:e}=this.$page.frontmatter;if("string"==typeof e)return e;const t=this.$siteTitle,o=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?o?o+" | "+t:t:o||"VuePress"}get $description(){const n=function(n){if(n){const e=n.filter(n=>"description"===n.name)[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}get $lang(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}get $localePath(){return this.$localeConfig.path||"/"}get $themeLocaleConfig(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}get $page(){return this.__page?this.__page:function(n,e){for(let t=0;t<n.length;t++){const o=n[t];if(o.path.toLowerCase()===e.toLowerCase())return o}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}},vl)),Vt.component("Content",Xc),Vt.component("ContentSlotsDistributor",Qc),Vt.component("OutboundLink",np),Vt.component("ClientOnly",ep),Vt.component("Layout",ii("Layout")),Vt.component("NotFound",ii("NotFound")),Vt.prototype.$withBase=function(n){const e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.9.9",hash:""},async function(n){const e="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:vl.routerBase||vl.base,t=new Gr({base:e,mode:"history",fallback:!1,routes:_l,scrollBehavior:(n,e,t)=>t||(n.hash?!Vt.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})});!function(n){n.beforeEach((e,t,o)=>{if(Yc(n,e.path))o();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){const t=e.path.replace(/\/$/,"")+".html";Yc(n,t)?o(t):o()}else o();else{const t=e.path+"/",a=e.path+".html";Yc(n,a)?o(a):Yc(n,t)?o(t):o()}})}(t);const o={};try{await Promise.all(Vc.filter(n=>"function"==typeof n).map(e=>e({Vue:Vt,options:o,router:t,siteData:vl,isServer:n})))}catch(n){console.error(n)}return{app:new Vt(Object.assign(o,{router:t,render:n=>n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},Zc.map(e=>n(e)))])})),router:t}}(!1).then(({app:n,router:e})=>{e.onReady(()=>{n.$mount("#app")})})}]);